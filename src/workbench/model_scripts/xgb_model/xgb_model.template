# Imports for XGB Model
import xgboost as xgb
import awswrangler as wr
import numpy as np

# Classification Encoder
from sklearn.preprocessing import LabelEncoder

# Scikit Learn Imports
from sklearn.model_selection import train_test_split

import json
import argparse
import joblib
import os
import pandas as pd

# Shared model script utilities
from model_script_utils import (
    check_dataframe,
    expand_proba_column,
    match_features_case_insensitive,
    convert_categorical_types,
    decompress_features,
    input_fn,
    output_fn,
    compute_regression_metrics,
    print_regression_metrics,
    compute_classification_metrics,
    print_classification_metrics,
    print_confusion_matrix,
)

# UQ Harness for uncertainty quantification (regression models only)
from uq_harness import (
    train_uq_models,
    save_uq_models,
    load_uq_models,
    predict_intervals,
    compute_confidence,
)

# Default Hyperparameters for XGBoost
DEFAULT_HYPERPARAMETERS = {
    # Core tree parameters
    "n_estimators": 200,  # More trees for better signal capture when we have lots of features
    "max_depth": 6,  # Medium depth
    "learning_rate": 0.05,  # Lower rate with more estimators for smoother learning

    # Sampling parameters
    "subsample": 0.7,  # Moderate row sampling to reduce overfitting
    "colsample_bytree": 0.6,  # More aggressive feature sampling given lots of features
    "colsample_bylevel": 0.8,  # Additional feature sampling at each tree level

    # Regularization
    "min_child_weight": 5,  # Higher to prevent overfitting on small groups
    "gamma": 0.2,  # Moderate pruning - you have real signal so don't over-prune
    "reg_alpha": 0.5,  # L1 for feature selection (useful with many features)
    "reg_lambda": 2.0,  # Strong L2 to smooth predictions

    # Random seed
    "random_state": 42,
}

# Template Parameters
TEMPLATE_PARAMS = {
    "model_type": "{{model_type}}",
    "target": "{{target_column}}",
    "features": "{{feature_list}}",
    "compressed_features": "{{compressed_features}}",
    "model_metrics_s3_path": "{{model_metrics_s3_path}}",
    "train_all_data": "{{train_all_data}}",
    "hyperparameters": "{{hyperparameters}}",
}


if __name__ == "__main__":
    """The main function is for training the XGBoost model"""

    # Harness Template Parameters
    target = TEMPLATE_PARAMS["target"]
    features = TEMPLATE_PARAMS["features"]
    orig_features = features.copy()
    compressed_features = TEMPLATE_PARAMS["compressed_features"]
    model_type = TEMPLATE_PARAMS["model_type"]
    model_metrics_s3_path = TEMPLATE_PARAMS["model_metrics_s3_path"]
    train_all_data = TEMPLATE_PARAMS["train_all_data"]
    hyperparameters = {**DEFAULT_HYPERPARAMETERS, **(TEMPLATE_PARAMS["hyperparameters"] or {})}
    validation_split = 0.2

    # Script arguments for input/output directories
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-dir", type=str, default=os.environ.get("SM_MODEL_DIR", "/opt/ml/model"))
    parser.add_argument("--train", type=str, default=os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train"))
    parser.add_argument(
        "--output-data-dir", type=str, default=os.environ.get("SM_OUTPUT_DATA_DIR", "/opt/ml/output/data")
    )
    args = parser.parse_args()

    # Read the training data into DataFrames
    training_files = [os.path.join(args.train, file) for file in os.listdir(args.train) if file.endswith(".csv")]
    print(f"Training Files: {training_files}")

    # Combine files and read them all into a single pandas dataframe
    all_df = pd.concat([pd.read_csv(file, engine="python") for file in training_files])

    # Check if the dataframe is empty
    check_dataframe(all_df, "training_df")

    # Features/Target output
    print(f"Target: {target}")
    print(f"Features: {str(features)}")

    # Convert any features that might be categorical to 'category' type
    all_df, category_mappings = convert_categorical_types(all_df, features)

    # If we have compressed features, decompress them
    if compressed_features:
        print(f"Decompressing features {compressed_features}...")
        all_df, features = decompress_features(all_df, features, compressed_features)

    # Do we want to train on all the data?
    if train_all_data:
        print("Training on ALL of the data")
        df_train = all_df.copy()
        df_val = all_df.copy()

    # Does the dataframe have a training column?
    elif "training" in all_df.columns:
        print("Found training column, splitting data based on training column")
        df_train = all_df[all_df["training"]]
        df_val = all_df[~all_df["training"]]
    else:
        # Just do a random training Split
        print("WARNING: No training column found, splitting data with random state=42")
        df_train, df_val = train_test_split(all_df, test_size=validation_split, random_state=42)
    print(f"FIT/TRAIN: {df_train.shape}")
    print(f"VALIDATION: {df_val.shape}")

    # Use any hyperparameters to set up both the trainer and model configurations
    print(f"Hyperparameters: {hyperparameters}")

    # Check for target transform (e.g., "log" for log1p/expm1 transform)
    target_transform = hyperparameters.pop("target_transform", None)
    if target_transform:
        print(f"Target transform: {target_transform}")

    # Now spin up our XGB Model
    if model_type == "classifier":
        xgb_model = xgb.XGBClassifier(enable_categorical=True, **hyperparameters)

        # Encode the target column
        label_encoder = LabelEncoder()
        df_train[target] = label_encoder.fit_transform(df_train[target])
        df_val[target] = label_encoder.transform(df_val[target])

    else:
        xgb_model = xgb.XGBRegressor(enable_categorical=True, **hyperparameters)
        label_encoder = None  # We don't need this for regression

    # Grab our Features, Target and Train the Model
    y_train = df_train[target].copy()
    X_train = df_train[features]

    # Apply target transform for training (regression only)
    if target_transform == "log" and model_type != "classifier":
        print("Applying log1p transform to target for training...")
        y_train = np.log1p(y_train)

    xgb_model.fit(X_train, y_train)

    # Make Predictions on the Validation Set
    print(f"Making Predictions on Validation Set...")
    y_validate = df_val[target]
    X_validate = df_val[features]
    preds = xgb_model.predict(X_validate)

    # Reverse transform predictions for regression
    if target_transform == "log" and model_type != "classifier":
        print("Applying expm1 (reverse log) transform to predictions...")
        preds = np.expm1(preds)

    if model_type == "classifier":
        # Also get the probabilities for each class
        print("Processing Probabilities...")
        probs = xgb_model.predict_proba(X_validate)
        df_val["pred_proba"] = [p.tolist() for p in probs]

        # Expand the pred_proba column into separate columns for each class
        print(df_val.columns)
        df_val = expand_proba_column(df_val, label_encoder.classes_)
        print(df_val.columns)

        # Decode the target and prediction labels
        y_validate = label_encoder.inverse_transform(y_validate)
        preds = label_encoder.inverse_transform(preds)

    # Save predictions to S3 (just the target, prediction, and '_proba' columns)
    df_val["prediction"] = preds
    output_columns = [target, "prediction"]
    output_columns += [col for col in df_val.columns if col.endswith("_proba")]
    wr.s3.to_csv(
        df_val[output_columns],
        path=f"{model_metrics_s3_path}/validation_predictions.csv",
        index=False,
    )

    # Report Performance Metrics
    if model_type == "classifier":
        label_names = label_encoder.classes_
        score_df = compute_classification_metrics(y_validate, preds, label_names, target)
        print_classification_metrics(score_df, target, label_names)
        print_confusion_matrix(y_validate, preds, label_names)

    else:
        # Calculate and print regression metrics
        metrics = compute_regression_metrics(y_validate, preds)
        print_regression_metrics(metrics)

        # ==========================================
        # Train UQ models for regression (uncertainty quantification)
        # ==========================================
        print("\n" + "=" * 50)
        print("Training UQ Models for Uncertainty Quantification")
        print("=" * 50)
        uq_models, uq_metadata = train_uq_models(X_train, y_train, X_validate, y_validate)

    # Now save the model to the standard place/name
    joblib.dump(xgb_model, os.path.join(args.model_dir, "xgb_model.joblib"))

    # Save the label encoder if we have one
    if label_encoder:
        joblib.dump(label_encoder, os.path.join(args.model_dir, "label_encoder.joblib"))

    # Save the features (this will validate input during predictions)
    with open(os.path.join(args.model_dir, "feature_columns.json"), "w") as fp:
        json.dump(orig_features, fp)  # We save the original features, not the decompressed ones

    # Save the category mappings
    with open(os.path.join(args.model_dir, "category_mappings.json"), "w") as fp:
        json.dump(category_mappings, fp)

    # Save the target transform if we have one
    if target_transform:
        with open(os.path.join(args.model_dir, "target_transform.json"), "w") as fp:
            json.dump({"transform": target_transform}, fp)

    # Save UQ models for regression
    if model_type != "classifier":
        save_uq_models(uq_models, uq_metadata, args.model_dir)
        print(f"\nModel training complete!")
        print(f"Saved XGBoost model and {len(uq_models)} UQ models to {args.model_dir}")


def model_fn(model_dir) -> dict:
    """Load XGBoost model and UQ models (if regression) from the specified directory.

    Returns:
        dict: Dictionary containing the XGBoost model and optionally UQ models
    """
    # Load XGBoost model
    model_path = os.path.join(model_dir, "xgb_model.joblib")
    xgb_model = joblib.load(model_path)

    # Load label encoder if it exists (classifier)
    label_encoder = None
    label_encoder_path = os.path.join(model_dir, "label_encoder.joblib")
    if os.path.exists(label_encoder_path):
        label_encoder = joblib.load(label_encoder_path)

    # Load category mappings
    category_mappings = {}
    category_path = os.path.join(model_dir, "category_mappings.json")
    if os.path.exists(category_path):
        with open(category_path) as fp:
            category_mappings = json.load(fp)

    # Load target transform if it exists
    target_transform = None
    transform_path = os.path.join(model_dir, "target_transform.json")
    if os.path.exists(transform_path):
        with open(transform_path) as fp:
            target_transform = json.load(fp).get("transform")

    # Load UQ models if they exist (regression only)
    uq_models = None
    uq_metadata = None
    uq_metadata_path = os.path.join(model_dir, "uq_metadata.json")
    if os.path.exists(uq_metadata_path):
        uq_models, uq_metadata = load_uq_models(model_dir)

    return {
        "xgb_model": xgb_model,
        "label_encoder": label_encoder,
        "category_mappings": category_mappings,
        "target_transform": target_transform,
        "uq_models": uq_models,
        "uq_metadata": uq_metadata,
    }


def predict_fn(df, models) -> pd.DataFrame:
    """Make Predictions with our XGB Model

    Args:
        df (pd.DataFrame): The input DataFrame
        models (dict): Dictionary containing XGBoost model and optionally UQ models

    Returns:
        pd.DataFrame: The DataFrame with the predictions added
    """
    compressed_features = TEMPLATE_PARAMS["compressed_features"]

    # Grab our feature columns (from training)
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")
    with open(os.path.join(model_dir, "feature_columns.json")) as fp:
        features = json.load(fp)
    print(f"Model Features: {features}")

    # Extract components from models dict
    xgb_model = models["xgb_model"]
    label_encoder = models.get("label_encoder")
    category_mappings = models.get("category_mappings", {})
    target_transform = models.get("target_transform")
    uq_models = models.get("uq_models")
    uq_metadata = models.get("uq_metadata")

    if target_transform:
        print(f"Target transform: {target_transform}")

    # We're going match features in a case-insensitive manner, accounting for all the permutations
    # - Model has a feature list that's any case ("Id", "taCos", "cOunT", "likes_tacos")
    # - Incoming data has columns that are mixed case ("ID", "Tacos", "Count", "Likes_Tacos")
    matched_df = match_features_case_insensitive(df, features)

    # Detect categorical types in the incoming DataFrame
    matched_df, _ = convert_categorical_types(matched_df, features, category_mappings)

    # If we have compressed features, decompress them
    if compressed_features:
        print("Decompressing features for prediction...")
        matched_df, features = decompress_features(matched_df, features, compressed_features)

    # Predict the features against our XGB Model
    X = matched_df[features]
    predictions = xgb_model.predict(X)

    # Reverse transform predictions if needed
    if target_transform == "log":
        print("Applying expm1 (reverse log) transform to predictions...")
        predictions = np.expm1(predictions)

    # If we have a label encoder, decode the predictions (classifier)
    if label_encoder:
        predictions = label_encoder.inverse_transform(predictions)

    # Set the predictions on the DataFrame
    df["prediction"] = predictions

    # Does our model have a 'predict_proba' method? (classifier)
    if getattr(xgb_model, "predict_proba", None) and label_encoder:
        probs = xgb_model.predict_proba(X)
        df["pred_proba"] = [p.tolist() for p in probs]

        # Expand the pred_proba column into separate columns for each class
        df = expand_proba_column(df, label_encoder.classes_)

    # Add UQ prediction intervals for regression models
    elif uq_models and uq_metadata:
        df = predict_intervals(df, X, uq_models, uq_metadata)

        # Compute confidence scores
        df = compute_confidence(
            df,
            median_interval_width=uq_metadata["median_interval_width"],
            lower_q="q_10",
            upper_q="q_90",
        )

    # All done, return the DataFrame with new columns for the predictions
    return df
