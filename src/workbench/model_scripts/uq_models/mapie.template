# Model: XGBoost for point predictions + LightGBM with MAPIE for conformalized intervals
from mapie.regression import ConformalizedQuantileRegressor
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split

import json
import argparse
import joblib
import os
import numpy as np
import pandas as pd

# Shared model script utilities
from model_script_utils import (
    check_dataframe,
    match_features_case_insensitive,
    convert_categorical_types,
    decompress_features,
    input_fn,
    output_fn,
    compute_regression_metrics,
    print_regression_metrics,
)

# Template Placeholders
TEMPLATE_PARAMS = {
    "target": "{{target_column}}",
    "features": "{{feature_list}}",
    "compressed_features": "{{compressed_features}}",
    "train_all_data": "{{train_all_data}}",
    "hyperparameters": "{{hyperparameters}}",
}


def compute_confidence(
        df: pd.DataFrame,
        median_interval_width: float,
        lower_q: str = "q_10",
        upper_q: str = "q_90",
        alpha: float = 1.0,
        beta: float = 1.0,
) -> pd.DataFrame:
    """
    Compute confidence scores (0.0 to 1.0) based on prediction interval width
    and distance from median using exponential decay.

    Args:
        df: DataFrame with 'prediction', 'q_50', and quantile columns
        median_interval_width: Pre-computed median interval width from training data
        lower_q: Lower quantile column name (default: 'q_10')
        upper_q: Upper quantile column name (default: 'q_90')
        alpha: Weight for interval width term (default: 1.0)
        beta: Weight for distance from median term (default: 1.0)

    Returns:
        DataFrame with added 'confidence' column
    """
    # Interval width
    interval_width = (df[upper_q] - df[lower_q]).abs()

    # Distance from median, normalized by interval width
    distance_from_median = (df['prediction'] - df['q_50']).abs()
    normalized_distance = distance_from_median / (interval_width + 1e-6)

    # Cap the distance penalty at 1.0
    normalized_distance = np.minimum(normalized_distance, 1.0)

    # Confidence using exponential decay
    interval_term = interval_width / median_interval_width
    df['confidence'] = np.exp(-(alpha * interval_term + beta * normalized_distance))

    return df


if __name__ == "__main__":
    # Template Parameters
    target = TEMPLATE_PARAMS["target"]
    features = TEMPLATE_PARAMS["features"]
    orig_features = features.copy()
    compressed_features = TEMPLATE_PARAMS["compressed_features"]
    train_all_data = TEMPLATE_PARAMS["train_all_data"]
    hyperparameters = TEMPLATE_PARAMS["hyperparameters"] or {}
    validation_split = 0.2

    # Script arguments for input/output directories
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-dir", type=str, default=os.environ.get("SM_MODEL_DIR", "/opt/ml/model"))
    parser.add_argument("--train", type=str, default=os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train"))
    parser.add_argument(
        "--output-data-dir", type=str, default=os.environ.get("SM_OUTPUT_DATA_DIR", "/opt/ml/output/data")
    )
    args = parser.parse_args()

    # Read the training data into DataFrames
    training_files = [os.path.join(args.train, file) for file in os.listdir(args.train) if file.endswith(".csv")]
    print(f"Training Files: {training_files}")

    # Combine files and read them all into a single pandas dataframe
    all_df = pd.concat([pd.read_csv(file, engine="python") for file in training_files])

    # Check if the dataframe is empty
    check_dataframe(all_df, "training_df")

    # Features/Target output
    print(f"Target: {target}")
    print(f"Features: {str(features)}")

    # Convert any features that might be categorical to 'category' type
    all_df, category_mappings = convert_categorical_types(all_df, features)

    # If we have compressed features, decompress them
    if compressed_features:
        print(f"Decompressing features {compressed_features}...")
        all_df, features = decompress_features(all_df, features, compressed_features)

    # Do we want to train on all the data?
    if train_all_data:
        print("Training on ALL of the data")
        df_train = all_df.copy()
        df_val = all_df.copy()

    # Does the dataframe have a training column?
    elif "training" in all_df.columns:
        print("Found training column, splitting data based on training column")
        df_train = all_df[all_df["training"]]
        df_val = all_df[~all_df["training"]]
    else:
        # Just do a random training Split
        print("WARNING: No training column found, splitting data with random state=42")
        df_train, df_val = train_test_split(all_df, test_size=validation_split, random_state=42)
    print(f"FIT/TRAIN: {df_train.shape}")
    print(f"VALIDATION: {df_val.shape}")

    # Extract sample weights if present
    if 'sample_weight' in df_train.columns:
        sample_weights = df_train['sample_weight']
        print(f"Using sample weights: min={sample_weights.min():.2f}, max={sample_weights.max():.2f}, mean={sample_weights.mean():.2f}")
    else:
        sample_weights = None
        print("No sample weights found, training with equal weights")

    # Prepare features and targets for training
    X_train = df_train[features]
    X_validate = df_val[features]
    y_train = df_train[target]
    y_validate = df_val[target]

    # Train XGBoost for point predictions
    print("\nTraining XGBoost for point predictions...")
    print(f"  Hyperparameters: {hyperparameters}")
    xgb_model = XGBRegressor(enable_categorical=True, **hyperparameters)
    xgb_model.fit(X_train, y_train, sample_weight=sample_weights)

    # Evaluate XGBoost performance
    y_pred_xgb = xgb_model.predict(X_validate)
    xgb_metrics = compute_regression_metrics(y_validate, y_pred_xgb)

    print(f"\nXGBoost Point Prediction Performance:")
    print_regression_metrics(xgb_metrics)

    # Define confidence levels we want to model
    confidence_levels = [0.50, 0.68, 0.80, 0.90, 0.95]  # 50%, 68%, 80%, 90%, 95% confidence intervals

    # Store MAPIE models for each confidence level
    mapie_models = {}

    # Train models for each confidence level
    for confidence_level in confidence_levels:
        alpha = 1 - confidence_level
        lower_q = alpha / 2
        upper_q = 1 - alpha / 2

        print(f"\nTraining quantile models for {confidence_level * 100:.0f}% confidence interval...")
        print(f"  Quantiles: {lower_q:.3f}, {upper_q:.3f}, 0.500")

        # Train three models for this confidence level
        quantile_estimators = []
        for q in [lower_q, upper_q, 0.5]:
            print(f"    Training model for quantile {q:.3f}...")
            est = LGBMRegressor(
                objective="quantile",
                alpha=q,
                n_estimators=1000,
                max_depth=6,
                learning_rate=0.01,
                num_leaves=31,
                min_child_samples=20,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                verbose=-1,
                force_col_wise=True,
            )
            est.fit(X_train, y_train)
            quantile_estimators.append(est)

        # Create MAPIE CQR model for this confidence level
        print(f"  Setting up MAPIE CQR for {confidence_level * 100:.0f}% confidence...")
        mapie_model = ConformalizedQuantileRegressor(
            quantile_estimators, confidence_level=confidence_level, prefit=True
        )

        # Conformalize the model
        print(f"  Conformalizing with validation data...")
        mapie_model.conformalize(X_validate, y_validate)

        # Store the model
        mapie_models[f"mapie_{confidence_level:.2f}"] = mapie_model

        # Validate coverage for this confidence level
        y_pred, y_pis = mapie_model.predict_interval(X_validate)
        coverage = np.mean((y_validate >= y_pis[:, 0, 0]) & (y_validate <= y_pis[:, 1, 0]))
        print(f"  Coverage: Target={confidence_level * 100:.0f}%, Empirical={coverage * 100:.1f}%")

    print(f"\nOverall Model Performance Summary:")
    print_regression_metrics(xgb_metrics)

    # Analyze interval widths across confidence levels
    print(f"\nInterval Width Analysis:")
    for conf_level in confidence_levels:
        model = mapie_models[f"mapie_{conf_level:.2f}"]
        _, y_pis = model.predict_interval(X_validate)
        widths = y_pis[:, 1, 0] - y_pis[:, 0, 0]
        print(f"  {conf_level * 100:.0f}% CI: Mean width={np.mean(widths):.3f}, Std={np.std(widths):.3f}")

    # Compute normalization statistics for confidence calculation
    print(f"\nComputing normalization statistics for confidence scores...")

    # Add predictions directly to validation dataframe
    df_val["prediction"] = xgb_model.predict(X_validate)

    # Add all quantile predictions
    for conf_level in confidence_levels:
        model_name = f"mapie_{conf_level:.2f}"
        model = mapie_models[model_name]
        y_pred, y_pis = model.predict_interval(X_validate)

        if conf_level == 0.50:
            df_val["q_25"] = y_pis[:, 0, 0]
            df_val["q_75"] = y_pis[:, 1, 0]
            # y_pred is the median prediction
            df_val["q_50"] = y_pred
        elif conf_level == 0.68:
            df_val["q_16"] = y_pis[:, 0, 0]
            df_val["q_84"] = y_pis[:, 1, 0]
        elif conf_level == 0.80:
            df_val["q_10"] = y_pis[:, 0, 0]
            df_val["q_90"] = y_pis[:, 1, 0]
        elif conf_level == 0.90:
            df_val["q_05"] = y_pis[:, 0, 0]
            df_val["q_95"] = y_pis[:, 1, 0]
        elif conf_level == 0.95:
            df_val["q_025"] = y_pis[:, 0, 0]
            df_val["q_975"] = y_pis[:, 1, 0]

    # Compute normalization stats using q_10 and q_90 (default range)
    interval_width = (df_val["q_90"] - df_val["q_10"]).abs()
    median_interval_width = float(interval_width.median())
    print(f"  Median interval width (q_10-q_90): {median_interval_width:.6f}")

    # Save median interval width for confidence calculation
    with open(os.path.join(args.model_dir, "median_interval_width.json"), "w") as fp:
        json.dump(median_interval_width, fp)

    # Save the trained XGBoost model
    joblib.dump(xgb_model, os.path.join(args.model_dir, "xgb_model.joblib"))

    # Save all MAPIE models
    for model_name, model in mapie_models.items():
        joblib.dump(model, os.path.join(args.model_dir, f"{model_name}.joblib"))

    # Save the feature list
    with open(os.path.join(args.model_dir, "feature_columns.json"), "w") as fp:
        json.dump(features, fp)

    # Save category mappings if any
    if category_mappings:
        with open(os.path.join(args.model_dir, "category_mappings.json"), "w") as fp:
            json.dump(category_mappings, fp)

    # Save model configuration
    model_config = {
        "model_type": "XGBoost_MAPIE_CQR_LightGBM",
        "confidence_levels": confidence_levels,
        "n_features": len(features),
        "target": target,
        "validation_metrics": {
            "xgb_rmse": float(xgb_metrics["rmse"]),
            "xgb_mae": float(xgb_metrics["mae"]),
            "xgb_r2": float(xgb_metrics["r2"]),
            "n_validation": len(df_val),
        },
    }
    with open(os.path.join(args.model_dir, "model_config.json"), "w") as fp:
        json.dump(model_config, fp, indent=2)

    print(f"\nModel training complete!")
    print(f"Saved 1 XGBoost model and {len(mapie_models)} MAPIE models to {args.model_dir}")


#
# Inference Section
#
def model_fn(model_dir) -> dict:
    """Load XGBoost and all MAPIE models from the specified directory."""

    # Load model configuration to know which models to load
    with open(os.path.join(model_dir, "model_config.json")) as fp:
        config = json.load(fp)

    # Load XGBoost regressor
    xgb_path = os.path.join(model_dir, "xgb_model.joblib")
    xgb_model = joblib.load(xgb_path)

    # Load all MAPIE models
    mapie_models = {}
    for conf_level in config["confidence_levels"]:
        model_name = f"mapie_{conf_level:.2f}"
        mapie_models[model_name] = joblib.load(os.path.join(model_dir, f"{model_name}.joblib"))

    # Load category mappings if they exist
    category_mappings = {}
    category_path = os.path.join(model_dir, "category_mappings.json")
    if os.path.exists(category_path):
        with open(category_path) as fp:
            category_mappings = json.load(fp)

    # Load median interval width for confidence calculation
    median_interval_width = None
    median_width_path = os.path.join(model_dir, "median_interval_width.json")
    if os.path.exists(median_width_path):
        with open(median_width_path) as fp:
            median_interval_width = json.load(fp)

    return {
        "xgb_model": xgb_model,
        "mapie_models": mapie_models,
        "confidence_levels": config["confidence_levels"],
        "category_mappings": category_mappings,
        "median_interval_width": median_interval_width,
    }


def predict_fn(df, models) -> pd.DataFrame:
    """Make predictions using XGBoost for point estimates and MAPIE for conformalized intervals

    Args:
        df (pd.DataFrame): The input DataFrame
        models (dict): Dictionary containing XGBoost and MAPIE models

    Returns:
        pd.DataFrame: DataFrame with XGBoost predictions and conformalized intervals
    """

    # Flag for outlier stretch adjustment for the prediction intervals
    # if the predicted values are outside the intervals
    outlier_stretch = False

    # Grab our feature columns (from training)
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")
    with open(os.path.join(model_dir, "feature_columns.json")) as fp:
        model_features = json.load(fp)

    # Match features in a case-insensitive manner
    matched_df = match_features_case_insensitive(df, model_features)

    # Apply categorical mappings if they exist
    if models.get("category_mappings"):
        matched_df, _ = convert_categorical_types(matched_df, model_features, models["category_mappings"])

    # Get features for prediction
    X = matched_df[model_features]

    # Get XGBoost point predictions
    df["prediction"] = models["xgb_model"].predict(X)

    # Get predictions from each MAPIE model for conformalized intervals
    for conf_level in models["confidence_levels"]:
        model_name = f"mapie_{conf_level:.2f}"
        model = models["mapie_models"][model_name]

        # Get conformalized predictions
        y_pred, y_pis = model.predict_interval(X)

        # Map confidence levels to quantile names
        if conf_level == 0.50:  # 50% CI
            df["q_25"] = y_pis[:, 0, 0]
            df["q_75"] = y_pis[:, 1, 0]
            # y_pred is the median prediction
            df["q_50"] = y_pred
        elif conf_level == 0.68:  # 68% CI
            df["q_16"] = y_pis[:, 0, 0]
            df["q_84"] = y_pis[:, 1, 0]
        elif conf_level == 0.80:  # 80% CI
            df["q_10"] = y_pis[:, 0, 0]
            df["q_90"] = y_pis[:, 1, 0]
        elif conf_level == 0.90:  # 90% CI
            df["q_05"] = y_pis[:, 0, 0]
            df["q_95"] = y_pis[:, 1, 0]
        elif conf_level == 0.95:  # 95% CI
            df["q_025"] = y_pis[:, 0, 0]
            df["q_975"] = y_pis[:, 1, 0]

    # Calculate a pseudo-standard deviation from the 68% interval width
    df["prediction_std"] = (df["q_84"] - df["q_16"]).abs() / 2.0

    # Reorder the quantile columns for easier reading
    quantile_cols = ["q_025", "q_05", "q_10", "q_16", "q_25", "q_50", "q_75", "q_84", "q_90", "q_95", "q_975"]
    other_cols = [col for col in df.columns if col not in quantile_cols]
    df = df[other_cols + quantile_cols]

    # Adjust the outer quantiles to ensure they encompass the prediction
    if outlier_stretch:
        # Lower intervals adjustments
        df["q_025"] = np.minimum(df["q_025"], df["prediction"])
        df["q_05"] = np.minimum(df["q_05"], df["prediction"])
        df["q_10"] = np.minimum(df["q_10"], df["prediction"])
        df["q_16"] = np.minimum(df["q_16"], df["prediction"])
        df["q_25"] = np.minimum(df["q_25"], df["prediction"])

        # Upper intervals adjustments
        df["q_75"] = np.maximum(df["q_75"], df["prediction"])
        df["q_84"] = np.maximum(df["q_84"], df["prediction"])
        df["q_90"] = np.maximum(df["q_90"], df["prediction"])
        df["q_95"] = np.maximum(df["q_95"], df["prediction"])
        df["q_975"] = np.maximum(df["q_975"], df["prediction"])

    # Compute confidence scores using pre-computed normalization stats
    df = compute_confidence(
        df,
        lower_q="q_10",
        upper_q="q_90",
        alpha=1.0,
        beta=1.0,
        median_interval_width=models["median_interval_width"],
    )

    return df
