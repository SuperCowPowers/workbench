# ChemProp Model Template for Workbench
# Uses ChemProp 2.x Message Passing Neural Networks for molecular property prediction
import os
import argparse
import json
from io import StringIO

import awswrangler as wr
import numpy as np
import pandas as pd
import torch
from lightning import pytorch as pl
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    mean_absolute_error,
    r2_score,
    root_mean_squared_error,
    precision_recall_fscore_support,
    confusion_matrix,
)
import joblib

# ChemProp imports
from chemprop import data, featurizers, models, nn

# Template Parameters
TEMPLATE_PARAMS = {
    "model_type": "{{model_type}}",
    "target": "{{target_column}}",
    "feature_list": "{{feature_list}}",
    "model_metrics_s3_path": "{{model_metrics_s3_path}}",
    "train_all_data": "{{train_all_data}}",
    "hyperparameters": "{{hyperparameters}}",
}


def check_dataframe(df: pd.DataFrame, df_name: str) -> None:
    """Check if the provided dataframe is empty and raise an exception if it is."""
    if df.empty:
        msg = f"*** The training data {df_name} has 0 rows! ***STOPPING***"
        print(msg)
        raise ValueError(msg)


def find_smiles_column(df: pd.DataFrame) -> str:
    """Find the SMILES column in a DataFrame (any capitalization)."""
    smiles_column = next((col for col in df.columns if col.lower() == "smiles"), None)
    if smiles_column is None:
        raise ValueError("Input DataFrame must have a 'smiles' column")
    return smiles_column


def expand_proba_column(df: pd.DataFrame, class_labels: list[str]) -> pd.DataFrame:
    """Expands a column containing a list of probabilities into separate columns.

    Handles None values for rows where predictions couldn't be made.
    """
    proba_column = "pred_proba"
    if proba_column not in df.columns:
        raise ValueError('DataFrame does not contain a "pred_proba" column')

    proba_splits = [f"{label}_proba" for label in class_labels]
    n_classes = len(class_labels)

    # Handle None values by replacing with list of NaNs
    proba_values = []
    for val in df[proba_column]:
        if val is None:
            proba_values.append([np.nan] * n_classes)
        else:
            proba_values.append(val)

    proba_df = pd.DataFrame(proba_values, columns=proba_splits)

    df = df.drop(columns=[proba_column] + proba_splits, errors="ignore")
    df = df.reset_index(drop=True)
    df = pd.concat([df, proba_df], axis=1)
    return df


def get_extra_features(feature_list: list[str]) -> list[str]:
    """Get the list of extra descriptor features (excludes 'smiles')."""
    return [f for f in feature_list if f.lower() != "smiles"]


def create_molecule_datapoints(
    smiles_list: list[str],
    targets: list[float] | None = None,
    extra_descriptors: np.ndarray | None = None,
) -> tuple[list[data.MoleculeDatapoint], list[int]]:
    """Create ChemProp MoleculeDatapoints from SMILES strings.

    Args:
        smiles_list: List of SMILES strings
        targets: Optional list of target values (for training)
        extra_descriptors: Optional array of extra features (n_samples, n_features)

    Returns:
        Tuple of (list of MoleculeDatapoint objects, list of valid indices)
    """
    from rdkit import Chem

    datapoints = []
    valid_indices = []
    invalid_count = 0

    for i, smi in enumerate(smiles_list):
        # Validate SMILES with RDKit first
        mol = Chem.MolFromSmiles(smi)
        if mol is None:
            invalid_count += 1
            continue

        # Build datapoint with optional target and extra descriptors
        y = [targets[i]] if targets is not None else None
        x_d = extra_descriptors[i] if extra_descriptors is not None else None

        dp = data.MoleculeDatapoint.from_smi(smi, y=y, x_d=x_d)
        datapoints.append(dp)
        valid_indices.append(i)

    if invalid_count > 0:
        print(f"Warning: Skipped {invalid_count} invalid SMILES strings")

    return datapoints, valid_indices


def build_mpnn_model(
    hyperparameters: dict,
    task: str = "regression",
    num_classes: int | None = None,
    n_extra_descriptors: int = 0,
    x_d_transform: nn.ScaleTransform | None = None,
) -> models.MPNN:
    """Build an MPNN model with the specified hyperparameters.

    Args:
        hyperparameters: Dictionary of model hyperparameters
        task: Either "regression" or "classification"
        num_classes: Number of classes for classification tasks
        n_extra_descriptors: Number of extra descriptor features (for hybrid mode)
        x_d_transform: Optional transform for extra descriptors (scaling)

    Returns:
        Configured MPNN model
    """
    # Model hyperparameters with defaults
    hidden_dim = hyperparameters.get("hidden_dim", 300)
    depth = hyperparameters.get("depth", 3)
    dropout = hyperparameters.get("dropout", 0.0)
    ffn_hidden_dim = hyperparameters.get("ffn_hidden_dim", 300)
    ffn_num_layers = hyperparameters.get("ffn_num_layers", 1)

    # Message passing component
    mp = nn.BondMessagePassing(d_h=hidden_dim, depth=depth, dropout=dropout)

    # Aggregation
    agg = nn.MeanAggregation()

    # FFN input_dim = message passing output + extra descriptors
    ffn_input_dim = hidden_dim + n_extra_descriptors

    # Build FFN based on task type
    if task == "classification" and num_classes is not None:
        # Multi-class classification
        ffn = nn.MulticlassClassificationFFN(
            n_classes=num_classes,
            input_dim=ffn_input_dim,
            hidden_dim=ffn_hidden_dim,
            n_layers=ffn_num_layers,
            dropout=dropout,
        )
    else:
        # Regression (uses MSE loss by default)
        ffn = nn.RegressionFFN(
            input_dim=ffn_input_dim,
            hidden_dim=ffn_hidden_dim,
            n_layers=ffn_num_layers,
            dropout=dropout,
        )

    # Create the MPNN model
    mpnn = models.MPNN(
        message_passing=mp,
        agg=agg,
        predictor=ffn,
        batch_norm=True,
        metrics=None,
        X_d_transform=x_d_transform,
    )

    return mpnn


def model_fn(model_dir: str) -> models.MPNN:
    """Load the ChemProp MPNN model from the specified directory.

    Args:
        model_dir: Directory containing the saved model

    Returns:
        Loaded MPNN model
    """
    model_path = os.path.join(model_dir, "chemprop_model.pt")
    model = models.MPNN.load_from_file(model_path)
    model.eval()
    return model


def input_fn(input_data, content_type: str) -> pd.DataFrame:
    """Parse input data and return a DataFrame."""
    if not input_data:
        raise ValueError("Empty input data is not supported!")

    if isinstance(input_data, bytes):
        input_data = input_data.decode("utf-8")

    if "text/csv" in content_type:
        return pd.read_csv(StringIO(input_data))
    elif "application/json" in content_type:
        return pd.DataFrame(json.loads(input_data))
    else:
        raise ValueError(f"{content_type} not supported!")


def output_fn(output_df: pd.DataFrame, accept_type: str) -> tuple[str, str]:
    """Supports both CSV and JSON output formats."""
    if "text/csv" in accept_type:
        csv_output = output_df.fillna("N/A").to_csv(index=False)
        return csv_output, "text/csv"
    elif "application/json" in accept_type:
        return output_df.to_json(orient="records"), "application/json"
    else:
        raise RuntimeError(f"{accept_type} accept type is not supported by this script.")


def predict_fn(df: pd.DataFrame, model: models.MPNN) -> pd.DataFrame:
    """Make predictions with the ChemProp MPNN model.

    Args:
        df: Input DataFrame containing SMILES column (and extra features if hybrid mode)
        model: The loaded MPNN model

    Returns:
        DataFrame with predictions added
    """
    model_type = TEMPLATE_PARAMS["model_type"]
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")

    # Load label encoder if present (classification)
    label_encoder = None
    label_encoder_path = os.path.join(model_dir, "label_encoder.joblib")
    if os.path.exists(label_encoder_path):
        label_encoder = joblib.load(label_encoder_path)

    # Load feature metadata and scaler if present (hybrid mode)
    feature_metadata = None
    x_d_scaler = None
    feature_metadata_path = os.path.join(model_dir, "feature_metadata.joblib")
    x_d_scaler_path = os.path.join(model_dir, "x_d_scaler.joblib")
    if os.path.exists(feature_metadata_path):
        feature_metadata = joblib.load(feature_metadata_path)
        x_d_scaler = joblib.load(x_d_scaler_path)
        print(f"Hybrid mode: using {len(feature_metadata['extra_feature_cols'])} extra features")

    # Find SMILES column
    smiles_column = find_smiles_column(df)

    smiles_list = df[smiles_column].tolist()

    # Track invalid SMILES
    valid_mask = []
    valid_smiles = []
    valid_indices = []
    for i, smi in enumerate(smiles_list):
        if smi and isinstance(smi, str) and len(smi.strip()) > 0:
            valid_mask.append(True)
            valid_smiles.append(smi.strip())
            valid_indices.append(i)
        else:
            valid_mask.append(False)

    valid_mask = np.array(valid_mask)
    print(f"Valid SMILES: {sum(valid_mask)} / {len(smiles_list)}")

    # Initialize prediction column (use object dtype for classifiers to avoid FutureWarning)
    if model_type == "classifier":
        df["prediction"] = pd.Series([None] * len(df), dtype=object)
    else:
        df["prediction"] = np.nan

    if sum(valid_mask) == 0:
        print("Warning: No valid SMILES to predict on")
        return df

    # Prepare extra features if in hybrid mode
    extra_features = None
    if feature_metadata is not None:
        extra_feature_cols = feature_metadata["extra_feature_cols"]
        col_means = np.array(feature_metadata["col_means"])

        # Check columns exist
        missing_cols = [col for col in extra_feature_cols if col not in df.columns]
        if missing_cols:
            print(f"Warning: Missing extra feature columns: {missing_cols}. Using mean values.")

        # Extract features for valid SMILES rows
        extra_features = np.zeros((len(valid_indices), len(extra_feature_cols)), dtype=np.float32)
        for j, col in enumerate(extra_feature_cols):
            if col in df.columns:
                values = df.iloc[valid_indices][col].values.astype(np.float32)
                # Fill NaN with training column means
                nan_mask = np.isnan(values)
                values[nan_mask] = col_means[j]
                extra_features[:, j] = values
            else:
                # Column missing, use training mean
                extra_features[:, j] = col_means[j]

        # Scale features using the saved scaler
        extra_features = x_d_scaler.transform(extra_features)

    # Create datapoints for prediction (filter out invalid SMILES)
    datapoints, rdkit_valid_indices = create_molecule_datapoints(valid_smiles, extra_descriptors=extra_features)

    if len(datapoints) == 0:
        print("Warning: No valid SMILES after RDKit validation")
        return df

    dataset = data.MoleculeDataset(datapoints)
    dataloader = data.build_dataloader(dataset, shuffle=False)

    # Make predictions
    trainer = pl.Trainer(
        accelerator="auto",
        logger=False,
        enable_progress_bar=False,
    )

    with torch.inference_mode():
        predictions = trainer.predict(model, dataloader)

    # Debug: check prediction shapes
    print(f"Inference: Number of prediction batches: {len(predictions)}")
    print(f"Inference: First batch shape: {predictions[0].shape}")

    # Concatenate batch predictions
    preds = np.concatenate([p.numpy() for p in predictions], axis=0)
    print(f"Inference: Predictions shape after concat: {preds.shape}")

    # ChemProp returns (n_samples, 1, n_classes) for multiclass - squeeze middle dim
    if preds.ndim == 3 and preds.shape[1] == 1:
        preds = preds.squeeze(axis=1)
        print(f"Inference: Squeezed predictions shape: {preds.shape}")

    # Map predictions back to valid_mask positions (accounting for RDKit-invalid SMILES)
    # rdkit_valid_indices tells us which of the valid_smiles were actually valid
    valid_positions = np.where(valid_mask)[0][rdkit_valid_indices]
    valid_mask = np.zeros(len(df), dtype=bool)
    valid_mask[valid_positions] = True

    if model_type == "classifier" and label_encoder is not None:
        # For classification, get class predictions and probabilities
        if preds.ndim == 2 and preds.shape[1] > 1:
            # Multi-class: preds are probabilities
            class_preds = np.argmax(preds, axis=1)
            decoded_preds = label_encoder.inverse_transform(class_preds)
            df.loc[valid_mask, "prediction"] = decoded_preds

            # Add probability columns
            proba_series = pd.Series([None] * len(df), index=df.index, dtype=object)
            proba_series.loc[valid_mask] = [p.tolist() for p in preds]
            df["pred_proba"] = proba_series
            df = expand_proba_column(df, label_encoder.classes_)
        else:
            # Binary or single output
            class_preds = (preds.flatten() > 0.5).astype(int)
            decoded_preds = label_encoder.inverse_transform(class_preds)
            df.loc[valid_mask, "prediction"] = decoded_preds
    else:
        # Regression: direct predictions
        df.loc[valid_mask, "prediction"] = preds.flatten()

    return df


if __name__ == "__main__":
    """Training script for ChemProp MPNN model"""
    from sklearn.preprocessing import StandardScaler

    # Template Parameters
    target = TEMPLATE_PARAMS["target"]
    model_type = TEMPLATE_PARAMS["model_type"]
    feature_list = TEMPLATE_PARAMS["feature_list"]
    model_metrics_s3_path = TEMPLATE_PARAMS["model_metrics_s3_path"]
    train_all_data = TEMPLATE_PARAMS["train_all_data"]
    hyperparameters = TEMPLATE_PARAMS["hyperparameters"]
    validation_split = 0.2

    # Determine extra features (all features except smiles)
    extra_feature_cols = get_extra_features(feature_list)
    use_extra_features = len(extra_feature_cols) > 0
    print(f"Feature List: {feature_list}")
    print(f"Extra Features (hybrid mode): {extra_feature_cols if use_extra_features else 'None (SMILES only)'}")

    # Script arguments for input/output directories
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-dir", type=str, default=os.environ.get("SM_MODEL_DIR", "/opt/ml/model"))
    parser.add_argument("--train", type=str, default=os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train"))
    parser.add_argument(
        "--output-data-dir", type=str, default=os.environ.get("SM_OUTPUT_DATA_DIR", "/opt/ml/output/data")
    )
    args = parser.parse_args()

    # Read the training data
    training_files = [os.path.join(args.train, f) for f in os.listdir(args.train) if f.endswith(".csv")]
    print(f"Training Files: {training_files}")

    all_df = pd.concat([pd.read_csv(f, engine="python") for f in training_files])
    print(f"All Data Shape: {all_df.shape}")

    check_dataframe(all_df, "training_df")

    # Find SMILES column
    smiles_column = find_smiles_column(all_df)

    # Drop rows with missing SMILES or target values
    initial_count = len(all_df)
    all_df = all_df.dropna(subset=[smiles_column, target])
    dropped = initial_count - len(all_df)
    if dropped > 0:
        print(f"Dropped {dropped} rows with missing SMILES or target values")

    print(f"Target: {target}")
    print(f"SMILES Column: {smiles_column}")
    print(f"Data Shape after cleaning: {all_df.shape}")

    # Set up label encoder for classification
    label_encoder = None
    if model_type == "classifier":
        label_encoder = LabelEncoder()
        all_df[target] = label_encoder.fit_transform(all_df[target])
        num_classes = len(label_encoder.classes_)
        print(f"Classification task with {num_classes} classes: {label_encoder.classes_}")
    else:
        num_classes = None

    # Split data
    if train_all_data:
        print("Training on ALL of the data")
        df_train = all_df.copy()
        df_val = all_df.copy()
    elif "training" in all_df.columns:
        print("Found training column, splitting data based on training column")
        df_train = all_df[all_df["training"]].copy()
        df_val = all_df[~all_df["training"]].copy()
    else:
        print("WARNING: No training column found, splitting data with random state=42")
        df_train, df_val = train_test_split(all_df, test_size=validation_split, random_state=42)

    print(f"TRAIN: {df_train.shape}")
    print(f"VALIDATION: {df_val.shape}")

    # Extract and scale extra features if using hybrid mode
    x_d_scaler = None
    x_d_transform = None
    train_extra_features = None
    val_extra_features = None

    if use_extra_features:
        # Check that all extra feature columns exist
        missing_cols = [col for col in extra_feature_cols if col not in df_train.columns]
        if missing_cols:
            raise ValueError(f"Missing extra feature columns in training data: {missing_cols}")

        # Extract features and handle NaN values
        train_extra_features = df_train[extra_feature_cols].values.astype(np.float32)
        val_extra_features = df_val[extra_feature_cols].values.astype(np.float32)

        # Fill NaN with column means from training data
        col_means = np.nanmean(train_extra_features, axis=0)
        for i in range(train_extra_features.shape[1]):
            train_nan_mask = np.isnan(train_extra_features[:, i])
            val_nan_mask = np.isnan(val_extra_features[:, i])
            train_extra_features[train_nan_mask, i] = col_means[i]
            val_extra_features[val_nan_mask, i] = col_means[i]

        # Scale extra features (we pre-scale here, so don't use X_d_transform in model)
        x_d_scaler = StandardScaler()
        train_extra_features = x_d_scaler.fit_transform(train_extra_features)
        val_extra_features = x_d_scaler.transform(val_extra_features)

        # Note: We set x_d_transform=None because we pre-scale the features ourselves
        # The scaler is saved for inference to apply the same scaling
        x_d_transform = None

        print(f"Extra features shape: train={train_extra_features.shape}, val={val_extra_features.shape}")

    # Create ChemProp datasets (invalid SMILES are filtered out)
    train_datapoints, train_valid_idx = create_molecule_datapoints(
        df_train[smiles_column].tolist(),
        df_train[target].tolist(),
        train_extra_features,
    )
    val_datapoints, val_valid_idx = create_molecule_datapoints(
        df_val[smiles_column].tolist(),
        df_val[target].tolist(),
        val_extra_features,
    )

    # Update dataframes to only include valid molecules
    df_train = df_train.iloc[train_valid_idx].reset_index(drop=True)
    df_val = df_val.iloc[val_valid_idx].reset_index(drop=True)
    print(f"TRAIN (after SMILES validation): {df_train.shape}")
    print(f"VALIDATION (after SMILES validation): {df_val.shape}")

    train_dataset = data.MoleculeDataset(train_datapoints)
    val_dataset = data.MoleculeDataset(val_datapoints)

    # Get batch size from hyperparameters
    batch_size = hyperparameters.get("batch_size", min(64, max(16, len(df_train) // 16)))

    train_loader = data.build_dataloader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = data.build_dataloader(val_dataset, batch_size=batch_size, shuffle=False)

    # Build the model
    print(f"Hyperparameters: {hyperparameters}")
    task = "classification" if model_type == "classifier" else "regression"
    n_extra = len(extra_feature_cols) if use_extra_features else 0
    mpnn = build_mpnn_model(
        hyperparameters,
        task=task,
        num_classes=num_classes,
        n_extra_descriptors=n_extra,
        x_d_transform=x_d_transform,
    )

    # Training configuration
    max_epochs = hyperparameters.get("max_epochs", 50)
    patience = hyperparameters.get("patience", 10)

    # Set up trainer
    callbacks = [
        pl.callbacks.EarlyStopping(monitor="val_loss", patience=patience, mode="min"),
        pl.callbacks.ModelCheckpoint(
            dirpath=args.model_dir,
            filename="best_model",
            monitor="val_loss",
            mode="min",
            save_top_k=1,
        ),
    ]

    trainer = pl.Trainer(
        accelerator="auto",
        max_epochs=max_epochs,
        callbacks=callbacks,
        logger=False,
        enable_progress_bar=True,
    )

    # Train the model
    print("Starting training...")
    trainer.fit(mpnn, train_loader, val_loader)
    print("Training complete!")

    # Load the best checkpoint (PyTorch 2.6+ requires weights_only=False for Lightning checkpoints)
    if trainer.checkpoint_callback and trainer.checkpoint_callback.best_model_path:
        best_ckpt_path = trainer.checkpoint_callback.best_model_path
        print(f"Loading best checkpoint from {best_ckpt_path}")
        checkpoint = torch.load(best_ckpt_path, weights_only=False)
        mpnn.load_state_dict(checkpoint["state_dict"])

    mpnn.eval()

    # Make predictions on validation set
    print("Making Predictions on Validation Set...")
    with torch.inference_mode():
        val_predictions = trainer.predict(mpnn, val_loader)

    # Debug: check prediction shapes
    print(f"Number of prediction batches: {len(val_predictions)}")
    print(f"First batch shape: {val_predictions[0].shape}")
    print(f"First batch ndim: {val_predictions[0].ndim}")

    preds = np.concatenate([p.numpy() for p in val_predictions], axis=0)
    y_validate = df_val[target].values

    print(f"Predictions shape after concat: {preds.shape}, y_validate shape: {y_validate.shape}")

    # ChemProp may return (n_samples, 1, n_classes) for multiclass - squeeze middle dim
    if preds.ndim == 3 and preds.shape[1] == 1:
        preds = preds.squeeze(axis=1)
        print(f"Squeezed predictions shape: {preds.shape}")

    if model_type == "classifier":
        # Classification metrics - handle multi-class output
        if preds.ndim == 2 and preds.shape[1] > 1:
            # Multi-class: preds is (n_samples, n_classes), take argmax
            class_preds = np.argmax(preds, axis=1)
        elif preds.ndim == 1:
            # Binary with single output
            class_preds = (preds > 0.5).astype(int)
        else:
            # Squeeze extra dimensions if needed
            preds = preds.squeeze()
            if preds.ndim == 2:
                class_preds = np.argmax(preds, axis=1)
            else:
                class_preds = (preds > 0.5).astype(int)

        print(f"class_preds shape: {class_preds.shape}")

        # Decode labels for metrics
        y_validate_decoded = label_encoder.inverse_transform(y_validate.astype(int))
        preds_decoded = label_encoder.inverse_transform(class_preds)

        # Calculate metrics
        label_names = label_encoder.classes_
        scores = precision_recall_fscore_support(y_validate_decoded, preds_decoded, average=None, labels=label_names)

        score_df = pd.DataFrame({
            target: label_names,
            "precision": scores[0],
            "recall": scores[1],
            "f1": scores[2],
            "support": scores[3],
        })

        # Output metrics per class
        metrics = ["precision", "recall", "f1", "support"]
        for t in label_names:
            for m in metrics:
                value = score_df.loc[score_df[target] == t, m].iloc[0]
                print(f"Metrics:{t}:{m} {value}")

        # Confusion matrix
        conf_mtx = confusion_matrix(y_validate_decoded, preds_decoded, labels=label_names)
        for i, row_name in enumerate(label_names):
            for j, col_name in enumerate(label_names):
                value = conf_mtx[i, j]
                print(f"ConfusionMatrix:{row_name}:{col_name} {value}")

        # Save validation predictions
        df_val = df_val.copy()
        df_val["prediction"] = preds_decoded
        if preds.ndim == 2 and preds.shape[1] > 1:
            df_val["pred_proba"] = [p.tolist() for p in preds]
            df_val = expand_proba_column(df_val, label_names)

    else:
        # Regression metrics
        preds_flat = preds.flatten()
        rmse = root_mean_squared_error(y_validate, preds_flat)
        mae = mean_absolute_error(y_validate, preds_flat)
        r2 = r2_score(y_validate, preds_flat)
        print(f"RMSE: {rmse:.3f}")
        print(f"MAE: {mae:.3f}")
        print(f"R2: {r2:.3f}")
        print(f"NumRows: {len(df_val)}")

        df_val = df_val.copy()
        df_val["prediction"] = preds_flat

    # Save validation predictions to S3
    output_columns = [target, "prediction"]
    output_columns += [col for col in df_val.columns if col.endswith("_proba")]
    wr.s3.to_csv(
        df_val[output_columns],
        path=f"{model_metrics_s3_path}/validation_predictions.csv",
        index=False,
    )

    # Save the model
    model_path = os.path.join(args.model_dir, "chemprop_model.pt")
    models.save_model(model_path, mpnn)
    print(f"Model saved to {model_path}")

    # Save label encoder if classification
    if label_encoder is not None:
        joblib.dump(label_encoder, os.path.join(args.model_dir, "label_encoder.joblib"))

    # Save extra feature metadata for inference (hybrid mode)
    if use_extra_features:
        feature_metadata = {
            "extra_feature_cols": extra_feature_cols,
            "col_means": col_means.tolist(),
        }
        joblib.dump(feature_metadata, os.path.join(args.model_dir, "feature_metadata.joblib"))
        joblib.dump(x_d_scaler, os.path.join(args.model_dir, "x_d_scaler.joblib"))
        print(f"Saved feature metadata for {len(extra_feature_cols)} extra features")
