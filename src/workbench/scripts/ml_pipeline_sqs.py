import argparse
import logging
import json
from pathlib import Path

# Workbench Imports
from workbench.core.cloud_platform.aws.aws_account_clamp import AWSAccountClamp
from workbench.utils.config_manager import ConfigManager
from workbench.utils.s3_utils import upload_content_to_s3

log = logging.getLogger("workbench")
cm = ConfigManager()
workbench_bucket = cm.get_config("WORKBENCH_BUCKET")


def submit_to_sqs(script_path: str, size: str = "small", realtime: bool = False, recreate: bool = False) -> None:
    """
    Upload script to S3 and submit message to SQS queue for processing.
    Args:
        script_path: Local path to the ML pipeline script
        size: Job size tier - "small" (default), "medium", or "large"
        realtime: If True, sets serverless=False for real-time processing (default: False, meaning serverless=True)
        recreate: If True, sets RECREATE=True in environment (default: False)
    """
    print(f"\n{'=' * 60}")
    print("ğŸš€  SUBMITTING ML PIPELINE JOB")
    print(f"{'=' * 60}")

    if size not in ["small", "medium", "large"]:
        raise ValueError(f"Invalid size '{size}'. Must be 'small', 'medium', or 'large'")
    # Validate script exists
    script_file = Path(script_path)
    if not script_file.exists():
        raise FileNotFoundError(f"Script not found: {script_path}")

    print(f"ğŸ“„  Script: {script_file.name}")
    print(f"ğŸ“  Size tier: {size}")
    print(f"âš¡  Mode: {'Real-time' if realtime else 'Serverless'} (serverless={'False' if realtime else 'True'})")
    print(f"ğŸ”„  Recreate: {recreate}")
    print(f"ğŸª£  Bucket: {workbench_bucket}")
    sqs = AWSAccountClamp().boto3_session.client("sqs")
    script_name = script_file.name

    # List Workbench queues
    print("\nğŸ“‹  Listing Workbench SQS queues...")
    try:
        queues = sqs.list_queues(QueueNamePrefix="workbench-")
        queue_urls = queues.get("QueueUrls", [])
        if queue_urls:
            print(f"âœ…  Found {len(queue_urls)} workbench queue(s):")
            for url in queue_urls:
                queue_name = url.split("/")[-1]
                print(f"   â€¢ {queue_name}")
        else:
            print("âš ï¸  No workbench queues found")
    except Exception as e:
        print(f"âŒ  Error listing queues: {e}")

    # Upload script to S3
    s3_path = f"s3://{workbench_bucket}/batch-jobs/{script_name}"
    print("\nğŸ“¤  Uploading script to S3...")
    print(f"   Source: {script_path}")
    print(f"   Destination: {s3_path}")

    try:
        upload_content_to_s3(script_file.read_text(), s3_path)
        print("âœ…  Script uploaded successfully")
    except Exception as e:
        print(f"âŒ  Upload failed: {e}")
        raise
    # Get queue URL and info
    queue_name = "workbench-ml-pipeline-queue.fifo"
    print("\nğŸ¯  Getting queue information...")
    print(f"   Queue name: {queue_name}")

    try:
        queue_url = sqs.get_queue_url(QueueName=queue_name)["QueueUrl"]
        print(f"   Queue URL: {queue_url}")

        # Get queue attributes for additional info
        attrs = sqs.get_queue_attributes(
            QueueUrl=queue_url, AttributeNames=["ApproximateNumberOfMessages", "ApproximateNumberOfMessagesNotVisible"]
        )
        messages_available = attrs["Attributes"].get("ApproximateNumberOfMessages", "0")
        messages_in_flight = attrs["Attributes"].get("ApproximateNumberOfMessagesNotVisible", "0")
        print(f"   Messages in queue: {messages_available}")
        print(f"   Messages in flight: {messages_in_flight}")

    except Exception as e:
        print(f"âŒ  Error accessing queue: {e}")
        raise

    # Prepare message
    message = {"script_path": s3_path, "size": size}

    # Set environment variables
    message["environment"] = {"SERVERLESS": "False" if realtime else "True"}
    if recreate:
        message["environment"]["RECREATE"] = "True"

    print("\nğŸ“¨  Sending message to SQS...")

    # Send the message to SQS
    try:
        response = sqs.send_message(
            QueueUrl=queue_url,
            MessageBody=json.dumps(message, indent=2),
            MessageGroupId="ml-pipeline-jobs",  # Required for FIFO
        )
        message_id = response["MessageId"]
        print("âœ…  Message sent successfully!")
        print(f"   Message ID: {message_id}")
    except Exception as e:
        print(f"âŒ  Failed to send message: {e}")
        raise

    # Success summary
    print(f"\n{'=' * 60}")
    print("âœ…  JOB SUBMISSION COMPLETE")
    print(f"{'=' * 60}")
    print(f"ğŸ“„  Script: {script_name}")
    print(f"ğŸ“  Size: {size}")
    print(f"âš¡  Mode: {'Real-time' if realtime else 'Serverless'} (SERVERLESS={'False' if realtime else 'True'})")
    print(f"ğŸ”„  Recreate: {recreate}")
    print(f"ğŸ†”  Message ID: {message_id}")
    print("\nğŸ”  MONITORING LOCATIONS:")
    print(f"   â€¢ SQS Queue: AWS Console â†’ SQS â†’ {queue_name}")
    print("   â€¢ Lambda Logs: AWS Console â†’ Lambda â†’ Functions")
    print("   â€¢ Batch Jobs: AWS Console â†’ Batch â†’ Jobs")
    print("   â€¢ CloudWatch: AWS Console â†’ CloudWatch â†’ Log groups")
    print("\nâ³  Your job should start processing soon...")


def main():
    """CLI entry point for submitting ML pipelines via SQS."""
    parser = argparse.ArgumentParser(description="Submit ML pipeline to SQS queue for Batch processing")
    parser.add_argument("script_file", help="Local path to ML pipeline script")
    parser.add_argument(
        "--size", default="small", choices=["small", "medium", "large"], help="Job size tier (default: small)"
    )
    parser.add_argument(
        "--realtime",
        action="store_true",
        help="Create realtime endpoints (default is serverless)",
    )
    parser.add_argument(
        "--recreate",
        action="store_true",
        help="Set RECREATE=True (will force recreation of resources)",
    )
    args = parser.parse_args()
    try:
        submit_to_sqs(args.script_file, args.size, realtime=args.realtime, recreate=args.recreate)
    except Exception as e:
        print(f"\nâŒ  ERROR: {e}")
        log.error(f"Error: {e}")
        exit(1)


if __name__ == "__main__":
    main()
