{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to ADMET Workbench","text":"<p>The ADMET Workbench framework makes AWS\u00ae both easier to use and more powerful. Workbench handles all the details around updating and managing a complex set of AWS Services. With a simple-to-use Python API and a beautiful set of web interfaces, Workbench makes creating AWS ML pipelines a snap. It also dramatically improves both the usability and visibility across the entire spectrum of services: Glue Jobs, Athena, Feature Store, Models, and Endpoints. Workbench makes it easy to build production ready, AWS powered, machine learning pipelines.</p> ADMET Workbench Dashboard: AWS Pipelines in a Whole New Light!"},{"location":"#full-aws-overview","title":"Full AWS OverView","text":"<ul> <li>Health Monitoring \ud83d\udfe2</li> <li>Dynamic Updates</li> <li>High Level Summary</li> </ul>"},{"location":"#drill-down-views","title":"Drill-Down Views","text":"<ul> <li>Glue Jobs</li> <li>DataSources</li> <li>FeatureSets</li> <li>Models</li> <li>Endpoints</li> </ul>"},{"location":"#private-saas-architecture","title":"Private SaaS Architecture","text":"<p>Secure your Data, Empower your ML Pipelines</p> <p>ADMET Workbench is architected as a Private SaaS. This hybrid architecture is the ultimate solution for businesses that prioritize data control and security. Workbench deploys as an AWS Stack within your own cloud environment, ensuring compliance with stringent corporate and regulatory standards. It offers the flexibility to tailor solutions to your specific business needs through our comprehensive plugin support, both components and full web interfaces. By using Workbench, you maintain absolute control over your data while benefiting from the power, security, and scalability of AWS cloud services. Workbench Private SaaS Architecture</p>"},{"location":"#dashboard-and-api","title":"Dashboard and API","text":"<p>The ADMET Workbench package has two main components, a Web Interface that provides visibility into AWS ML PIpelines and a Python API that makes creation and usage of the AWS ML Services easier than using/learning the services directly.</p>"},{"location":"#web-interfaces","title":"Web Interfaces","text":"<p>The ADMET Workbench Dashboard has a set of web interfaces that give visibility into the AWS Glue and SageMaker Services. There are currently 5 web interfaces available:</p> <ul> <li>Top Level Dashboard: Shows all AWS ML Artifacts (Glue and SageMaker)</li> <li>DataSources: DataSource Column Details, Distributions and Correlations</li> <li>FeatureSets: FeatureSet Details, Distributions and Correlations</li> <li>Model: Model details, performance metric, and inference plots</li> <li>Endpoints: Endpoint details, realtime charts of endpoint performance and latency</li> </ul>"},{"location":"#python-api","title":"Python API","text":"<p>Workbench API Documentation: Workbench API Classes </p> <p>The main functionality of the Python API is to encapsulate and manage a set of AWS services underneath a Python Object interface. The Python Classes are used to create and interact with Machine Learning Pipeline Artifacts.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>ADMET Workbench will need some initial setup when you first start using it. See our Getting Started guide on how to connect Workbench to your AWS Account.</p>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>Getting Started: Getting Started </li> <li>Workbench API Classes: API Classes</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"admin/aws_service_limits/","title":"AWS Service Limits","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>In general Workbench works well, out of the box, with the standard set of limits for AWS accounts. Workbench supports throttling, timeouts, and a broad set of AWS error handling routines for general purpose usage.</p> <p>When using Workbench for large scale deployments there are a set of AWS Service limits that will need to be increased.</p>"},{"location":"admin/aws_service_limits/#serverless-endpoints","title":"ServerLess Endpoints","text":"<p>There are two serverless endpoint quotas that will need to be adjusted.</p> <ul> <li>Maximum number of serverless endpoints (defaults to 5): Set to 50-100</li> <li>Maximum total concurrency serverless endpoints (defaults to 10): Set to 100-200</li> </ul> <p></p>"},{"location":"admin/aws_service_limits/#parallel-featuregroup-creation","title":"Parallel FeatureGroup Creation","text":"<p>When running a large set of parallel Glue/Batch Jobs that are creating FeatureGroups, some clients have hit this limit. </p> <p>\"ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateFeatureGroup operation: The account-level service limit 'Maximum number of feature group creation workflows executing in parallel' is 4 FeatureGroups, with current utilization of 4 FeatureGroups and a request delta of 1 FeatureGroups. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.\"}</p> <p>Unfortunately this one is not adjustable through the AWS Service Quota console and you'll have to initiate an AWS Support ticket.</p>"},{"location":"admin/aws_service_limits/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"admin/base_docker_push/","title":"Workbench Base Docker Build and Push","text":"<p>Notes and information on how to do the Docker Builds and Push to AWS ECR.</p>"},{"location":"admin/base_docker_push/#update-workbench-version","title":"Update Workbench Version","text":"<pre><code>vi Dockerfile\n\nARG WORKBENCH_VERSION=0.8.110 &lt;-- change this in TWO places\n</code></pre>"},{"location":"admin/base_docker_push/#run-the-deploy-script","title":"Run the Deploy Script","text":"<p>Note: For a client specific config file you'll need to copy it locally so that it's within Dockers 'build context'. If you're building the 'vanilla' open source Docker image, then you can use the <code>open_source_config.json</code> that's in the directory already.</p> <pre><code>./deploy_docker.sh\n</code></pre>"},{"location":"admin/base_docker_push/#update-the-stable-tag","title":"Update the 'stable' tag","text":"<p>This is obviously only when you want to mark a version as stable. Meaning that it seems to 'be good and stable (ish)' :)</p> <pre><code>./deploy.sh --stable\n</code></pre>"},{"location":"admin/base_docker_push/#test-the-ecr-image","title":"Test the ECR Image","text":"<p>You have a <code>docker_ecr_base</code> alias in your <code>~/.zshrc</code> :)</p>"},{"location":"admin/dashboard_docker_push/","title":"Dashboard Docker Build and Push","text":"<p>Notes and information on how to do the Dashboard Docker Builds and Push to AWS ECR.</p>"},{"location":"admin/dashboard_docker_push/#update-workbench-version","title":"Update Workbench Version","text":"<pre><code>cd applications/aws_dashboard\nvi Dockerfile\n...\n\n# Install workbench[ui] in separate layer (dependencies already satisfied)\nARG WORKBENCH_VERSION=0.8.227  &lt;-- change this\n</code></pre>"},{"location":"admin/dashboard_docker_push/#run-the-deploy-script","title":"Run the Deploy Script","text":"<p>Note: For a client specific config file you'll need to copy it locally so that it's within Dockers 'build context'. If you're building the 'vanilla' open source Docker image, then you can use the <code>open_source_config.json</code> that's in the directory already.</p> <pre><code>./deploy.sh 0_8_110   (match version to above)\n</code></pre> <p>Custom Plugins: If you're using custom plugins those are simply 'pushed' to S3, visit our Dashboard with Plugins) page.</p>"},{"location":"admin/dashboard_docker_push/#update-the-stable-tag","title":"Update the 'stable' tag","text":"<p>This is obviously only when you want to mark a version as stable. Meaning that it seems to 'be good and stable (ish)' :)</p> <pre><code>./deploy.sh 0_8_110 --stable\n</code></pre>"},{"location":"admin/dashboard_docker_push/#test-the-ecr-image","title":"Test the ECR Image","text":"<p>You have a <code>docker_ecr_dashboard</code> alias in your <code>~/.zshrc</code> :)</p>"},{"location":"admin/dashboard_s3_plugins/","title":"Deploying S3 Plugins with the Dashboard","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>Notes and information on how to include S3 based plugins with your Workbench Dashboard. </p> <p>Deploying your Dashboard plugins via an S3 bucket allows plugin developers to modify and improve plugins without a bunch of Docker builds, ECR, and CDK Deploy.</p>"},{"location":"admin/dashboard_s3_plugins/#check-your-dashboard","title":"Check your Dashboard","text":"<p>Make sure your Dashboard is configured to pull plugin pages, views, and components from an S3 bucket. Go to your main dashboard page and there's a 'secret link' when you click on the main title that brings up the Dashboard Status Page.</p> <p></p> <p>Dashboard Status Page Okay now check your Plugin Path: config and make sure it points to the S3 bucket you're expecting.</p> <p></p>"},{"location":"admin/dashboard_s3_plugins/#develop-your-plugins","title":"Develop your Plugins","text":"<p>During development it's good to both unit testing and local dashboard testing. Please see our main plugin docs for how to do local testing Plugins General.</p> <p>When you're ready to 'deploy' the plugins you can copy them up to the S3 bucket/prefix. You want to copy recursively so if you're plugin directory looks like the listing below you want to copy all the files/directories, so that the dashboard picks up everything.</p> <pre><code>- my_plugins\n   - pages\n      - page_1.py\n   - views\n      - view_1.py\n   - components\n       -component_1.py\n       -component_2.py\n</code></pre> <p>Careful with Plugin Copy</p> <p>In particular, pay attention to additional files in the directory structure. You do not want to copy _pycache_ and *.pyc files. So we recommend using this CLI.</p> <pre><code>cd my_plugins\naws s3 cp . s3://my_bucket/prefix --recursive --exclude \"*\" --include \"*.py\"\n</code></pre>"},{"location":"admin/dashboard_s3_plugins/#restart-the-ecs-service","title":"Restart the ECS Service","text":"<p>Okay, so this is a bit heavy handed, but automatically removing/adding/modifying the plugin pages, views, and components was 'amazingly complicated' and will be a feature request for a later date. :)</p> <p>Getting Cluster and Service Names</p> <p>You can go to the AWS Console, Elastic Container Service, find the cluster, click on that and find the service.</p> <p>The cluster will be something like:</p> <pre><code>WorkbenchDashboard-WorkbenchCluster123456\n</code></pre> <p>and the service will be something like:</p> <pre><code>WorkbenchDashboard-WorkbenchService789123\n</code></pre> <p>Anyway, find those two things and run this command below (Note: You probably need admin permisions)</p> <pre><code>aws ecs update-service --cluster your-cluster-name \\\n--service your-service-name --force-new-deployment\n</code></pre> <p>Important: Even though this command will finish immediately, the ECS service will slowly flip over to the new instance (like 5-10 minutes), so wait a bit before testing the changes.</p>"},{"location":"admin/dashboard_s3_plugins/#verify-new-plugin-changes","title":"Verify new Plugin changes","text":"<p>Okay now that the ECS service has restarted (which can take a bit) you can now go to the Dashboard and test/verify that the changes you made now show up on the Dashboard.</p>"},{"location":"admin/dashboard_s3_plugins/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"admin/explorer_docker_push/","title":"Explorer Docker Build and Push","text":"<p>Notes and information on how to do the Dashboard Docker Builds and Push to AWS ECR.</p>"},{"location":"admin/explorer_docker_push/#update-workbench-version","title":"Update Workbench Version","text":"<pre><code>cd applications/compound_explorer\nvi Dockerfile\n\n# Install Workbench (changes often)\nRUN pip install --no-cache-dir workbench==0.8.90 &lt;-- change this\n</code></pre>"},{"location":"admin/explorer_docker_push/#build-the-docker-image","title":"Build the Docker Image","text":"<p>Note: For a client specific config file you'll need to copy it locally so that it's within Dockers 'build context'. If you're building the 'vanilla' open source Docker image, then you can use the <code>open_source_config.json</code> that's in the directory already.</p> <pre><code>docker build --build-arg WORKBENCH_CONFIG=open_source_config.json -t \\\ncompound_explorer:v0_8_90_amd64 --platform linux/amd64 .\n</code></pre>"},{"location":"admin/explorer_docker_push/#test-the-image-locally","title":"Test the Image Locally","text":"<p>You have a <code>docker_local_dashboard</code> alias in your <code>~/.zshrc</code> :)</p>"},{"location":"admin/explorer_docker_push/#login-to-ecr","title":"Login to ECR","text":"<pre><code>aws ecr-public get-login-password --region us-east-1 --profile \\\nscp_sandbox_admin | docker login --username AWS \\\n--password-stdin public.ecr.aws\n</code></pre>"},{"location":"admin/explorer_docker_push/#tagpush-the-image-to-aws-ecr","title":"Tag/Push the Image to AWS ECR","text":"<p><pre><code>docker tag compound_explorer:v0_8_90_amd64 \\\npublic.ecr.aws/m6i5k1r2/compound_explorer:v0_8_90_amd64\n</code></pre> <pre><code>docker push public.ecr.aws/m6i5k1r2/compound_explorer:v0_8_90_amd64\n</code></pre></p>"},{"location":"admin/explorer_docker_push/#update-the-latest-tag","title":"Update the 'latest' tag","text":"<p><pre><code>docker tag public.ecr.aws/m6i5k1r2/compound_explorer:v0_8_90_amd64 \\\npublic.ecr.aws/m6i5k1r2/compound_explorer:latest\n</code></pre> <pre><code>docker push public.ecr.aws/m6i5k1r2/compound_explorer:latest\n</code></pre></p>"},{"location":"admin/explorer_docker_push/#update-the-stable-tag","title":"Update the 'stable' tag","text":"<p>This is obviously only when you want to mark a version as stable. Meaning that it seems to 'be good and stable (ish)' :)</p> <p><pre><code>docker tag public.ecr.aws/m6i5k1r2/compound_explorer:v0_8_90_amd64 \\\npublic.ecr.aws/m6i5k1r2/compound_explorer:stable\n</code></pre> <pre><code>docker push public.ecr.aws/m6i5k1r2/workbench_dashboard:stable\n</code></pre></p>"},{"location":"admin/explorer_docker_push/#test-the-ecr-image","title":"Test the ECR Image","text":"<p>You have a <code>docker_ecr_dashboard</code> alias in your <code>~/.zshrc</code> :)</p>"},{"location":"admin/pypi_release/","title":"PyPI Release Notes","text":"<p>Notes and information on how to do the PyPI release for the SageMaker project. For full details on packaging you can reference this page Packaging</p> <p>The following instructions should work, but things change :)</p>"},{"location":"admin/pypi_release/#package-requirements","title":"Package Requirements","text":"<ul> <li>pip install tox</li> <li>pip install --upgrade wheel build twine</li> </ul>"},{"location":"admin/pypi_release/#setup-pypirc","title":"Setup pypirc","text":"<p>The easiest thing to do is setup a \\~/.pypirc file with the following contents</p> <pre><code>[distutils]\nindex-servers =\n  pypi\n  testpypi\n\n[pypi]\nusername = __token__\npassword = pypi-AgEIcH...\n\n[testpypi]\nusername = __token__\npassword = pypi-AgENdG...\n</code></pre>"},{"location":"admin/pypi_release/#tox-background","title":"Tox Background","text":"<p>Tox will install the SageMaker Sandbox package into a blank virtualenv and then execute all the tests against the newly installed package. So if everything goes okay, you know the pypi package installed fine and the tests (which pulls from the installed <code>workbench</code> package) also ran okay.</p>"},{"location":"admin/pypi_release/#make-sure-all-tests-pass","title":"Make sure ALL tests pass","text":"<pre><code>$ cd workbench\n$ tox \n</code></pre> <p>If ALL the test above pass...</p>"},{"location":"admin/pypi_release/#clean-any-previous-distribution-files","title":"Clean any previous distribution files","text":"<pre><code>make clean\n</code></pre>"},{"location":"admin/pypi_release/#tag-the-new-version","title":"Tag the New Version","text":"<pre><code>git tag v0.1.8 (or whatever)\ngit push --tags\n</code></pre>"},{"location":"admin/pypi_release/#create-the-test-pypi-release","title":"Create the TEST PyPI Release","text":"<pre><code>python -m build\ntwine upload dist/* -r testpypi\n</code></pre>"},{"location":"admin/pypi_release/#install-the-test-pypi-release","title":"Install the TEST PyPI Release","text":"<pre><code>pip install --index-url https://test.pypi.org/simple workbench\n</code></pre>"},{"location":"admin/pypi_release/#create-the-real-pypi-release","title":"Create the REAL PyPI Release","text":"<pre><code>twine upload dist/* -r pypi\n</code></pre>"},{"location":"admin/pypi_release/#push-any-possible-changes-to-github","title":"Push any possible changes to Github","text":"<pre><code>git push\n</code></pre>"},{"location":"admin/workbench_docker_for_lambdas/","title":"Workbench Docker Image for Lambdas","text":"<p>Using the Workbench Docker Image for AWS Lambda Jobs allows your Lambda Jobs to use and create AWS ML Pipeline Artifacts with Workbench.</p> <p>AWS, for some reason, does not allow Public ECRs to be used for Lambda Docker images. So you'll have to copy the Docker image into your private ECR. </p>"},{"location":"admin/workbench_docker_for_lambdas/#creating-a-private-ecr","title":"Creating a Private ECR","text":"<p>You only need to do this if you don't already have a private ECR.</p>"},{"location":"admin/workbench_docker_for_lambdas/#aws-console-to-create-private-ecr","title":"AWS Console to create Private ECR","text":"<ol> <li>Open the Amazon ECR console.</li> <li>Choose \"Create repository\".</li> <li>For \"Repository name\", enter <code>workbench_base</code>.</li> <li>Ensure \"Private\" is selected.</li> <li>Choose \"Create repository\".</li> </ol>"},{"location":"admin/workbench_docker_for_lambdas/#command-line-to-create-private-ecr","title":"Command Line to create Private ECR","text":"<p>Create the ECR repository using the AWS CLI:</p> <pre><code>aws ecr create-repository --repository-name workbench_base --region &lt;region&gt;\n</code></pre>"},{"location":"admin/workbench_docker_for_lambdas/#pulling-docker-image-into-private-ecr","title":"Pulling Docker Image into Private ECR","text":"<p>Note: You'll only need to do this when you want to update the Workbench Docker image</p> <p>Pull the Workbench Public ECR Image</p> <pre><code>docker pull public.ecr.aws/m6i5k1r2/workbench_base:latest\n</code></pre> <p>Tag the image for your private ECR</p> <pre><code>docker tag public.ecr.aws/m6i5k1r2/workbench_base:latest \\\n&lt;your-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;private-repo&gt;:latest\n</code></pre> <p>Push the image to your private ECR</p> <pre><code>aws ecr get-login-password --region &lt;region&gt; --profile &lt;profile&gt; | \\\ndocker login --username AWS --password-stdin &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n\ndocker push &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;private-repo&gt;:&lt;tag&gt;\n</code></pre>"},{"location":"admin/workbench_docker_for_lambdas/#using-the-docker-image-for-your-lambdas","title":"Using the Docker Image for your Lambdas","text":"<p>Okay, now that you have the Workbench Docker image in your private ECR, here's how you use that image for your Lambda jobs.</p>"},{"location":"admin/workbench_docker_for_lambdas/#aws-console","title":"AWS Console","text":"<ol> <li>Open the AWS Lambda console.</li> <li>Create a new function.</li> <li>Select \"Container image\".</li> <li>Use the ECR image URI: <code>&lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;private-repo&gt;:&lt;tag&gt;</code>.</li> </ol>"},{"location":"admin/workbench_docker_for_lambdas/#command-line","title":"Command Line","text":"<p>Create the Lambda function using the AWS CLI:</p> <pre><code>aws lambda create-function \\\n --region &lt;region&gt; \\\n --function-name myLambdaFunction \\\n --package-type Image \\\n --code ImageUri=&lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;private-repo&gt;:&lt;tag&gt; \\\n --role arn:aws:iam::&lt;account-id&gt;:role/&lt;execution-role&gt;\n</code></pre>"},{"location":"admin/workbench_docker_for_lambdas/#python-cdk","title":"Python CDK","text":"<p>Define the Lambda function in your CDK app:</p> <pre><code>from aws_cdk import (\n   aws_lambda as _lambda,\n   core\n)\n\nclass MyLambdaStack(core.Stack):\n   def __init__(self, scope: core.Construct, id: str, **kwargs) -&gt; None:\n       super().__init__(scope, id, **kwargs)\n\n       _lambda.Function(self, \"MyLambdaFunction\",\n                        code=_lambda.Code.from_ecr_image(\"&lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;private-repo&gt;:&lt;tag&gt;\"),\n                        handler=_lambda.Handler.FROM_IMAGE,\n                        runtime=_lambda.Runtime.FROM_IMAGE,\n                        role=iam.Role.from_role_arn(self, \"LambdaRole\", \"arn:aws:iam::&lt;account-id&gt;:role/&lt;execution-role&gt;\"))\n\napp = core.App()\nMyLambdaStack(app, \"MyLambdaStack\")\napp.synth()\n</code></pre>"},{"location":"admin/workbench_docker_for_lambdas/#cloudformation","title":"Cloudformation","text":"<p>Define the Lambda function in your CloudFormation template.</p> <pre><code>Resources:\n MyLambdaFunction:\n   Type: AWS::Lambda::Function\n   Properties:\n     Code:\n       ImageUri: &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;private-repo&gt;:&lt;tag&gt;\n     Role: arn:aws:iam::&lt;account-id&gt;:role/&lt;execution-role&gt;\n     PackageType: Image\n</code></pre>"},{"location":"advanced_models/","title":"Advanced Models","text":"<p>OpenADMET Challenge</p> <p>ChemProp was used by many of the top performers on the OpenADMET Leaderboard. Workbench makes it easy to train and deploy ChemProp models to AWS\u00ae.</p> <p>Workbench supports advanced model frameworks including PyTorch neural networks and ChemProp message passing neural networks (MPNNs). These models can be trained and deployed to AWS\u00ae with the same simple API as other Workbench models.</p>"},{"location":"advanced_models/#available-model-frameworks","title":"Available Model Frameworks","text":"Model Framework Description XGBoost Gradient boosted trees on RDKit molecular descriptors PyTorch Neural network on RDKit molecular descriptors ChemProp Message Passing Neural Network (MPNN) on molecular graphs ChemProp Hybrid MPNN combined with top RDKit descriptors ChemProp Multi-Task Single MPNN predicting multiple endpoints simultaneously"},{"location":"advanced_models/#why-chemprop","title":"Why ChemProp?","text":"<p>Traditional models treat molecules as a list of computed descriptors. ChemProp takes a different approach\u2014it operates directly on the molecular graph structure, using atoms as nodes and bonds as edges. This allows the model to learn representations from the molecular topology itself.</p> <p>In the OpenADMET Challenge, ChemProp models consistently performed well across the ADMET endpoints given for the contest:</p> <ul> <li>LogD (Lipophilicity)</li> <li>KSOL (Kinetic Solubility)</li> <li>HLM/MLM CLint (Liver Clearance)</li> <li>Caco-2 Permeability &amp; Efflux</li> <li>Plasma &amp; Brain Protein Binding</li> </ul>"},{"location":"advanced_models/#deploying-a-chemprop-model","title":"Deploying a ChemProp Model","text":"<p>Creating and deploying a ChemProp model follows the standard Workbench pattern:</p> <pre><code>from workbench.api import DataSource, FeatureSet, ModelType, ModelFramework\n\n# Create a DataSource and FeatureSet\nds = DataSource(\"my_molecules.csv\", name=\"admet_data\")\nfs = ds.to_features(\"admet_features\", id_column=\"mol_id\")\n\n# Create a ChemProp model\nmodel = FeatureSet(\"admet_features\").to_model(\n    name=\"my-chemprop-model\",\n    model_type=ModelType.REGRESSOR,\n    model_framework=ModelFramework.CHEMPROP,\n    target_column=\"logd\",\n    feature_list=[\"smiles\"],\n    description=\"ChemProp model for LogD prediction\",\n)\n\n# Deploy to an AWS Endpoint\nendpoint = model.to_endpoint()\n</code></pre>"},{"location":"advanced_models/#multi-task-models","title":"Multi-Task Models","text":"<p>ChemProp supports multi-task learning, where a single model predicts multiple endpoints simultaneously. This can improve performance when endpoints are related and share underlying molecular features.</p> <pre><code># Define multiple target columns for multi-task learning\nADMET_TARGETS = [\n    'logd', 'ksol', 'hlm_clint', 'mlm_clint',\n    'caco_2_papp_a_b', 'caco_2_efflux',\n    'mppb', 'mbpb', 'mgmb'\n]\n\nmodel = feature_set.to_model(\n    name=\"admet-multi-task\",\n    model_type=ModelType.REGRESSOR,\n    model_framework=ModelFramework.CHEMPROP,\n    target_column=ADMET_TARGETS,  # List enables multi-task\n    feature_list=[\"smiles\"],\n    tags=[\"chemprop\", \"multitask\"],\n)\n</code></pre>"},{"location":"advanced_models/#confidence-estimates","title":"Confidence Estimates","text":"<p>All Workbench models include built-in uncertainty quantification. This provides confidence estimates alongside predictions, which is valuable for drug discovery workflows:</p> <ul> <li>High confidence: Predictions can be trusted for decision-making</li> <li>Low confidence: The molecule may be outside the training domain; consider gathering additional data</li> </ul> Confidence Models: All our models provide confidence metrics to identify predictions where the model is unsure or needs more data."},{"location":"advanced_models/#pytorch-models","title":"PyTorch Models","text":"<p>For some assays, PyTorch models on RDKit descriptors can outperform ChemProp. These models train faster and work well when molecular descriptors capture the relevant features:</p> <pre><code>model = feature_set.to_model(\n    name=\"my-pytorch-model\",\n    model_type=ModelType.REGRESSOR,\n    model_framework=ModelFramework.PYTORCH,\n    target_column=\"logd\",\n    feature_list=fs.feature_columns,\n)\n</code></pre> <p>Examples</p> <p>All Workbench model examples are in the repository under the <code>examples/models</code> directory. For full code listings, visit Workbench Model Examples.</p> <p>Beta Software</p> <p>Workbench is currently in beta. We're actively looking for beta testers! If you're interested in early access, contact us at workbench@supercowpowers.com.</p>"},{"location":"advanced_models/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS\u00ae and Workbench.</p> <ul> <li>Support: workbench@supercowpowers.com</li> <li>Discord: Join us on Discord</li> <li>Website: supercowpowers.com</li> </ul>"},{"location":"advanced_models/#references","title":"References","text":"<ul> <li>ChemProp: Yang et al. \"Analyzing Learned Molecular Representations for Property Prediction\" J. Chem. Inf. Model. 2019 \u2014 GitHub | Paper</li> <li>PyTorch: Paszke et al. \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\" NeurIPS 2019 \u2014 pytorch.org | Paper</li> <li>OpenADMET Challenge: Community benchmark for ADMET property prediction \u2014 Leaderboard | GitHub</li> </ul> <p>\u00ae Amazon Web Services, AWS, the Powered by AWS logo, are trademarks of Amazon.com, Inc. or its affiliates</p>"},{"location":"api_classes/data_source/","title":"DataSource","text":"<p>DataSource Examples</p> <p>Examples of using the DataSource class are in the Examples section at the bottom of this page. S3 data, local files, and Pandas dataframes, DataSource can read data from many different sources.</p> <p>DataSource: Manages AWS Data Catalog creation and management. DataSources are set up so that can easily be queried with AWS Athena. All DataSources are run through a full set of Exploratory Data Analysis (EDA) techniques (data quality, distributions, stats, outliers, etc.) DataSources can be viewed and explored within the Workbench Dashboard UI.</p>"},{"location":"api_classes/data_source/#workbench.api.data_source.DataSource","title":"<code>DataSource</code>","text":"<p>               Bases: <code>AthenaSource</code></p> <p>DataSource: Workbench DataSource API Class</p> Common Usage <pre><code>my_data = DataSource(name_of_source)\nmy_data.details()\nmy_features = my_data.to_features()\n</code></pre> Source code in <code>src/workbench/api/data_source.py</code> <pre><code>class DataSource(AthenaSource):\n    \"\"\"DataSource: Workbench DataSource API Class\n\n    Common Usage:\n        ```python\n        my_data = DataSource(name_of_source)\n        my_data.details()\n        my_features = my_data.to_features()\n        ```\n    \"\"\"\n\n    def __init__(self, source: Union[str, pd.DataFrame], name: str = None, tags: list = None, **kwargs):\n        \"\"\"\n        Initializes a new DataSource object.\n\n        Args:\n            source (Union[str, pd.DataFrame]): Source of data (existing name, filepath, S3 path, or a Pandas DataFrame)\n            name (str): The name of the data source (must be lowercase). If not specified, a name will be generated\n            tags (list[str]): A list of tags associated with the data source. If not specified tags will be generated.\n        \"\"\"\n\n        # Ensure the ds_name is valid\n        if name:\n            Artifact.is_name_valid(name)\n\n        # If the data source name wasn't given, generate it\n        else:\n            name = extract_data_source_basename(source)\n            name = Artifact.generate_valid_name(name)\n\n            # Sanity check for dataframe sources\n            if name == \"dataframe\":\n                msg = \"Set the 'name' argument in the constructor: DataSource(df, name='my_data')\"\n                self.log.critical(msg)\n                raise ValueError(msg)\n\n        # Set the tags and load the source\n        tags = [name] if tags is None else tags\n        self._load_source(source, name, tags)\n\n        # Call superclass init\n        super().__init__(name, **kwargs)\n\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"DataSource Details\n\n        Returns:\n            dict: A dictionary of details about the DataSource\n        \"\"\"\n        return super().details(**kwargs)\n\n    def query(self, query: str) -&gt; pd.DataFrame:\n        \"\"\"Query the AthenaSource\n\n        Args:\n            query (str): The query to run against the DataSource\n\n        Returns:\n            pd.DataFrame: The results of the query\n        \"\"\"\n        return super().query(query)\n\n    def pull_dataframe(self, limit: int = 50000, include_aws_columns=False) -&gt; pd.DataFrame:\n        \"\"\"Return a DataFrame of ALL the data from this DataSource\n\n        Args:\n            limit (int): Limit the number of rows returned (default: 50000)\n            include_aws_columns (bool): Include the AWS columns in the DataFrame (default: False)\n\n        Returns:\n            pd.DataFrame: A DataFrame of ALL the data from this DataSource\n\n        Note:\n            Obviously this is not recommended for large datasets :)\n        \"\"\"\n\n        # Get the table associated with the data\n        self.log.info(f\"Pulling data from {self.name}...\")\n        table = super().table\n        pull_query = f'SELECT * FROM \"{table}\" LIMIT {limit}'\n        df = self.query(pull_query)\n\n        # Drop any columns generated from AWS\n        if not include_aws_columns:\n            aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n            df = df.drop(columns=aws_cols, errors=\"ignore\")\n        return df\n\n    def to_features(\n        self,\n        name: str,\n        id_column: str = None,\n        tags: list = None,\n        event_time_column: str = None,\n        one_hot_columns: list = None,\n    ) -&gt; Union[FeatureSet, None]:\n        \"\"\"\n        Convert the DataSource to a FeatureSet\n\n        Args:\n            name (str): Set the name for feature set (must be lowercase).\n            id_column (str, optional): The ID column (if not specified, an 'auto_id' will be generated).\n            tags (list, optional): Set the tags for the feature set. If not specified tags will be generated.\n            event_time_column (str, optional): Set the event time for feature set. If not specified will be generated\n            one_hot_columns (list, optional): Set the columns to be one-hot encoded. (default: None)\n\n        Returns:\n            FeatureSet: The FeatureSet created from the DataSource (or None if the FeatureSet isn't created)\n        \"\"\"\n\n        # Ensure the feature_set_name is valid\n        if not Artifact.is_name_valid(name):\n            self.log.critical(f\"Invalid FeatureSet name: {name}, not creating FeatureSet!\")\n            return None\n\n        # Set the Tags\n        tags = [name] if tags is None else tags\n\n        # Transform the DataSource to a FeatureSet\n        data_to_features = DataToFeaturesLight(self.name, name)\n        data_to_features.set_output_tags(tags)\n        data_to_features.transform(\n            id_column=id_column,\n            event_time_column=event_time_column,\n            one_hot_columns=one_hot_columns,\n        )\n\n        # Return the FeatureSet (which will now be up-to-date)\n        return FeatureSet(name)\n\n    def _load_source(self, source: str, name: str, tags: list):\n        \"\"\"Load the source of the data\"\"\"\n        self.log.info(f\"Loading source: {source}...\")\n\n        # Pandas DataFrame Source\n        if isinstance(source, pd.DataFrame):\n            my_loader = PandasToData(name)\n            my_loader.set_input(source)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n\n        # S3 Source\n        source = source if isinstance(source, str) else str(source)\n        if source.startswith(\"s3://\"):\n            my_loader = S3ToDataSourceLight(source, name)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n\n        # File Source\n        elif os.path.isfile(source):\n            my_loader = CSVToDataSource(source, name)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n</code></pre>"},{"location":"api_classes/data_source/#workbench.api.data_source.DataSource.__init__","title":"<code>__init__(source, name=None, tags=None, **kwargs)</code>","text":"<p>Initializes a new DataSource object.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, DataFrame]</code> <p>Source of data (existing name, filepath, S3 path, or a Pandas DataFrame)</p> required <code>name</code> <code>str</code> <p>The name of the data source (must be lowercase). If not specified, a name will be generated</p> <code>None</code> <code>tags</code> <code>list[str]</code> <p>A list of tags associated with the data source. If not specified tags will be generated.</p> <code>None</code> Source code in <code>src/workbench/api/data_source.py</code> <pre><code>def __init__(self, source: Union[str, pd.DataFrame], name: str = None, tags: list = None, **kwargs):\n    \"\"\"\n    Initializes a new DataSource object.\n\n    Args:\n        source (Union[str, pd.DataFrame]): Source of data (existing name, filepath, S3 path, or a Pandas DataFrame)\n        name (str): The name of the data source (must be lowercase). If not specified, a name will be generated\n        tags (list[str]): A list of tags associated with the data source. If not specified tags will be generated.\n    \"\"\"\n\n    # Ensure the ds_name is valid\n    if name:\n        Artifact.is_name_valid(name)\n\n    # If the data source name wasn't given, generate it\n    else:\n        name = extract_data_source_basename(source)\n        name = Artifact.generate_valid_name(name)\n\n        # Sanity check for dataframe sources\n        if name == \"dataframe\":\n            msg = \"Set the 'name' argument in the constructor: DataSource(df, name='my_data')\"\n            self.log.critical(msg)\n            raise ValueError(msg)\n\n    # Set the tags and load the source\n    tags = [name] if tags is None else tags\n    self._load_source(source, name, tags)\n\n    # Call superclass init\n    super().__init__(name, **kwargs)\n</code></pre>"},{"location":"api_classes/data_source/#workbench.api.data_source.DataSource.details","title":"<code>details(**kwargs)</code>","text":"<p>DataSource Details</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the DataSource</p> Source code in <code>src/workbench/api/data_source.py</code> <pre><code>def details(self, **kwargs) -&gt; dict:\n    \"\"\"DataSource Details\n\n    Returns:\n        dict: A dictionary of details about the DataSource\n    \"\"\"\n    return super().details(**kwargs)\n</code></pre>"},{"location":"api_classes/data_source/#workbench.api.data_source.DataSource.pull_dataframe","title":"<code>pull_dataframe(limit=50000, include_aws_columns=False)</code>","text":"<p>Return a DataFrame of ALL the data from this DataSource</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Limit the number of rows returned (default: 50000)</p> <code>50000</code> <code>include_aws_columns</code> <code>bool</code> <p>Include the AWS columns in the DataFrame (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame of ALL the data from this DataSource</p> Note <p>Obviously this is not recommended for large datasets :)</p> Source code in <code>src/workbench/api/data_source.py</code> <pre><code>def pull_dataframe(self, limit: int = 50000, include_aws_columns=False) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame of ALL the data from this DataSource\n\n    Args:\n        limit (int): Limit the number of rows returned (default: 50000)\n        include_aws_columns (bool): Include the AWS columns in the DataFrame (default: False)\n\n    Returns:\n        pd.DataFrame: A DataFrame of ALL the data from this DataSource\n\n    Note:\n        Obviously this is not recommended for large datasets :)\n    \"\"\"\n\n    # Get the table associated with the data\n    self.log.info(f\"Pulling data from {self.name}...\")\n    table = super().table\n    pull_query = f'SELECT * FROM \"{table}\" LIMIT {limit}'\n    df = self.query(pull_query)\n\n    # Drop any columns generated from AWS\n    if not include_aws_columns:\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n        df = df.drop(columns=aws_cols, errors=\"ignore\")\n    return df\n</code></pre>"},{"location":"api_classes/data_source/#workbench.api.data_source.DataSource.query","title":"<code>query(query)</code>","text":"<p>Query the AthenaSource</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to run against the DataSource</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The results of the query</p> Source code in <code>src/workbench/api/data_source.py</code> <pre><code>def query(self, query: str) -&gt; pd.DataFrame:\n    \"\"\"Query the AthenaSource\n\n    Args:\n        query (str): The query to run against the DataSource\n\n    Returns:\n        pd.DataFrame: The results of the query\n    \"\"\"\n    return super().query(query)\n</code></pre>"},{"location":"api_classes/data_source/#workbench.api.data_source.DataSource.to_features","title":"<code>to_features(name, id_column=None, tags=None, event_time_column=None, one_hot_columns=None)</code>","text":"<p>Convert the DataSource to a FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Set the name for feature set (must be lowercase).</p> required <code>id_column</code> <code>str</code> <p>The ID column (if not specified, an 'auto_id' will be generated).</p> <code>None</code> <code>tags</code> <code>list</code> <p>Set the tags for the feature set. If not specified tags will be generated.</p> <code>None</code> <code>event_time_column</code> <code>str</code> <p>Set the event time for feature set. If not specified will be generated</p> <code>None</code> <code>one_hot_columns</code> <code>list</code> <p>Set the columns to be one-hot encoded. (default: None)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FeatureSet</code> <code>Union[FeatureSet, None]</code> <p>The FeatureSet created from the DataSource (or None if the FeatureSet isn't created)</p> Source code in <code>src/workbench/api/data_source.py</code> <pre><code>def to_features(\n    self,\n    name: str,\n    id_column: str = None,\n    tags: list = None,\n    event_time_column: str = None,\n    one_hot_columns: list = None,\n) -&gt; Union[FeatureSet, None]:\n    \"\"\"\n    Convert the DataSource to a FeatureSet\n\n    Args:\n        name (str): Set the name for feature set (must be lowercase).\n        id_column (str, optional): The ID column (if not specified, an 'auto_id' will be generated).\n        tags (list, optional): Set the tags for the feature set. If not specified tags will be generated.\n        event_time_column (str, optional): Set the event time for feature set. If not specified will be generated\n        one_hot_columns (list, optional): Set the columns to be one-hot encoded. (default: None)\n\n    Returns:\n        FeatureSet: The FeatureSet created from the DataSource (or None if the FeatureSet isn't created)\n    \"\"\"\n\n    # Ensure the feature_set_name is valid\n    if not Artifact.is_name_valid(name):\n        self.log.critical(f\"Invalid FeatureSet name: {name}, not creating FeatureSet!\")\n        return None\n\n    # Set the Tags\n    tags = [name] if tags is None else tags\n\n    # Transform the DataSource to a FeatureSet\n    data_to_features = DataToFeaturesLight(self.name, name)\n    data_to_features.set_output_tags(tags)\n    data_to_features.transform(\n        id_column=id_column,\n        event_time_column=event_time_column,\n        one_hot_columns=one_hot_columns,\n    )\n\n    # Return the FeatureSet (which will now be up-to-date)\n    return FeatureSet(name)\n</code></pre>"},{"location":"api_classes/data_source/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Create a DataSource from an S3 Path or File Path</p> datasource_from_s3.py<pre><code>from workbench.api.data_source import DataSource\n\n# Create a DataSource from an S3 Path (or a local file)\nsource_path = \"s3://workbench-public-data/common/abalone.csv\"\n# source_path = \"/full/path/to/local/file.csv\"\n\nmy_data = DataSource(source_path)\nprint(my_data.details())\n</code></pre> <p>Create a DataSource from a Pandas Dataframe</p> datasource_from_df.py<pre><code>from workbench.utils.test_data_generator import TestDataGenerator\nfrom workbench.api.data_source import DataSource\n\n# Create a DataSource from a Pandas DataFrame\ngen_data = TestDataGenerator()\ndf = gen_data.person_data()\n\ntest_data = DataSource(df, name=\"test_data\")\nprint(test_data.details())\n</code></pre> <p>Query a DataSource</p> <p>All Workbench DataSources use AWS Athena, so any query that you can make with Athena is accessible through the DataSource API.</p> datasource_query.py<pre><code>from workbench.api.data_source import DataSource\n\n# Grab a DataSource\nmy_data = DataSource(\"abalone_data\")\n\n# Make some queries using the Athena backend\ndf = my_data.query(\"select * from abalone_data where height &gt; .3\")\nprint(df.head())\n\ndf = my_data.query(\"select * from abalone_data where class_number_of_rings &lt; 3\")\nprint(df.head())\n</code></pre> <p>Output</p> <pre><code>  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  shell_weight  class_number_of_rings\n0   M   0.705     0.565   0.515         2.210          1.1075          0.4865        0.5120                     10\n1   F   0.455     0.355   1.130         0.594          0.3320          0.1160        0.1335                      8\n\n  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  shell_weight  class_number_of_rings\n0   I   0.075     0.055   0.010         0.002          0.0010          0.0005        0.0015                      1\n1   I   0.150     0.100   0.025         0.015          0.0045          0.0040        0.0050                      2\n</code></pre> <p>Create a FeatureSet from a DataSource</p> datasource_to_featureset.py<pre><code>from workbench.api.data_source import DataSource\n\n# Convert the Data Source to a Feature Set\ntest_data = DataSource('test_data')\nmy_features = test_data.to_features(\"test_features\")\nprint(my_features.details())\n</code></pre>"},{"location":"api_classes/data_source/#workbench-ui","title":"Workbench UI","text":"<p>Whenever a DataSource is created Workbench performs a comprehensive set of Exploratory Data Analysis techniques on your data, pushes the results into AWS, and provides a detailed web visualization of the results.</p> Workbench Dashboard: DataSources <p>Not Finding a particular method?</p> <p>The Workbench API Classes use the 'Core' Classes Internally, so for an extensive listing of all the methods available please take a deep dive into: Workbench Core Classes</p>"},{"location":"api_classes/df_store/","title":"Workbench DataFrame Storage","text":""},{"location":"api_classes/df_store/#why-dataframe-storage","title":"Why DataFrame Storage?","text":"<p>Great question, there's a couple of reasons. The first is that the Parameter Store in AWS has a 4KB limit, so that won't support any kind of 'real data'. The second reason is that DataFrames are commonly used as part of the data engineering, science, and ML pipeline construction process. Providing storage of named DataFrames in an accessible location that can be inspected and used by your ML Team comes in super handy.</p>"},{"location":"api_classes/df_store/#efficient-storage","title":"Efficient Storage","text":"<p>All DataFrames are stored in the Parquet format using 'snappy' storage. Parquet is a columnar storage format that efficiently handles large datasets, and using Snappy compression reduces file size while maintaining fast read/write speeds.</p>"},{"location":"api_classes/df_store/#examples","title":"Examples","text":"<p>These example show how to use the <code>DFStore()</code> class to list, add, and get dataframes from AWS Storage.</p> <p>Workbench REPL</p> <p>If you'd like to experiment with listing, adding, and getting dataframe with the <code>DFStore()</code> class, you can spin up the Workbench REPL, use the class and test out all the methods. Try it out! Workbench REPL</p> Using DataFrame Store<pre><code>from workbench.api mport DFStore\ndf_store = DFStore()\n\n# List DataFrames\ndf_store().list()\n\nOut[1]:\nml/confustion_matrix  (0.002MB/2024-09-23 16:44:48)\nml/hold_out_ids  (0.094MB/2024-09-23 16:57:01)\nml/my_awesome_df  (0.002MB/2024-09-23 16:43:30)\nml/shap_values  (0.019MB/2024-09-23 16:57:21)\n\n# Add a DataFrame\ndf = pd.DataFrame({\"A\": [1]*1000, \"B\": [3]*1000})\ndf_store.upsert(\"test/test_df\", df)\n\n# List DataFrames (we can just use the REPR)\ndf_store\n\nOut[2]:\nml/confustion_matrix  (0.002MB/2024-09-23 16:44:48)\nml/hold_out_ids  (0.094MB/2024-09-23 16:57:01)\nml/my_awesome_df  (0.002MB/2024-09-23 16:43:30)\nml/shap_values  (0.019MB/2024-09-23 16:57:21)\ntest/test_df  (0.002MB/2024-09-23 16:59:27)\n\n# Retrieve dataframes\nreturn_df = df_store.get(\"test/test_df\")\nreturn_df.head()\n\nOut[3]:\n   A  B\n0  1  3\n1  1  3\n2  1  3\n3  1  3\n4  1  3\n\n# Delete dataframes\ndf_store.delete(\"test/test_df\")\n</code></pre> <p>Compressed Storage is Automatic</p> <p>All DataFrames are stored in the Parquet format using 'snappy' storage. Parquet is a columnar storage format that efficiently handles large datasets, and using Snappy compression reduces file size while maintaining fast read/write speeds.</p> <p>DFStore: Fast/efficient storage of DataFrames using AWS S3/Parquet/Snappy</p>"},{"location":"api_classes/df_store/#workbench.api.df_store.DFStore","title":"<code>DFStore</code>","text":"<p>               Bases: <code>DFStoreCore</code></p> <p>DFStore: Fast/efficient storage of DataFrames using AWS S3/Parquet/Snappy</p> <pre><code>Common Usage:\n</code></pre> <p><code>python         df_store = DFStore()          # List Data         df_store.list()          # Add DataFrame         df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})         df_store.upsert(\"/test/my_data\", df)          # Retrieve DataFrame         df = df_store.get(\"/test/my_data\")         print(df)          # Delete Data         df_store.delete(\"/test/my_data\")</code></p> Source code in <code>src/workbench/api/df_store.py</code> <pre><code>class DFStore(DFStoreCore):\n    \"\"\"DFStore: Fast/efficient storage of DataFrames using AWS S3/Parquet/Snappy\n\n        Common Usage:\n    ```python\n            df_store = DFStore()\n\n            # List Data\n            df_store.list()\n\n            # Add DataFrame\n            df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n            df_store.upsert(\"/test/my_data\", df)\n\n            # Retrieve DataFrame\n            df = df_store.get(\"/test/my_data\")\n            print(df)\n\n            # Delete Data\n            df_store.delete(\"/test/my_data\")\n    ```\n    \"\"\"\n\n    def __init__(self, path_prefix: Union[str, None] = None):\n        \"\"\"DFStore Init Method\n\n        Args:\n            path_prefix (Union[str, None], optional): Add a path prefix to storage locations (Defaults to None)\n        \"\"\"\n        super().__init__(path_prefix=path_prefix)\n</code></pre>"},{"location":"api_classes/df_store/#workbench.api.df_store.DFStore.__init__","title":"<code>__init__(path_prefix=None)</code>","text":"<p>DFStore Init Method</p> <p>Parameters:</p> Name Type Description Default <code>path_prefix</code> <code>Union[str, None]</code> <p>Add a path prefix to storage locations (Defaults to None)</p> <code>None</code> Source code in <code>src/workbench/api/df_store.py</code> <pre><code>def __init__(self, path_prefix: Union[str, None] = None):\n    \"\"\"DFStore Init Method\n\n    Args:\n        path_prefix (Union[str, None], optional): Add a path prefix to storage locations (Defaults to None)\n    \"\"\"\n    super().__init__(path_prefix=path_prefix)\n</code></pre>"},{"location":"api_classes/endpoint/","title":"Endpoint","text":"<p>Endpoint Examples</p> <p>Examples of using the Endpoint class are listed at the bottom of this page Examples.</p> <p>Endpoint: Manages AWS Endpoint creation and deployment. Endpoints are automatically set up and provisioned for deployment into AWS. Endpoints can be viewed in the AWS Sagemaker interfaces or in the Workbench Dashboard UI, which provides additional model details and performance metrics</p>"},{"location":"api_classes/endpoint/#workbench.api.endpoint.Endpoint","title":"<code>Endpoint</code>","text":"<p>               Bases: <code>EndpointCore</code></p> <p>Endpoint: Workbench Endpoint API Class</p> Common Usage <pre><code>my_endpoint = Endpoint(name)\nmy_endpoint.details()\nmy_endpoint.inference(eval_df)\n</code></pre> Source code in <code>src/workbench/api/endpoint.py</code> <pre><code>class Endpoint(EndpointCore):\n    \"\"\"Endpoint: Workbench Endpoint API Class\n\n    Common Usage:\n        ```python\n        my_endpoint = Endpoint(name)\n        my_endpoint.details()\n        my_endpoint.inference(eval_df)\n        ```\n    \"\"\"\n\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"Endpoint Details\n\n        Returns:\n            dict: A dictionary of details about the Endpoint\n        \"\"\"\n        return super().details(**kwargs)\n\n    def inference(\n        self,\n        eval_df: pd.DataFrame,\n        capture_name: str = None,\n        id_column: str = None,\n        drop_error_rows: bool = False,\n        include_quantiles: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Run inference on the Endpoint using the provided DataFrame\n\n        Args:\n            eval_df (pd.DataFrame): The DataFrame to run predictions on\n            capture_name (str, optional): The Name of the capture to use (default: None)\n            id_column (str, optional): The name of the column to use as the ID (default: None)\n            drop_error_rows (bool): Whether to drop rows with errors (default: False)\n            include_quantiles (bool): Include q_* quantile columns in saved output (default: False)\n\n        Returns:\n            pd.DataFrame: The DataFrame with predictions\n        \"\"\"\n        return super().inference(eval_df, capture_name, id_column, drop_error_rows, include_quantiles)\n\n    def auto_inference(self) -&gt; pd.DataFrame:\n        \"\"\"Run inference on the Endpoint using the test data from the model training view\n\n        Returns:\n            pd.DataFrame: The DataFrame with predictions\n        \"\"\"\n        return super().auto_inference()\n\n    def full_inference(self) -&gt; pd.DataFrame:\n        \"\"\"Run inference on the Endpoint using the full data from the model training view\n\n        Returns:\n            pd.DataFrame: The DataFrame with predictions\n        \"\"\"\n        return super().full_inference()\n\n    def fast_inference(self, eval_df: pd.DataFrame, threads: int = 4) -&gt; pd.DataFrame:\n        \"\"\"Run inference on the Endpoint using the provided DataFrame\n\n        Args:\n            eval_df (pd.DataFrame): The DataFrame to run predictions on\n            threads (int): The number of threads to use (default: 4)\n\n        Returns:\n            pd.DataFrame: The DataFrame with predictions\n\n        Note:\n            There's no sanity checks or error handling... just FAST Inference!\n        \"\"\"\n        return super().fast_inference(eval_df, threads=threads)\n\n    def cross_fold_inference(self, include_quantiles: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Pull cross-fold inference from model associated with this Endpoint\n\n        Args:\n            include_quantiles (bool): Include q_* quantile columns in saved output (default: False)\n\n        Returns:\n            pd.DataFrame: A DataFrame with cross fold predictions\n        \"\"\"\n        return super().cross_fold_inference(include_quantiles)\n</code></pre>"},{"location":"api_classes/endpoint/#workbench.api.endpoint.Endpoint.auto_inference","title":"<code>auto_inference()</code>","text":"<p>Run inference on the Endpoint using the test data from the model training view</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with predictions</p> Source code in <code>src/workbench/api/endpoint.py</code> <pre><code>def auto_inference(self) -&gt; pd.DataFrame:\n    \"\"\"Run inference on the Endpoint using the test data from the model training view\n\n    Returns:\n        pd.DataFrame: The DataFrame with predictions\n    \"\"\"\n    return super().auto_inference()\n</code></pre>"},{"location":"api_classes/endpoint/#workbench.api.endpoint.Endpoint.cross_fold_inference","title":"<code>cross_fold_inference(include_quantiles=False)</code>","text":"<p>Pull cross-fold inference from model associated with this Endpoint</p> <p>Parameters:</p> Name Type Description Default <code>include_quantiles</code> <code>bool</code> <p>Include q_* quantile columns in saved output (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with cross fold predictions</p> Source code in <code>src/workbench/api/endpoint.py</code> <pre><code>def cross_fold_inference(self, include_quantiles: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Pull cross-fold inference from model associated with this Endpoint\n\n    Args:\n        include_quantiles (bool): Include q_* quantile columns in saved output (default: False)\n\n    Returns:\n        pd.DataFrame: A DataFrame with cross fold predictions\n    \"\"\"\n    return super().cross_fold_inference(include_quantiles)\n</code></pre>"},{"location":"api_classes/endpoint/#workbench.api.endpoint.Endpoint.details","title":"<code>details(**kwargs)</code>","text":"<p>Endpoint Details</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the Endpoint</p> Source code in <code>src/workbench/api/endpoint.py</code> <pre><code>def details(self, **kwargs) -&gt; dict:\n    \"\"\"Endpoint Details\n\n    Returns:\n        dict: A dictionary of details about the Endpoint\n    \"\"\"\n    return super().details(**kwargs)\n</code></pre>"},{"location":"api_classes/endpoint/#workbench.api.endpoint.Endpoint.fast_inference","title":"<code>fast_inference(eval_df, threads=4)</code>","text":"<p>Run inference on the Endpoint using the provided DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>eval_df</code> <code>DataFrame</code> <p>The DataFrame to run predictions on</p> required <code>threads</code> <code>int</code> <p>The number of threads to use (default: 4)</p> <code>4</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with predictions</p> Note <p>There's no sanity checks or error handling... just FAST Inference!</p> Source code in <code>src/workbench/api/endpoint.py</code> <pre><code>def fast_inference(self, eval_df: pd.DataFrame, threads: int = 4) -&gt; pd.DataFrame:\n    \"\"\"Run inference on the Endpoint using the provided DataFrame\n\n    Args:\n        eval_df (pd.DataFrame): The DataFrame to run predictions on\n        threads (int): The number of threads to use (default: 4)\n\n    Returns:\n        pd.DataFrame: The DataFrame with predictions\n\n    Note:\n        There's no sanity checks or error handling... just FAST Inference!\n    \"\"\"\n    return super().fast_inference(eval_df, threads=threads)\n</code></pre>"},{"location":"api_classes/endpoint/#workbench.api.endpoint.Endpoint.full_inference","title":"<code>full_inference()</code>","text":"<p>Run inference on the Endpoint using the full data from the model training view</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with predictions</p> Source code in <code>src/workbench/api/endpoint.py</code> <pre><code>def full_inference(self) -&gt; pd.DataFrame:\n    \"\"\"Run inference on the Endpoint using the full data from the model training view\n\n    Returns:\n        pd.DataFrame: The DataFrame with predictions\n    \"\"\"\n    return super().full_inference()\n</code></pre>"},{"location":"api_classes/endpoint/#workbench.api.endpoint.Endpoint.inference","title":"<code>inference(eval_df, capture_name=None, id_column=None, drop_error_rows=False, include_quantiles=False)</code>","text":"<p>Run inference on the Endpoint using the provided DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>eval_df</code> <code>DataFrame</code> <p>The DataFrame to run predictions on</p> required <code>capture_name</code> <code>str</code> <p>The Name of the capture to use (default: None)</p> <code>None</code> <code>id_column</code> <code>str</code> <p>The name of the column to use as the ID (default: None)</p> <code>None</code> <code>drop_error_rows</code> <code>bool</code> <p>Whether to drop rows with errors (default: False)</p> <code>False</code> <code>include_quantiles</code> <code>bool</code> <p>Include q_* quantile columns in saved output (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with predictions</p> Source code in <code>src/workbench/api/endpoint.py</code> <pre><code>def inference(\n    self,\n    eval_df: pd.DataFrame,\n    capture_name: str = None,\n    id_column: str = None,\n    drop_error_rows: bool = False,\n    include_quantiles: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Run inference on the Endpoint using the provided DataFrame\n\n    Args:\n        eval_df (pd.DataFrame): The DataFrame to run predictions on\n        capture_name (str, optional): The Name of the capture to use (default: None)\n        id_column (str, optional): The name of the column to use as the ID (default: None)\n        drop_error_rows (bool): Whether to drop rows with errors (default: False)\n        include_quantiles (bool): Include q_* quantile columns in saved output (default: False)\n\n    Returns:\n        pd.DataFrame: The DataFrame with predictions\n    \"\"\"\n    return super().inference(eval_df, capture_name, id_column, drop_error_rows, include_quantiles)\n</code></pre>"},{"location":"api_classes/endpoint/#examples","title":"Examples","text":"<p>Run Inference on an Endpoint</p> endpoint_inference.py<pre><code>from workbench.api import Endpoint\nfrom workbench.utils.endpoint_utils import get_evaluation_data\n\n# Grab an existing Endpoint\nendpoint = Endpoint(\"abalone-regression-end\")\n\n# Workbench has full ML Pipeline provenance, so we can backtrack the inputs,\n# get a DataFrame of data (not used for training) and run inference\ndf = get_evaluation_data(endpoint)\n\n# Run inference/predictions on the Endpoint\nresults_df = endpoint.inference(df)\n\n# Run inference/predictions and capture the results\nresults_df = endpoint.inference(df, capture=True)\n\n# Run inference/predictions using the FeatureSet evaluation data\nresults_df = endpoint.auto_inference()\n</code></pre> <p>Output</p> <p><pre><code>Processing...\n     class_number_of_rings  prediction\n0                       13   11.477922\n1                       12   12.316887\n2                        8    7.612847\n3                        8    9.663341\n4                        9    9.075263\n..                     ...         ...\n839                      8    8.069856\n840                     15   14.915502\n841                     11   10.977605\n842                     10   10.173433\n843                      7    7.297976\n</code></pre> Endpoint Details</p> <p>The details() method</p> <p>The <code>detail()</code> method on the Endpoint class provides a lot of useful information. All of the Workbench classes have a <code>details()</code> method try it out!</p> endpoint_details.py<pre><code>from workbench.api.endpoint import Endpoint\nfrom pprint import pprint\n\n# Get Endpoint and print out it's details\nendpoint = Endpoint(\"abalone-regression-end\")\npprint(endpoint.details())\n</code></pre> <p>Output</p> <pre><code>{\n 'input': 'abalone-regression',\n 'instance': 'Serverless (2GB/5)',\n 'model_metrics':   metric_name  value\n            0        RMSE  2.190\n            1         MAE  1.544\n            2          R2  0.504,\n 'model_name': 'abalone-regression',\n 'model_type': 'regressor',\n 'modified': datetime.datetime(2023, 12, 29, 17, 48, 35, 115000, tzinfo=datetime.timezone.utc),\n     class_number_of_rings  prediction\n0                        9    8.648378\n1                       11    9.717787\n2                       11   10.933070\n3                       10    9.899738\n4                        9   10.014504\n..                     ...         ...\n495                     10   10.261657\n496                      9   10.788254\n497                     13    7.779886\n498                     12   14.718514\n499                     13   10.637320\n 'workbench_tags': ['abalone', 'regression'],\n 'status': 'InService',\n 'name': 'abalone-regression-end',\n 'variant': 'AllTraffic'}\n</code></pre> <p>Endpoint Metrics</p> endpoint_metrics.py<pre><code>from workbench.api.endpoint import Endpoint\n\n# Grab an existing Endpoint\nendpoint = Endpoint(\"abalone-regression-end\")\n\n# Workbench tracks both Model performance and Endpoint Metrics\nmodel_metrics = endpoint.details()[\"model_metrics\"]\nendpoint_metrics = endpoint.endpoint_metrics()\nprint(model_metrics)\nprint(endpoint_metrics)\n</code></pre> <p>Output</p> <pre><code>  metric_name  value\n0        RMSE  2.190\n1         MAE  1.544\n2          R2  0.504\n\n    Invocations  ModelLatency  OverheadLatency  ModelSetupTime  Invocation5XXErrors\n29          0.0          0.00             0.00            0.00                  0.0\n30          1.0          1.11            23.73           23.34                  0.0\n31          0.0          0.00             0.00            0.00                  0.0\n48          0.0          0.00             0.00            0.00                  0.0\n49          5.0          0.45             9.64           23.57                  0.0\n50          2.0          0.57             0.08            0.00                  0.0\n51          0.0          0.00             0.00            0.00                  0.0\n60          4.0          0.33             5.80           22.65                  0.0\n61          1.0          1.11            23.35           23.10                  0.0\n62          0.0          0.00             0.00            0.00                  0.0\n...\n</code></pre>"},{"location":"api_classes/endpoint/#workbench-ui","title":"Workbench UI","text":"<p>Running these few lines of code creates and deploys an AWS Endpoint. The Endpoint artifacts can be viewed in the Sagemaker Console/Notebook interfaces or in the Workbench Dashboard UI. Workbench will monitor the endpoint, plot invocations, latencies, and tracks error metrics.</p> Workbench Dashboard: Endpoints <p>Not Finding a particular method?</p> <p>The Workbench API Classes use the 'Core' Classes Internally, so for an extensive listing of all the methods available please take a deep dive into: Workbench Core Classes</p>"},{"location":"api_classes/feature_set/","title":"FeatureSet","text":"<p>FeatureSet Examples</p> <p>Examples of using the FeatureSet Class are in the Examples section at the bottom of this page. AWS Feature Store and Feature Groups are quite complicated to set up manually but the Workbench FeatureSet makes it a breeze!</p> <p>FeatureSet: Manages AWS Feature Store/Group creation and management. FeatureSets are set up so they can easily be queried with AWS Athena. All FeatureSets are run through a full set of Exploratory Data Analysis (EDA) techniques (data quality, distributions, stats, outliers, etc.) FeatureSets can be viewed and explored within the Workbench Dashboard UI.</p>"},{"location":"api_classes/feature_set/#workbench.api.feature_set.FeatureSet","title":"<code>FeatureSet</code>","text":"<p>               Bases: <code>FeatureSetCore</code></p> <p>FeatureSet: Workbench FeatureSet API Class</p> Common Usage <pre><code>my_features = FeatureSet(name)\nmy_features.details()\nmy_features.to_model(\n    name=\"abalone-regression\",\n    model_type=ModelType.REGRESSOR,\n    target_column=\"class_number_of_rings\"\n    feature_list=[\"my\", \"best\", \"features\"])\n)\n</code></pre> Source code in <code>src/workbench/api/feature_set.py</code> <pre><code>class FeatureSet(FeatureSetCore):\n    \"\"\"FeatureSet: Workbench FeatureSet API Class\n\n    Common Usage:\n        ```python\n        my_features = FeatureSet(name)\n        my_features.details()\n        my_features.to_model(\n            name=\"abalone-regression\",\n            model_type=ModelType.REGRESSOR,\n            target_column=\"class_number_of_rings\"\n            feature_list=[\"my\", \"best\", \"features\"])\n        )\n        ```\n    \"\"\"\n\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"FeatureSet Details\n\n        Returns:\n            dict: A dictionary of details about the FeatureSet\n        \"\"\"\n        return super().details(**kwargs)\n\n    def query(self, query: str, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Query the AthenaSource\n\n        Args:\n            query (str): The query to run against the FeatureSet\n\n        Returns:\n            pd.DataFrame: The results of the query\n        \"\"\"\n        return super().query(query, **kwargs)\n\n    def pull_dataframe(self, limit: int = 50000, include_aws_columns=False) -&gt; pd.DataFrame:\n        \"\"\"Return a DataFrame of ALL the data from this FeatureSet\n\n        Args:\n            limit (int): Limit the number of rows returned (default: 50000)\n            include_aws_columns (bool): Include the AWS columns in the DataFrame (default: False)\n\n        Returns:\n            pd.DataFrame: A DataFrame of all the data from this FeatureSet up to the limit\n        \"\"\"\n\n        # Get the table associated with the data\n        self.log.info(f\"Pulling data from {self.name}...\")\n        pull_query = f'SELECT * FROM \"{self.athena_table}\" LIMIT {limit}'\n        df = self.query(pull_query)\n\n        # Drop any columns generated from AWS\n        if not include_aws_columns:\n            aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n            df = df.drop(columns=aws_cols, errors=\"ignore\")\n        return df\n\n    def to_model(\n        self,\n        name: str,\n        model_type: ModelType,\n        model_framework: ModelFramework = ModelFramework.XGBOOST,\n        tags: list = None,\n        description: str = None,\n        feature_list: list = None,\n        target_column: Union[str, list[str]] = None,\n        model_class: str = None,\n        model_import_str: str = None,\n        custom_script: Union[str, Path] = None,\n        custom_args: dict = None,\n        **kwargs,\n    ) -&gt; Union[Model, None]:\n        \"\"\"Create a Model from the FeatureSet\n\n        Args:\n\n            name (str): The name of the Model to create\n            model_type (ModelType): The type of model to create (See workbench.model.ModelType)\n            model_framework (ModelFramework, optional): The framework to use for the model (default: XGBOOST)\n            tags (list, optional): Set the tags for the model.  If not given tags will be generated.\n            description (str, optional): Set the description for the model. If not give a description is generated.\n            feature_list (list, optional): Set the feature list for the model. If not given a feature list is generated.\n            target_column (str or list[str], optional): Target column(s) for the model (use None for unsupervised model)\n            model_class (str, optional): Model class to use (e.g. \"KMeans\", default: None)\n            model_import_str (str, optional): The import for the model (e.g. \"from sklearn.cluster import KMeans\")\n            custom_script (str, optional): The custom script to use for the model (default: None)\n            kwargs (dict, optional): Additional keyword arguments to pass to the model\n\n        Returns:\n            Model: The Model created from the FeatureSet (or None if the Model could not be created)\n        \"\"\"\n\n        # Ensure the model_name is valid\n        if name:\n            if not Artifact.is_name_valid(name, delimiter=\"-\", lower_case=False):\n                self.log.critical(f\"Invalid Model name: {name}, not creating Model!\")\n                return None\n\n        # If the model_name wasn't given generate it\n        else:\n            name = self.name.replace(\"_features\", \"\") + \"-model\"\n            name = Artifact.generate_valid_name(name, delimiter=\"-\")\n\n        # Create the Model Tags\n        tags = [name] if tags is None else tags\n\n        # Set training/inference images based on model framework\n        if model_framework in (ModelFramework.PYTORCH, ModelFramework.CHEMPROP):\n            training_image = \"pytorch_training\"\n            inference_image = \"pytorch_inference\"\n            inference_arch = \"x86_64\"\n        else:\n            training_image = \"training\"\n            inference_image = \"inference\"\n            inference_arch = \"x86_64\"\n\n        # Transform the FeatureSet into a Model\n        features_to_model = FeaturesToModel(\n            feature_name=self.name,\n            model_name=name,\n            model_type=model_type,\n            model_framework=model_framework,\n            model_class=model_class,\n            model_import_str=model_import_str,\n            custom_script=custom_script,\n            custom_args=custom_args,\n            training_image=training_image,\n            inference_image=inference_image,\n            inference_arch=inference_arch,\n        )\n        features_to_model.set_output_tags(tags)\n        features_to_model.transform(\n            target_column=target_column, description=description, feature_list=feature_list, **kwargs\n        )\n\n        # Return the Model\n        return Model(name)\n\n    def prox_model(\n        self, target: str, features: list, include_all_columns: bool = False\n    ) -&gt; \"FeatureSpaceProximity\":  # noqa: F821\n        \"\"\"Create a local FeatureSpaceProximity Model for this FeatureSet\n\n        Args:\n           target (str): The target column name\n           features (list): The list of feature column names\n           include_all_columns (bool): Include all DataFrame columns in results (default: False)\n\n        Returns:\n           FeatureSpaceProximity: A local FeatureSpaceProximity Model\n        \"\"\"\n        from workbench.algorithms.dataframe.feature_space_proximity import FeatureSpaceProximity  # noqa: F401\n\n        # Create the Proximity Model from the full FeatureSet dataframe\n        full_df = self.pull_dataframe()\n\n        # Create and return the FeatureSpaceProximity Model\n        return FeatureSpaceProximity(\n            full_df, id_column=self.id_column, features=features, target=target, include_all_columns=include_all_columns\n        )\n\n    def fp_prox_model(\n        self,\n        target: str,\n        fingerprint_column: str = None,\n        include_all_columns: bool = False,\n        radius: int = 2,\n        n_bits: int = 1024,\n        counts: bool = False,\n    ) -&gt; \"FingerprintProximity\":  # noqa: F821\n        \"\"\"Create a local FingerprintProximity Model for this FeatureSet\n\n        Args:\n           target (str): The target column name\n           fingerprint_column (str): Column containing fingerprints. If None, uses existing 'fingerprint'\n                                     column or computes from SMILES column.\n           include_all_columns (bool): Include all DataFrame columns in results (default: False)\n           radius (int): Radius for Morgan fingerprint computation (default: 2)\n           n_bits (int): Number of bits for fingerprint (default: 1024)\n           counts (bool): Whether to use count simulation (default: False)\n\n        Returns:\n           FingerprintProximity: A local FingerprintProximity Model\n        \"\"\"\n        from workbench.algorithms.dataframe.fingerprint_proximity import FingerprintProximity  # noqa: F401\n\n        # Create the Proximity Model from the full FeatureSet dataframe\n        full_df = self.pull_dataframe()\n\n        # Create and return the FingerprintProximity Model\n        return FingerprintProximity(\n            full_df,\n            id_column=self.id_column,\n            fingerprint_column=fingerprint_column,\n            target=target,\n            include_all_columns=include_all_columns,\n            radius=radius,\n            n_bits=n_bits,\n        )\n\n    def cleanlab_model(\n        self,\n        target: str,\n        features: list,\n        model_type: ModelType = ModelType.REGRESSOR,\n    ) -&gt; \"CleanLearning\":  # noqa: F821\n        \"\"\"Create a CleanLearning model for detecting label issues in this FeatureSet\n\n        Args:\n           target (str): The target column name\n           features (list): The list of feature column names\n           model_type (ModelType): The model type (REGRESSOR or CLASSIFIER). Defaults to REGRESSOR.\n\n        Returns:\n           CleanLearning: A fitted cleanlab model. Use get_label_issues() to get\n           a DataFrame with id_column, label_quality, predicted_label, given_label, is_label_issue.\n        \"\"\"\n        from workbench.algorithms.models.cleanlab_model import create_cleanlab_model  # noqa: F401\n\n        # Get the full FeatureSet dataframe\n        full_df = self.pull_dataframe()\n\n        # Create and return the CleanLearning model\n        return create_cleanlab_model(full_df, self.id_column, features, target, model_type=model_type)\n</code></pre>"},{"location":"api_classes/feature_set/#workbench.api.feature_set.FeatureSet.cleanlab_model","title":"<code>cleanlab_model(target, features, model_type=ModelType.REGRESSOR)</code>","text":"<p>Create a CleanLearning model for detecting label issues in this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>The target column name</p> required <code>features</code> <code>list</code> <p>The list of feature column names</p> required <code>model_type</code> <code>ModelType</code> <p>The model type (REGRESSOR or CLASSIFIER). Defaults to REGRESSOR.</p> <code>REGRESSOR</code> <p>Returns:</p> Name Type Description <code>CleanLearning</code> <code>CleanLearning</code> <p>A fitted cleanlab model. Use get_label_issues() to get</p> <code>CleanLearning</code> <p>a DataFrame with id_column, label_quality, predicted_label, given_label, is_label_issue.</p> Source code in <code>src/workbench/api/feature_set.py</code> <pre><code>def cleanlab_model(\n    self,\n    target: str,\n    features: list,\n    model_type: ModelType = ModelType.REGRESSOR,\n) -&gt; \"CleanLearning\":  # noqa: F821\n    \"\"\"Create a CleanLearning model for detecting label issues in this FeatureSet\n\n    Args:\n       target (str): The target column name\n       features (list): The list of feature column names\n       model_type (ModelType): The model type (REGRESSOR or CLASSIFIER). Defaults to REGRESSOR.\n\n    Returns:\n       CleanLearning: A fitted cleanlab model. Use get_label_issues() to get\n       a DataFrame with id_column, label_quality, predicted_label, given_label, is_label_issue.\n    \"\"\"\n    from workbench.algorithms.models.cleanlab_model import create_cleanlab_model  # noqa: F401\n\n    # Get the full FeatureSet dataframe\n    full_df = self.pull_dataframe()\n\n    # Create and return the CleanLearning model\n    return create_cleanlab_model(full_df, self.id_column, features, target, model_type=model_type)\n</code></pre>"},{"location":"api_classes/feature_set/#workbench.api.feature_set.FeatureSet.details","title":"<code>details(**kwargs)</code>","text":"<p>FeatureSet Details</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the FeatureSet</p> Source code in <code>src/workbench/api/feature_set.py</code> <pre><code>def details(self, **kwargs) -&gt; dict:\n    \"\"\"FeatureSet Details\n\n    Returns:\n        dict: A dictionary of details about the FeatureSet\n    \"\"\"\n    return super().details(**kwargs)\n</code></pre>"},{"location":"api_classes/feature_set/#workbench.api.feature_set.FeatureSet.fp_prox_model","title":"<code>fp_prox_model(target, fingerprint_column=None, include_all_columns=False, radius=2, n_bits=1024, counts=False)</code>","text":"<p>Create a local FingerprintProximity Model for this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>The target column name</p> required <code>fingerprint_column</code> <code>str</code> <p>Column containing fingerprints. If None, uses existing 'fingerprint'                        column or computes from SMILES column.</p> <code>None</code> <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in results (default: False)</p> <code>False</code> <code>radius</code> <code>int</code> <p>Radius for Morgan fingerprint computation (default: 2)</p> <code>2</code> <code>n_bits</code> <code>int</code> <p>Number of bits for fingerprint (default: 1024)</p> <code>1024</code> <code>counts</code> <code>bool</code> <p>Whether to use count simulation (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FingerprintProximity</code> <code>FingerprintProximity</code> <p>A local FingerprintProximity Model</p> Source code in <code>src/workbench/api/feature_set.py</code> <pre><code>def fp_prox_model(\n    self,\n    target: str,\n    fingerprint_column: str = None,\n    include_all_columns: bool = False,\n    radius: int = 2,\n    n_bits: int = 1024,\n    counts: bool = False,\n) -&gt; \"FingerprintProximity\":  # noqa: F821\n    \"\"\"Create a local FingerprintProximity Model for this FeatureSet\n\n    Args:\n       target (str): The target column name\n       fingerprint_column (str): Column containing fingerprints. If None, uses existing 'fingerprint'\n                                 column or computes from SMILES column.\n       include_all_columns (bool): Include all DataFrame columns in results (default: False)\n       radius (int): Radius for Morgan fingerprint computation (default: 2)\n       n_bits (int): Number of bits for fingerprint (default: 1024)\n       counts (bool): Whether to use count simulation (default: False)\n\n    Returns:\n       FingerprintProximity: A local FingerprintProximity Model\n    \"\"\"\n    from workbench.algorithms.dataframe.fingerprint_proximity import FingerprintProximity  # noqa: F401\n\n    # Create the Proximity Model from the full FeatureSet dataframe\n    full_df = self.pull_dataframe()\n\n    # Create and return the FingerprintProximity Model\n    return FingerprintProximity(\n        full_df,\n        id_column=self.id_column,\n        fingerprint_column=fingerprint_column,\n        target=target,\n        include_all_columns=include_all_columns,\n        radius=radius,\n        n_bits=n_bits,\n    )\n</code></pre>"},{"location":"api_classes/feature_set/#workbench.api.feature_set.FeatureSet.prox_model","title":"<code>prox_model(target, features, include_all_columns=False)</code>","text":"<p>Create a local FeatureSpaceProximity Model for this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>The target column name</p> required <code>features</code> <code>list</code> <p>The list of feature column names</p> required <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in results (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FeatureSpaceProximity</code> <code>FeatureSpaceProximity</code> <p>A local FeatureSpaceProximity Model</p> Source code in <code>src/workbench/api/feature_set.py</code> <pre><code>def prox_model(\n    self, target: str, features: list, include_all_columns: bool = False\n) -&gt; \"FeatureSpaceProximity\":  # noqa: F821\n    \"\"\"Create a local FeatureSpaceProximity Model for this FeatureSet\n\n    Args:\n       target (str): The target column name\n       features (list): The list of feature column names\n       include_all_columns (bool): Include all DataFrame columns in results (default: False)\n\n    Returns:\n       FeatureSpaceProximity: A local FeatureSpaceProximity Model\n    \"\"\"\n    from workbench.algorithms.dataframe.feature_space_proximity import FeatureSpaceProximity  # noqa: F401\n\n    # Create the Proximity Model from the full FeatureSet dataframe\n    full_df = self.pull_dataframe()\n\n    # Create and return the FeatureSpaceProximity Model\n    return FeatureSpaceProximity(\n        full_df, id_column=self.id_column, features=features, target=target, include_all_columns=include_all_columns\n    )\n</code></pre>"},{"location":"api_classes/feature_set/#workbench.api.feature_set.FeatureSet.pull_dataframe","title":"<code>pull_dataframe(limit=50000, include_aws_columns=False)</code>","text":"<p>Return a DataFrame of ALL the data from this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Limit the number of rows returned (default: 50000)</p> <code>50000</code> <code>include_aws_columns</code> <code>bool</code> <p>Include the AWS columns in the DataFrame (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame of all the data from this FeatureSet up to the limit</p> Source code in <code>src/workbench/api/feature_set.py</code> <pre><code>def pull_dataframe(self, limit: int = 50000, include_aws_columns=False) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame of ALL the data from this FeatureSet\n\n    Args:\n        limit (int): Limit the number of rows returned (default: 50000)\n        include_aws_columns (bool): Include the AWS columns in the DataFrame (default: False)\n\n    Returns:\n        pd.DataFrame: A DataFrame of all the data from this FeatureSet up to the limit\n    \"\"\"\n\n    # Get the table associated with the data\n    self.log.info(f\"Pulling data from {self.name}...\")\n    pull_query = f'SELECT * FROM \"{self.athena_table}\" LIMIT {limit}'\n    df = self.query(pull_query)\n\n    # Drop any columns generated from AWS\n    if not include_aws_columns:\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n        df = df.drop(columns=aws_cols, errors=\"ignore\")\n    return df\n</code></pre>"},{"location":"api_classes/feature_set/#workbench.api.feature_set.FeatureSet.query","title":"<code>query(query, **kwargs)</code>","text":"<p>Query the AthenaSource</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to run against the FeatureSet</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The results of the query</p> Source code in <code>src/workbench/api/feature_set.py</code> <pre><code>def query(self, query: str, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Query the AthenaSource\n\n    Args:\n        query (str): The query to run against the FeatureSet\n\n    Returns:\n        pd.DataFrame: The results of the query\n    \"\"\"\n    return super().query(query, **kwargs)\n</code></pre>"},{"location":"api_classes/feature_set/#workbench.api.feature_set.FeatureSet.to_model","title":"<code>to_model(name, model_type, model_framework=ModelFramework.XGBOOST, tags=None, description=None, feature_list=None, target_column=None, model_class=None, model_import_str=None, custom_script=None, custom_args=None, **kwargs)</code>","text":"<p>Create a Model from the FeatureSet</p> <p>Args:</p> <pre><code>name (str): The name of the Model to create\nmodel_type (ModelType): The type of model to create (See workbench.model.ModelType)\nmodel_framework (ModelFramework, optional): The framework to use for the model (default: XGBOOST)\ntags (list, optional): Set the tags for the model.  If not given tags will be generated.\ndescription (str, optional): Set the description for the model. If not give a description is generated.\nfeature_list (list, optional): Set the feature list for the model. If not given a feature list is generated.\ntarget_column (str or list[str], optional): Target column(s) for the model (use None for unsupervised model)\nmodel_class (str, optional): Model class to use (e.g. \"KMeans\", default: None)\nmodel_import_str (str, optional): The import for the model (e.g. \"from sklearn.cluster import KMeans\")\ncustom_script (str, optional): The custom script to use for the model (default: None)\nkwargs (dict, optional): Additional keyword arguments to pass to the model\n</code></pre> <p>Returns:</p> Name Type Description <code>Model</code> <code>Union[Model, None]</code> <p>The Model created from the FeatureSet (or None if the Model could not be created)</p> Source code in <code>src/workbench/api/feature_set.py</code> <pre><code>def to_model(\n    self,\n    name: str,\n    model_type: ModelType,\n    model_framework: ModelFramework = ModelFramework.XGBOOST,\n    tags: list = None,\n    description: str = None,\n    feature_list: list = None,\n    target_column: Union[str, list[str]] = None,\n    model_class: str = None,\n    model_import_str: str = None,\n    custom_script: Union[str, Path] = None,\n    custom_args: dict = None,\n    **kwargs,\n) -&gt; Union[Model, None]:\n    \"\"\"Create a Model from the FeatureSet\n\n    Args:\n\n        name (str): The name of the Model to create\n        model_type (ModelType): The type of model to create (See workbench.model.ModelType)\n        model_framework (ModelFramework, optional): The framework to use for the model (default: XGBOOST)\n        tags (list, optional): Set the tags for the model.  If not given tags will be generated.\n        description (str, optional): Set the description for the model. If not give a description is generated.\n        feature_list (list, optional): Set the feature list for the model. If not given a feature list is generated.\n        target_column (str or list[str], optional): Target column(s) for the model (use None for unsupervised model)\n        model_class (str, optional): Model class to use (e.g. \"KMeans\", default: None)\n        model_import_str (str, optional): The import for the model (e.g. \"from sklearn.cluster import KMeans\")\n        custom_script (str, optional): The custom script to use for the model (default: None)\n        kwargs (dict, optional): Additional keyword arguments to pass to the model\n\n    Returns:\n        Model: The Model created from the FeatureSet (or None if the Model could not be created)\n    \"\"\"\n\n    # Ensure the model_name is valid\n    if name:\n        if not Artifact.is_name_valid(name, delimiter=\"-\", lower_case=False):\n            self.log.critical(f\"Invalid Model name: {name}, not creating Model!\")\n            return None\n\n    # If the model_name wasn't given generate it\n    else:\n        name = self.name.replace(\"_features\", \"\") + \"-model\"\n        name = Artifact.generate_valid_name(name, delimiter=\"-\")\n\n    # Create the Model Tags\n    tags = [name] if tags is None else tags\n\n    # Set training/inference images based on model framework\n    if model_framework in (ModelFramework.PYTORCH, ModelFramework.CHEMPROP):\n        training_image = \"pytorch_training\"\n        inference_image = \"pytorch_inference\"\n        inference_arch = \"x86_64\"\n    else:\n        training_image = \"training\"\n        inference_image = \"inference\"\n        inference_arch = \"x86_64\"\n\n    # Transform the FeatureSet into a Model\n    features_to_model = FeaturesToModel(\n        feature_name=self.name,\n        model_name=name,\n        model_type=model_type,\n        model_framework=model_framework,\n        model_class=model_class,\n        model_import_str=model_import_str,\n        custom_script=custom_script,\n        custom_args=custom_args,\n        training_image=training_image,\n        inference_image=inference_image,\n        inference_arch=inference_arch,\n    )\n    features_to_model.set_output_tags(tags)\n    features_to_model.transform(\n        target_column=target_column, description=description, feature_list=feature_list, **kwargs\n    )\n\n    # Return the Model\n    return Model(name)\n</code></pre>"},{"location":"api_classes/feature_set/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Create a FeatureSet from a Datasource</p> datasource_to_featureset.py<pre><code>from workbench.api.data_source import DataSource\n\n# Convert the Data Source to a Feature Set\nds = DataSource('test_data')\nfs = ds.to_features(\"test_features\", id_column=\"id\")\nprint(fs.details())\n</code></pre> <p>FeatureSet EDA Statistics</p> <p>featureset_eda.py<pre><code>from workbench.api.feature_set import FeatureSet\nimport pandas as pd\n\n# Grab a FeatureSet and pull some of the EDA Stats\nmy_features = FeatureSet('test_features')\n\n# Grab some of the EDA Stats\ncorr_data = my_features.correlations()\ncorr_df = pd.DataFrame(corr_data)\nprint(corr_df)\n\n# Get some outliers\noutliers = my_features.outliers()\npprint(outliers.head())\n\n# Full set of EDA Stats\neda_stats = my_features.column_stats()\npprint(eda_stats)\n</code></pre> Output</p> <pre><code>                 age  food_pizza  food_steak  food_sushi  food_tacos    height        id  iq_score\nage              NaN   -0.188645   -0.256356    0.263048    0.054211  0.439678 -0.054948 -0.295513\nfood_pizza -0.188645         NaN   -0.288175   -0.229591   -0.196818 -0.494380  0.137282  0.395378\nfood_steak -0.256356   -0.288175         NaN   -0.374920   -0.321403 -0.002542 -0.005199  0.076477\nfood_sushi  0.263048   -0.229591   -0.374920         NaN   -0.256064  0.536396  0.038279 -0.435033\nfood_tacos  0.054211   -0.196818   -0.321403   -0.256064         NaN -0.091493 -0.051398  0.033364\nheight      0.439678   -0.494380   -0.002542    0.536396   -0.091493       NaN -0.117372 -0.655210\nid         -0.054948    0.137282   -0.005199    0.038279   -0.051398 -0.117372       NaN  0.106020\niq_score   -0.295513    0.395378    0.076477   -0.435033    0.033364 -0.655210  0.106020       NaN\n\n        name     height      weight         salary  age    iq_score  likes_dogs  food_pizza  food_steak  food_sushi  food_tacos outlier_group\n0  Person 96  57.582840  148.461349   80000.000000   43  150.000000           1           0           0           0           0    height_low\n1  Person 68  73.918663  189.527313  219994.000000   80  100.000000           0           0           0           1           0  iq_score_low\n2  Person 49  70.381790  261.237000  175633.703125   49  107.933998           0           0           0           1           0  iq_score_low\n3  Person 90  73.488739  193.840698  227760.000000   72  110.821541           1           0           0           0           0   salary_high\n\n&lt;lots of EDA data and statistics&gt;\n</code></pre> <p>Query a FeatureSet</p> <p>All Workbench FeatureSet have an 'offline' store that uses AWS Athena, so any query that you can make with Athena is accessible through the FeatureSet API.</p> featureset_query.py<pre><code>from workbench.api.feature_set import FeatureSet\n\n# Grab a FeatureSet\nmy_features = FeatureSet(\"abalone_features\")\n\n# Make some queries using the Athena backend\ndf = my_features.query(\"select * from abalone_features where height &gt; .3\")\nprint(df.head())\n\ndf = my_features.query(\"select * from abalone_features where class_number_of_rings &lt; 3\")\nprint(df.head())\n</code></pre> <p>Output</p> <pre><code>  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  shell_weight  class_number_of_rings\n0   M   0.705     0.565   0.515         2.210          1.1075          0.4865        0.5120                     10\n1   F   0.455     0.355   1.130         0.594          0.3320          0.1160        0.1335                      8\n\n  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  shell_weight  class_number_of_rings\n0   I   0.075     0.055   0.010         0.002          0.0010          0.0005        0.0015                      1\n1   I   0.150     0.100   0.025         0.015          0.0045          0.0040         0.0050                      2\n</code></pre> <p>Create a Model from a FeatureSet</p> featureset_to_model.py<pre><code>from workbench.api.feature_set import FeatureSet\nfrom workbench.api.model import ModelType\nfrom pprint import pprint\n\n# Grab a FeatureSet\nmy_features = FeatureSet('test_features')\n\n# Create a Model from the FeatureSet\n# Note: ModelTypes can be CLASSIFIER, REGRESSOR, \n#       UNSUPERVISED, or TRANSFORMER\nmy_model = my_features.to_model(name=\"test-model\", \n                                model_type=ModelType.REGRESSOR, \n                                target_column=\"iq_score\")\npprint(my_model.details())\n</code></pre> <p>Output</p> <pre><code>{'approval_status': 'Approved',\n 'content_types': ['text/csv'],\n ...\n 'inference_types': ['ml.t2.medium'],\n 'input': 'test_features',\n 'model_metrics':   metric_name  value\n                0        RMSE  7.924\n                1         MAE  6.554,\n                2          R2  0.604,\n 'regression_predictions':       iq_score  prediction\n                            0   136.519012  139.964460\n                            1   133.616974  130.819950\n                            2   122.495415  124.967834\n                            3   133.279510  121.010284\n                            4   127.881073  113.825005\n    ...\n 'response_types': ['text/csv'],\n 'workbench_tags': ['test-model'],\n 'shapley_values': None,\n 'size': 0.0,\n 'status': 'Completed',\n 'transform_types': ['ml.m5.large'],\n 'name': 'test-model',\n 'version': 1}\n</code></pre>"},{"location":"api_classes/feature_set/#workbench-ui","title":"Workbench UI","text":"<p>Whenever a FeatureSet is created Workbench performs a comprehensive set of Exploratory Data Analysis techniques on your data, pushes the results into AWS, and provides a detailed web visualization of the results.</p> Workbench Dashboard: FeatureSets <p>Not Finding a particular method?</p> <p>The Workbench API Classes use the 'Core' Classes Internally, so for an extensive listing of all the methods available please take a deep dive into: Workbench Core Classes</p>"},{"location":"api_classes/meta/","title":"Meta","text":"<p>Meta Examples</p> <p>Examples of using the Meta class are listed at the bottom of this page Examples.</p> <p>Meta: A class that provides high level information and summaries of Cloud Platform Artifacts. The Meta class provides 'account' information, configuration, etc. It also provides metadata for Artifacts, such as Data Sources, Feature Sets, Models, and Endpoints.</p>"},{"location":"api_classes/meta/#workbench.api.meta.Meta","title":"<code>Meta</code>","text":"<p>               Bases: <code>CloudMeta</code></p> <p>Meta: A class that provides metadata functionality for Cloud Platform Artifacts.</p> Common Usage <pre><code>from workbench.api import Meta\nmeta = Meta()\n\n# Get the AWS Account Info\nmeta.account()\nmeta.config()\n\n# These are 'list' methods\nmeta.etl_jobs()\nmeta.data_sources()\nmeta.feature_sets(details=True/False)\nmeta.models(details=True/False)\nmeta.endpoints(details=True/False)\nmeta.views()\nmeta.pipelines()\n\n# These are 'describe' methods\nmeta.data_source(\"abalone_data\")\nmeta.feature_set(\"abalone_features\")\nmeta.model(\"abalone-regression\")\nmeta.endpoint(\"abalone-endpoint\")\n</code></pre> Source code in <code>src/workbench/api/meta.py</code> <pre><code>class Meta(CloudMeta):\n    \"\"\"Meta: A class that provides metadata functionality for Cloud Platform Artifacts.\n\n    Common Usage:\n       ```python\n       from workbench.api import Meta\n       meta = Meta()\n\n       # Get the AWS Account Info\n       meta.account()\n       meta.config()\n\n       # These are 'list' methods\n       meta.etl_jobs()\n       meta.data_sources()\n       meta.feature_sets(details=True/False)\n       meta.models(details=True/False)\n       meta.endpoints(details=True/False)\n       meta.views()\n       meta.pipelines()\n\n       # These are 'describe' methods\n       meta.data_source(\"abalone_data\")\n       meta.feature_set(\"abalone_features\")\n       meta.model(\"abalone-regression\")\n       meta.endpoint(\"abalone-endpoint\")\n       ```\n    \"\"\"\n\n    def account(self) -&gt; dict:\n        \"\"\"Cloud Platform Account Info\n\n        Returns:\n            dict: Cloud Platform Account Info\n        \"\"\"\n        return super().account()\n\n    def config(self) -&gt; dict:\n        \"\"\"Return the current Workbench Configuration\n\n        Returns:\n            dict: The current Workbench Configuration\n        \"\"\"\n        return super().config()\n\n    def incoming_data(self) -&gt; pd.DataFrame:\n        \"\"\"Get summary data about data in the incoming raw data\n\n        Returns:\n            pd.DataFrame: A summary of the incoming raw data\n        \"\"\"\n        return super().incoming_data()\n\n    def etl_jobs(self) -&gt; pd.DataFrame:\n        \"\"\"Get summary data about Extract, Transform, Load (ETL) Jobs\n\n        Returns:\n            pd.DataFrame: A summary of the ETL Jobs deployed in the Cloud Platform\n        \"\"\"\n        return super().etl_jobs()\n\n    def data_sources(self) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the Data Sources deployed in the Cloud Platform\n\n        Returns:\n            pd.DataFrame: A summary of the Data Sources deployed in the Cloud Platform\n        \"\"\"\n        return super().data_sources()\n\n    def views(self, database: str = \"workbench\") -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the all the Views, for the given database, in AWS\n\n        Args:\n            database (str, optional): Glue database. Defaults to 'workbench'.\n\n        Returns:\n            pd.DataFrame: A summary of all the Views, for the given database, in AWS\n        \"\"\"\n        return super().views(database=database)\n\n    def feature_sets(self, details: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the Feature Sets deployed in the Cloud Platform\n\n        Args:\n            details (bool, optional): Include detailed information. Defaults to False.\n\n        Returns:\n            pd.DataFrame: A summary of the Feature Sets deployed in the Cloud Platform\n        \"\"\"\n        return super().feature_sets(details=details)\n\n    def models(self, details: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the Models deployed in the Cloud Platform\n\n        Args:\n            details (bool, optional): Include detailed information. Defaults to False.\n\n        Returns:\n            pd.DataFrame: A summary of the Models deployed in the Cloud Platform\n        \"\"\"\n        return super().models(details=details)\n\n    def endpoints(self, details: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the Endpoints deployed in the Cloud Platform\n\n        Args:\n            details (bool, optional): Include detailed information. Defaults to False.\n\n        Returns:\n            pd.DataFrame: A summary of the Endpoints in the Cloud Platform\n        \"\"\"\n        return super().endpoints(details=details)\n\n    def pipelines(self) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the ML Pipelines deployed in the Cloud Platform\n\n        Returns:\n            pd.DataFrame: A summary of the Pipelines in the Cloud Platform\n        \"\"\"\n        return super().pipelines()\n\n    def glue_job(self, job_name: str) -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Glue Job\n\n        Args:\n            job_name (str): The name of the Glue Job\n\n        Returns:\n            dict: The details of the Glue Job (None if not found)\n        \"\"\"\n        return super().glue_job(job_name=job_name)\n\n    def data_source(self, data_source_name: str, database: str = \"workbench\") -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Data Source\n\n        Args:\n            data_source_name (str): The name of the Data Source\n            database (str, optional): The Glue database. Defaults to 'workbench'.\n\n        Returns:\n            dict: The details of the Data Source (None if not found)\n        \"\"\"\n        return super().data_source(data_source_name=data_source_name, database=database)\n\n    def feature_set(self, feature_set_name: str) -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Feature Set\n\n        Args:\n            feature_set_name (str): The name of the Feature Set\n\n        Returns:\n            dict: The details of the Feature Set (None if not found)\n        \"\"\"\n        return super().feature_set(feature_group_name=feature_set_name)\n\n    def model(self, model_name: str) -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Model\n\n        Args:\n            model_name (str): The name of the Model\n\n        Returns:\n            dict: The details of the Model (None if not found)\n        \"\"\"\n        return super().model(model_group_name=model_name)\n\n    def endpoint(self, endpoint_name: str) -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Endpoint\n\n        Args:\n            endpoint_name (str): The name of the Endpoint\n\n        Returns:\n            dict: The details of the Endpoint (None if not found)\n        \"\"\"\n        return super().endpoint(endpoint_name=endpoint_name)\n\n    def __repr__(self):\n        return f\"Meta()\\n\\t{super().__repr__()}\"\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.account","title":"<code>account()</code>","text":"<p>Cloud Platform Account Info</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Cloud Platform Account Info</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def account(self) -&gt; dict:\n    \"\"\"Cloud Platform Account Info\n\n    Returns:\n        dict: Cloud Platform Account Info\n    \"\"\"\n    return super().account()\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.config","title":"<code>config()</code>","text":"<p>Return the current Workbench Configuration</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The current Workbench Configuration</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Return the current Workbench Configuration\n\n    Returns:\n        dict: The current Workbench Configuration\n    \"\"\"\n    return super().config()\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.data_source","title":"<code>data_source(data_source_name, database='workbench')</code>","text":"<p>Get the details of a specific Data Source</p> <p>Parameters:</p> Name Type Description Default <code>data_source_name</code> <code>str</code> <p>The name of the Data Source</p> required <code>database</code> <code>str</code> <p>The Glue database. Defaults to 'workbench'.</p> <code>'workbench'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Data Source (None if not found)</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def data_source(self, data_source_name: str, database: str = \"workbench\") -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Data Source\n\n    Args:\n        data_source_name (str): The name of the Data Source\n        database (str, optional): The Glue database. Defaults to 'workbench'.\n\n    Returns:\n        dict: The details of the Data Source (None if not found)\n    \"\"\"\n    return super().data_source(data_source_name=data_source_name, database=database)\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.data_sources","title":"<code>data_sources()</code>","text":"<p>Get a summary of the Data Sources deployed in the Cloud Platform</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Data Sources deployed in the Cloud Platform</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def data_sources(self) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the Data Sources deployed in the Cloud Platform\n\n    Returns:\n        pd.DataFrame: A summary of the Data Sources deployed in the Cloud Platform\n    \"\"\"\n    return super().data_sources()\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.endpoint","title":"<code>endpoint(endpoint_name)</code>","text":"<p>Get the details of a specific Endpoint</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the Endpoint</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Endpoint (None if not found)</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def endpoint(self, endpoint_name: str) -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Endpoint\n\n    Args:\n        endpoint_name (str): The name of the Endpoint\n\n    Returns:\n        dict: The details of the Endpoint (None if not found)\n    \"\"\"\n    return super().endpoint(endpoint_name=endpoint_name)\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.endpoints","title":"<code>endpoints(details=False)</code>","text":"<p>Get a summary of the Endpoints deployed in the Cloud Platform</p> <p>Parameters:</p> Name Type Description Default <code>details</code> <code>bool</code> <p>Include detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Endpoints in the Cloud Platform</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def endpoints(self, details: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the Endpoints deployed in the Cloud Platform\n\n    Args:\n        details (bool, optional): Include detailed information. Defaults to False.\n\n    Returns:\n        pd.DataFrame: A summary of the Endpoints in the Cloud Platform\n    \"\"\"\n    return super().endpoints(details=details)\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.etl_jobs","title":"<code>etl_jobs()</code>","text":"<p>Get summary data about Extract, Transform, Load (ETL) Jobs</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the ETL Jobs deployed in the Cloud Platform</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def etl_jobs(self) -&gt; pd.DataFrame:\n    \"\"\"Get summary data about Extract, Transform, Load (ETL) Jobs\n\n    Returns:\n        pd.DataFrame: A summary of the ETL Jobs deployed in the Cloud Platform\n    \"\"\"\n    return super().etl_jobs()\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.feature_set","title":"<code>feature_set(feature_set_name)</code>","text":"<p>Get the details of a specific Feature Set</p> <p>Parameters:</p> Name Type Description Default <code>feature_set_name</code> <code>str</code> <p>The name of the Feature Set</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Feature Set (None if not found)</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def feature_set(self, feature_set_name: str) -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Feature Set\n\n    Args:\n        feature_set_name (str): The name of the Feature Set\n\n    Returns:\n        dict: The details of the Feature Set (None if not found)\n    \"\"\"\n    return super().feature_set(feature_group_name=feature_set_name)\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.feature_sets","title":"<code>feature_sets(details=False)</code>","text":"<p>Get a summary of the Feature Sets deployed in the Cloud Platform</p> <p>Parameters:</p> Name Type Description Default <code>details</code> <code>bool</code> <p>Include detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Feature Sets deployed in the Cloud Platform</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def feature_sets(self, details: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the Feature Sets deployed in the Cloud Platform\n\n    Args:\n        details (bool, optional): Include detailed information. Defaults to False.\n\n    Returns:\n        pd.DataFrame: A summary of the Feature Sets deployed in the Cloud Platform\n    \"\"\"\n    return super().feature_sets(details=details)\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.glue_job","title":"<code>glue_job(job_name)</code>","text":"<p>Get the details of a specific Glue Job</p> <p>Parameters:</p> Name Type Description Default <code>job_name</code> <code>str</code> <p>The name of the Glue Job</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Glue Job (None if not found)</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def glue_job(self, job_name: str) -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Glue Job\n\n    Args:\n        job_name (str): The name of the Glue Job\n\n    Returns:\n        dict: The details of the Glue Job (None if not found)\n    \"\"\"\n    return super().glue_job(job_name=job_name)\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.incoming_data","title":"<code>incoming_data()</code>","text":"<p>Get summary data about data in the incoming raw data</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the incoming raw data</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def incoming_data(self) -&gt; pd.DataFrame:\n    \"\"\"Get summary data about data in the incoming raw data\n\n    Returns:\n        pd.DataFrame: A summary of the incoming raw data\n    \"\"\"\n    return super().incoming_data()\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.model","title":"<code>model(model_name)</code>","text":"<p>Get the details of a specific Model</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the Model</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Model (None if not found)</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def model(self, model_name: str) -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Model\n\n    Args:\n        model_name (str): The name of the Model\n\n    Returns:\n        dict: The details of the Model (None if not found)\n    \"\"\"\n    return super().model(model_group_name=model_name)\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.models","title":"<code>models(details=False)</code>","text":"<p>Get a summary of the Models deployed in the Cloud Platform</p> <p>Parameters:</p> Name Type Description Default <code>details</code> <code>bool</code> <p>Include detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Models deployed in the Cloud Platform</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def models(self, details: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the Models deployed in the Cloud Platform\n\n    Args:\n        details (bool, optional): Include detailed information. Defaults to False.\n\n    Returns:\n        pd.DataFrame: A summary of the Models deployed in the Cloud Platform\n    \"\"\"\n    return super().models(details=details)\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.pipelines","title":"<code>pipelines()</code>","text":"<p>Get a summary of the ML Pipelines deployed in the Cloud Platform</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Pipelines in the Cloud Platform</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def pipelines(self) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the ML Pipelines deployed in the Cloud Platform\n\n    Returns:\n        pd.DataFrame: A summary of the Pipelines in the Cloud Platform\n    \"\"\"\n    return super().pipelines()\n</code></pre>"},{"location":"api_classes/meta/#workbench.api.meta.Meta.views","title":"<code>views(database='workbench')</code>","text":"<p>Get a summary of the all the Views, for the given database, in AWS</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>Glue database. Defaults to 'workbench'.</p> <code>'workbench'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of all the Views, for the given database, in AWS</p> Source code in <code>src/workbench/api/meta.py</code> <pre><code>def views(self, database: str = \"workbench\") -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the all the Views, for the given database, in AWS\n\n    Args:\n        database (str, optional): Glue database. Defaults to 'workbench'.\n\n    Returns:\n        pd.DataFrame: A summary of all the Views, for the given database, in AWS\n    \"\"\"\n    return super().views(database=database)\n</code></pre>"},{"location":"api_classes/meta/#examples","title":"Examples","text":"<p>These example show how to use the <code>Meta()</code> class to pull lists of artifacts from AWS. DataSources, FeatureSets, Models, Endpoints and more. If you're building a web interface plugin, the Meta class is a great place to start.</p> <p>Workbench REPL</p> <p>If you'd like to see exactly what data/details you get back from the <code>Meta()</code> class, you can spin up the Workbench REPL, use the class and test out all the methods. Try it out! Workbench REPL</p> Using Workbench REPL<pre><code>meta = Meta()\nmodel_df = meta.models()\nmodel_df\n               Model Group   Health Owner  ...             Input     Status                Description\n0      wine-classification  healthy     -  ...     wine_features  Completed  Wine Classification Model\n1  abalone-regression-full  healthy     -  ...  abalone_features  Completed   Abalone Regression Model\n2       abalone-regression  healthy     -  ...  abalone_features  Completed   Abalone Regression Model\n\n[3 rows x 10 columns]\n</code></pre> <p>List the Models in AWS</p> meta_list_models.py<pre><code>from workbench.api import Meta\n\n# Create our Meta Class and get a list of our Models\nmeta = Meta()\nmodel_df = meta.models()\n\nprint(f\"Number of Models: {len(model_df)}\")\nprint(model_df)\n\n# Get more details data on the Models\nmodel_names = model_df[\"Model Group\"].tolist()\nfor name in model_names:\n    pprint(meta.model(name))\n</code></pre> <p>Output</p> <pre><code>Number of Models: 3\n               Model Group   Health Owner  ...             Input     Status                Description\n0      wine-classification  healthy     -  ...     wine_features  Completed  Wine Classification Model\n1  abalone-regression-full  healthy     -  ...  abalone_features  Completed   Abalone Regression Model\n2       abalone-regression  healthy     -  ...  abalone_features  Completed   Abalone Regression Model\n\n[3 rows x 10 columns]\nwine-classification\nabalone-regression-full\nabalone-regression\n</code></pre> <p>Getting Model Performance Metrics</p> meta_model_metrics.py<pre><code>from workbench.api import Meta\n\n# Create our Meta Class and get a list of our Models\nmeta = Meta()\nmodel_df = meta.models()\n\nprint(f\"Number of Models: {len(model_df)}\")\nprint(model_df)\n\n# Get more details data on the Models\nmodel_names = model_df[\"Model Group\"].tolist()\nfor name in model_names[:5]:\n    model_details = meta.model(name)\n    print(f\"\\n\\nModel: {name}\")\n    performance_metrics = model_details[\"workbench_meta\"][\"workbench_inference_metrics\"]\n    print(f\"\\tPerformance Metrics: {performance_metrics}\")\n</code></pre> <p>Output</p> <pre><code>wine-classification\n    ARN: arn:aws:sagemaker:us-west-2:507740646243:model-package-group/wine-classification\n    Description: Wine Classification Model\n    Tags: wine::classification\n    Performance Metrics:\n        [{'wine_class': 'TypeA', 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'support': 12}, {'wine_class': 'TypeB', 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'support': 14}, {'wine_class': 'TypeC', 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'support': 9}]\n\nabalone-regression\n    ARN: arn:aws:sagemaker:us-west-2:507740646243:model-package-group/abalone-regression\n    Description: Abalone Regression Model\n    Tags: abalone::regression\n    Performance Metrics:\n        [{'MAE': 1.64, 'RMSE': 2.246, 'R2': 0.502, 'MAPE': 16.393, 'MedAE': 1.209, 'NumRows': 834}]\n</code></pre> <p>List the Endpoints in AWS</p> meta_list_endpoints.py<pre><code>from pprint import pprint\nfrom workbench.api import Meta\n\n# Create our Meta Class and get a list of our Endpoints\nmeta = Meta()\nendpoint_df = meta.endpoints()\nprint(f\"Number of Endpoints: {len(endpoint_df)}\")\nprint(endpoint_df)\n\n# Get more details data on the Endpoints\nendpoint_names = endpoint_df[\"Name\"].tolist()\nfor name in endpoint_names:\n    pprint(meta.endpoint(name))\n</code></pre> <p>Output</p> <pre><code>Number of Endpoints: 2\n                      Name   Health            Instance           Created  ...     Status     Variant Capture Samp(%)\n0  wine-classification-end  healthy  Serverless (2GB/5)  2024-03-23 23:09  ...  InService  AllTraffic   False       -\n1   abalone-regression-end  healthy  Serverless (2GB/5)  2024-03-23 21:11  ...  InService  AllTraffic   False       -\n\n[2 rows x 10 columns]\nwine-classification-end\n&lt;lots of details about endpoints&gt;\n</code></pre> <p>Not Finding some particular AWS Data?</p> <p>The Workbench Meta API Class also has <code>(details=True)</code> arguments, so make sure to check those out.</p>"},{"location":"api_classes/model/","title":"Model","text":"<p>Model Examples</p> <p>Examples of using the Model Class are in the Examples section at the bottom of this page. AWS Model setup and deployment are quite complicated to do manually but the Workbench Model Class makes it a breeze!</p> <p>Model: Manages AWS Model Package/Group creation and management.</p> <p>Models are automatically set up and provisioned for deployment into AWS. Models can be viewed in the AWS Sagemaker interfaces or in the Workbench Dashboard UI, which provides additional model details and performance metrics</p>"},{"location":"api_classes/model/#workbench.api.model.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>ModelCore</code></p> <p>Model: Workbench Model API Class.</p> Common Usage <pre><code>my_model = Model(name)\nmy_model.details()\nmy_model.to_endpoint()\n</code></pre> Source code in <code>src/workbench/api/model.py</code> <pre><code>class Model(ModelCore):\n    \"\"\"Model: Workbench Model API Class.\n\n    Common Usage:\n        ```python\n        my_model = Model(name)\n        my_model.details()\n        my_model.to_endpoint()\n        ```\n    \"\"\"\n\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the Model Details.\n\n        Returns:\n            dict: A dictionary of details about the Model\n        \"\"\"\n        return super().details(**kwargs)\n\n    def to_endpoint(\n        self,\n        name: str = None,\n        tags: list = None,\n        serverless: bool = True,\n        mem_size: int = 2048,\n        max_concurrency: int = 5,\n        instance: str = None,\n        data_capture: bool = False,\n    ) -&gt; Endpoint:\n        \"\"\"Create an Endpoint from the Model.\n\n        Args:\n            name (str): Set the name for the endpoint. If not specified, an automatic name will be generated\n            tags (list): Set the tags for the endpoint. If not specified automatic tags will be generated.\n            serverless (bool): Set the endpoint to be serverless (default: True)\n            mem_size (int): The memory size for the Endpoint in MB (default: 2048)\n            max_concurrency (int): The maximum concurrency for the Endpoint (default: 5)\n            instance (str): The instance type for Realtime Endpoints (default: None = auto-select based on model)\n            data_capture (bool): Enable data capture for the Endpoint (default: False)\n\n        Returns:\n            Endpoint: The Endpoint created from the Model\n        \"\"\"\n\n        # Ensure the endpoint_name is valid\n        if name:\n            Artifact.is_name_valid(name, delimiter=\"-\", lower_case=False)\n\n        # If the endpoint_name wasn't given generate it\n        else:\n            name = self.name.replace(\"_features\", \"\") + \"\"\n            name = Artifact.generate_valid_name(name, delimiter=\"-\")\n\n        # Create the Endpoint Tags\n        tags = [name] if tags is None else tags\n\n        # Create an Endpoint from the Model\n        model_to_endpoint = ModelToEndpoint(self.name, name, serverless=serverless, instance=instance)\n        model_to_endpoint.set_output_tags(tags)\n        model_to_endpoint.transform(\n            mem_size=mem_size,\n            max_concurrency=max_concurrency,\n            data_capture=data_capture,\n        )\n\n        # Set the Endpoint Owner and Return the Endpoint\n        end = Endpoint(name)\n        end.set_owner(self.get_owner())\n        return end\n\n    def prox_model(self, include_all_columns: bool = False) -&gt; FeatureSpaceProximity:\n        \"\"\"Create a local Proximity Model for this Model\n\n        Args:\n            include_all_columns (bool): Include all DataFrame columns in results (default: False)\n\n        Returns:\n            FeatureSpaceProximity: A local FeatureSpaceProximity Model\n        \"\"\"\n        return proximity_model_local(self, include_all_columns=include_all_columns)\n\n    def fp_prox_model(\n        self,\n        include_all_columns: bool = False,\n        radius: int = 2,\n        n_bits: int = 1024,\n        counts: bool = False,\n    ) -&gt; FingerprintProximity:\n        \"\"\"Create a local Fingerprint Proximity Model for this Model\n\n        Args:\n            include_all_columns (bool): Include all DataFrame columns in results (default: False)\n            radius (int): Morgan fingerprint radius (default: 2)\n            n_bits (int): Number of bits for the fingerprint (default: 1024)\n            counts (bool): Use count fingerprints instead of binary (default: False)\n\n        Returns:\n            FingerprintProximity: A local FingerprintProximity Model\n        \"\"\"\n        return fingerprint_prox_model_local(\n            self, include_all_columns=include_all_columns, radius=radius, n_bits=n_bits, counts=counts\n        )\n\n    def noise_model(self) -&gt; NoiseModel:\n        \"\"\"Create a local Noise Model for this Model\n\n        Returns:\n            NoiseModel: A local Noise Model\n        \"\"\"\n        return noise_model_local(self)\n\n    def cleanlab_model(self) -&gt; CleanlabModels:\n        \"\"\"Create a CleanlabModels instance for this Model's training data.\n\n        Returns:\n            CleanlabModels: Factory providing access to CleanLearning and Datalab models. Use get_label_issues() to get\n                a DataFrame with id_column, label_quality, predicted_label, given_label, is_label_issue.\n        \"\"\"\n        return cleanlab_model_local(self)\n</code></pre>"},{"location":"api_classes/model/#workbench.api.model.Model.cleanlab_model","title":"<code>cleanlab_model()</code>","text":"<p>Create a CleanlabModels instance for this Model's training data.</p> <p>Returns:</p> Name Type Description <code>CleanlabModels</code> <code>CleanlabModels</code> <p>Factory providing access to CleanLearning and Datalab models. Use get_label_issues() to get a DataFrame with id_column, label_quality, predicted_label, given_label, is_label_issue.</p> Source code in <code>src/workbench/api/model.py</code> <pre><code>def cleanlab_model(self) -&gt; CleanlabModels:\n    \"\"\"Create a CleanlabModels instance for this Model's training data.\n\n    Returns:\n        CleanlabModels: Factory providing access to CleanLearning and Datalab models. Use get_label_issues() to get\n            a DataFrame with id_column, label_quality, predicted_label, given_label, is_label_issue.\n    \"\"\"\n    return cleanlab_model_local(self)\n</code></pre>"},{"location":"api_classes/model/#workbench.api.model.Model.details","title":"<code>details(**kwargs)</code>","text":"<p>Retrieve the Model Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the Model</p> Source code in <code>src/workbench/api/model.py</code> <pre><code>def details(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the Model Details.\n\n    Returns:\n        dict: A dictionary of details about the Model\n    \"\"\"\n    return super().details(**kwargs)\n</code></pre>"},{"location":"api_classes/model/#workbench.api.model.Model.fp_prox_model","title":"<code>fp_prox_model(include_all_columns=False, radius=2, n_bits=1024, counts=False)</code>","text":"<p>Create a local Fingerprint Proximity Model for this Model</p> <p>Parameters:</p> Name Type Description Default <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in results (default: False)</p> <code>False</code> <code>radius</code> <code>int</code> <p>Morgan fingerprint radius (default: 2)</p> <code>2</code> <code>n_bits</code> <code>int</code> <p>Number of bits for the fingerprint (default: 1024)</p> <code>1024</code> <code>counts</code> <code>bool</code> <p>Use count fingerprints instead of binary (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FingerprintProximity</code> <code>FingerprintProximity</code> <p>A local FingerprintProximity Model</p> Source code in <code>src/workbench/api/model.py</code> <pre><code>def fp_prox_model(\n    self,\n    include_all_columns: bool = False,\n    radius: int = 2,\n    n_bits: int = 1024,\n    counts: bool = False,\n) -&gt; FingerprintProximity:\n    \"\"\"Create a local Fingerprint Proximity Model for this Model\n\n    Args:\n        include_all_columns (bool): Include all DataFrame columns in results (default: False)\n        radius (int): Morgan fingerprint radius (default: 2)\n        n_bits (int): Number of bits for the fingerprint (default: 1024)\n        counts (bool): Use count fingerprints instead of binary (default: False)\n\n    Returns:\n        FingerprintProximity: A local FingerprintProximity Model\n    \"\"\"\n    return fingerprint_prox_model_local(\n        self, include_all_columns=include_all_columns, radius=radius, n_bits=n_bits, counts=counts\n    )\n</code></pre>"},{"location":"api_classes/model/#workbench.api.model.Model.noise_model","title":"<code>noise_model()</code>","text":"<p>Create a local Noise Model for this Model</p> <p>Returns:</p> Name Type Description <code>NoiseModel</code> <code>NoiseModel</code> <p>A local Noise Model</p> Source code in <code>src/workbench/api/model.py</code> <pre><code>def noise_model(self) -&gt; NoiseModel:\n    \"\"\"Create a local Noise Model for this Model\n\n    Returns:\n        NoiseModel: A local Noise Model\n    \"\"\"\n    return noise_model_local(self)\n</code></pre>"},{"location":"api_classes/model/#workbench.api.model.Model.prox_model","title":"<code>prox_model(include_all_columns=False)</code>","text":"<p>Create a local Proximity Model for this Model</p> <p>Parameters:</p> Name Type Description Default <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in results (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FeatureSpaceProximity</code> <code>FeatureSpaceProximity</code> <p>A local FeatureSpaceProximity Model</p> Source code in <code>src/workbench/api/model.py</code> <pre><code>def prox_model(self, include_all_columns: bool = False) -&gt; FeatureSpaceProximity:\n    \"\"\"Create a local Proximity Model for this Model\n\n    Args:\n        include_all_columns (bool): Include all DataFrame columns in results (default: False)\n\n    Returns:\n        FeatureSpaceProximity: A local FeatureSpaceProximity Model\n    \"\"\"\n    return proximity_model_local(self, include_all_columns=include_all_columns)\n</code></pre>"},{"location":"api_classes/model/#workbench.api.model.Model.to_endpoint","title":"<code>to_endpoint(name=None, tags=None, serverless=True, mem_size=2048, max_concurrency=5, instance=None, data_capture=False)</code>","text":"<p>Create an Endpoint from the Model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Set the name for the endpoint. If not specified, an automatic name will be generated</p> <code>None</code> <code>tags</code> <code>list</code> <p>Set the tags for the endpoint. If not specified automatic tags will be generated.</p> <code>None</code> <code>serverless</code> <code>bool</code> <p>Set the endpoint to be serverless (default: True)</p> <code>True</code> <code>mem_size</code> <code>int</code> <p>The memory size for the Endpoint in MB (default: 2048)</p> <code>2048</code> <code>max_concurrency</code> <code>int</code> <p>The maximum concurrency for the Endpoint (default: 5)</p> <code>5</code> <code>instance</code> <code>str</code> <p>The instance type for Realtime Endpoints (default: None = auto-select based on model)</p> <code>None</code> <code>data_capture</code> <code>bool</code> <p>Enable data capture for the Endpoint (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Endpoint</code> <code>Endpoint</code> <p>The Endpoint created from the Model</p> Source code in <code>src/workbench/api/model.py</code> <pre><code>def to_endpoint(\n    self,\n    name: str = None,\n    tags: list = None,\n    serverless: bool = True,\n    mem_size: int = 2048,\n    max_concurrency: int = 5,\n    instance: str = None,\n    data_capture: bool = False,\n) -&gt; Endpoint:\n    \"\"\"Create an Endpoint from the Model.\n\n    Args:\n        name (str): Set the name for the endpoint. If not specified, an automatic name will be generated\n        tags (list): Set the tags for the endpoint. If not specified automatic tags will be generated.\n        serverless (bool): Set the endpoint to be serverless (default: True)\n        mem_size (int): The memory size for the Endpoint in MB (default: 2048)\n        max_concurrency (int): The maximum concurrency for the Endpoint (default: 5)\n        instance (str): The instance type for Realtime Endpoints (default: None = auto-select based on model)\n        data_capture (bool): Enable data capture for the Endpoint (default: False)\n\n    Returns:\n        Endpoint: The Endpoint created from the Model\n    \"\"\"\n\n    # Ensure the endpoint_name is valid\n    if name:\n        Artifact.is_name_valid(name, delimiter=\"-\", lower_case=False)\n\n    # If the endpoint_name wasn't given generate it\n    else:\n        name = self.name.replace(\"_features\", \"\") + \"\"\n        name = Artifact.generate_valid_name(name, delimiter=\"-\")\n\n    # Create the Endpoint Tags\n    tags = [name] if tags is None else tags\n\n    # Create an Endpoint from the Model\n    model_to_endpoint = ModelToEndpoint(self.name, name, serverless=serverless, instance=instance)\n    model_to_endpoint.set_output_tags(tags)\n    model_to_endpoint.transform(\n        mem_size=mem_size,\n        max_concurrency=max_concurrency,\n        data_capture=data_capture,\n    )\n\n    # Set the Endpoint Owner and Return the Endpoint\n    end = Endpoint(name)\n    end.set_owner(self.get_owner())\n    return end\n</code></pre>"},{"location":"api_classes/model/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Create a Model from a FeatureSet</p> featureset_to_model.py<pre><code>from workbench.api.feature_set import FeatureSet\nfrom workbench.api.model import ModelType\nfrom pprint import pprint\n\n# Grab a FeatureSet\nmy_features = FeatureSet(\"test_features\")\n\n# Create a Model from the FeatureSet\n# Note: ModelTypes can be CLASSIFIER, REGRESSOR (XGBoost is default)\nmy_model = my_features.to_model(name=\"test-model\",\n                                model_type=ModelType.REGRESSOR, \n                                target_column=\"iq_score\")\npprint(my_model.details())\n</code></pre> <p>Output</p> <pre><code>{'approval_status': 'Approved',\n 'content_types': ['text/csv'],\n ...\n 'inference_types': ['ml.t2.medium'],\n 'input': 'test_features',\n 'model_metrics':   metric_name  value\n                0        RMSE  7.924\n                1         MAE  6.554,\n                2          R2  0.604,\n 'regression_predictions':       iq_score  prediction\n                            0   136.519012  139.964460\n                            1   133.616974  130.819950\n                            2   122.495415  124.967834\n                            3   133.279510  121.010284\n                            4   127.881073  113.825005\n    ...\n 'response_types': ['text/csv'],\n 'workbench_tags': ['test-model'],\n 'shapley_values': None,\n 'size': 0.0,\n 'status': 'Completed',\n 'transform_types': ['ml.m5.large'],\n 'name': 'test-model',\n 'version': 1}\n</code></pre> <p>Use a specific Scikit-Learn Model</p> <p>featureset_to_knn.py<pre><code>from workbench.api.feature_set import FeatureSet\nfrom pprint import pprint\n\n# Grab a FeatureSet\nmy_features = FeatureSet(\"abalone_features\")\n\n# Transform FeatureSet into KNN Regression Model\n# Note: model_class can be any sckit-learn model \n#  \"KNeighborsRegressor\", \"BayesianRidge\",\n#  \"GaussianNB\", \"AdaBoostClassifier\", etc\nmy_model = my_features.to_model(\n    model_class=\"KNeighborsRegressor\",\n    model_import_str=\"from sklearn.neighbors import KNeighborsRegressor\",\n    target_column=\"class_number_of_rings\",\n    name=\"abalone-knn-reg\",\n    description=\"Abalone KNN Regression\",\n    tags=[\"abalone\", \"knn\"],\n    train_all_data=True,\n)\npprint(my_model.details())\n</code></pre> Another Scikit-Learn Example</p> featureset_to_rfc.py<pre><code>from workbench.api.feature_set import FeatureSet\nfrom pprint import pprint\n\n# Grab a FeatureSet\nmy_features = FeatureSet(\"wine_features\")\n\n# Using a Scikit-Learn Model\n# Note: model_class can be any sckit-learn model (\"KMeans\", \"BayesianRidge\",\n#       \"GaussianNB\", \"AdaBoostClassifier\", \"Ridge, \"Lasso\", \"SVC\", \"SVR\", etc...)\nmy_model = my_features.to_model(\n    model_class=\"RandomForestClassifier\",\n    model_import_str=\"from sklearn.ensemble import RandomForestClassifier\",\n    target_column=\"wine_class\",\n    name=\"wine-rfc-class\",\n    description=\"Wine RandomForest Classification\",\n    tags=[\"wine\", \"rfc\"]\n)\npprint(my_model.details())\n</code></pre> <p>Create an Endpoint from a Model</p> <p>Endpoint Costs</p> <p>Serverless endpoints are a great option, they have no AWS charges when not running. A realtime endpoint has less latency (no cold start) but AWS charges an hourly fee which can add up quickly!</p> model_to_endpoint.py<pre><code>from workbench.api.model import Model\n\n# Grab the abalone regression Model\nmodel = Model(\"abalone-regression\")\n\n# By default, an Endpoint is serverless, you can\n# make a realtime endpoint with serverless=False\nmodel.to_endpoint(name=\"abalone-regression-end\",\n                  tags=[\"abalone\", \"regression\"],\n                  serverless=True)\n</code></pre> <p>Model Health Check and Metrics</p> model_metrics.py<pre><code>from workbench.api.model import Model\n\n# Grab the abalone-regression Model\nmodel = Model(\"abalone-regression\")\n\n# Perform a health check on the model\n# Note: The health_check() method returns 'issues' if there are any\n#       problems, so if there are no issues, the model is healthy\nhealth_issues = model.health_check()\nif not health_issues:\n    print(\"Model is Healthy\")\nelse:\n    print(\"Model has issues\")\n    print(health_issues)\n\n# Get the model metrics and regression predictions\nprint(model.model_metrics())\nprint(model.regression_predictions())\n</code></pre> <p>Output</p> <pre><code>Model is Healthy\n  metric_name  value\n0        RMSE  2.190\n1         MAE  1.544\n2          R2  0.504\n\n     class_number_of_rings  prediction\n0                        9    8.648378\n1                       11    9.717787\n2                       11   10.933070\n3                       10    9.899738\n4                        9   10.014504\n..                     ...         ...\n495                     10   10.261657\n496                      9   10.788254\n497                     13    7.779886\n498                     12   14.718514\n499                     13   10.637320\n</code></pre>"},{"location":"api_classes/model/#workbench-ui","title":"Workbench UI","text":"<p>Running these few lines of code creates an AWS Model Package Group and an AWS Model Package. These model artifacts can be viewed in the Sagemaker Console/Notebook interfaces or in the Workbench Dashboard UI.</p> Workbench Dashboard: Models <p>Not Finding a particular method?</p> <p>The Workbench API Classes use the 'Core' Classes Internally, so for an extensive listing of all the methods available please take a deep dive into: Workbench Core Classes</p>"},{"location":"api_classes/monitor/","title":"Monitor","text":"<p>Monitor Examples</p> <p>Examples of using the Monitor class are listed at the bottom of this page Examples.</p> <p>Monitor: Manages AWS Endpoint Monitor creation and deployment. Endpoints Monitors are set up and provisioned for deployment into AWS. Monitors can be viewed in the AWS Sagemaker interfaces or in the Workbench Dashboard UI, which provides additional monitor details and performance metrics</p>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor","title":"<code>Monitor</code>","text":"<p>               Bases: <code>MonitorCore</code></p> <p>Monitor: Workbench Monitor API Class</p> Common Usage <pre><code>mon = Endpoint(name).monitor()  # Pull from endpoint OR\nmon = Monitor(name)                 # Create using Endpoint Name\nmon.summary()\nmon.details()\n\n# One time setup methods\nmon.enable_data_capture()\nmon.create_baseline()\nmon.create_monitoring_schedule()\n\n# Pull information from the monitor\nbaseline_df = mon.get_baseline()\nconstraints_df = mon.get_constraints()\nstats_df = mon.get_statistics()\n</code></pre> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>class Monitor(MonitorCore):\n    \"\"\"Monitor: Workbench Monitor API Class\n\n    Common Usage:\n       ```\n       mon = Endpoint(name).monitor()  # Pull from endpoint OR\n       mon = Monitor(name)                 # Create using Endpoint Name\n       mon.summary()\n       mon.details()\n\n       # One time setup methods\n       mon.enable_data_capture()\n       mon.create_baseline()\n       mon.create_monitoring_schedule()\n\n       # Pull information from the monitor\n       baseline_df = mon.get_baseline()\n       constraints_df = mon.get_constraints()\n       stats_df = mon.get_statistics()\n       ```\n    \"\"\"\n\n    def summary(self) -&gt; dict:\n        \"\"\"Monitor Summary\n\n        Returns:\n            dict: A dictionary of summary information about the Monitor\n        \"\"\"\n        return super().summary()\n\n    def details(self) -&gt; dict:\n        \"\"\"Monitor Details\n\n        Returns:\n            dict: A dictionary of details about the Monitor\n        \"\"\"\n        return super().details()\n\n    def enable_data_capture(self, capture_percentage=100):\n        \"\"\"\n        Enable data capture configuration for this Monitor/endpoint.\n\n        Args:\n            capture_percentage (int): Percentage of data to capture. Defaults to 100.\n        \"\"\"\n        super().enable_data_capture(capture_percentage)\n\n    def create_baseline(self, recreate: bool = False):\n        \"\"\"Code to create a baseline for monitoring\n\n        Args:\n            recreate (bool): If True, recreate the baseline even if it already exists\n\n        Notes:\n            This will create/write three files to the baseline_dir:\n            - baseline.csv\n            - constraints.json\n            - statistics.json\n        \"\"\"\n        super().create_baseline(recreate)\n\n    def create_monitoring_schedule(self, schedule: str = \"hourly\"):\n        \"\"\"\n        Sets up the monitoring schedule for the model endpoint.\n\n        Args:\n            schedule (str): The schedule for the monitoring job (hourly or daily, defaults to hourly).\n        \"\"\"\n        super().create_monitoring_schedule(schedule)\n\n    def get_baseline(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Code to get the baseline CSV from the S3 baseline directory\n\n        Returns:\n            pd.DataFrame: The baseline CSV as a DataFrame (None if it doesn't exist)\n        \"\"\"\n        return super().get_baseline()\n\n    def get_constraints(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Code to get the constraints from the baseline\n\n        Returns:\n           pd.DataFrame: The constraints from the baseline (constraints.json) (None if it doesn't exist)\n        \"\"\"\n        return super().get_constraints()\n\n    def get_statistics(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Code to get the statistics from the baseline\n\n        Returns:\n            pd.DataFrame: The statistics from the baseline (statistics.json) (None if it doesn't exist)\n        \"\"\"\n        return super().get_statistics()\n</code></pre>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor.create_baseline","title":"<code>create_baseline(recreate=False)</code>","text":"<p>Code to create a baseline for monitoring</p> <p>Parameters:</p> Name Type Description Default <code>recreate</code> <code>bool</code> <p>If True, recreate the baseline even if it already exists</p> <code>False</code> Notes <p>This will create/write three files to the baseline_dir: - baseline.csv - constraints.json - statistics.json</p> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>def create_baseline(self, recreate: bool = False):\n    \"\"\"Code to create a baseline for monitoring\n\n    Args:\n        recreate (bool): If True, recreate the baseline even if it already exists\n\n    Notes:\n        This will create/write three files to the baseline_dir:\n        - baseline.csv\n        - constraints.json\n        - statistics.json\n    \"\"\"\n    super().create_baseline(recreate)\n</code></pre>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor.create_monitoring_schedule","title":"<code>create_monitoring_schedule(schedule='hourly')</code>","text":"<p>Sets up the monitoring schedule for the model endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>str</code> <p>The schedule for the monitoring job (hourly or daily, defaults to hourly).</p> <code>'hourly'</code> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>def create_monitoring_schedule(self, schedule: str = \"hourly\"):\n    \"\"\"\n    Sets up the monitoring schedule for the model endpoint.\n\n    Args:\n        schedule (str): The schedule for the monitoring job (hourly or daily, defaults to hourly).\n    \"\"\"\n    super().create_monitoring_schedule(schedule)\n</code></pre>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor.details","title":"<code>details()</code>","text":"<p>Monitor Details</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the Monitor</p> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>def details(self) -&gt; dict:\n    \"\"\"Monitor Details\n\n    Returns:\n        dict: A dictionary of details about the Monitor\n    \"\"\"\n    return super().details()\n</code></pre>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor.enable_data_capture","title":"<code>enable_data_capture(capture_percentage=100)</code>","text":"<p>Enable data capture configuration for this Monitor/endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>capture_percentage</code> <code>int</code> <p>Percentage of data to capture. Defaults to 100.</p> <code>100</code> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>def enable_data_capture(self, capture_percentage=100):\n    \"\"\"\n    Enable data capture configuration for this Monitor/endpoint.\n\n    Args:\n        capture_percentage (int): Percentage of data to capture. Defaults to 100.\n    \"\"\"\n    super().enable_data_capture(capture_percentage)\n</code></pre>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor.get_baseline","title":"<code>get_baseline()</code>","text":"<p>Code to get the baseline CSV from the S3 baseline directory</p> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: The baseline CSV as a DataFrame (None if it doesn't exist)</p> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>def get_baseline(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Code to get the baseline CSV from the S3 baseline directory\n\n    Returns:\n        pd.DataFrame: The baseline CSV as a DataFrame (None if it doesn't exist)\n    \"\"\"\n    return super().get_baseline()\n</code></pre>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor.get_constraints","title":"<code>get_constraints()</code>","text":"<p>Code to get the constraints from the baseline</p> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: The constraints from the baseline (constraints.json) (None if it doesn't exist)</p> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>def get_constraints(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Code to get the constraints from the baseline\n\n    Returns:\n       pd.DataFrame: The constraints from the baseline (constraints.json) (None if it doesn't exist)\n    \"\"\"\n    return super().get_constraints()\n</code></pre>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Code to get the statistics from the baseline</p> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: The statistics from the baseline (statistics.json) (None if it doesn't exist)</p> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>def get_statistics(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Code to get the statistics from the baseline\n\n    Returns:\n        pd.DataFrame: The statistics from the baseline (statistics.json) (None if it doesn't exist)\n    \"\"\"\n    return super().get_statistics()\n</code></pre>"},{"location":"api_classes/monitor/#workbench.api.monitor.Monitor.summary","title":"<code>summary()</code>","text":"<p>Monitor Summary</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of summary information about the Monitor</p> Source code in <code>src/workbench/api/monitor.py</code> <pre><code>def summary(self) -&gt; dict:\n    \"\"\"Monitor Summary\n\n    Returns:\n        dict: A dictionary of summary information about the Monitor\n    \"\"\"\n    return super().summary()\n</code></pre>"},{"location":"api_classes/monitor/#examples","title":"Examples","text":"<p>Initial Setup of the Endpoint Monitor</p> monitor_setup.py<pre><code>from workbench.api.monitor import Monitor\n\n# Create an Endpoint Monitor Class and perform initial Setup\nendpoint_name = \"abalone-regression-end-rt\"\nmon = Monitor(endpoint_name)\n\n# Add data capture to the endpoint\nmon.enable_data_capture(capture_percentage=100)\n\n# Create a baseline for monitoring\nmon.create_baseline()\n\n# Set up the monitoring schedule\nmon.create_monitoring_schedule(schedule=\"hourly\")\n</code></pre> <p>Pulling Information from an Existing Monitor</p> monitor_usage.py<pre><code>from workbench.api.monitor import Monitor\nfrom workbench.api.endpoint import Endpoint\n\n# Construct a Monitor Class in one of Two Ways\nmon = Endpoint(\"abalone-regression-end-rt\").monitor()\nmon = Monitor(\"abalone-regression-end-rt\")\n\n# Check the summary and details of the monitoring class\nmon.summary()\nmon.details()\n\n# Check the baseline outputs (baseline, constraints, statistics)\nbase_df = mon.get_baseline()\nbase_df.head()\n\nconstraints_df = mon.get_constraints()\nconstraints_df.head()\n\nstatistics_df = mon.get_statistics()\nstatistics_df.head()\n\n# Get the latest data capture (inputs and outputs)\ninput_df, output_df = mon.get_latest_data_capture()\ninput_df.head()\noutput_df.head()\n</code></pre>"},{"location":"api_classes/monitor/#workbench-ui","title":"Workbench UI","text":"<p>Running these few lines of code creates and deploys an AWS Endpoint Monitor. The Monitor status and outputs can be viewed in the Sagemaker Console interfaces or in the Workbench Dashboard UI. Workbench will use the monitor to track various metrics including Data Quality, Model Bias, etc...</p> Workbench Dashboard: Endpoints <p>Not Finding a particular method?</p> <p>The Workbench API Classes use the 'Core' Classes Internally, so for an extensive listing of all the methods available please take a deep dive into: Workbench Core Classes</p>"},{"location":"api_classes/overview/","title":"Overview","text":"<p>Just Getting Started?</p> <p>You're in the right place, the Workbench API Classes are the best way to get started with Workbench!</p>"},{"location":"api_classes/overview/#welcome-to-the-workbench-api-classes","title":"Welcome to the Workbench API Classes","text":"<p>These classes provide high-level APIs for the Workbench package, they enable your team to build full AWS Machine Learning Pipelines. They handle all the details around updating and managing a complex set of AWS Services. Each class provides an essential component of the overall ML Pipline. Simply combine the classes to build production ready, AWS powered, machine learning pipelines. </p> <ul> <li>DataSource: Manages AWS Data Catalog and Athena</li> <li>FeatureSet: Manages AWS Feature Store and Feature Groups</li> <li>Model: Manages the training and deployment of AWS Model Groups and Packages</li> <li>Endpoint: Manages the deployment and invocations/inference on AWS Endpoints</li> <li>Monitor: Manages the setup and deployment of AWS Endpoint Monitors</li> </ul> <p></p>"},{"location":"api_classes/overview/#example-ml-pipline","title":"Example ML Pipline","text":"full_ml_pipeline.py<pre><code>from workbench.api import DataSource, FeatureSet, Model, ModelType, Endpoint\n\n# Create the abalone_data DataSource\nds = DataSource(\"s3://workbench-public-data/common/abalone.csv\")\n\n# Now create a FeatureSet\nds.to_features(\"abalone_features\")\n\n# Create the abalone_regression Model\nfs = FeatureSet(\"abalone_features\")\nfs.to_model(\n    name=\"abalone-regression\",\n    model_type=ModelType.REGRESSOR,\n    target_column=\"class_number_of_rings\",\n    tags=[\"abalone\", \"regression\"],\n    description=\"Abalone Regression Model\",\n)\n\n# Create the abalone_regression Endpoint\nmodel = Model(\"abalone-regression\")\nmodel.to_endpoint(name=\"abalone-regression-end\", tags=[\"abalone\", \"regression\"])\n\n# Now we'll run inference on the endpoint\nendpoint = Endpoint(\"abalone-regression-end\")\n\n# Run inference on the Endpoint\nresults = endpoint.auto_inference()\nprint(results[[\"class_number_of_rings\", \"prediction\"]])\n</code></pre> <p>Output</p> <pre><code>Processing...\n     class_number_of_rings  prediction\n0                       12   10.477794\n1                       11    11.11835\n2                       14   13.605763\n3                       12   11.744759\n4                       17    15.55189\n..                     ...         ...\n826                      7    7.981503\n827                     11   11.246113\n828                      9    9.592911\n829                      6    6.129388\n830                      8    7.628252\n</code></pre> <p>Full AWS ML Pipeline Achievement Unlocked!</p> <p>Bing! You just built and deployed a full AWS Machine Learning Pipeline. You can now use the Workbench Dashboard web interface to inspect your AWS artifacts. A comprehensive set of Exploratory Data Analysis techniques and Model Performance Metrics are available for your entire team to review, inspect and interact with.</p> <p></p> <p>Examples</p> <p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p>"},{"location":"api_classes/parameter_store/","title":"Workbench Parameter Storage","text":"<p>The Parameter Store is a great place to publish data (strings, lists, dictionaries, etc). The service is provided on all AWS accounts and allows ML pipelines to create, store, and read data. </p>"},{"location":"api_classes/parameter_store/#bypassing-the-4k-limit","title":"Bypassing the 4k Limit","text":"<p>AWS Parameter Storage has a 4k limit on values, the Workbench class bypasses this limit by detecting large values (strings, data, whatever) and compressing those on the fly. The decompressing is also handled automatically.</p>"},{"location":"api_classes/parameter_store/#examples","title":"Examples","text":"<p>These example show how to use the <code>ParameterStore()</code> class to list, add, and get parameters from the AWS Parameter Store Service.</p> <p>Workbench REPL</p> <p>If you'd like to experiment with listing, adding, and getting data with the <code>ParameterStore()</code> class, you can spin up the Workbench REPL, use the class and test out all the methods. Try it out! Workbench REPL</p> Using ParameterStore<pre><code>from workbench.api import ParameterStore\nparams = ParameterStore()\n\n# List Parameters\nparams.list()\n\n['/workbench/abalone_info',\n '/workbench/my_data',\n '/workbench/test',\n '/workbench/pipelines/my_pipeline']\n\n# Add Key\nparams.upsert(\"key\", \"value\")\nvalue = params.get(\"key\")\n\n# Add any data (lists, dictionaries, etc..)\nmy_data = {\"key\": \"value\", \"number\": 4.2, \"list\": [1,2,3]}\nparams.upsert(\"my_data\", my_data)\n\n# Retrieve data\nreturn_value = params.get(\"my_data\")\npprint(return_value)\n\n{'key': 'value', 'list': [1, 2, 3], 'number': 4.2}\n\n# Delete parameters\nparam_store.delete(\"my_data\")\n</code></pre> <p><code>list()</code> not showing ALL parameters?</p> <p>If you want access to ALL the parameters in the parameter store set <code>prefix=None</code> and everything will show up.</p> <pre><code>params = ParameterStore(prefix=None)\nparams.list()\n&lt;all the keys&gt;\n</code></pre> <p>ParameterStore: Manages Workbench parameters in a Cloud Based Parameter Store.</p>"},{"location":"api_classes/parameter_store/#workbench.api.parameter_store.ParameterStore","title":"<code>ParameterStore</code>","text":"<p>               Bases: <code>ParameterStoreCore</code></p> <p>ParameterStore: Manages Workbench parameters in a Cloud Based Parameter Store.</p> Common Usage <pre><code>params = ParameterStore()\n\n# List Parameters\nparams.list()\n\n['/workbench/abalone_info',\n '/workbench/my_data',\n '/workbench/test',\n '/workbench/pipelines/my_pipeline']\n\n# Add Key\nparams.upsert(\"key\", \"value\")\nvalue = params.get(\"key\")\n\n# Add any data (lists, dictionaries, etc..)\nmy_data = {\"key\": \"value\", \"number\": 4.2, \"list\": [1,2,3]}\nparams.upsert(\"my_data\", my_data)\n\n# Retrieve data\nreturn_value = params.get(\"my_data\")\npprint(return_value)\n\n{'key': 'value', 'list': [1, 2, 3], 'number': 4.2}\n\n# Delete parameters\nparam_store.delete(\"my_data\")\n</code></pre> Source code in <code>src/workbench/api/parameter_store.py</code> <pre><code>class ParameterStore(ParameterStoreCore):\n    \"\"\"ParameterStore: Manages Workbench parameters in a Cloud Based Parameter Store.\n\n    Common Usage:\n        ```python\n        params = ParameterStore()\n\n        # List Parameters\n        params.list()\n\n        ['/workbench/abalone_info',\n         '/workbench/my_data',\n         '/workbench/test',\n         '/workbench/pipelines/my_pipeline']\n\n        # Add Key\n        params.upsert(\"key\", \"value\")\n        value = params.get(\"key\")\n\n        # Add any data (lists, dictionaries, etc..)\n        my_data = {\"key\": \"value\", \"number\": 4.2, \"list\": [1,2,3]}\n        params.upsert(\"my_data\", my_data)\n\n        # Retrieve data\n        return_value = params.get(\"my_data\")\n        pprint(return_value)\n\n        {'key': 'value', 'list': [1, 2, 3], 'number': 4.2}\n\n        # Delete parameters\n        param_store.delete(\"my_data\")\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"ParameterStore Init Method\"\"\"\n\n        # Initialize parent class\n        super().__init__()\n</code></pre>"},{"location":"api_classes/parameter_store/#workbench.api.parameter_store.ParameterStore.__init__","title":"<code>__init__()</code>","text":"<p>ParameterStore Init Method</p> Source code in <code>src/workbench/api/parameter_store.py</code> <pre><code>def __init__(self):\n    \"\"\"ParameterStore Init Method\"\"\"\n\n    # Initialize parent class\n    super().__init__()\n</code></pre>"},{"location":"api_classes/pipelines/","title":"Pipelines","text":"<p>Pipeline Examples</p> <p>Examples of using the Pipeline classes are listed at the bottom of this page Examples.</p> <p>Pipelines store sequences of Workbench transforms. So if you have a nightly ML workflow you can capture that as a Pipeline. Here's an example pipeline:</p> nightly_sol_pipeline_v1.json<pre><code>{\n    \"data_source\": {\n         \"name\": \"nightly_data\",\n         \"tags\": [\"solubility\", \"foo\"],\n         \"s3_input\": \"s3://blah/blah.csv\"\n    },\n    \"feature_set\": {\n          \"name\": \"nightly_features\",\n          \"tags\": [\"blah\", \"blah\"],\n          \"input\": \"nightly_data\"\n          \"schema\": \"mol_descriptors_v1\"\n    },\n    \"model\": {\n          \"name\": \u201cnightly_model\u201d,\n          \"tags\": [\"blah\", \"blah\"],\n          \"features\": [\"col1\", \"col2\"],\n          \"target\": \u201csol\u201d,\n          \"input\": \u201cnightly_features\u201d\n    \"endpoint\": {\n          ...\n}    \n</code></pre> <p>Pipeline: Manages the details around a Workbench Pipeline, including Execution</p>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.log","title":"<code>log = logging.getLogger('workbench')</code>  <code>module-attribute</code>","text":"<p>my_pipeline = Pipeline(\"aqsol_pipeline_v1\") my_pipeline.set_input(\"s3://workbench-public-data/comp_chem/aqsol_public_data.csv\") my_pipeline.execute_partial([\"model\", \"endpoint\"]) exit(0)</p>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline: Workbench Pipeline API Class</p> Common Usage <pre><code>my_pipeline = Pipeline(\"name\")\nmy_pipeline.details()\nmy_pipeline.execute()  # Execute entire pipeline\nmy_pipeline.execute_partial([\"data_source\", \"feature_set\"])\nmy_pipeline.execute_partial([\"model\", \"endpoint\"])\n</code></pre> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Pipeline: Workbench Pipeline API Class\n\n    Common Usage:\n        ```python\n        my_pipeline = Pipeline(\"name\")\n        my_pipeline.details()\n        my_pipeline.execute()  # Execute entire pipeline\n        my_pipeline.execute_partial([\"data_source\", \"feature_set\"])\n        my_pipeline.execute_partial([\"model\", \"endpoint\"])\n        ```\n    \"\"\"\n\n    def __init__(self, name: str):\n        \"\"\"Pipeline Init Method\"\"\"\n        self.log = logging.getLogger(\"workbench\")\n        self.name = name\n\n        # Spin up a Parameter Store for Pipelines\n        self.prefix = \"/workbench/pipelines\"\n        self.params = ParameterStore()\n        self.pipeline = self.params.get(f\"{self.prefix}/{self.name}\")\n\n    def summary(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the Pipeline Summary.\n\n        Returns:\n            dict: A dictionary of details about the Pipeline\n        \"\"\"\n        return self.pipeline\n\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the Pipeline Details.\n\n        Returns:\n            dict: A dictionary of details about the Pipeline\n        \"\"\"\n        return self.pipeline\n\n    def health_check(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the Pipeline Health Check.\n\n        Returns:\n            dict: A dictionary of health check details for the Pipeline\n        \"\"\"\n        return {}\n\n    def set_input(self, input: Union[str, pd.DataFrame], artifact: str = \"data_source\"):\n        \"\"\"Set the input for the Pipeline\n\n        Args:\n            input (Union[str, pd.DataFrame]): The input for the Pipeline\n            artifact (str): The artifact to set the input for (default: \"data_source\")\n        \"\"\"\n        self.pipeline[artifact][\"input\"] = input\n\n    def set_training_holdouts(self, id_column: str, holdout_ids: list[str]):\n        \"\"\"Set the input for the Pipeline\n\n        Args:\n            id_column (str): The column name of the unique identifier\n            holdout_ids (list[str]): The list of unique identifiers to hold out\n        \"\"\"\n        self.pipeline[\"feature_set\"][\"id_column\"] = id_column\n        self.pipeline[\"feature_set\"][\"holdout_ids\"] = holdout_ids\n\n    def execute(self):\n        \"\"\"Execute the entire Pipeline\n\n        Raises:\n            RunTimeException: If the pipeline execution fails in any way\n        \"\"\"\n        pipeline_executor = PipelineExecutor(self)\n        pipeline_executor.execute()\n\n    def execute_partial(self, subset: list):\n        \"\"\"Execute a partial Pipeline\n\n        Args:\n            subset (list): A subset of the pipeline to execute\n\n        Raises:\n            RunTimeException: If the pipeline execution fails in any way\n        \"\"\"\n        pipeline_executor = PipelineExecutor(self)\n        pipeline_executor.execute_partial(subset)\n\n    def report_settable_fields(self, pipeline: dict = {}, path: str = \"\") -&gt; None:\n        \"\"\"\n        Recursively finds and prints keys with settable fields in a JSON-like dictionary.\n\n        Args:\n        pipeline (dict): pipeline (or sub pipeline) to process.\n        path (str): Current path to the key, used for nested dictionaries.\n        \"\"\"\n        # Grab the entire pipeline if not provided (first call)\n        if not pipeline:\n            self.log.important(f\"Checking Pipeline: {self.name}...\")\n            pipeline = self.pipeline\n        for key, value in pipeline.items():\n            if isinstance(value, dict):\n                # Recurse into sub-dictionary\n                self.report_settable_fields(value, path + key + \" -&gt; \")\n            elif isinstance(value, str) and value.startswith(\"&lt;&lt;\") and value.endswith(\"&gt;&gt;\"):\n                # Check if required or optional\n                required = \"[Required]\" if \"required\" in value else \"[Optional]\"\n                self.log.important(f\"{required} Path: {path + key}\")\n\n    def delete(self):\n        \"\"\"Pipeline Deletion\"\"\"\n        self.log.info(f\"Deleting Pipeline: {self.name}...\")\n        self.params.delete(f\"{self.prefix}/{self.name}\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of this pipeline\n\n        Returns:\n            str: String representation of this pipeline\n        \"\"\"\n        # Class name and details\n        class_name = self.__class__.__name__\n        pipeline_details = json.dumps(self.pipeline, indent=4)\n        return f\"{class_name}({pipeline_details})\"\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.__init__","title":"<code>__init__(name)</code>","text":"<p>Pipeline Init Method</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"Pipeline Init Method\"\"\"\n    self.log = logging.getLogger(\"workbench\")\n    self.name = name\n\n    # Spin up a Parameter Store for Pipelines\n    self.prefix = \"/workbench/pipelines\"\n    self.params = ParameterStore()\n    self.pipeline = self.params.get(f\"{self.prefix}/{self.name}\")\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of this pipeline</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of this pipeline</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of this pipeline\n\n    Returns:\n        str: String representation of this pipeline\n    \"\"\"\n    # Class name and details\n    class_name = self.__class__.__name__\n    pipeline_details = json.dumps(self.pipeline, indent=4)\n    return f\"{class_name}({pipeline_details})\"\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.delete","title":"<code>delete()</code>","text":"<p>Pipeline Deletion</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def delete(self):\n    \"\"\"Pipeline Deletion\"\"\"\n    self.log.info(f\"Deleting Pipeline: {self.name}...\")\n    self.params.delete(f\"{self.prefix}/{self.name}\")\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.details","title":"<code>details(**kwargs)</code>","text":"<p>Retrieve the Pipeline Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the Pipeline</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def details(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the Pipeline Details.\n\n    Returns:\n        dict: A dictionary of details about the Pipeline\n    \"\"\"\n    return self.pipeline\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.execute","title":"<code>execute()</code>","text":"<p>Execute the entire Pipeline</p> <p>Raises:</p> Type Description <code>RunTimeException</code> <p>If the pipeline execution fails in any way</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def execute(self):\n    \"\"\"Execute the entire Pipeline\n\n    Raises:\n        RunTimeException: If the pipeline execution fails in any way\n    \"\"\"\n    pipeline_executor = PipelineExecutor(self)\n    pipeline_executor.execute()\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.execute_partial","title":"<code>execute_partial(subset)</code>","text":"<p>Execute a partial Pipeline</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>list</code> <p>A subset of the pipeline to execute</p> required <p>Raises:</p> Type Description <code>RunTimeException</code> <p>If the pipeline execution fails in any way</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def execute_partial(self, subset: list):\n    \"\"\"Execute a partial Pipeline\n\n    Args:\n        subset (list): A subset of the pipeline to execute\n\n    Raises:\n        RunTimeException: If the pipeline execution fails in any way\n    \"\"\"\n    pipeline_executor = PipelineExecutor(self)\n    pipeline_executor.execute_partial(subset)\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.health_check","title":"<code>health_check(**kwargs)</code>","text":"<p>Retrieve the Pipeline Health Check.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of health check details for the Pipeline</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def health_check(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the Pipeline Health Check.\n\n    Returns:\n        dict: A dictionary of health check details for the Pipeline\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.report_settable_fields","title":"<code>report_settable_fields(pipeline={}, path='')</code>","text":"<p>Recursively finds and prints keys with settable fields in a JSON-like dictionary.</p> <p>Args: pipeline (dict): pipeline (or sub pipeline) to process. path (str): Current path to the key, used for nested dictionaries.</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def report_settable_fields(self, pipeline: dict = {}, path: str = \"\") -&gt; None:\n    \"\"\"\n    Recursively finds and prints keys with settable fields in a JSON-like dictionary.\n\n    Args:\n    pipeline (dict): pipeline (or sub pipeline) to process.\n    path (str): Current path to the key, used for nested dictionaries.\n    \"\"\"\n    # Grab the entire pipeline if not provided (first call)\n    if not pipeline:\n        self.log.important(f\"Checking Pipeline: {self.name}...\")\n        pipeline = self.pipeline\n    for key, value in pipeline.items():\n        if isinstance(value, dict):\n            # Recurse into sub-dictionary\n            self.report_settable_fields(value, path + key + \" -&gt; \")\n        elif isinstance(value, str) and value.startswith(\"&lt;&lt;\") and value.endswith(\"&gt;&gt;\"):\n            # Check if required or optional\n            required = \"[Required]\" if \"required\" in value else \"[Optional]\"\n            self.log.important(f\"{required} Path: {path + key}\")\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.set_input","title":"<code>set_input(input, artifact='data_source')</code>","text":"<p>Set the input for the Pipeline</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, DataFrame]</code> <p>The input for the Pipeline</p> required <code>artifact</code> <code>str</code> <p>The artifact to set the input for (default: \"data_source\")</p> <code>'data_source'</code> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def set_input(self, input: Union[str, pd.DataFrame], artifact: str = \"data_source\"):\n    \"\"\"Set the input for the Pipeline\n\n    Args:\n        input (Union[str, pd.DataFrame]): The input for the Pipeline\n        artifact (str): The artifact to set the input for (default: \"data_source\")\n    \"\"\"\n    self.pipeline[artifact][\"input\"] = input\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.set_training_holdouts","title":"<code>set_training_holdouts(id_column, holdout_ids)</code>","text":"<p>Set the input for the Pipeline</p> <p>Parameters:</p> Name Type Description Default <code>id_column</code> <code>str</code> <p>The column name of the unique identifier</p> required <code>holdout_ids</code> <code>list[str]</code> <p>The list of unique identifiers to hold out</p> required Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def set_training_holdouts(self, id_column: str, holdout_ids: list[str]):\n    \"\"\"Set the input for the Pipeline\n\n    Args:\n        id_column (str): The column name of the unique identifier\n        holdout_ids (list[str]): The list of unique identifiers to hold out\n    \"\"\"\n    self.pipeline[\"feature_set\"][\"id_column\"] = id_column\n    self.pipeline[\"feature_set\"][\"holdout_ids\"] = holdout_ids\n</code></pre>"},{"location":"api_classes/pipelines/#workbench.api.pipeline.Pipeline.summary","title":"<code>summary(**kwargs)</code>","text":"<p>Retrieve the Pipeline Summary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the Pipeline</p> Source code in <code>src/workbench/api/pipeline.py</code> <pre><code>def summary(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the Pipeline Summary.\n\n    Returns:\n        dict: A dictionary of details about the Pipeline\n    \"\"\"\n    return self.pipeline\n</code></pre>"},{"location":"api_classes/pipelines/#examples","title":"Examples","text":"<p>Make a Pipeline</p> <p>Pipelines are just JSON files (see <code>workbench/examples/pipelines/</code>). You can copy one and make changes to fit your objects/use case, or if you have a set of Workbench artifacts created you can 'backtrack' from the Endpoint and have it create the Pipeline for you.</p> pipeline_manager.py<pre><code>from workbench.api.pipeline_manager import PipelineManager\n\n # Create a PipelineManager\nmy_manager = PipelineManager()\n\n# List the Pipelines\npprint(my_manager.list_pipelines())\n\n# Create a Pipeline from an Endpoint\nabalone_pipeline = my_manager.create_from_endpoint(\"abalone-regression-end\")\n\n# Publish the Pipeline\nmy_manager.publish_pipeline(\"abalone_pipeline_v1\", abalone_pipeline)\n</code></pre> <p>Output</p> <p><pre><code>Listing Pipelines...\n[{'last_modified': datetime.datetime(2024, 4, 16, 21, 10, 6, tzinfo=tzutc()),\n  'name': 'abalone_pipeline_v1',\n  'size': 445}]\n</code></pre> Pipeline Details</p> pipeline_details.py<pre><code>from workbench.api.pipeline import Pipeline\n\n# Retrieve an existing Pipeline\nmy_pipeline = Pipeline(\"abalone_pipeline_v1\")\npprint(my_pipeline.details())\n</code></pre> <p>Output</p> <pre><code>{\n    \"name\": \"abalone_pipeline_v1\",\n    \"s3_path\": \"s3://sandbox/pipelines/abalone_pipeline_v1.json\",\n    \"pipeline\": {\n        \"data_source\": {\n            \"name\": \"abalone_data\",\n            \"tags\": [\n                \"abalone_data\"\n            ],\n            \"input\": \"/Users/briford/work/workbench/data/abalone.csv\"\n        },\n        \"feature_set\": {\n            \"name\": \"abalone_features\",\n            \"tags\": [\n                \"abalone_features\"\n            ],\n            \"input\": \"abalone_data\"\n        },\n        \"model\": {\n            \"name\": \"abalone-regression\",\n            \"tags\": [\n                \"abalone\",\n                \"regression\"\n            ],\n            \"input\": \"abalone_features\"\n        },\n        ...\n    }\n}\n</code></pre> <p>Pipeline Execution</p> <p>Pipeline Execution</p> <p>Executing the Pipeline is obviously the most important reason for creating one. If gives you a reproducible way to capture, inspect, and run the same ML pipeline on different data (nightly).</p> pipeline_execution.py<pre><code>from workbench.api.pipeline import Pipeline\n\n# Retrieve an existing Pipeline\nmy_pipeline = Pipeline(\"abalone_pipeline_v1\")\n\n# Execute the Pipeline\nmy_pipeline.execute()  # Full execution\n\n# Partial executions\nmy_pipeline.execute_partial([\"data_source\", \"feature_set\"])\nmy_pipeline.execute_partial([\"model\", \"endpoint\"])\n</code></pre>"},{"location":"api_classes/pipelines/#pipelines-advanced","title":"Pipelines Advanced","text":"<p>As part of the flexible architecture sometimes DataSources or FeatureSets can be created with a Pandas DataFrame. To support a DataFrame as input to a pipeline we can call the <code>set_input()</code> method to the pipeline object. If you'd like to specify the <code>set_hold_out_ids()</code> you can also provide a list of ids.</p> <pre><code>    def set_input(self, input: Union[str, pd.DataFrame], artifact: str = \"data_source\"):\n        \"\"\"Set the input for the Pipeline\n\n        Args:\n            input (Union[str, pd.DataFrame]): The input for the Pipeline\n            artifact (str): The artifact to set the input for (default: \"data_source\")\n        \"\"\"\n        self.pipeline[artifact][\"input\"] = input\n\n    def set_hold_out_ids(self, id_list: list):\n        \"\"\"Set the input for the Pipeline\n\n        Args:\n           id_list (list): The list of hold out ids\n        \"\"\"\n        self.pipeline[\"feature_set\"][\"hold_out_ids\"] = id_list\n</code></pre> <p>Running a pipeline creates and deploys a set of Workbench Artifacts, DataSource, FeatureSet, Model and Endpoint. These artifacts can be viewed in the Sagemaker Console/Notebook interfaces or in the Workbench Dashboard UI.</p> <p>Not Finding a particular method?</p> <p>The Workbench API Classes use the 'Core' Classes Internally, so for an extensive listing of all the methods available please take a deep dive into: Workbench Core Classes</p>"},{"location":"api_classes/views/","title":"Views","text":"<p>View Examples</p> <p>Examples of using the Views classes to extend the functionality of Workbench Artifacts are in the Examples section at the bottom of this page. </p> <p>Views are a powerful way to filter and agument your DataSources and FeatureSets. With Views you can subset columns, rows, and even add data to existing Workbench Artifacts. If you want to compute outliers, runs some statistics or engineer some new features, Views are an easy way to change, modify, and add to DataSources and FeatureSets.</p> <p>View: Read from a view (training, display, etc) for DataSources and FeatureSets.</p>"},{"location":"api_classes/views/#workbench.core.views.view.View","title":"<code>View</code>","text":"<p>View: Read from a view (training, display, etc) for DataSources and FeatureSets.</p> Common Usage <pre><code># Grab the Display View for a DataSource\ndisplay_view = ds.view(\"display\")\nprint(display_view.columns)\n\n# Pull a DataFrame for the view\ndf = display_view.pull_dataframe()\n\n# Views also work with FeatureSets\ncomp_view = fs.view(\"computation\")\ncomp_df = comp_view.pull_dataframe()\n\n# Query the view with a custom SQL query\nquery = f\"SELECT * FROM {comp_view.table} WHERE age &gt; 30\"\ndf = comp_view.query(query)\n\n# Delete the view\ncomp_view.delete()\n</code></pre> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>class View:\n    \"\"\"View: Read from a view (training, display, etc) for DataSources and FeatureSets.\n\n    Common Usage:\n        ```python\n\n        # Grab the Display View for a DataSource\n        display_view = ds.view(\"display\")\n        print(display_view.columns)\n\n        # Pull a DataFrame for the view\n        df = display_view.pull_dataframe()\n\n        # Views also work with FeatureSets\n        comp_view = fs.view(\"computation\")\n        comp_df = comp_view.pull_dataframe()\n\n        # Query the view with a custom SQL query\n        query = f\"SELECT * FROM {comp_view.table} WHERE age &gt; 30\"\n        df = comp_view.query(query)\n\n        # Delete the view\n        comp_view.delete()\n        ```\n    \"\"\"\n\n    # Class attributes\n    log = logging.getLogger(\"workbench\")\n    meta = Meta()\n\n    def __init__(self, artifact: Union[DataSource, FeatureSet], view_name: str, **kwargs):\n        \"\"\"View Constructor: Retrieve a View for the given artifact\n\n        Args:\n            artifact (Union[DataSource, FeatureSet]): A DataSource or FeatureSet object\n            view_name (str): The name of the view to retrieve (e.g. \"training\")\n        \"\"\"\n\n        # Set the view name\n        self.view_name = view_name\n\n        # Is this a DataSource or a FeatureSet?\n        self.is_feature_set = isinstance(artifact, FeatureSetCore)\n        self.auto_id_column = artifact.id_column if self.is_feature_set else None\n\n        # Get the data_source from the artifact\n        self.artifact_name = artifact.name\n        self.data_source = artifact.data_source if self.is_feature_set else artifact\n        self.database = self.data_source.database\n\n        # Construct our base_table_name\n        self.base_table_name = self.data_source.table\n\n        # Check if the view should be auto created\n        self.auto_created = False\n        if kwargs.get(\"auto_create_view\", True) and not self.exists():\n\n            # A direct double check before we auto-create\n            if not self.exists(skip_cache=True):\n                self.log.important(\n                    f\"View {self.view_name} for {self.artifact_name} doesn't exist, attempting to auto-create...\"\n                )\n                self.auto_created = self._auto_create_view()\n\n                # Check for failure of the auto-creation\n                if not self.auto_created:\n                    self.log.error(\n                        f\"View {self.view_name} for {self.artifact_name} doesn't exist and cannot be auto-created...\"\n                    )\n                    self.view_name = self.columns = self.column_types = self.source_table = self.base_table_name = (\n                        self.join_view\n                    ) = None\n                    return\n\n        # Now fill some details about the view\n        self.columns, self.column_types, self.source_table, self.join_view = view_details(\n            self.table, self.data_source.database, self.data_source.boto3_session\n        )\n\n    def pull_dataframe(self, limit: int = 100000) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Pull a DataFrame based on the view type\n\n        Args:\n            limit (int): The maximum number of rows to pull (default: 100000)\n\n        Returns:\n            Union[pd.DataFrame, None]: The DataFrame for the view or None if it doesn't exist\n        \"\"\"\n\n        # Pull the DataFrame\n        pull_query = f'SELECT * FROM \"{self.table}\" LIMIT {limit}'\n        df = self.data_source.query(pull_query)\n        return df\n\n    def query(self, query: str) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Query the view with a custom SQL query\n\n        Args:\n            query (str): The SQL query to execute\n\n        Returns:\n            Union[pd.DataFrame, None]: The DataFrame for the query or None if it doesn't exist\n        \"\"\"\n        return self.data_source.query(query)\n\n    def column_details(self) -&gt; dict:\n        \"\"\"Return a dictionary of the column names and types for this view\n\n        Returns:\n            dict: A dictionary of the column names and types\n        \"\"\"\n        return dict(zip(self.columns, self.column_types))\n\n    @property\n    def table(self) -&gt; str:\n        \"\"\"Construct the view table name for the given view type\n\n        Returns:\n            str: The view table name\n        \"\"\"\n        if self.view_name is None:\n            return None\n        if self.view_name == \"base\":\n            return self.base_table_name\n        return f\"{self.base_table_name}___{self.view_name}\"\n\n    def delete(self):\n        \"\"\"Delete the database view (and supplemental data) if it exists.\"\"\"\n\n        # List any supplemental tables for this data source\n        supplemental_tables = list_supplemental_data_tables(self.base_table_name, self.database)\n        for table in supplemental_tables:\n            if self.view_name in table:\n                self.log.important(f\"Deleting Supplemental Table {table}...\")\n                delete_table(table, self.database, self.data_source.boto3_session)\n\n        # Now drop the view\n        self.log.important(f\"Dropping View {self.table}...\")\n        drop_view_query = f'DROP VIEW \"{self.table}\"'\n\n        # Execute the DROP VIEW query\n        try:\n            self.data_source.execute_statement(drop_view_query, silence_errors=True)\n        except wr.exceptions.QueryFailed as e:\n            if \"View not found\" in str(e):\n                self.log.info(f\"View {self.table} not found, this is fine...\")\n            else:\n                raise\n\n        # We want to do a small sleep so that AWS has time to catch up\n        self.log.info(\"Sleeping for 3 seconds after dropping view to allow AWS to catch up...\")\n        time.sleep(3)\n\n    def exists(self, skip_cache: bool = False) -&gt; bool:\n        \"\"\"Check if the view exists in the database\n\n        Args:\n            skip_cache (bool): Skip the cache and check the database directly (default: False)\n        Returns:\n            bool: True if the view exists, False otherwise.\n        \"\"\"\n        # The BaseView always exists\n        if self.view_name == \"base\":\n            return True\n\n        # If we're skipping the cache, we need to check the database directly\n        if skip_cache:\n            return self._check_database()\n\n        # Use the meta class to see if the view exists\n        views_df = self.meta.views(self.database)\n\n        # Check if we have ANY views\n        if views_df.empty:\n            return False\n\n        # Check if the view exists\n        return self.table in views_df[\"Name\"].values\n\n    def ensure_exists(self):\n        \"\"\"Ensure if the view exists by making a query directly to the database. If it doesn't exist, create it\"\"\"\n\n        # The BaseView always exists\n        if self.view_name == \"base\":\n            return\n\n        # Check the database directly\n        if not self._check_database():\n            self._auto_create_view()\n\n    def copy(self, dest_view_name: str) -&gt; \"View\":\n        \"\"\"Copy this view to a new view with a different name\n\n        Args:\n            dest_view_name (str): The destination view name (e.g. \"training_v1\")\n\n        Returns:\n            View: A new View object for the destination view\n        \"\"\"\n        # Can't copy the base view\n        if self.view_name == \"base\":\n            self.log.error(\"Cannot copy the base view\")\n            return None\n\n        # Get the view definition\n        get_view_query = f\"\"\"\n        SELECT view_definition\n        FROM information_schema.views\n        WHERE table_schema = '{self.database}'\n        AND table_name = '{self.table}'\n        \"\"\"\n        df = self.data_source.query(get_view_query)\n\n        if df.empty:\n            self.log.error(f\"View {self.table} not found\")\n            return None\n\n        view_definition = df.iloc[0][\"view_definition\"]\n\n        # Create the new view with the destination name\n        dest_table = f\"{self.base_table_name}___{dest_view_name.lower()}\"\n        create_view_query = f'CREATE OR REPLACE VIEW \"{dest_table}\" AS {view_definition}'\n\n        self.log.important(f\"Copying view {self.table} to {dest_table}...\")\n        self.data_source.execute_statement(create_view_query)\n\n        # Return a new View object for the destination\n        artifact = FeatureSet(self.artifact_name) if self.is_feature_set else DataSource(self.artifact_name)\n        return View(artifact, dest_view_name, auto_create_view=False)\n\n    def _check_database(self) -&gt; bool:\n        \"\"\"Internal: Check if the view exists in the database\n\n        Returns:\n            bool: True if the view exists, False otherwise\n        \"\"\"\n        # Query to check if the table/view exists\n        check_table_query = f\"\"\"\n        SELECT table_name\n        FROM information_schema.tables\n        WHERE table_schema = '{self.database}' AND table_name = '{self.table}'\n        \"\"\"\n        _df = self.data_source.query(check_table_query)\n        return not _df.empty\n\n    def _auto_create_view(self) -&gt; bool:\n        \"\"\"Internal: Automatically create a view training, display, and computation views\n\n        Returns:\n            bool: True if the view was created, False otherwise\n\n        Raises:\n            ValueError: If the view type is not supported\n        \"\"\"\n        from workbench.core.views import DisplayView, ComputationView, TrainingView\n\n        # First if we're going to auto-create, we need to make sure the data source exists\n        if not self.data_source.exists():\n            self.log.error(f\"Data Source {self.data_source.name} does not exist...\")\n            return False\n\n        # DisplayView\n        if self.view_name == \"display\":\n            self.log.important(f\"Auto creating View {self.view_name} for {self.data_source.name}...\")\n            DisplayView.create(self.data_source)\n            return True\n\n        # ComputationView\n        if self.view_name == \"computation\":\n            self.log.important(f\"Auto creating View {self.view_name} for {self.data_source.name}...\")\n            ComputationView.create(self.data_source)\n            return True\n\n        # TrainingView\n        if self.view_name == \"training\":\n            # We're only going to create training views for FeatureSets\n            if self.is_feature_set:\n                self.log.important(f\"Auto creating View {self.view_name} for {self.data_source.name}...\")\n                TrainingView.create(self.data_source, id_column=self.auto_id_column)\n                return True\n            else:\n                self.log.warning(\"Training Views are only supported for FeatureSets...\")\n                return False\n\n        # If we get here, we don't support auto-creating this view\n        self.log.warning(f\"Auto-Create for {self.view_name} not implemented yet...\")\n        return False\n\n    def __repr__(self):\n        \"\"\"Return a string representation of this object\"\"\"\n\n        # Set up various details that we want to print out\n        auto = \"(Auto-Created)\" if self.auto_created else \"\"\n        artifact = \"FeatureSet\" if self.is_feature_set else \"DataSource\"\n\n        info = f'View: \"{self.view_name}\" for {artifact}(\"{self.artifact_name}\")\\n'\n        info += f\"      Database: {self.database}\\n\"\n        info += f\"      Table: {self.table}{auto}\\n\"\n        info += f\"      Source Table: {self.source_table}\\n\"\n        info += f\"      Join View: {self.join_view}\"\n        return info\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.table","title":"<code>table</code>  <code>property</code>","text":"<p>Construct the view table name for the given view type</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The view table name</p>"},{"location":"api_classes/views/#workbench.core.views.view.View.__init__","title":"<code>__init__(artifact, view_name, **kwargs)</code>","text":"<p>View Constructor: Retrieve a View for the given artifact</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Union[DataSource, FeatureSet]</code> <p>A DataSource or FeatureSet object</p> required <code>view_name</code> <code>str</code> <p>The name of the view to retrieve (e.g. \"training\")</p> required Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def __init__(self, artifact: Union[DataSource, FeatureSet], view_name: str, **kwargs):\n    \"\"\"View Constructor: Retrieve a View for the given artifact\n\n    Args:\n        artifact (Union[DataSource, FeatureSet]): A DataSource or FeatureSet object\n        view_name (str): The name of the view to retrieve (e.g. \"training\")\n    \"\"\"\n\n    # Set the view name\n    self.view_name = view_name\n\n    # Is this a DataSource or a FeatureSet?\n    self.is_feature_set = isinstance(artifact, FeatureSetCore)\n    self.auto_id_column = artifact.id_column if self.is_feature_set else None\n\n    # Get the data_source from the artifact\n    self.artifact_name = artifact.name\n    self.data_source = artifact.data_source if self.is_feature_set else artifact\n    self.database = self.data_source.database\n\n    # Construct our base_table_name\n    self.base_table_name = self.data_source.table\n\n    # Check if the view should be auto created\n    self.auto_created = False\n    if kwargs.get(\"auto_create_view\", True) and not self.exists():\n\n        # A direct double check before we auto-create\n        if not self.exists(skip_cache=True):\n            self.log.important(\n                f\"View {self.view_name} for {self.artifact_name} doesn't exist, attempting to auto-create...\"\n            )\n            self.auto_created = self._auto_create_view()\n\n            # Check for failure of the auto-creation\n            if not self.auto_created:\n                self.log.error(\n                    f\"View {self.view_name} for {self.artifact_name} doesn't exist and cannot be auto-created...\"\n                )\n                self.view_name = self.columns = self.column_types = self.source_table = self.base_table_name = (\n                    self.join_view\n                ) = None\n                return\n\n    # Now fill some details about the view\n    self.columns, self.column_types, self.source_table, self.join_view = view_details(\n        self.table, self.data_source.database, self.data_source.boto3_session\n    )\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of this object</p> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation of this object\"\"\"\n\n    # Set up various details that we want to print out\n    auto = \"(Auto-Created)\" if self.auto_created else \"\"\n    artifact = \"FeatureSet\" if self.is_feature_set else \"DataSource\"\n\n    info = f'View: \"{self.view_name}\" for {artifact}(\"{self.artifact_name}\")\\n'\n    info += f\"      Database: {self.database}\\n\"\n    info += f\"      Table: {self.table}{auto}\\n\"\n    info += f\"      Source Table: {self.source_table}\\n\"\n    info += f\"      Join View: {self.join_view}\"\n    return info\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.column_details","title":"<code>column_details()</code>","text":"<p>Return a dictionary of the column names and types for this view</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of the column names and types</p> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def column_details(self) -&gt; dict:\n    \"\"\"Return a dictionary of the column names and types for this view\n\n    Returns:\n        dict: A dictionary of the column names and types\n    \"\"\"\n    return dict(zip(self.columns, self.column_types))\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.copy","title":"<code>copy(dest_view_name)</code>","text":"<p>Copy this view to a new view with a different name</p> <p>Parameters:</p> Name Type Description Default <code>dest_view_name</code> <code>str</code> <p>The destination view name (e.g. \"training_v1\")</p> required <p>Returns:</p> Name Type Description <code>View</code> <code>View</code> <p>A new View object for the destination view</p> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def copy(self, dest_view_name: str) -&gt; \"View\":\n    \"\"\"Copy this view to a new view with a different name\n\n    Args:\n        dest_view_name (str): The destination view name (e.g. \"training_v1\")\n\n    Returns:\n        View: A new View object for the destination view\n    \"\"\"\n    # Can't copy the base view\n    if self.view_name == \"base\":\n        self.log.error(\"Cannot copy the base view\")\n        return None\n\n    # Get the view definition\n    get_view_query = f\"\"\"\n    SELECT view_definition\n    FROM information_schema.views\n    WHERE table_schema = '{self.database}'\n    AND table_name = '{self.table}'\n    \"\"\"\n    df = self.data_source.query(get_view_query)\n\n    if df.empty:\n        self.log.error(f\"View {self.table} not found\")\n        return None\n\n    view_definition = df.iloc[0][\"view_definition\"]\n\n    # Create the new view with the destination name\n    dest_table = f\"{self.base_table_name}___{dest_view_name.lower()}\"\n    create_view_query = f'CREATE OR REPLACE VIEW \"{dest_table}\" AS {view_definition}'\n\n    self.log.important(f\"Copying view {self.table} to {dest_table}...\")\n    self.data_source.execute_statement(create_view_query)\n\n    # Return a new View object for the destination\n    artifact = FeatureSet(self.artifact_name) if self.is_feature_set else DataSource(self.artifact_name)\n    return View(artifact, dest_view_name, auto_create_view=False)\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.delete","title":"<code>delete()</code>","text":"<p>Delete the database view (and supplemental data) if it exists.</p> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the database view (and supplemental data) if it exists.\"\"\"\n\n    # List any supplemental tables for this data source\n    supplemental_tables = list_supplemental_data_tables(self.base_table_name, self.database)\n    for table in supplemental_tables:\n        if self.view_name in table:\n            self.log.important(f\"Deleting Supplemental Table {table}...\")\n            delete_table(table, self.database, self.data_source.boto3_session)\n\n    # Now drop the view\n    self.log.important(f\"Dropping View {self.table}...\")\n    drop_view_query = f'DROP VIEW \"{self.table}\"'\n\n    # Execute the DROP VIEW query\n    try:\n        self.data_source.execute_statement(drop_view_query, silence_errors=True)\n    except wr.exceptions.QueryFailed as e:\n        if \"View not found\" in str(e):\n            self.log.info(f\"View {self.table} not found, this is fine...\")\n        else:\n            raise\n\n    # We want to do a small sleep so that AWS has time to catch up\n    self.log.info(\"Sleeping for 3 seconds after dropping view to allow AWS to catch up...\")\n    time.sleep(3)\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.ensure_exists","title":"<code>ensure_exists()</code>","text":"<p>Ensure if the view exists by making a query directly to the database. If it doesn't exist, create it</p> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def ensure_exists(self):\n    \"\"\"Ensure if the view exists by making a query directly to the database. If it doesn't exist, create it\"\"\"\n\n    # The BaseView always exists\n    if self.view_name == \"base\":\n        return\n\n    # Check the database directly\n    if not self._check_database():\n        self._auto_create_view()\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.exists","title":"<code>exists(skip_cache=False)</code>","text":"<p>Check if the view exists in the database</p> <p>Parameters:</p> Name Type Description Default <code>skip_cache</code> <code>bool</code> <p>Skip the cache and check the database directly (default: False)</p> <code>False</code> <p>Returns:     bool: True if the view exists, False otherwise.</p> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def exists(self, skip_cache: bool = False) -&gt; bool:\n    \"\"\"Check if the view exists in the database\n\n    Args:\n        skip_cache (bool): Skip the cache and check the database directly (default: False)\n    Returns:\n        bool: True if the view exists, False otherwise.\n    \"\"\"\n    # The BaseView always exists\n    if self.view_name == \"base\":\n        return True\n\n    # If we're skipping the cache, we need to check the database directly\n    if skip_cache:\n        return self._check_database()\n\n    # Use the meta class to see if the view exists\n    views_df = self.meta.views(self.database)\n\n    # Check if we have ANY views\n    if views_df.empty:\n        return False\n\n    # Check if the view exists\n    return self.table in views_df[\"Name\"].values\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.pull_dataframe","title":"<code>pull_dataframe(limit=100000)</code>","text":"<p>Pull a DataFrame based on the view type</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of rows to pull (default: 100000)</p> <code>100000</code> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>Union[pd.DataFrame, None]: The DataFrame for the view or None if it doesn't exist</p> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def pull_dataframe(self, limit: int = 100000) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Pull a DataFrame based on the view type\n\n    Args:\n        limit (int): The maximum number of rows to pull (default: 100000)\n\n    Returns:\n        Union[pd.DataFrame, None]: The DataFrame for the view or None if it doesn't exist\n    \"\"\"\n\n    # Pull the DataFrame\n    pull_query = f'SELECT * FROM \"{self.table}\" LIMIT {limit}'\n    df = self.data_source.query(pull_query)\n    return df\n</code></pre>"},{"location":"api_classes/views/#workbench.core.views.view.View.query","title":"<code>query(query)</code>","text":"<p>Query the view with a custom SQL query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query to execute</p> required <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>Union[pd.DataFrame, None]: The DataFrame for the query or None if it doesn't exist</p> Source code in <code>src/workbench/core/views/view.py</code> <pre><code>def query(self, query: str) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Query the view with a custom SQL query\n\n    Args:\n        query (str): The SQL query to execute\n\n    Returns:\n        Union[pd.DataFrame, None]: The DataFrame for the query or None if it doesn't exist\n    \"\"\"\n    return self.data_source.query(query)\n</code></pre>"},{"location":"api_classes/views/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Listing Views</p> views.py<pre><code>from workbench.api.data_source import DataSource\n\n# Convert the Data Source to a Feature Set\ntest_data = DataSource('test_data')\ntest_data.views()\n[\"display\", \"training\", \"computation\"]\n</code></pre> <p>Getting a Particular View</p> views.py<pre><code>from workbench.api.feature_set import FeatureSet\n\nfs = FeatureSet('test_features')\n\n# Grab the columns for the display view\ndisplay_view = fs.view(\"display\")\ndisplay_view.columns\n['id', 'name', 'height', 'weight', 'salary', ...]\n\n# Pull the dataframe for this view\ndf = display_view.pull_dataframe()\n    id       name     height      weight         salary ...\n0   58  Person 58  71.781227  275.088196  162053.140625  \n</code></pre> <p>View Queries</p> <p>All Workbench Views are stored in AWS Athena, so any query that you can make with Athena is accessible through the View Query API.</p> view_query.py<pre><code>from workbench.api.feature_set import FeatureSet\n\n# Grab a FeatureSet View\nfs = FeatureSet(\"abalone_features\")\nd_view = fs.view(\"display\")\n\n# Make some queries using the Athena backend\ndf = d_view(f\"select * from {d_view.table} where height &gt; .3\")\nprint(df.head())\n\ndf = t_view.query(\"select * from abalone_features where class_number_of_rings &lt; 3\")\nprint(df.head())\n</code></pre> <p>Output</p> <pre><code>  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  shell_weight  class_number_of_rings\n0   M   0.705     0.565   0.515         2.210          1.1075          0.4865        0.5120                     10\n1   F   0.455     0.355   1.130         0.594          0.3320          0.1160        0.1335                      8\n\n  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  shell_weight  class_number_of_rings\n0   I   0.075     0.055   0.010         0.002          0.0010          0.0005        0.0015                      1\n1   I   0.150     0.100   0.025         0.015          0.0045          0.0040         0.0050                      2\n</code></pre> <p>Classes to construct View</p> <p>The Workbench Classes used to construct viewss are currently in 'Core'. So you can check out the documentation for those classes here: Workbench View Creators</p> <p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"aws_setup/aws_access_management/","title":"AWS Acesss Management","text":"<p>Need AWS Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>This page gives an overview of how Workbench sets up roles and policies in a granular way that provides 'least priviledge' and also provides a unified framework for AWS access management.</p>"},{"location":"aws_setup/aws_access_management/#conceptual-slide-deck","title":"Conceptual Slide Deck","text":"<p>Workbench AWS Acesss Management</p>"},{"location":"aws_setup/aws_access_management/#aws-resources","title":"AWS Resources","text":"<ul> <li>AWS Identity Center</li> <li>Users and Groups</li> <li>Permission Sets</li> <li>SSO Command Line/Python Configure</li> </ul>"},{"location":"aws_setup/aws_client_vpn/","title":"Setting Up AWS Client VPN","text":"<p>Follow the steps below to set up and connect using AWS Client VPN.</p>"},{"location":"aws_setup/aws_client_vpn/#step-1-create-a-client-vpn-endpoint-in-aws","title":"Step 1: Create a Client VPN Endpoint in AWS","text":"<ol> <li>Go to the VPC Dashboard in the AWS Management Console.</li> <li>Select Client VPN Endpoints and click Create Client VPN Endpoint.</li> <li>Fill in the required details:</li> <li>Client IPv4 CIDR: Choose an IP range (e.g., <code>10.0.0.0/22</code>) that doesn\u2019t overlap with your VPC CIDR.</li> <li>Server Certificate ARN: Use or create an SSL certificate using AWS Certificate Manager (ACM).</li> <li>Authentication Options: Use either Mutual Authentication (client certificates) or Active Directory (for user-based authentication).</li> <li>Connection Log Options: Optional; you can set up CloudWatch logs.</li> <li>Click Create Client VPN Endpoint.</li> </ol>"},{"location":"aws_setup/aws_client_vpn/#step-2-associate-the-client-vpn-endpoint-with-a-vpc-subnet","title":"Step 2: Associate the Client VPN Endpoint with a VPC Subnet","text":"<ol> <li>Once the endpoint is created, select it and click on Associations.</li> <li>Choose a Target Network Association (a subnet in the VPC where you want VPN clients to access).</li> <li>This allows traffic from the VPN clients to route through your chosen VPC subnet.</li> </ol>"},{"location":"aws_setup/aws_client_vpn/#step-3-authorize-vpn-clients-to-access-the-vpc","title":"Step 3: Authorize VPN Clients to Access the VPC","text":"<ol> <li>Under the Authorization Rules tab, click Add Authorization Rule.</li> <li>For Destination network, specify <code>0.0.0.0/0</code> to allow access to all resources in the VPC.</li> <li>Set Grant access to to <code>Allow access</code> and specify the group you created or allow all users.</li> </ol>"},{"location":"aws_setup/aws_client_vpn/#step-4-download-and-install-aws-vpn-client","title":"Step 4: Download and Install AWS VPN Client","text":"<ol> <li>Download the AWS Client VPN for macOS.</li> <li>Install the client on your Mac.</li> </ol>"},{"location":"aws_setup/aws_client_vpn/#step-5-download-the-client-configuration-file","title":"Step 5: Download the Client Configuration File","text":"<ol> <li>In the AWS Console, go to your Client VPN Endpoint.</li> <li>Click on Download Client Configuration. This file contains the connection details required by the VPN client.</li> </ol>"},{"location":"aws_setup/aws_client_vpn/#step-6-import-the-configuration-file-and-connect","title":"Step 6: Import the Configuration File and Connect","text":"<ol> <li>Open the AWS Client VPN app on your Mac.</li> <li>Click File -&gt; Manage Profiles -&gt; Add Profile.</li> <li>Import the configuration file you downloaded.</li> <li>Enter your credentials if required (depending on the authentication method you chose).</li> <li>Click Connect.</li> </ol>"},{"location":"aws_setup/aws_client_vpn/#benefits-of-using-aws-client-vpn","title":"Benefits of Using AWS Client VPN","text":"<ul> <li>Simple Setup: Minimal steps and no need for additional infrastructure.</li> <li>Secure: Uses TLS to secure connections, and you control who has access.</li> <li>Direct Access: Provides direct access to your AWS resources, including the Redis cluster.</li> </ul>"},{"location":"aws_setup/aws_client_vpn/#conclusion","title":"Conclusion","text":"<p>AWS Client VPN is a straightforward, secure, and effective solution for connecting your laptop to an AWS VPC. It requires minimal setup and provides all the security controls you need, making it ideal for a single laptop and user.</p>"},{"location":"aws_setup/aws_setup/","title":"AWS Setup","text":"<p>Need AWS Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"aws_setup/aws_setup/#get-some-information","title":"Get some information","text":"<ul> <li>Go to your AWS Identity Center in the AWS Console</li> <li>On the right side there will be two important pieces of information<ul> <li>Start URL</li> <li>Region </li> </ul> </li> </ul> <p>Write these values down, you'll need them as part of this AWS setup.</p>"},{"location":"aws_setup/aws_setup/#install-aws-cli","title":"Install AWS CLI","text":"<p>AWS CLI Instructions</p>"},{"location":"aws_setup/aws_setup/#running-the-sso-configuration","title":"Running the SSO Configuration","text":"<p>Note: You only need to do this once! Also this will create a NEW profile, so name the profile something like <code>aws_sso</code>.</p> <pre><code>aws configure sso --profile &lt;whatever&gt; (e.g. aws_sso)\nSSO session name (Recommended): sso-session\nSSO start URL []: &lt;the Start URL from info above&gt;\nSSO region []: &lt;the Region from info above&gt;\nSSO registration scopes [sso:account:access]: &lt;just hit return&gt;\n</code></pre> <p>You will get a browser open/redirect at this point and get a list of available accounts.. something like below, just pick the correct account</p> <pre><code>There are 2 AWS accounts available to you.\n&gt; SCP_Sandbox, briford+sandbox@supercowpowers.com (XXXX40646YYY)\n  SCP_Main, briford@supercowpowers.com (XXX576391YYY)\n</code></pre> <p>Now pick the role that you're going to use</p> <pre><code>There are 2 roles available to you.\n&gt; DataScientist\n  AdministratorAccess\n\nCLI default client Region [None]: &lt;same region as above&gt;\nCLI default output format [None]: json\n</code></pre>"},{"location":"aws_setup/aws_setup/#setting-up-some-aliases-for-bashzsh","title":"Setting up some aliases for bash/zsh","text":"<p>Edit your favorite ~/.bashrc ~/.zshrc and add these nice aliases/helper</p> <pre><code># AWS Aliases\nalias aws_sso='export AWS_PROFILE=aws_sso'\n\n# Default AWS Profile\nexport AWS_PROFILE=aws_sso\n</code></pre>"},{"location":"aws_setup/aws_setup/#testing-your-new-aws-profile","title":"Testing your new AWS Profile","text":"<p>Make sure your profile is active/set</p> <p><pre><code>env | grep AWS\nAWS_PROFILE=&lt;aws_sso or whatever&gt;\n</code></pre> Now you can list the S3 buckets in the AWS Account</p> <p><pre><code>aws ls s3\n</code></pre> If you get some message like this...</p> <pre><code>The SSO session associated with this profile has\nexpired or is otherwise invalid. To refresh this SSO\nsession run aws sso login with the corresponding\nprofile.\n</code></pre> <p>This is fine/good, a browser will open up and you can refresh your SSO Token.</p> <p>After that you should get a listing of the S3 buckets without needed to refresh your token.</p> <pre><code>aws s3 ls\n\u276f aws s3 ls\n2023-03-20 20:06:53 aws-athena-query-results-XXXYYY-us-west-2\n2023-03-30 13:22:28 sagemaker-studio-XXXYYY-dbgyvq8ruka\n2023-03-24 22:05:55 sagemaker-us-west-2-XXXYYY\n2023-04-30 13:43:29 scp-workbench-artifacts\n</code></pre>"},{"location":"aws_setup/aws_setup/#back-to-initial-setup","title":"Back to Initial Setup","text":"<p>If you're doing the initial setup of Workbench you should now go back and finish that process: Getting Started</p>"},{"location":"aws_setup/aws_setup/#aws-resources","title":"AWS Resources","text":"<ul> <li>AWS Identity Center</li> <li>Users and Groups</li> <li>Permission Sets</li> <li>SSO Command Line/Python Configure</li> </ul>"},{"location":"aws_setup/aws_tips_and_tricks/","title":"AWS Tips and Tricks","text":"<p>Need AWS Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>This page tries to give helpful guidance when setting up AWS Accounts, Users, and Groups. In general AWS can be a bit tricky to set up the first time. Feel free to use any material in this guide but we're more than happy to help clients get their AWS Setup ready to go for FREE. Below are some guides for setting up a new AWS account for Workbench and also setting up SSO Users and Groups within AWS.</p>"},{"location":"aws_setup/aws_tips_and_tricks/#new-aws-account-with-aws-organizations-easy","title":"New AWS Account (with AWS Organizations: easy)","text":"<ul> <li>If you already have an AWS Account you can activate the AWS Identity Center/Organization functionality.</li> <li>Now go to AWS Organizations page and hit 'Add an AWS Account' button</li> <li>Add a new User with permissions that allows AWS Stack creation</li> </ul> <p>Email Trick</p> <p>AWS will often not allow the same email to be used for different accounts. If you need a 'new' email just add a plus sign '+' at the end of your existing email (e.g. bob.smith+aws@gmail.com). This email will 'auto forward' to bob.smith@gmail.com.</p>"},{"location":"aws_setup/aws_tips_and_tricks/#new-aws-account-without-aws-organizations-a-bit-harder","title":"New AWS Account (without AWS Organizations: a bit harder)","text":"<ul> <li>Goto: https://aws.amazon.com/free and hit the big button 'Create a Free Account'</li> <li>Enter email and the account name you'd like (anything is fine)</li> <li>You'll get a validation email and go through the rest of the Account setup procedure</li> <li>Add a new User with permissions that allows AWS Stack creation</li> </ul>"},{"location":"aws_setup/aws_tips_and_tricks/#sso-users-and-groups","title":"SSO Users and Groups","text":"<p>AWS SSO (Single Sign-On) is a cloud-based service that allows users to manage access to multiple AWS accounts and business applications using a single set of credentials. It simplifies the authentication process for users and provides centralized management of permissions and access control across various AWS resources. With AWS SSO, users can log in once and access all the applications and accounts they need, streamlining the user experience and increasing productivity. AWS SSO also enables IT administrators to manage access more efficiently by providing a single point of control for managing user access, permissions, and policies, reducing the risk of unauthorized access or security breaches.</p>"},{"location":"aws_setup/aws_tips_and_tricks/#setting-up-sso-users","title":"Setting up SSO Users","text":"<ul> <li>Log in to your AWS account and go to the AWS Identity Center console.</li> <li>Click on the \"Users\" tab and then click on the \"Add user\" button.</li> </ul> <p>The 'Add User' setup is fairly straight forward but here are some screen shots:</p> <p>On the first panel you can fill in the users information.</p> <p></p>"},{"location":"aws_setup/aws_tips_and_tricks/#groups","title":"Groups","text":"<p>On the second panel we suggest that you have at LEAST two groups:</p> <ul> <li>Admin group</li> <li>DataScientists group</li> </ul>"},{"location":"aws_setup/aws_tips_and_tricks/#setting-up-groups","title":"Setting up Groups","text":"<p>This allows you to put most of the users into the DataScientists group that has AWS policies based on their job role. AWS uses 'permission sets' and you assign AWS Policies. This approach makes it easy to give a group of users a set of relevant policies for their tasks. </p> <p>Our standard setup is to have two permission sets with the following policies:</p> <ul> <li>IAM Identity Center --&gt; Permission sets --&gt; DataScientist </li> <li> <p>Add Policy: arn:aws:iam::aws:policy/job-function/DataScientist</p> </li> <li> <p>IAM Identity Center --&gt; Permission sets --&gt; AdministratorAccess </p> </li> <li>Add Policy: arn:aws:iam::aws:policy/job-function/AdministratorAccess</li> </ul> <p>See: Permission Sets for more details and instructions.</p> <p>Another benefit of creating groups is that you can include that group in 'Trust Policy (assume_role)' for the Workbench-ExecutionRole (this gets deployed as part of the Workbench AWS Stack). This means that the management of what Workbench can do/see/read/write is completely done through the Workbench-ExecutionRole.</p>"},{"location":"aws_setup/aws_tips_and_tricks/#back-to-adding-user","title":"Back to Adding User","text":"<p>Okay now that we have our groups set up we can go back to our original goal of adding a user. So here's the second panel with the groups and now we can hit 'Next'</p> <p></p> <p>On the third panel just review the details and hit the 'Add User' button at the bottom. The user will get an email giving them instructions on how to log on to their AWS account.</p> <p></p>"},{"location":"aws_setup/aws_tips_and_tricks/#aws-console","title":"AWS Console","text":"<p>Now when the user logs onto the AWS Console they should see something like this: </p>"},{"location":"aws_setup/aws_tips_and_tricks/#aws-clisso-setup-for-command-linepython-usage","title":"AWS CLI/SSO Setup for Command Line/Python Usage","text":"<p>Please see our AWS Setup</p>"},{"location":"aws_setup/aws_tips_and_tricks/#aws-resources","title":"AWS Resources","text":"<ul> <li>AWS Identity Center</li> <li>Users and Groups</li> <li>Permission Sets</li> <li>SSO Command Line/Python Configure</li> </ul>"},{"location":"aws_setup/core_stack/","title":"WorkbenchCore AWS Stack","text":"<p>Welcome to the Workbench AWS Setup Guide. Workbench is deployed as an AWS Stack following the well architected system practices of AWS. </p> <p>AWS Setup can be a bit complex</p> <p>Setting up Workbench with AWS can be a bit complex, but this only needs to be done ONCE for your entire company. The install uses standard CDK --&gt; AWS Stacks and Workbench tries to make it straight forward. If you have any troubles at all feel free to contact us a workbench@supercowpowers.com or on Discord and we're happy to help you with AWS for FREE.</p>"},{"location":"aws_setup/core_stack/#two-main-options-when-using-workbench","title":"Two main options when using Workbench","text":"<ol> <li>Spin up a new AWS Account for the Workbench Stacks (Make a New Account)</li> <li>Deploy Workbench Stacks into your existing AWS Account</li> </ol> <p>Either of these options are fully supported, but we highly suggest a NEW account as it gives the following benefits:</p> <ul> <li>AWS Data Isolation: Data Scientists will feel empowered to play in the sandbox without impacting production services.</li> <li>AWS Cost Accounting: Monitor and Track all those new ML Pipelines that your team creates with Workbench :)</li> </ul>"},{"location":"aws_setup/core_stack/#setting-up-users-and-groups","title":"Setting up Users and Groups","text":"<p>If your AWS Account already has users and groups set up you can skip this but here's our recommendations on setting up SSO Users and Groups</p>"},{"location":"aws_setup/core_stack/#onboarding-workbench-to-your-aws-account","title":"Onboarding Workbench to your AWS Account","text":"<p>Pulling down the Workbench Repo   <pre><code>git clone https://github.com/SuperCowPowers/workbench.git\n</code></pre></p>"},{"location":"aws_setup/core_stack/#workbench-uses-aws-python-cdk-for-deployments","title":"Workbench uses AWS Python CDK for Deployments","text":"<p>If you don't have AWS CDK already installed you can do these steps:</p> <p>Mac</p> <p><pre><code>brew install node \nnpm install -g aws-cdk\n</code></pre> Linux</p> <p><pre><code>sudo apt install nodejs\nsudo npm install -g aws-cdk\n</code></pre> For more information on Linux installs see Digital Ocean NodeJS</p>"},{"location":"aws_setup/core_stack/#create-an-s3-bucket-for-workbench","title":"Create an S3 Bucket for Workbench","text":"<p>Workbench pushes and pulls data from AWS, it will use this S3 Bucket for storage and processing. You should create a NEW S3 Bucket for EACH account, we suggest names like:</p> <ul> <li><code>&lt;company-name&gt;-dev-workbench</code></li> <li><code>&lt;company-name&gt;-stage-workbench</code></li> <li><code>&lt;company-name&gt;-prod-workbench</code></li> </ul>"},{"location":"aws_setup/core_stack/#deploying-the-workbench-core-stack","title":"Deploying the Workbench Core Stack","text":"<p>This stack has the <code>Workbench-Execution-Role</code> and an associated role for AWS Glue Jobs.</p> <p>You'll need to set some environmental vars before deploying the stack.</p> <pre><code>export WORKBENCH_BUCKET=name-of-workbench-bucket\nexport WORKBENCH_SSO_GROUPS=DataScientists,DataEngineers (no spaces between commas)\n</code></pre> <p>Optional ENV Vars</p> <pre><code>export WORKBENCH_ADDITIONAL_BUCKETS=&lt;comma separated list of buckets&gt;\n</code></pre> <p>AWS Stuff</p> <p>Activate your AWS Account that's used for Workbench deployment. For this one time install you should use an Admin Account (or an account that had permissions to create/update AWS Stacks)</p> <pre><code>cd workbench/aws_setup/workbench_core\npip install -r requirements.txt\ncdk bootstrap\ncdk deploy\n</code></pre>"},{"location":"aws_setup/core_stack/#important","title":"Important","text":"<p>The first time you run the core stack it will barf a bunch of messages about not being able to assume the workbench execution role, something like this...</p> <p><pre><code>    raise RuntimeError(msg) from e\nRuntimeError: Failed to Assume Workbench Role: Check AWS_PROFILE and/or Renew SSO Token..\n</code></pre> Please ignore this when running this for the first time. After the WorkbenchCore stack is installed this set of error messages goes away.</p>"},{"location":"aws_setup/core_stack/#enable-users-to-assume-workbench-executionrole","title":"Enable Users to Assume Workbench-ExecutionRole","text":"<p>Now that the <code>Workbench-ExecutionRole</code> has been deployed via AWS Stack. These guides walk you through setting up access for both SSO users and IAM users to assume the Workbench-ExecutionRole in your AWS account.</p> <ul> <li>Set up SSO Users</li> <li>Set up IAM Users (not recommend, but contact us we'll help you out)</li> </ul>"},{"location":"aws_setup/core_stack/#aws-account-setup","title":"AWS Account Setup","text":"<p>After deploying the Workbench Core Stack and setting up users to assume that Role, you can run this account setup script. If the results ends with <code>INFO AWS Account Clamp: AOK!</code> you're in good shape. If not feel free to contact us on Discord and we'll get it straightened out for you :)</p> <pre><code>pip install workbench (if not already installed)\ncd workbench/aws_setup\npython aws_account_setup.py\n&lt;lot of print outs for various checks&gt;\nINFO AWS Account Clamp: AOK!\n</code></pre>"},{"location":"aws_setup/core_stack/#important_1","title":"Important","text":"<p>The first time your run this it will barf some error messages at you. These are just ensuring that certain Glue Catalogs exist. Just cut/paste the last error message into your console to create these databases.</p> <p><pre><code>ERROR Access denied while trying to create/access the catalog database 'workbench'.\nERROR Create the database manually in the AWS Glue Console, or run this command:\nERROR aws glue create-database --database-input '{\"Name\": \"workbench\"}'\n</code></pre> So for this message you would just cut/paste this into your command line</p> <p><pre><code>aws glue create-database --database-input '{\"Name\": \"workbench\"}'\n</code></pre> Just rerun the script after doing this and after (2 or 3) of these you should see the script run successfully and give a message <code>AWS Account Clamp: AOK!</code></p> <p>Success</p> <p>Congratulations: Workbench is now deployed to your AWS Account. Deploying the AWS Stack only needs to be done once. Now that this is complete your developers can simply <code>pip install workbench</code> and start using the API.</p> <p>If you ran into any issues with this procedure please contact us via Discord or email workbench@supercowpowers.com and the SCP team will provide free setup and support for new Workbench users.</p>"},{"location":"aws_setup/dashboard_stack/","title":"Deploy the Workbench Dashboard Stack","text":"<p>Deploying the Dashboard Stack is reasonably straight forward, it's the same approach as the Core Stack that you've already deployed.</p> <p>Please review the Stack Details section to understand all the AWS components that are included and utilized in the Workbench Dashboard Stack.</p>"},{"location":"aws_setup/dashboard_stack/#deploying-the-dashboard-stack","title":"Deploying the Dashboard Stack","text":"<p>AWS Stuff</p> <p>Activate your AWS Account that's used for Workbench deployment. For this one time install you should use an Admin Account (or an account that had permissions to create/update AWS Stacks)</p> <pre><code>cd workbench/aws_setup/workbench_dashboard_full\nexport WORKBENCH_CONFIG=/full/path/to/config.json\npip install -r requirements.txt\ncdk bootstrap\ncdk deploy\n</code></pre>"},{"location":"aws_setup/dashboard_stack/#stack-details","title":"Stack Details","text":"<p>AWS Questions?</p> <p>There's quite a bit to unpack when deploying an AWS powered Web Service. We're happy to help walk you through the details and options. Contact us anytime for a free consultation.</p> <ul> <li>ECS Fargate</li> <li>Load Balancer</li> <li>2 Availability Zones</li> <li>VPCs / Nat Gateways</li> <li>ElasticCache Cluster (shared Redis Caching)</li> </ul>"},{"location":"aws_setup/dashboard_stack/#aws-stack-benefits","title":"AWS Stack Benefits","text":"<ol> <li>Scalability: Includes an Application Load Balancer and uses ECS with Fargate, and ElasticCache for more robust scaling options.</li> <li>Higher Security: Utilizes security groups for both the ECS tasks, load balancer, plus VPC private subnets for Redis and the utilization of NAT Gateways.</li> </ol> <p>AWS Costs</p> <p>Deploying the Workbench Dashboard does incur some monthly AWS costs. If you're on a tight budget you can deploy the 'lite' version of the Dashboard Stack.</p> <pre><code>cd workbench/aws_setup/workbench_dashboard_lite\nexport WORKBENCH_CONFIG=/full/path/to/config.json\npip install -r requirements.txt\ncdk bootstrap\ncdk deploy\n</code></pre>"},{"location":"aws_setup/domain_cert_setup/","title":"AWS Domain and Certificate Instructions","text":"<p>Need AWS Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>This page tries to give helpful guidance when setting up a new domain and SSL Certificate in your AWS Account.</p>"},{"location":"aws_setup/domain_cert_setup/#new-domain","title":"New Domain","text":"<p>You'll want the Workbench Dashboard to have a domain for your companies internal use. Customers will typically use a domain like <code>&lt;company_name&gt;-ml-dashboard.com</code> but you are free to choose any domain you'd like.</p> <p>Domains are tied to AWS Accounts</p> <p>When you create a new domain in AWS Route 53, that domain is tied to that AWS Account. You can do a cross account setup for domains but it's a bit more tricky. We recommend that each account where Workbench gets deployed owns the domain for that Dashboard.</p>"},{"location":"aws_setup/domain_cert_setup/#multiple-aws-accounts","title":"Multiple AWS Accounts","text":"<p>Many customers will have a dev/stage/prod set of AWS accounts, if that the case then the best practice is to make a domain specific to each account. So for instance:</p> <ul> <li>The AWS Dev Account gets: <code>&lt;company_name&gt;-ml-dashboard-dev.com</code> </li> <li>The AWS Prod Account gets:  <code>&lt;company_name&gt;-ml-dashboard-prod.com</code>.</li> </ul> <p>This means that when you go to that Dashboard it's super obvious which environment your on.</p>"},{"location":"aws_setup/domain_cert_setup/#register-the-domain","title":"Register the Domain","text":"<ul> <li> <p>Open Route 53 Console Route 53 Console</p> </li> <li> <p>Register your New Domain</p> <ul> <li>Click on Registered domains in the left navigation pane.</li> <li>Click on Register Domain.</li> <li>Enter your desired domain name and check for availability.</li> <li>Follow the prompts to complete the domain registration process.</li> <li>After registration, your domain will be listed under Registered domains.</li> </ul> </li> </ul>"},{"location":"aws_setup/domain_cert_setup/#request-a-ssl-certificate-from-acm","title":"Request a SSL Certificate from ACM","text":"<ol> <li> <p>Open ACM Console: AWS Certificate Manager (ACM) Console</p> </li> <li> <p>Request a Certificate:</p> <ul> <li>Click on Request a certificate.</li> <li>Select Request a public certificate and click Next.</li> </ul> </li> <li> <p>Add Domain Names:</p> <ul> <li>Enter the domain name you registered (e.g., <code>yourdomain.com</code>).</li> <li>Add any additional subdomains if needed (e.g., <code>www.yourdomain.com</code>).</li> </ul> </li> <li> <p>Validation Method:</p> <ul> <li>Choose DNS validation (recommended).</li> <li>ACM will provide CNAME records that you need to add to your Route 53 hosted zone.</li> </ul> </li> <li> <p>Add Tags (Optional):</p> <ul> <li>Add any tags if you want to organize your resources.</li> </ul> </li> <li> <p>Review and Request:</p> <ul> <li>Review your request and click Confirm and request.</li> </ul> </li> </ol>"},{"location":"aws_setup/domain_cert_setup/#adding-cname-records-to-route-53","title":"Adding CNAME Records to Route 53","text":"<p>To complete the domain validation process for your SSL/TLS certificate, you need to add the CNAME records provided by AWS Certificate Manager (ACM) to your Route 53 hosted zone. This step ensures that you own the domain and allows ACM to issue the certificate.</p>"},{"location":"aws_setup/domain_cert_setup/#finding-cname-record-names-and-values","title":"Finding CNAME Record Names and Values","text":"<p>You can find the CNAME record names and values in the AWS Certificate Manager (ACM) console:</p> <ol> <li> <p>Open ACM Console: AWS Certificate Manager (ACM) Console</p> </li> <li> <p>Select Your Certificate:</p> <ul> <li>Click on the certificate that is in the Pending Validation state.</li> </ul> </li> <li> <p>View Domains Section:</p> <ul> <li>Under the Domains section, you will see the CNAME record names and values that you need to add to your Route 53 hosted zone.</li> </ul> </li> </ol>"},{"location":"aws_setup/domain_cert_setup/#adding-cname-records-to-domain","title":"Adding CName Records to Domain","text":"<ol> <li> <p>Open Route 53 Console: Route 53 Console</p> </li> <li> <p>Select Your Hosted Zone:</p> <ul> <li>Find and select the hosted zone for your domain (e.g., <code>yourdomain.com</code>).</li> <li>Click on Create record.</li> </ul> </li> <li> <p>Add the First CNAME Record:</p> <ul> <li>For the Record name, enter the name provided by ACM (e.g., <code>_3e8623442477e9eeec.your-domain.com</code>). Note: they might already have <code>your-domain.com</code> next to this box, if so don't repeat it :)</li> <li>For the Record type, select <code>CNAME</code>.</li> <li>For the Value, enter the value provided by ACM (e.g., <code>_0908c89646d92.sdgjtdhdhz.acm-validations.aws.</code>) (include the trailing dot).</li> <li>Leave the default settings for TTL.</li> <li>Click on Create records.</li> </ul> </li> <li> <p>Add the Second CNAME Record:</p> <ul> <li>Repeat the process for the second CNAME record.</li> <li>For the Record name, enter the second name provided by ACM (e.g., <code>_75cd9364c643caa.www.your-domain.com</code>).</li> <li>For the Record type, select <code>CNAME</code>.</li> <li>For the Value, enter the second value provided by ACM (e.g., <code>_f72f8cff4fb20f4.sdgjhdhz.acm-validations.aws.</code>)  (include the trailing dot).</li> <li>Leave the default settings for TTL.</li> <li>Click on Create records.</li> </ul> </li> </ol> <p>DNS Propagation and Cert Validation</p> <p>After adding the CNAME records, these DNS records will propagate through the DNS system and ACM will automatically detect the validation records and validate the domain. This process can take a few minutes or up to an hour.</p>"},{"location":"aws_setup/domain_cert_setup/#certificate-states","title":"Certificate States","text":"<p>After requesting a certificate, it will go through the following states:</p> <ul> <li> <p>Pending Validation: The initial state after you request a certificate and before you complete the validation process. ACM is waiting for you to prove domain ownership by adding the CNAME records.</p> </li> <li> <p>Issued: This state indicates that the certificate has been successfully validated and issued. You can now use this certificate with your AWS resources.</p> </li> <li> <p>Validation Timed Out: If you do not complete the validation process within a specified period (usually 72 hours), the certificate request times out and enters this state.</p> </li> <li> <p>Revoked: This state indicates that the certificate has been revoked and is no longer valid.</p> </li> <li> <p>Failed: If the validation process fails for any reason, the certificate enters this state.</p> </li> <li> <p>Inactive: This state indicates that the certificate is not currently in use.</p> </li> </ul> <p>The certificate status should obviously be in the Issued state, if not please contact Workbench Support Team.</p>"},{"location":"aws_setup/domain_cert_setup/#retrieving-the-certificate-arn","title":"Retrieving the Certificate ARN","text":"<ol> <li> <p>Open ACM Console:</p> <ul> <li>Go back to the AWS Certificate Manager (ACM) Console.</li> </ul> </li> <li> <p>Check the Status:</p> <ul> <li>Once the CNAME records are added, ACM will automatically validate the domain.</li> <li>Refresh the ACM console to see the updated status.</li> <li>The status will change to \"Issued\" once validation is complete.</li> </ul> </li> <li> <p>Copy the Certificate ARN:</p> <ul> <li>Click on your issued certificate.</li> <li>Copy the Amazon Resource Name (ARN) from the certificate details.</li> </ul> </li> </ol> <p>You now have the ARN for your certificate, which you can use in your AWS resources such as API Gateway, LoadBalancer, CloudFront, etc. Specifically for the Workbench-Dashboard Stack you will need to put this into your Workbench Config file when you deploy/update the stack.</p> <pre><code>\"WORKBENCH_ROLE\": \"Workbench-ExecutionRole\",\n\"WORKBENCH_CERTIFICATE_ARN\": \"arn:aws:acm:&lt;region&gt;:&lt;account&gt;:certificate/123-987-456-123-456789012\",\n</code></pre>"},{"location":"aws_setup/domain_cert_setup/#update-route-53-to-point-to-new-load-balancer","title":"Update Route 53 to Point to New Load Balancer","text":"<p>If you deploy a new stack (new load balancer), you'll have to set up DNS 'A' records to point to that new load balancer.</p>"},{"location":"aws_setup/domain_cert_setup/#updating-a-record-in-route53","title":"Updating A Record in Route53","text":"<ul> <li>Go to Route 53 Console</li> <li>Click Hosted zones (on left panel)</li> </ul>"},{"location":"aws_setup/domain_cert_setup/#new-a-records","title":"New A Record(s)","text":"<ul> <li>Create Record (orange button on right)</li> <li>Leave <code>subdomain</code> blank (for first A record)</li> <li>Click the 'Alias' button (important)</li> <li> <p>Routes traffic to </p> <ul> <li>Alias to Application and Classic Load Balanacer</li> <li>AWS Region</li> <li>Chooser Box (find LB domain)</li> </ul> </li> <li> <p>If you have another subdomain like <code>www.blah.com</code> then just go through the above steps again.</p> </li> </ul> <p>Note: The LB domain should looks something like <code>dualstack.workbe-workb-xyzabc-123456.us-west-2.elb.amazonaws.com</code></p>"},{"location":"aws_setup/domain_cert_setup/#change-a-record","title":"Change A Record","text":"<ul> <li>Click (or add) A records to point to your load balancer internal domain.</li> <li>Leave most of default options</li> <li>For Route Traffic To<ul> <li>Alias to Application and Classic Load Balancer</li> <li>AWS Region</li> <li>Chooser Box (find LB domain)</li> </ul> </li> </ul> <p>Note: The LB domain should looks something like <code>dualstack.workbe-workb-xyzabc-123456.us-west-2.elb.amazonaws.com</code></p>"},{"location":"aws_setup/domain_cert_setup/#aws-resources","title":"AWS Resources","text":"<ul> <li>AWS Adding or Changing DNS Records</li> <li>AWS Certificate Manager (ACM) Documentation</li> <li>Requesting a Public Certificate</li> <li>Validating Domain Ownership</li> <li>AWS Route 53 Documentation</li> <li>AWS API Gateway Documentation</li> </ul>"},{"location":"aws_setup/domain_cert_setup/#reference-materials","title":"Reference Materials","text":""},{"location":"aws_setup/domain_cert_setup/#finding-the-load-balancer-internal-domain","title":"Finding the Load Balancer Internal Domain","text":"<p>Note: This is only for NEW A records. When you update an existing A Record, it should already 'find' this for you and present it as an option.</p> <p>To find the internal domain of your load balancer in AWS:</p> <ol> <li> <p>Go to the AWS Console:</p> <ul> <li>Navigate to EC2 Console (yes, Load Balancers are under EC2).</li> </ul> </li> <li> <p>Find the Load Balancer:</p> <ul> <li>Under Load Balancing, click on Load Balancers.</li> <li>Look for the load balancer associated with your stack.</li> </ul> </li> <li> <p>Check the DNS Name:</p> <ul> <li>Select the load balancer.</li> <li>In the Description tab, look for the DNS Name field. This is the internal domain you\u2019re looking for (something like: Workbe-Workb-xyzabc-123456.us-west-2.elb.amazonaws.com).</li> </ul> </li> <li> <p>Use It for Your A Record:</p> <ul> <li>Copy this DNS name and create the new A record in Route 53 or your DNS provider.</li> </ul> </li> </ol>"},{"location":"aws_setup/full_pipeline/","title":"Testing Full ML Pipeline","text":"<p>Now that the core Workbench AWS Stack has been deployed. Let's test out Workbench by building a full entire AWS ML Pipeline from start to finish. The script <code>build_ml_pipeline.py</code> uses the Workbench API to quickly and easily build an AWS Modeling Pipeline.</p> <p>Taste the Awesome</p> <p>The Workbench \"hello world\" builds a full AWS ML Pipeline. From S3 to deployed model and endpoint. If you have any troubles at all feel free to contact us at workbench email or on Discord and we're happy to help you for FREE.</p> <ul> <li>DataLoader(abalone.csv) --&gt; DataSource</li> <li>DataToFeatureSet Transform --&gt; FeatureSet</li> <li>FeatureSetToModel Transform --&gt; Model</li> <li>ModelToEndpoint Transform --&gt; Endpoint</li> </ul> <p>This script will take a LONG TiME to run, most of the time is waiting on AWS to finalize FeatureGroups, train Models or deploy Endpoints.</p> <p><pre><code>\u276f python build_ml_pipeline.py\n&lt;lot of building ML pipeline outputs&gt;\n</code></pre> After the script completes you will see that it's built out an AWS ML Pipeline and testing artifacts.</p>"},{"location":"aws_setup/full_pipeline/#run-the-workbench-dashboard-local","title":"Run the Workbench Dashboard (Local)","text":"<p>Dashboard AWS Stack</p> <p>Deploying the Dashboard Stack is straight-forward and provides a robust AWS Web Server with Load Balancer, Elastic Container Service, VPC Networks, etc. (see AWS Dashboard Stack)</p> <p>For testing it's nice to run the Dashboard locally, but for longterm use the Workbench Dashboard should be deployed as an AWS Stack. The deployed Stack allows everyone in the company to use, view, and interact with the AWS Machine Learning Artifacts created with Workbench.</p> <p><pre><code>cd workbench/application/aws_dashboard\n./dashboard\n</code></pre> This will open a browser to http://localhost:8000</p> <p> Workbench Dashboard: AWS Pipelines in a Whole New Light! <p>Success</p> <p>Congratulations: Workbench is now deployed to your AWS Account. Deploying the AWS Stack only needs to be done once. Now that this is complete your developers can simply <code>pip install workbench</code> and start using the API.</p> <p>If you ran into any issues with this procedure please contact us via Discord or email workbench@supercowpowers.com and the SCP team will provide free setup and support for new Workbench users.</p>"},{"location":"aws_setup/iam_assume_role/","title":"Setting Up IAM Users to Access Workbench-ExecutionRole","text":"<p>This guide provides step-by-step instructions to configure IAM users to assume the <code>Workbench-ExecutionRole</code> in your AWS account.</p>"},{"location":"aws_setup/iam_assume_role/#prerequisites","title":"Prerequisites","text":"<ul> <li>Administrator permissions to update IAM users and policies.</li> <li>The <code>Workbench-ExecutionRole</code> must already be deployed via the Workbench AWS CDK stack.</li> </ul>"},{"location":"aws_setup/iam_assume_role/#steps-to-update-iam-user-permissions","title":"Steps to Update IAM User Permissions","text":""},{"location":"aws_setup/iam_assume_role/#1-log-in-to-the-aws-management-console","title":"1. Log in to the AWS Management Console","text":"<ul> <li>Navigate to the IAM Console.</li> </ul>"},{"location":"aws_setup/iam_assume_role/#2-select-the-iam-user","title":"2. Select the IAM User","text":"<ol> <li>In the left-hand menu, select Users.</li> <li>Locate and select the IAM user who needs access to the <code>Workbench-ExecutionRole</code>.</li> </ol>"},{"location":"aws_setup/iam_assume_role/#3-attach-an-inline-policy","title":"3. Attach an Inline Policy","text":"<ol> <li>Navigate to the Permissions tab for the IAM user.</li> <li>Click Add inline policy.</li> <li> <p>Select the JSON editor and paste the following policy:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam::&lt;account-id&gt;:role/Workbench-ExecutionRole\"\n        }\n    ]\n}\n</code></pre> <ul> <li>Replace <code>&lt;account-id&gt;</code> with your AWS account ID.</li> </ul> </li> <li> <p>Review and save the policy.</p> </li> </ol>"},{"location":"aws_setup/iam_assume_role/#verifying-access-for-iam-users","title":"Verifying Access for IAM Users","text":"<ol> <li>Log in to the AWS Management Console as the IAM user.</li> <li> <p>Use the following CLI command to test access:</p> <pre><code>aws sts assume-role \\\n    --role-arn arn:aws:iam::&lt;account-id&gt;:role/Workbench-ExecutionRole \\\n    --role-session-name TestSession\n</code></pre> <ul> <li>Replace <code>&lt;account-id&gt;</code> with your AWS account ID.</li> </ul> </li> <li> <p>If successful, you will receive temporary credentials for the <code>Workbench-ExecutionRole</code>.</p> </li> </ol>"},{"location":"aws_setup/iam_assume_role/#troubleshooting","title":"Troubleshooting","text":""},{"location":"aws_setup/iam_assume_role/#common-issues","title":"Common Issues","text":"<ul> <li>Permission Denied: Ensure the correct inline policy is attached to the IAM user.</li> <li>Role Not Found: Verify that the <code>Workbench-ExecutionRole</code> has been deployed correctly.</li> </ul>"},{"location":"aws_setup/iam_assume_role/#contact-support","title":"Contact Support","text":"<p>If you encounter issues, please contact your AWS administrator or reach out to the Workbench support team.</p>"},{"location":"aws_setup/sso_assume_role/","title":"Setting Up SSO Users to Access Workbench-ExecutionRole","text":"<p>This guide provides step-by-step instructions to configure AWS SSO users to assume the <code>Workbench-ExecutionRole</code> in your AWS account.</p>"},{"location":"aws_setup/sso_assume_role/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the management account in your AWS Organization.</li> <li>Administrative permissions to modify AWS SSO permission sets.</li> <li>The <code>Workbench-ExecutionRole</code> must already be deployed via the Workbench AWS CDK stack.</li> </ul>"},{"location":"aws_setup/sso_assume_role/#steps-to-update-sso-permissions","title":"Steps to Update SSO Permissions","text":""},{"location":"aws_setup/sso_assume_role/#1-log-in-to-the-aws-sso-console","title":"1. Log in to the AWS SSO Console","text":"<ul> <li>Login to your 'main' organization AWS Account.</li> <li>Go to the IAM Identity Center</li> </ul>"},{"location":"aws_setup/sso_assume_role/#2-find-the-relevant-permission-set","title":"2. Find the Relevant Permission Set","text":"<ul> <li>In the left menu, select Permission Sets.</li> <li>Locate the permission set used by the group needing access (e.g., <code>DataScientist</code> or another relevant group).</li> </ul>"},{"location":"aws_setup/sso_assume_role/#3-edit-the-permission-set","title":"3. Edit the Permission Set","text":"<ol> <li>Select the permission set</li> <li>Scroll down to Inline policy and click the the Edit button.</li> <li> <p>Add an inline policy with the following content:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam::&lt;account-id&gt;:role/Workbench-ExecutionRole\"\n            \"Resource\": \"arn:aws:iam::&lt;account-id&gt;:role/Workbench-ReadOnlyRole\"\n        }\n    ]\n}\n</code></pre> </li> </ol> <p>Replace <code>&lt;account-id&gt;</code> with your AWS account ID, OR for SSO Groups that span multiple accounts you can add both lines for each account.</p> <pre><code>    \"Resource\": \"arn:aws:iam::&lt;account-1&gt;:role/Workbench-ExecutionRole\"\n    \"Resource\": \"arn:aws:iam::&lt;account-1&gt;:role/Workbench-ReadOnlyRole\"\n\n    \"Resource\": \"arn:aws:iam::&lt;account-2&gt;:role/Workbench-ExecutionRole\"\n    \"Resource\": \"arn:aws:iam::&lt;account-2&gt;:role/Workbench-ReadOnlyRole\"\\\n\n    \"Resource\": \"arn:aws:iam::&lt;account-3&gt;:role/Workbench-ExecutionRole\"\n    \"Resource\": \"arn:aws:iam::&lt;account-3&gt;:role/Workbench-ReadOnlyRole\"\n</code></pre> <p>Please consult with your AWS SSO Administrator for guidance on this process.</p>"},{"location":"aws_setup/sso_assume_role/#4-save-changes","title":"4. Save Changes","text":"<ul> <li>Save the updated permission set.</li> <li>AWS SSO will automatically propagate the changes to all users in the associated group.</li> </ul>"},{"location":"aws_setup/sso_assume_role/#verifying-access-for-sso-users","title":"Verifying Access for SSO Users","text":"<ol> <li>Activate an AWS Profile for the configured SSO group.</li> <li> <p>Use the following CLI command to test access:</p> <pre><code>aws sts assume-role \\\n    --role-arn arn:aws:iam::&lt;account-id&gt;:role/Workbench-ExecutionRole \\\n    --role-session-name TestSession\n\naws sts assume-role \\\n    --role-arn arn:aws:iam::&lt;account-id&gt;:role/Workbench-ReadOnlyRole \\\n    --role-session-name TestSession\n</code></pre> <p>Replace <code>&lt;account-id&gt;</code> with your AWS account ID.</p> </li> <li> <p>If successful, you will receive temporary credentials.</p> </li> </ol>"},{"location":"aws_setup/sso_assume_role/#troubleshooting","title":"Troubleshooting","text":""},{"location":"aws_setup/sso_assume_role/#common-issues","title":"Common Issues","text":"<ul> <li>Permission Denied: Ensure the correct permission set is updated.</li> <li>Role Not Found: Verify that the <code>Workbench-ExecutionRole</code> has been deployed correctly.</li> </ul>"},{"location":"aws_setup/sso_assume_role/#contact-support","title":"Contact Support","text":"<p>If you encounter issues, please contact your AWS administrator or reach out to the Workbench support team.</p>"},{"location":"blogs/","title":"Workbench Blogs","text":"<p>Just Getting Started?</p> <p>Workbench Blogs are a great way to see what's possible with Workbench. When you're ready to jump in, the API Classes will give you details on the Workbench ML Pipeline Classes.</p> <p>Workbench blogs highlight interesting functionality and approaches that might be useful to a broader audience. Each blog gives a high-level overview of the topic with drilldowns into the trickier bits. Whether you're looking for implementation details, architecture decisions, or practical tips for deploying ML models on AWS, these posts cover the real-world challenges we've solved.</p>"},{"location":"blogs/#blogs","title":"Blogs","text":"<ul> <li> <p>Confusion Explorer: Beyond the Confusion Matrix: The standard confusion matrix tells you what your model gets wrong \u2014 the Confusion Explorer shows you why. We pair a residual-colored matrix with an interactive ternary probability plot, linked through a confidence slider. Filter to high-confidence predictions, click a cell to isolate misclassified compounds, and hover to see molecular structures.</p> </li> <li> <p>Model Confidence: Building on Conformal Prediction: How does Workbench approach prediction uncertainty? We walk through our current pipeline \u2014 5-fold ensemble disagreement, conformal calibration for coverage guarantees, and percentile-rank confidence scoring \u2014 discuss the trade-offs, and point to the foundational work we're building on.</p> </li> <li> <p>SHAP Values for ChemProp Models: How do you explain a graph neural network? In this blog we explore our per-bit ablation approach for computing SHAP values on ChemProp MPNN models. We walk through the technical approach, show key code snippets, and analyze real SHAP output from a LogD regression model \u2014 validating that the model independently learns known structure-lipophilicity relationships.</p> </li> <li> <p>Inside a Workbench AWS Endpoint: A deep dive into endpoint architecture \u2014 comparing the default SageMaker stack (Nginx, Gunicorn, Flask) with Workbench's modern ASGI stack (Uvicorn, FastAPI). We cover the custom image with pre-loaded chemistry packages, and Workbench's binary-search error handling that isolates bad rows instead of failing entire batches.</p> </li> <li> <p>Molecular Standardization: Why molecular standardization matters for ML pipelines. We walk through Workbench's four-step pipeline \u2014 cleanup, salt handling, charge neutralization, and tautomer canonicalization. We also describe 2D and 3D molecular descriptors computed by our feature endpoints.</p> </li> <li> <p>Feature Endpoints: From Training to LiveDesign: How Workbench uses SageMaker-hosted feature endpoints to guarantee identical feature computation \u2014 whether the request comes from a training pipeline, an inference endpoint, or a drug discovery platform like LiveDesign or StarDrop. We compare this approach to feature stores, platform UDFs (Databricks/Tecton), and open-source alternatives.</p> </li> </ul>"},{"location":"blogs/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"blogs/aws_endpoint_architecture/","title":"Inside a Workbench AWS Endpoint: A Modern Web Stack for ML Inference","text":"<p>Deploy Your Own</p> <p>Ready to deploy? The Endpoint API walks you through creating and managing endpoints on AWS.</p> <p>When you deploy a model endpoint on AWS SageMaker, the default architecture gives you a battle-tested but aging web stack. Workbench takes a different approach \u2014 building custom container images with a modern ASGI stack that delivers better performance and native async support. In this blog we'll compare the two architectures and explain why the Workbench stack is a better foundation for production ML inference.</p> The layered architecture of a Workbench endpoint: from the custom image down to the model artifacts."},{"location":"blogs/aws_endpoint_architecture/#the-default-sagemaker-stack-nginx-gunicorn-flask","title":"The Default SageMaker Stack: Nginx + Gunicorn + Flask","text":"<p>When you follow AWS's canonical \"bring your own container\" pattern for SageMaker endpoints, you get a three-tier web stack that's been the reference architecture for years:</p>"},{"location":"blogs/aws_endpoint_architecture/#nginx-reverse-proxy","title":"Nginx (Reverse Proxy)","text":"<p>Nginx sits at the front, listening on port 8080 (SageMaker's required port). It accepts incoming HTTP requests from the SageMaker runtime infrastructure and forwards them to the application server over a Unix socket. It handles buffering, connection management, and returns 404 for anything that isn't <code>/ping</code> or <code>/invocations</code>.</p>"},{"location":"blogs/aws_endpoint_architecture/#gunicorn-wsgi-application-server","title":"Gunicorn (WSGI Application Server)","text":"<p>Gunicorn is a pre-fork worker server that spawns multiple copies of the Flask application \u2014 typically one worker per CPU core. Each worker is an independent OS process running the synchronous WSGI protocol, meaning it handles exactly one request at a time. When a worker is processing an inference request, it's blocked until that request completes.</p>"},{"location":"blogs/aws_endpoint_architecture/#flask-web-framework","title":"Flask (Web Framework)","text":"<p>Flask defines the two required endpoints: <code>GET /ping</code> for health checks and <code>POST /invocations</code> for inference. It's lightweight and well-understood, but it's a synchronous WSGI framework \u2014 no native support for async I/O, streaming responses, or WebSocket connections.</p>"},{"location":"blogs/aws_endpoint_architecture/#the-request-flow","title":"The Request Flow","text":"<p>Each request passes through three layers of process/socket boundaries before reaching your model code. Gunicorn's sync workers mean that concurrency is limited to the number of worker processes \u2014 and each worker loads a full copy of the model into memory.</p>"},{"location":"blogs/aws_endpoint_architecture/#whats-wrong-with-the-default-stack","title":"What's Wrong with the Default Stack?","text":"<p>The Nginx/Gunicorn/Flask stack works, but it has real limitations for modern ML inference workloads:</p> <p>Synchronous-only processing. WSGI is a synchronous protocol from 2003. Each Gunicorn worker blocks on a single request. If your inference involves any I/O \u2014 loading data, calling external services, batching \u2014 the worker sits idle waiting instead of handling other requests.</p> <p>No native streaming support. SageMaker now supports response streaming via <code>InvokeEndpointWithResponseStream</code>, but WSGI and Flask can't do this natively. You need a fundamentally different server architecture to stream tokens or partial results back to the client.</p> <p>Memory-heavy concurrency. Gunicorn achieves concurrency by forking worker processes. Each process loads the full Python interpreter and model into memory. Want 8 concurrent requests? You need 8 copies of your model in RAM.</p> <p>An aging ecosystem. Flask and Gunicorn are mature and stable, but the Python web ecosystem has moved on. The ASGI standard, async/await, and frameworks like FastAPI represent the modern approach to building high-performance Python web services.</p>"},{"location":"blogs/aws_endpoint_architecture/#the-workbench-stack-uvicorn-fastapi","title":"The Workbench Stack: Uvicorn + FastAPI","text":"<p>Workbench endpoints replace the entire default stack with a modern ASGI architecture built on two components:</p>"},{"location":"blogs/aws_endpoint_architecture/#uvicorn-asgi-server","title":"Uvicorn (ASGI Server)","text":"<p>Uvicorn is a high-performance ASGI server built on <code>uvloop</code> (a fast, drop-in replacement for Python's <code>asyncio</code> event loop) and <code>httptools</code> (a fast HTTP parser based on Node.js's http-parser). It handles HTTP connections directly \u2014 no Nginx reverse proxy needed.</p> <p>Key advantages over Gunicorn + Nginx:</p> <ul> <li>Async I/O: A single Uvicorn worker can handle many concurrent connections using Python's <code>async/await</code>. While one request waits on model loading or I/O, other requests proceed.</li> <li>Fewer moving parts: One server process replaces two (Nginx + Gunicorn). Fewer processes means simpler debugging, fewer configuration files, and fewer failure modes.</li> <li>Native HTTP/1.1 and WebSocket support: ASGI natively supports streaming responses and bidirectional communication \u2014 critical for streaming inference results.</li> </ul>"},{"location":"blogs/aws_endpoint_architecture/#fastapi-asgi-web-framework","title":"FastAPI (ASGI Web Framework)","text":"<p>FastAPI is a modern Python web framework built on ASGI and Pydantic. It's what makes Workbench endpoints type-safe and production-ready:</p> <ul> <li>Pydantic models for request/response validation: Input data is validated against typed schemas before your model code ever sees it. Bad requests get clear error messages automatically.</li> <li>Dependency injection: Shared resources (model loading, configuration) are managed cleanly without global state or singletons.</li> <li>Native async/await: Endpoint handlers can be <code>async def</code>, enabling non-blocking I/O throughout the request lifecycle.</li> </ul>"},{"location":"blogs/aws_endpoint_architecture/#the-workbench-request-flow","title":"The Workbench Request Flow","text":"<p>Two layers instead of three. Async instead of sync. Typed schemas instead of manual parsing.</p>"},{"location":"blogs/aws_endpoint_architecture/#dataframe-in-dataframe-out","title":"DataFrame In, DataFrame Out","text":"<p>At the heart of every Workbench endpoint is a simple contract: send a DataFrame, get a DataFrame back. The model script layer handles the translation between HTTP and pandas, so your inference code works with familiar DataFrames rather than raw bytes or JSON blobs.</p> <p>The FastAPI <code>/invocations</code> handler orchestrates three functions that every model script defines:</p> <pre><code>@app.post(\"/invocations\")\nasync def invoke(request: Request):\n    body = await request.body()\n    data = inference_module.input_fn(body, content_type)      # \u2192 DataFrame\n    result = inference_module.predict_fn(data, model)         # \u2192 DataFrame\n    output_data = inference_module.output_fn(result, accept)  # \u2192 CSV/JSON\n    return Response(content=output_data)\n</code></pre> <ul> <li><code>input_fn</code> parses the raw request body into a DataFrame \u2014 supports both CSV and JSON. </li> <li><code>predict_fn</code> runs dataframe through the model, and returns a new DataFrame with predictions appended. </li> <li><code>output_fn</code> serializes the result back to CSV or JSON for the response.</li> </ul> <p>This pattern means model scripts are simplier and that calling inference on an endpoint is as easy as sending a DataFrame:</p> <pre><code># Grab Endpoint\nend = Endpoint(\"my_awesome_endpoint\")\n\n# Grab data that I want to predict on\nmy_data = &lt;my inference data&gt;\n\n# Send DataFrame to endpoint and get predictions back as a new DataFrame\npredictions = end.inference(my_data)\n</code></pre>"},{"location":"blogs/aws_endpoint_architecture/#why-dataframes-matter","title":"Why DataFrames Matter","text":"<p>This isn't just a convenience \u2014 it's a design decision that pays off across the entire ML lifecycle:</p> <p>Column preservation. The input DataFrame passes through prediction intact. ID columns, target values, metadata \u2014 everything comes back alongside the predictions. No need to rejoin predictions to your original data by row index and hope the alignment is correct.</p> <p>Case-insensitive feature matching. Workbench model scripts use <code>match_features_case_insensitive()</code> to handle column name variations. If your FeatureSet has <code>LogP</code> but your eval DataFrame has <code>logp</code>, inference still works \u2014 the model script renames columns to match the model's expectations automatically.</p> <p>Type handling across the wire. CSV serialization strips type information \u2014 everything becomes a string. The client-side <code>_predict</code> method handles the round-trip automatically: numeric columns are converted back with <code>pd.to_numeric()</code>, <code>N/A</code> placeholders (used because CSV can't represent <code>NaN</code> natively) are restored to proper <code>NaN</code> values, <code>__NA__</code> placeholders for <code>pd.NA</code> survive the round-trip, and boolean strings (<code>\"true\"/\"false\"</code>) are converted back to Python booleans. The result is a DataFrame that looks like you never serialized it at all.</p> <p>Consistent interface across model frameworks. Whether your model is XGBoost, PyTorch, or ChemProp, the contract is the same: DataFrame in, DataFrame out. The model script handles framework-specific details (loading XGBoost models, running ChemProp graph inference, expanding classifier probability columns) while the caller always works with the same pandas interface.</p>"},{"location":"blogs/aws_endpoint_architecture/#custom-image-more-than-just-the-web-stack","title":"Custom Image: More Than Just the Web Stack","text":"<p>The Workbench custom image isn't only about Uvicorn and FastAPI \u2014 it's a purpose-built environment for computational chemistry and ADMET modeling. The image comes pre-loaded with:</p> Package Purpose RDKitMolecular parsing, descriptor computation, substructure search MordredAdditional molecular descriptors (ADMET-focused modules) ChemPropMessage-passing neural network (MPNN) inference for molecular property prediction XGBoostGradient-boosted tree model inference PyTorchNeural network inference (ChemProp backend) scikit-learnClassical ML model inference and preprocessing <p>This means endpoint model scripts can import these packages directly without bundling them into the model artifact. The container image handles the complex dependency chain (RDKit's C++ extensions, PyTorch's CUDA bindings, Mordred's dependency on NetworkX) so your model script stays focused on inference logic.</p>"},{"location":"blogs/aws_endpoint_architecture/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"Aspect Default Stack Workbench Stack Web ServerNginx + GunicornUvicorn FrameworkFlask (WSGI)FastAPI (ASGI) ProtocolWSGI (synchronous)ASGI (async-native) Concurrency ModelProcess forking (1 req/worker)Async event loop (many req/worker) StreamingNot natively supportedNative ASGI streaming Request ValidationManualAutomatic (Pydantic) Process Count3 (Nginx + Gunicorn + Flask)1 (Uvicorn + FastAPI) Chemistry PackagesInstall yourselfPre-loaded (RDKit, Mordred, ChemProp)"},{"location":"blogs/aws_endpoint_architecture/#why-it-matters-for-ml-inference","title":"Why It Matters for ML Inference","text":"<p>The architecture differences aren't academic \u2014 they translate directly to operational benefits:</p> <p>Simpler debugging. One server process with structured FastAPI logging beats digging through Nginx access logs, Gunicorn error logs, and Flask tracebacks across three processes.</p> <p>Better resource utilization. Async I/O means a single worker can overlap model loading, preprocessing, and response serialization. You're not paying for idle workers blocked on I/O.</p> <p>Ready for advanced patterns. Streaming inference results, health checks with detailed model status, batch preprocessing with async gathering \u2014 these patterns fall out naturally from the ASGI architecture. With WSGI, each one requires workarounds.</p> <p>Production-ready chemistry stack. The custom image means you don't spend hours debugging RDKit compilation issues or Mordred version conflicts inside a SageMaker container. Deploy your model script, and the chemistry packages are already there.</p>"},{"location":"blogs/aws_endpoint_architecture/#robust-error-handling-binary-search-for-bad-rows","title":"Robust Error Handling: Binary Search for Bad Rows","text":"<p>Production inference means dealing with messy data \u2014 invalid SMILES strings, malformed feature values, and edge cases that crash model scripts. The default SageMaker pattern gives you nothing here: if a single bad row exists in your batch, the entire request fails with a <code>ModelError</code> and you get no predictions back.</p> <p>Workbench takes a fundamentally different approach. When the endpoint returns a <code>ModelError</code>, Workbench automatically bisects the batch and retries each half recursively. This binary search narrows down to the exact problematic row(s) while still returning predictions for every valid row:</p> <pre><code>def _endpoint_error_handling(self, predictor, feature_df, drop_error_rows=False):\n    try:\n        results = predictor.predict(csv_buffer.getvalue())\n        return pd.DataFrame.from_records(results[1:], columns=results[0])\n\n    except botocore.exceptions.ClientError as err:\n        if err.response[\"Error\"][\"Code\"] == \"ModelError\":\n            # Base case: single row that fails\n            if len(feature_df) == 1:\n                if drop_error_rows:\n                    return pd.DataFrame(columns=feature_df.columns)\n                return self._fill_with_nans(feature_df)  # NaN placeholder\n\n            # Binary search: split and retry both halves\n            mid_point = len(feature_df) // 2\n            first_half = self._endpoint_error_handling(predictor, feature_df.iloc[:mid_point])\n            second_half = self._endpoint_error_handling(predictor, feature_df.iloc[mid_point:])\n            return pd.concat([first_half, second_half], ignore_index=True)\n</code></pre> <p>The algorithm handles several scenarios gracefully:</p> <ul> <li><code>ModelNotReadyException</code>: Sleeps and retries \u2014 common with serverless endpoints that cold-start.</li> <li><code>ModelError</code> with multiple rows: Bisects the batch recursively until the bad row(s) are isolated.</li> <li>Single bad row: Either fills with NaN placeholders (preserving row alignment) or drops the row entirely, based on the <code>drop_error_rows</code> parameter.</li> <li>Unexpected errors: Logs full error context and raises \u2014 no silent failures.</li> </ul> <p>This means you can send 10,000 rows to an endpoint, have 3 of them contain invalid data, and get back 10,000 predictions \u2014 9,997 real values and 3 NaN placeholders. The alternative? Manually chunking your data, catching errors, and hoping you can figure out which rows caused the failure. Workbench handles all of this automatically with logarithmic overhead (a batch of 1,000 with one bad row requires only ~10 extra endpoint calls to isolate it).</p> <p>The <code>N/A</code> \u2192 <code>NaN</code> conversion and automatic type recovery in <code>_predict</code> further smooths over the rough edges of CSV serialization \u2014 numeric columns that SageMaker's CSV deserializer returns as strings get converted back to proper numeric types, and <code>pd.NA</code> placeholders survive the round-trip through <code>__NA__</code> encoding.</p>"},{"location":"blogs/aws_endpoint_architecture/#summary","title":"Summary","text":"<p>AWS SageMaker's default endpoint architecture \u2014 Nginx, Gunicorn, and Flask \u2014 is a proven stack that's been serving models reliably for years. But it's a synchronous, WSGI-era design that shows its age when you need async processing, or streaming responses.</p> <p>Workbench replaces this with Uvicorn and FastAPI: a modern ASGI stack that's simpler (fewer processes), more capable (async, streaming, auto-docs), and purpose-built for computational chemistry workloads. The DataFrame-in/DataFrame-out contract means model scripts work with familiar pandas code while the framework handles serialization, type recovery, and case-insensitive feature matching automatically. On top of that, robust error handling uses binary search to isolate bad rows in inference batches \u2014 so a single malformed SMILES string doesn't take down your entire prediction run. Combined with a custom image pre-loaded with RDKit, Mordred, and ChemProp, Workbench endpoints give you a production-ready ML inference platform without the infrastructure headaches.</p>"},{"location":"blogs/aws_endpoint_architecture/#references","title":"References","text":"<ul> <li>FastAPI: Ram\u00edrez, S. FastAPI: Modern, Fast (high-performance) Web Framework for Building APIs. https://fastapi.tiangolo.com/</li> <li>Uvicorn: Encode. Uvicorn: An ASGI Web Server. https://www.uvicorn.org/</li> <li>ASGI Specification: Django Software Foundation. Asynchronous Server Gateway Interface. https://asgi.readthedocs.io/</li> <li>SageMaker Custom Containers: AWS. Use Your Own Inference Code with Hosting Services. https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html</li> <li>Gunicorn: Chesneau, B. Gunicorn: Python WSGI HTTP Server for UNIX. https://gunicorn.org/</li> <li>RDKit: https://github.com/rdkit/rdkit</li> <li>ChemProp: https://github.com/chemprop/chemprop</li> <li>Mordred: https://github.com/mordred-descriptor/mordred</li> </ul>"},{"location":"blogs/aws_endpoint_architecture/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"blogs/chemprop_shap/","title":"SHAP Values for ChemProp Models","text":"<p>ChemProp in Workbench</p> <p>Workbench wraps ChemProp as a first-class model type. See the Model API for details on training and inference.</p> <p>In this blog we'll explore how Workbench computes SHAP (SHapley Additive exPlanations) values for ChemProp message-passing neural network (MPNN) models. ChemProp models operate directly on molecular graphs rather than tabular feature vectors, which makes interpretability a unique challenge. We'll walk through the per-bit ablation approach we developed, show some key code snippets, and then analyze real SHAP output from a LogD regression model.</p> Workbench provides visibility and drilldowns into Chemprop models deployed to AWS Endpoints."},{"location":"blogs/chemprop_shap/#the-challenge-explaining-a-graph-neural-network","title":"The Challenge: Explaining a Graph Neural Network","text":"<p>Traditional ML models (XGBoost, PyTorch feed-forward networks) take a fixed-length feature vector as input. Computing SHAP values is straightforward: mask or permute individual features and measure the prediction change.</p> <p>ChemProp is different. It takes a molecular graph as input \u2014 atoms are nodes, bonds are edges, and each carries a multi-hot encoded feature vector. There's no single \"feature matrix\" to permute. So how do we ask: \"Which molecular properties matter most for this prediction?\"</p>"},{"location":"blogs/chemprop_shap/#our-approach-per-bit-feature-ablation","title":"Our Approach: Per-Bit Feature Ablation","text":"<p>ChemProp's default v2 featurizers encode each atom as a 72-bit vector and each bond as a 14-bit vector (86 bits total). Each bit represents a specific chemical property:</p> Category Example Bits Atom type<code>atom=C</code>, <code>atom=N</code>, <code>atom=O</code>, <code>atom=S</code>, <code>atom=Cl</code>, <code>atom=F</code> Degree<code>degree=1</code>, <code>degree=2</code>, <code>degree=3</code>, <code>degree=4</code> Hybridization<code>hybrid=SP</code>, <code>hybrid=SP2</code>, <code>hybrid=SP3</code> Hydrogen count<code>num_Hs=0</code>, <code>num_Hs=1</code>, <code>num_Hs=2</code>, <code>num_Hs=3</code> Bond type<code>bond=SINGLE</code>, <code>bond=DOUBLE</code>, <code>bond=AROMATIC</code> Ring/conjugation<code>is_aromatic</code>, <code>is_in_ring</code>, <code>is_conjugated</code> Chirality<code>chiral=CW</code>, <code>chiral=CCW</code> <p>Our approach selectively zeroes out individual bits across all atoms/bonds in a molecule, then measures the prediction change. If zeroing out the \"atom=N\" bit (effectively hiding all nitrogen atoms from the model) causes a large shift in the predicted value, then nitrogen content is important for that molecule's prediction.</p> The ablation pipeline: bit masking happens at the feature vector level, after RDKit has already constructed a valid molecular graph. <p>What this measures. Because the ablation operates on feature vectors rather than the molecular graph itself, zeroing out a bit like <code>num_Hs</code> can create atom representations that don't correspond to real chemistry \u2014 the graph topology stays intact but the feature annotations become chemically implausible. This means we're measuring \"how sensitive is the model to this feature bit\" rather than \"what happens when we make a specific chemical substitution.\" It's an approximation, but a useful one for validating that the model has learned sensible structure-property relationships.</p>"},{"location":"blogs/chemprop_shap/#custom-ablation-featurizers","title":"Custom Ablation Featurizers","text":"<p>We extend ChemProp's built-in featurizers with a <code>keep_mask</code> \u2014 a boolean array where <code>False</code> means \"zero out this bit\":</p> <pre><code>class BitAblationAtomFeaturizer(MultiHotAtomFeaturizer):\n    def __init__(self, keep_mask=None, **kwargs):\n        super().__init__(**kwargs)\n        self.keep_mask = keep_mask or np.ones(len(self), dtype=bool)\n\n    def __call__(self, atom):\n        x = super().__call__(atom)  # Normal featurization\n        x[~self.keep_mask] = 0     # Zero out ablated bits\n        return x\n</code></pre> <p>The same pattern applies for <code>BitAblationBondFeaturizer</code>. This lets us selectively ablate any combination of features while keeping the rest of the molecular graph intact.</p>"},{"location":"blogs/chemprop_shap/#active-bit-filtering","title":"Active-Bit Filtering","text":"<p>Not all 86 bits are used by every dataset. A dataset of organic drug-like molecules won't contain silicon or germanium atoms, so those bits are always zero. We scan the sampled molecules to find which bits are actually \"active\" in the dataset, typically reducing the feature count from 86 to around 25-35 active features. This keeps the SHAP computation focused and the output readable.</p> <pre><code>def _analyze_molecules(smiles_list, atom_feat, bond_feat):\n    \"\"\"Scan molecules to find active bits and compute feature fractions.\"\"\"\n    for mol_idx, smi in enumerate(smiles_list):\n        mol = Chem.MolFromSmiles(smi)\n        for atom in mol.GetAtoms():\n            vec = atom_feat(atom)\n            atom_active |= (vec != 0)   # Track which bits fire\n            atom_sum += np.abs(vec)      # Accumulate for fractions\n        feature_fractions[mol_idx] = atom_sum / n_atoms  # Per-molecule fraction\n    ...\n</code></pre>"},{"location":"blogs/chemprop_shap/#shap-computation-with-permutationexplainer","title":"SHAP Computation with PermutationExplainer","text":"<p>For each molecule, we wrap the ablation logic in a callable that SHAP's <code>PermutationExplainer</code> can drive. The explainer systematically toggles feature bits on/off and measures the prediction impact:</p> <pre><code>explainer = shap.PermutationExplainer(model_wrapper, masker=binary_masker)\nexplanation = explainer(all_features_on, max_evals=100)\n</code></pre> <p>We use <code>PermutationExplainer</code> rather than <code>GradientExplainer</code> or <code>KernelExplainer</code> because the feature ablation happens at the graph-construction level (not at a differentiable tensor), so gradient-based methods don't apply.</p>"},{"location":"blogs/chemprop_shap/#meaningful-feature-values-for-the-beeswarm-plot","title":"Meaningful Feature Values for the Beeswarm Plot","text":"<p>A SHAP beeswarm plot colors each dot by the feature's value for that sample \u2014 high values in red, low values in blue. But what does \"feature value\" mean for a graph-level bit like <code>atom=N</code>?</p> <p>We compute per-molecule feature fractions: for each molecule, the fraction of atoms (or bonds) that activate each bit. For example:</p> <ul> <li>A molecule with 2 nitrogen atoms out of 20 total atoms gets <code>atom=N = 0.10</code></li> <li>A molecule with 6 aromatic bonds out of 8 total bonds gets <code>bond=AROMATIC = 0.75</code></li> </ul> <p>This gives each dot on the beeswarm plot a meaningful color: molecules with high nitrogen content are colored differently from low-nitrogen molecules on the <code>atom=N</code> row.</p>"},{"location":"blogs/chemprop_shap/#filtering-constant-features","title":"Filtering Constant Features","text":"<p>Some features have the same fraction across virtually all molecules in a dataset (e.g., <code>charge=+0</code> \u2248 1.0 for nearly every organic molecule). These constant-fraction features can't help differentiate why one molecule behaves differently from another, so we filter them from the beeswarm plot while keeping them in the importance rankings for completeness.</p>"},{"location":"blogs/chemprop_shap/#analyzing-a-logd-model","title":"Analyzing a LogD Model","text":"<p>Here's the SHAP output from a ChemProp model trained to predict LogD (the distribution coefficient \u2014 a measure of lipophilicity at physiological pH). LogD is a critical ADME property in drug discovery: it influences membrane permeability, protein binding, metabolism, and oral absorption.</p> SHAP beeswarm plot for a ChemProp LogD regression model. Features are ranked by mean |SHAP| (most impactful at top)."},{"location":"blogs/chemprop_shap/#model-insights-what-the-shap-values-reveal","title":"Model Insights: What the SHAP Values Reveal","text":"<p>The SHAP values reveal that the ChemProp model has independently learned the fundamental structure-lipophilicity relationships that medicinal chemists rely on:</p>"},{"location":"blogs/chemprop_shap/#carbon-content-atomc-top-feature","title":"Carbon Content (<code>atom=C</code>) \u2014 Top Feature","text":"<p>Carbon fraction is the dominant predictor, and the pattern is clear: higher carbon fraction (red dots) pushes LogD positive (more lipophilic). This is textbook \u2014 carbon-rich molecules are hydrophobic. The spread of SHAP values (roughly 0.5 to 3.5) shows this feature has good discriminating power.</p>"},{"location":"blogs/chemprop_shap/#nitrogen-content-atomn-strong-negative-driver","title":"Nitrogen Content (<code>atom=N</code>) \u2014 Strong Negative Driver","text":"<p>Nitrogen is the second most important feature and shows the opposite pattern: higher nitrogen fraction (red dots) pushes LogD negative (more hydrophilic). Nitrogen atoms introduce hydrogen-bond donors/acceptors that increase water solubility. The two red outliers far to the left (SHAP \u2248 -4 to -5) are likely molecules with very high nitrogen density (e.g., tetrazoles, triazines).</p>"},{"location":"blogs/chemprop_shap/#hydrogen-count-num_hs0-num_hs2","title":"Hydrogen Count (<code>num_Hs=0</code>, <code>num_Hs=2</code>)","text":"<p>Atoms with zero attached hydrogens (<code>num_Hs=0</code>) appear third \u2014 these are typically heteroatoms in aromatic rings or heavily substituted carbons. High fractions push LogD up slightly, consistent with aromatic/hydrophobic character. Meanwhile, <code>num_Hs=2</code> (methylene groups, primary amines) has a more mixed effect.</p>"},{"location":"blogs/chemprop_shap/#hybridization-and-aromaticity","title":"Hybridization and Aromaticity","text":"<p><code>hybrid=SP2</code> and <code>is_aromatic</code> both rank highly. SP2-hybridized atoms and aromatic rings are flat, planar structures with significant hydrophobic surface area that resists solvation by water, driving lipophilicity. The model captures this: high aromaticity generally pushes LogD positive.</p>"},{"location":"blogs/chemprop_shap/#bond-types-bondaromatic-vs-bondsingle","title":"Bond Types (<code>bond=AROMATIC</code> vs <code>bond=SINGLE</code>)","text":"<p>Aromatic bonds rank higher than single bonds in importance. Molecules rich in aromatic bonds (red dots on the <code>bond=AROMATIC</code> row) tend toward higher LogD \u2014 again consistent with the lipophilic nature of aromatic ring systems.</p>"},{"location":"blogs/chemprop_shap/#degree-of-substitution-degree3-degree2","title":"Degree of Substitution (<code>degree=3</code>, <code>degree=2</code>)","text":"<p>Higher connectivity (degree=3: branching points, ring junctions) appears with moderate importance. Branched and ring-fused structures are common in lipophilic drug scaffolds.</p>"},{"location":"blogs/chemprop_shap/#oxygen-and-halogens-atomo-atomcl-atoms-atomf","title":"Oxygen and Halogens (<code>atom=O</code>, <code>atom=Cl</code>, <code>atom=S</code>, <code>atom=F</code>)","text":"<p>These appear lower in the ranking but tell a coherent story:</p> <ul> <li>Oxygen (<code>atom=O</code>): Pushes LogD negative \u2014 hydroxyl groups, carbonyls, and ethers increase polarity and water solubility</li> <li>Chlorine (<code>atom=Cl</code>): Moderate positive push \u2014 halogens increase lipophilicity, and chlorine substituents are classic LogP-raising groups</li> <li>Sulfur (<code>atom=S</code>): Small effect, consistent with sulfur's moderate lipophilicity contribution</li> <li>Fluorine (<code>atom=F</code>): Appears near the bottom \u2014 fluorine has complex effects on LogD (can increase or decrease depending on context)</li> </ul>"},{"location":"blogs/chemprop_shap/#the-big-picture","title":"The Big Picture","text":"<p>The model has independently learned the fundamental structure-lipophilicity relationships that medicinal chemists rely on:</p> <ol> <li>More carbon \u2192 more lipophilic (higher LogD)</li> <li>More nitrogen/oxygen \u2192 more hydrophilic (lower LogD)</li> <li>Aromaticity and SP2 character \u2192 more lipophilic</li> <li>Halogens (Cl) \u2192 more lipophilic</li> </ol> <p>These aren't rules we gave the model \u2014 they emerged from the MPNN learning on molecular graphs. The fact that SHAP reveals these well-established SAR trends validates both the model and the interpretability approach.</p>"},{"location":"blogs/chemprop_shap/#summary","title":"Summary","text":"<p>Computing SHAP values for graph neural networks requires creative approaches since traditional feature-perturbation methods don't directly apply. Our per-bit ablation strategy bridges this gap by operating at the featurizer level \u2014 selectively hiding individual chemical properties and measuring the prediction impact. Combined with per-molecule feature fractions for meaningful beeswarm coloring and automatic filtering of constant features, this gives practitioners an interpretable window into what their ChemProp models have learned.</p>"},{"location":"blogs/chemprop_shap/#references","title":"References","text":"<p>ChemProp v2 \u2014 The message-passing neural network framework that Workbench uses for molecular property prediction:</p> <ul> <li>Chemprop v2 Paper: Iunusova, E., Pang, H.W., Greenman, K.P., et al. \"Chemprop v2: An Efficient, Modular Machine Learning Package for Chemical Property Prediction.\" Journal of Chemical Information and Modeling (2025). DOI: 10.1021/acs.jcim.5c02332</li> <li>Chemprop v1 Paper: Heid, E., Greenman, K.P., Chung, Y., et al. \"Chemprop: A Machine Learning Package for Chemical Property Prediction.\" Journal of Chemical Information and Modeling 64(1), 9\u201317 (2024). DOI: 10.1021/acs.jcim.3c01250</li> <li>GitHub: https://github.com/chemprop/chemprop</li> </ul> <p>SHAP \u2014 The interpretability framework for computing Shapley values:</p> <ul> <li>Lundberg, S.M. &amp; Lee, S.-I. \"A Unified Approach to Interpreting Model Predictions.\" Advances in Neural Information Processing Systems 30 (NeurIPS 2017). https://arxiv.org/abs/1705.07874</li> <li>GitHub: https://github.com/shap/shap</li> </ul> <p>ChemProp Shapley Value Notebook \u2014 The official chemprop v2 notebook that our per-bit ablation approach is based on:</p> <ul> <li>\"Shapley Value Analysis for Chemprop Models\" \u2014 demonstrates customized featurizers for SHAP analysis with PermutationExplainer. Chemprop Docs: Shapley Value with Customized Featurizers</li> <li>Li, W., Wu, H., et al. \"When Do Quantum Mechanical Descriptors Help Graph Neural Networks to Predict Chemical Properties?\" Journal of the American Chemical Society 146(33), 23103\u201323120 (2024). DOI: 10.1021/jacs.4c04670</li> </ul>"},{"location":"blogs/chemprop_shap/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"blogs/confusion_explorer/","title":"Confusion Explorer: Beyond the Confusion Matrix","text":"<p>How Does Confidence Work?</p> <p>The confidence slider is powered by VGMU \u2014 see our Model Confidence blog for the full details on how Workbench scores prediction uncertainty.</p> <p>Classification models get a lot of mileage out of the standard confusion matrix \u2014 it's simple, familiar, and gives you a quick read on where your model is getting things right and wrong. But when you want to understand why certain compounds are being misclassified and how confident the model was when it got them wrong, the traditional matrix falls short. The Confusion Explorer pairs an enhanced confusion matrix with an interactive ternary probability plot, giving you a linked view that connects aggregate performance to individual predictions.</p> Residual-colored confusion matrix linked to a ternary probability plot. Cell opacity reflects count, color indicates error severity. Each point in the triangle is one prediction positioned by its class probabilities."},{"location":"blogs/confusion_explorer/#the-ternary-probability-plot","title":"The Ternary Probability Plot","text":"<p>The right side of the explorer shows a ternary plot \u2014 each point is a single prediction, positioned according to its three class probabilities. Points in the corners have high confidence for a single class; points near the center are uncertain. The dashed lines partition the triangle into the three decision regions.</p> <p>Coloring by residual (the default) makes misclassifications immediately visible. Correct predictions appear in blue/purple and cluster within their class's decision region. Green and red points are off-by-one and off-by-two errors, respectively. Even at a glance, you can see the spatial structure of your model's mistakes \u2014 misclassified compounds tend to sit near decision boundaries, exactly where you'd expect the model to struggle.</p>"},{"location":"blogs/confusion_explorer/#confidence-aware-analysis","title":"Confidence-Aware Analysis","text":"<p>Confidence Slider</p> <p>The confidence slider filters both the matrix and triangle simultaneously. Drag the lower bound up to focus on only the predictions the model is most sure about.</p> <p>A key insight in model evaluation is that not all predictions deserve equal scrutiny. A model that's uncertain about a compound and gets it wrong is behaving reasonably \u2014 that's an expected failure mode. But a model that's highly confident and wrong? That's where you should focus your attention.</p> <p>The confidence slider at the top lets you filter predictions by their confidence score. When you slide the lower bound up to 0.5 or higher, you're asking: \"Among the predictions the model is most confident about, how does it perform?\"</p> Filtering to high-confidence predictions (above 0.5) yields zero misclassifications. Well-separated clusters in each corner confirm the model is accurate and confident. <p>In the screenshot above, filtering to high-confidence predictions yields a perfect confusion matrix \u2014 zero misclassifications. The ternary plot confirms this: every point sits deep in its correct corner, far from the decision boundaries. This is exactly what you want to see \u2014 it means the model's confidence scores are well-calibrated and can be used to prioritize which predictions to trust.</p>"},{"location":"blogs/confusion_explorer/#drilling-down-on-errors","title":"Drilling Down on Errors","text":"<p>Clicking any cell in the confusion matrix filters the triangle to show only the compounds in that cell. Non-matching compounds are dimmed, and the selected cell gets a highlight border. This is where the real investigative work happens.</p> Clicking the \"low actual / high predicted\" cell highlights those 5 errors on the triangle. Hovering reveals compound A-5550, a sulfonic acid misclassified from low to high solubility. <p>In this example, we've clicked the off-diagonal cell showing 5 compounds that were actually \"low\" solubility but predicted as \"high.\" The triangle plot zooms in on just those 5 points, and hovering reveals the molecular structure. This closes the loop from aggregate metrics to actionable chemistry \u2014 you can inspect exactly which compounds are confusing the model and look for structural patterns that might explain the misclassification.</p>"},{"location":"blogs/confusion_explorer/#visual-design-choices","title":"Visual Design Choices","text":"<p>A few design details that make the explorer more informative:</p> <ul> <li>Residual coloring: The confusion matrix uses the same colorscale as the ternary plot \u2014 diagonal cells (correct) are muted, off-diagonal cells get progressively warmer colors based on distance from the diagonal. This makes error severity visible at a glance.</li> <li>Log-scaled opacity: Cell opacity is proportional to <code>log(count)</code>, so high-count cells stand out while low-count cells (including zeros) fade into the background. This prevents rare misclassifications from visually competing with dominant cells.</li> <li>Linked interaction: The confidence slider and matrix clicks both act as filters \u2014 they update the triangle without resetting your color selection. You can switch between coloring by residual, prediction, confidence, or confusion without losing your current filter state.</li> </ul>"},{"location":"blogs/confusion_explorer/#under-the-hood-vgmu-confidence","title":"Under the Hood: VGMU Confidence","text":"<p>The confidence scores driving the slider come from VGMU (Variance-Gated Margin Uncertainty), which combines two signals from the 5-model ensemble: the probability margin between the top two classes and the ensemble's disagreement on those probabilities. This produces scores where high confidence means both a clear winner and model agreement \u2014 not just a high max probability. The scores are then calibrated via isotonic regression so that a confidence of 0.85 genuinely reflects ~85% accuracy. For the full details on how Workbench computes confidence for both classification and regression models, see our Model Confidence blog.</p>"},{"location":"blogs/confusion_explorer/#references","title":"References","text":"<ul> <li>Plotly Ternary Plots \u2014 Plotly's ternary plot documentation, the visualization framework underlying the Confusion Explorer</li> <li>Weiss et al., \"Variance-Gated Ensembles: An Epistemic-Aware Framework\" (2025) \u2014 The VGMU approach for combining margin and ensemble variance in classification confidence</li> <li>Galil et al., \"What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?\" (2022) \u2014 Analysis showing max probability alone is suboptimal for detecting incorrect predictions</li> <li>scikit-learn ConfusionMatrixDisplay \u2014 The standard confusion matrix visualization that the Confusion Explorer builds upon</li> </ul>"},{"location":"blogs/confusion_explorer/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"blogs/feature_endpoints/","title":"Feature Endpoints: From Training to LiveDesign","text":"<p>Already Using Workbench?</p> <p>Feature endpoints are created and managed through the Model and Endpoint APIs. The Molecular Standardization blog covers what happens inside the 2D and 3D descriptor pipelines.</p> <p>When a medicinal chemist draws a compound in LiveDesign or StarDrop and requests an ADMET prediction, the molecular descriptors powering that prediction need to be exactly the same as the ones the model was trained on. The same is true for batch training pipelines, inference endpoints, and scheduled jobs. Any gap between how features are computed across these consumers \u2014 known as training/inference skew \u2014 is one of the most common sources of silent model degradation in production.</p> <p>Workbench eliminates this problem with feature endpoints: SageMaker-hosted services whose only job is to compute features. Whether the request comes from a training notebook, a deployed model, or a drug discovery platform, every consumer calls the same endpoint and gets identical features by construction. In this blog we'll explain how they work, why the architecture looks the way it does, and how it compares to how other platforms approach the same problem.</p>"},{"location":"blogs/feature_endpoints/#the-problem-skew-happens-quietly","title":"The Problem: Skew Happens Quietly","text":"<p>Training/inference skew is insidious because it rarely causes errors \u2014 it causes drift. Here's how it typically plays out:</p> <ol> <li>A data scientist writes preprocessing code in a notebook to compute molecular descriptors from SMILES</li> <li>The descriptors go into a training DataFrame, the model trains, metrics look great</li> <li>The model gets deployed, and someone rewrites (or copies) the preprocessing code into the inference pipeline</li> <li>Six months later, someone updates RDKit in the training environment but not in production \u2014 or vice versa</li> <li>A Mordred version bump changes how a descriptor handles edge cases</li> <li>Nobody notices because predictions still come back \u2014 they're just slightly off</li> </ol> <p>The root cause is simple: two copies of the same logic, maintained independently, running in different environments. Even with perfect discipline, dependency versions drift. Code paths diverge. Edge cases get handled differently.</p>"},{"location":"blogs/feature_endpoints/#workbenchs-approach-aws-endpoints-serverless-or-realtime","title":"Workbench's Approach: AWS Endpoints (Serverless or Realtime)","text":"<p>A Workbench feature endpoint is a SageMaker model endpoint that doesn't contain a trained model at all. The entire purpose of the endpoint is to generate features from raw input data. The \"model\" is really a feature transformer \u2014 it takes a DataFrame with SMILES strings and returns a DataFrame with molecular descriptors (RDKIT, Mordred, 3D) appended. The model type is <code>ModelType.TRANSFORMER</code> \u2014 a signal to Workbench that this endpoint transforms data rather than making predictions.</p>"},{"location":"blogs/feature_endpoints/#how-it-gets-used","title":"How It Gets Used","text":"<p>When you build a predictive model in Workbench, the training pipeline calls the feature endpoint to compute descriptors for the training data. When that model is deployed and receives inference requests, the inference pipeline calls the same feature endpoint to compute descriptors for the incoming data. The features are identical because they come from the same code, running in the same container, with the same library versions:</p> <pre><code># Training: features computed by calling the endpoint\ndf = load_training_data()\nfeature_endpoint = Endpoint(\"smiles-to-taut-md-stereo-v1\")\ndf_features = feature_endpoint.inference(df)\n\n# Create a FeatureSet and deploy a model that uses those features\nto_features = PandasToFeatures(\"open_admet_mppb\")\nto_features.set_input(df_features, id_column=\"molecule_name\")\nto_features.set_output_tags([\"open_admet\", assay])\nto_features.transform()\n\n# Now use the features to train a model as usual\nfeature_set = FeatureSet(\"open_admet_mppb\")\npytorch_model = fs.to_model(\n    name=chemprop_model_name,\n    model_type=ModelType.UQ_REGRESSOR,\n    model_framework=ModelFramework.PYTORCH,\n    ...\n)\n</code></pre> <p>The inference path is the same \u2014 for the input data we call the feature endpoint to compute the features for the input SMILES before running the prediction:</p> <pre><code># Inference: same endpoint called for features\nfeature_endpoint = Endpoint(\"smiles-to-taut-md-stereo-v1\")\ndf_features = feature_endpoint.inference(input_df)\n\n# Now run the model prediction on the features\nend = Endpoint(\"my_admet_model\")\npredictions = end.inference(df_features)\n</code></pre>"},{"location":"blogs/feature_endpoints/#feature-endpoint-can-be-customized","title":"Feature Endpoint can be Customized","text":"<p>Most clients use variants similar to those listed below but we have the flexibility to deploy custom feature endpoints for specific use cases.</p> Endpoint Features Use Case smiles-to-taut-md-stereo~315 2D descriptorsStandard ADMET modeling (salt extraction, tautomer canonicalization) smiles-to-taut-md-stereo-keep-salts~315 2D descriptorsSalt-sensitive modeling (solubility, formulation) smiles-to-3d-descriptors75 3D descriptorsShape/pharmacophore features (permeability, transporter interactions) <p>The 2D and 3D endpoints can be combined \u2014 run both and concatenate the results for a ~390-feature descriptor set covering topological, electronic, and geometric properties.</p>"},{"location":"blogs/feature_endpoints/#why-a-deployed-endpoint","title":"Why a Deployed Endpoint?","text":"<p>You might ask: why not just share a Python function? Or package the code into a library? The endpoint architecture gives you several things that shared code doesn't:</p> Every Workbench endpoint \u2014 including feature endpoints \u2014 runs on a modern ASGI stack. Any client that can make an HTTP request gets the same features. <p>Pinned dependencies at the container level. The feature endpoint runs inside a Docker container with exact versions of RDKit, Mordred, NumPy, and every other dependency. Updating your local Python environment doesn't change what the endpoint computes. This is especially important for chemistry libraries \u2014 RDKit descriptor implementations do change between releases, and Mordred edge-case handling varies by version.</p> <p>Version management through naming. Deploy <code>smiles-to-taut-md-stereo-v2</code> alongside <code>v1</code>, and let downstream models pin whichever version they were trained against. When you improve the descriptor pipeline, existing models keep working with their original features while new models can use the updated set.</p> <p>Any consumer can call it. A notebook, a training pipeline, an inference endpoint, a scheduled batch job, or an external drug discovery platform \u2014 anything that can make an HTTP request gets the same features. No need to install RDKit locally, manage conda environments, or worry about platform-specific compilation issues. A simple <code>requests.post()</code> call with a CSV payload is all it takes.</p> <p>Scaling is handled by AWS. The endpoint can run serverless (cost-efficient for intermittent use) or on dedicated instances (higher throughput for batch processing). The 3D endpoint, which is compute-intensive (~1-2 molecules/second for conformer generation), benefits from this \u2014 you can scale up for a big batch run and scale back down without managing infrastructure.</p> <p>Built on the Workbench endpoint stack. Feature endpoints run on the same modern ASGI stack as every other Workbench endpoint \u2014 Uvicorn and FastAPI instead of the default SageMaker Nginx/Gunicorn/Flask stack. They follow the same DataFrame-in, DataFrame-out contract: send a DataFrame with SMILES, get back a DataFrame with descriptors appended.</p>"},{"location":"blogs/feature_endpoints/#integration-with-drug-discovery-platforms","title":"Integration with Drug Discovery Platforms","text":"<p>Just an HTTP Call</p> <p>Any platform that can make an HTTP <code>POST</code> with a CSV or JSON payload can call a feature endpoint \u2014 no RDKit install, no conda environment, no chemistry stack required.</p> <p>This \"any consumer can call it\" property is especially powerful for integration with external platforms. The ADMET Workbench will often manage hundreds of models across dozens of ADMET properties \u2014 solubility, permeability, metabolic stability, transporter interactions, toxicity endpoints, and more. These models aren't just called from Workbench itself \u2014 they're integrated into drug discovery platforms like Schr\u00f6dinger's LiveDesign and Optibrium's StarDrop, where medicinal chemists run predictions directly from their molecular design workflows.</p> <p>Because the feature endpoint is just an HTTP service, the integration is straightforward. When a chemist draws a compound in LiveDesign or StarDrop and requests an ADMET prediction, the platform makes a request to a Workbench model endpoint, which calls the feature endpoint to compute descriptors, then runs the model. LiveDesign and StarDrop don't need to install RDKit, bundle Mordred, or know anything about standardization pipelines \u2014 they send SMILES and get predictions back. All the complexity of feature computation is behind the endpoint boundary.</p> <p>This also means feature consistency is guaranteed across every integration point. Whether the request came from LiveDesign, StarDrop, a Workbench notebook, or a batch training pipeline, the descriptors come from the same endpoint. Without feature endpoints, each integration would need its own copy of the descriptor pipeline \u2014 and keeping those copies in sync across platforms is exactly the kind of coordination that breaks down over time.</p>"},{"location":"blogs/feature_endpoints/#how-other-platforms-approach-this","title":"How Other Platforms Approach This","text":"<p>The training/inference skew problem is well-recognized across the ML industry, and different platforms have developed thoughtful solutions. Here's how the major approaches compare:</p>"},{"location":"blogs/feature_endpoints/#feature-stores-pre-compute-and-look-up","title":"Feature Stores: Pre-Compute and Look Up","text":"<p>Platforms like AWS SageMaker Feature Store and Google Vertex AI Feature Store pre-compute features in batch and store them for low-latency lookup by entity ID. This works well for slowly-changing entity features (user demographics, item metadata), but not for molecular descriptors \u2014 you can't pre-compute features for every possible molecule when the chemical space is effectively infinite.</p>"},{"location":"blogs/feature_endpoints/#on-demand-feature-transforms-udfs-inside-the-platform","title":"On-Demand Feature Transforms: UDFs Inside the Platform","text":"<p>Databricks (Unity Catalog) and Tecton (On-Demand Feature Views) let you register Python functions that run at both training and serving time \u2014 architecturally similar to Workbench's approach. The key difference is coupling: these UDFs run inside the platform's managed runtime, tying your feature computation to that ecosystem. Workbench's endpoint is a standalone HTTP service that any client can call, independent of platform.</p>"},{"location":"blogs/feature_endpoints/#feast-and-hopsworks-open-source-feature-engineering","title":"Feast and Hopsworks: Open-Source Feature Engineering","text":"<p>Feast and Hopsworks support on-demand transformations at both training and serving time \u2014 Feast via a sidecar Transformation Server, Hopsworks via UDFs attached to feature views. These are solid general-purpose approaches, but for domain-specific computation requiring RDKit's C++ extensions and Mordred's descriptor modules, a containerized endpoint gives you more control over the execution environment.</p>"},{"location":"blogs/feature_endpoints/#summary-comparison","title":"Summary Comparison","text":"Approach Skew Prevention On-Demand Compute Environment Isolation Reusability Pre-computed Feature Store\u2705 Same store\u274c Batch only\u274c Varies\u2705 Any consumer Platform UDFs (Databricks/Tecton)\u2705 Same function\u2705 At request time\u26a0\ufe0f Platform-managed\u26a0\ufe0f Within platform Inference Pipeline\u2705 Same container\u2705 At request time\u2705 Container-level\u274c Per model Open-Source (Feast/Hopsworks)\u2705 Same transform\u2705 At request time\u26a0\ufe0f Sidecar/UDF\u2705 Any consumer Workbench Feature Endpoint\u2705 Same endpoint\u2705 At request time\u2705 Container-level\u2705 Any consumer"},{"location":"blogs/feature_endpoints/#under-the-hood-feature-endpoint-details","title":"Under the Hood: Feature Endpoint Details","text":"<p>Combine 2D + 3D</p> <p>Run both the 2D and 3D endpoints and concatenate the results for a ~390-feature descriptor set covering topological, electronic, and geometric properties.</p> <p>Most Feature Endpoints run a full molecular processing pipeline.</p> <ol> <li>Standardization: Cleanup, salt extraction, charge neutralization, tautomer canonicalization (see Molecular Standardization)</li> <li>RDKit descriptors (~220): Constitutional, topological, electronic, lipophilicity, pharmacophore, and ADMET-specific properties</li> <li>Mordred descriptors (~85): Five ADMET-focused modules \u2014 AcidBase, Aromatic, Constitutional, Chi connectivity, and CarbonTypes</li> <li>Stereochemistry features (10): R/S center counts, E/Z bond counts, stereo complexity, fraction-defined metrics</li> </ol> <p>Our 3D endpoint typically includes conformer generation using RDKit's ETKDGv3 algorithm and computes 75 additional descriptors covering molecular shape (PMI, NPR, asphericity), charged partial surface area (CPSA), pharmacophore spatial distribution (amphiphilic moment, intramolecular H-bond potential), and conformer ensemble statistics.</p>"},{"location":"blogs/feature_endpoints/#references","title":"References","text":"<ul> <li>Training-Inference Skew: Sculley, D., et al. \"Hidden Technical Debt in Machine Learning Systems.\" NeurIPS 2015. Paper</li> <li>Tecton Feature Platform: https://docs.tecton.ai/</li> <li>Feast Feature Store: https://feast.dev/</li> <li>Databricks Feature Serving: https://docs.databricks.com/en/machine-learning/feature-store/</li> <li>SageMaker Inference Pipelines: https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html</li> <li>RDKit: https://github.com/rdkit/rdkit</li> <li>Mordred: https://github.com/mordred-descriptor/mordred</li> <li>Schr\u00f6dinger LiveDesign: https://www.schrodinger.com/platform/products/livedesign/</li> <li>Optibrium StarDrop: https://www.optibrium.com/stardrop/</li> </ul>"},{"location":"blogs/feature_endpoints/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"blogs/model_confidence/","title":"Model Confidence: From Ensemble Disagreement to Calibrated Scores","text":"<p>See It in Action</p> <p>The Confusion Explorer uses these confidence scores to let you filter predictions by certainty and drill down on errors interactively.</p> <p>A prediction without a confidence score is just a number. In drug discovery, knowing how much to trust a prediction is often more valuable than the prediction itself \u2014 it determines whether you synthesize a compound, run an experiment, or move on. In this blog we'll walk through how Workbench approaches model confidence, the trade-offs we've considered, and where we think there's room to improve.</p> A target vs. prediction scatter from a ChemProp MLM CLint model. Points are colored by confidence \u2014 the high confidence points cluster along the diagonal, low confidence (blue) are scattered."},{"location":"blogs/model_confidence/#the-core-idea-ensemble-disagreement","title":"The Core Idea: Ensemble Disagreement","text":"<p>Every Workbench model \u2014 whether XGBoost, PyTorch, or ChemProp \u2014 is actually a 5-model ensemble trained via cross-validation. Each fold produces a model that saw a slightly different slice of the training data. At inference time, all 5 models make a prediction and we take the average.</p> <p>The idea behind using ensemble disagreement as an uncertainty signal is well-established in the ML literature (see Lakshminarayanan et al., 2017): when the models disagree, the prediction is less reliable. If all 5 models predict log CLint = 2.4 \u00b1 0.02, we have reason to be confident. If they predict 2.4 \u00b1 0.71, something about that compound is tricky and we should be cautious.</p> <p>This ensemble standard deviation (<code>prediction_std</code>) is the raw uncertainty signal. It comes directly from the model itself \u2014 not from an external surrogate or statistical assumption. In our testing, it correlates strongly with actual prediction error (Spearman r &gt; 0.85 for ChemProp on MLM CLint from the OpenADMET Blind Challenge), though your mileage will vary depending on the dataset and model type.</p>"},{"location":"blogs/model_confidence/#the-problem-raw-std-isnt-calibrated","title":"The Problem: Raw Std Isn't Calibrated","text":"<p>Ensemble std tells you which predictions to trust more, but the raw numbers don't correspond to meaningful intervals. If std = 0.3, does that mean the true value is within \u00b1 0.3? \u00b1 0.6? There's no guarantee.</p> <p>This is the classic calibration vs. discrimination trade-off described in Gneiting et al., 2007:</p> <ul> <li>Discrimination (ranking): Can you tell which predictions are better? Ensemble std tends to do this well.</li> <li>Calibration (coverage): Do your 80% intervals actually contain 80% of true values? Raw std alone doesn't guarantee this.</li> </ul> <p>We need both. That's where conformal prediction comes in.</p>"},{"location":"blogs/model_confidence/#conformal-calibration","title":"Conformal Calibration","text":"<p>Conformal prediction is a distribution-free framework for turning any uncertainty estimate into calibrated prediction intervals. Originally developed by Vovk et al. and made accessible by Angelopoulos &amp; Bates (2021), the core idea is elegant and the math is straightforward:</p> <ol> <li>Compute nonconformity scores on held-out validation data: <code>score = |actual - predicted| / std</code></li> <li>Find scaling factors: For each confidence level (50%, 68%, 80%, 90%, 95%), find the quantile of scores that achieves the target coverage</li> <li>Build intervals: <code>prediction \u00b1 scale_factor \u00d7 std</code></li> </ol> <p>The scaling factors are computed once during training and stored as metadata. At inference, building intervals is a simple multiply \u2014 no extra models to run.</p> <p>In practice, this gives us intervals that inherit the ensemble's discrimination (width varies per-compound based on model disagreement) but are calibrated to have correct coverage, an 80% interval should contain ~80% of true values.</p>"},{"location":"blogs/model_confidence/#confidence-scores","title":"Confidence Scores","text":"<p>With calibrated intervals in hand, we compute a confidence score between 0 and 1 for every prediction. We explored several approaches (exponential decay, z-score normalization) and settled on a simple percentile-rank method inspired by the nonparametric statistics literature:</p> <p>Specifically, confidence is the percentile rank of the prediction's <code>prediction_std</code> within the training set's std distribution:</p> <pre><code>confidence = 1 - percentile_rank(prediction_std)\n</code></pre> <p>A confidence of 0.7 means this prediction's ensemble disagreement is lower than 70% of the training set \u2014 it's a relatively tight prediction. A confidence of 0.1 means 90% of training predictions had lower uncertainty \u2014 this compound is an outlier in some way.</p> <p>We like this approach for a few reasons:</p> <ul> <li>Full range: Confidence scores spread across the entire 0\u20131 range, rather than clustering near zero</li> <li>Directly interpretable: \"confidence 0.7\" means \"tighter than 70% of training predictions\"</li> <li>No arbitrary parameters: No decay rates or normalization constants to tune</li> <li>Grounded in the calibration data: Derived from the same distribution used for interval calibration</li> </ul> <p>That said, percentile-rank has its own limitations \u2014 it's relative to the training set, so a confidence of 0.7 from two different models isn't directly comparable. We think this is an acceptable trade-off for now, but it's an area we're continuing to think about.</p> Confidence vs. prediction residual \u2014 high-confidence predictions (right) cluster near zero error, while low-confidence predictions (left) show the largest residuals. <p>You'll notice the outlier around confidence ~0.55 with a residual near 1.0 \u2014 the model is moderately confident on that compound but clearly getting it wrong. We're not going to pretend this doesn't happen. The value of this plot is that it gives us visibility into exactly these cases, so we can investigate individual compounds where the model's confidence doesn't match reality.</p>"},{"location":"blogs/model_confidence/#classification-confidence","title":"Classification Confidence","text":"<p>Everything above applies to regression models \u2014 where <code>prediction_std</code> gives us a natural uncertainty signal. But what about classifiers? A classification ensemble doesn't predict a continuous value with a standard deviation; it produces class probabilities. We need a different approach.</p>"},{"location":"blogs/model_confidence/#the-challenge","title":"The Challenge","text":"<p>For classification, each of the 5 ensemble members outputs a softmax probability distribution over classes. We average those distributions to get the final <code>_proba</code> columns. But how do we turn that into a single confidence score?</p> <p>Simple approaches like using the maximum predicted probability (<code>max(p)</code>) are tempting but have known issues \u2014 Galil et al. (2022) showed that max probability alone is suboptimal for detecting incorrect predictions, especially under distribution shift. It ignores both the shape of the probability distribution and whether the ensemble actually agrees.</p>"},{"location":"blogs/model_confidence/#vgmu-variance-gated-margin-uncertainty","title":"VGMU: Variance-Gated Margin Uncertainty","text":"<p>We use VGMU (Variance-Gated Margin Uncertainty), introduced in the Variance-Gated Ensembles paper (2025). The idea is to combine two signals:</p> <ul> <li>Margin: How much does the ensemble prefer its top class over the runner-up?</li> <li>Agreement: Do the 5 models agree on those probabilities, or are they all over the place?</li> </ul> <p>The formula computes a signal-to-noise ratio between the margin and the ensemble disagreement:</p> <pre><code>SNR = (p_top1 - p_top2) / (std_top1 + std_top2 + \u03b5)\ngamma = 1 - exp(-SNR)\nraw_confidence = gamma \u00d7 p_top1\n</code></pre> <p>Where <code>p_top1</code> and <code>p_top2</code> are the mean probabilities for the top two classes, and <code>std_top1</code> and <code>std_top2</code> are the standard deviations of those probabilities across the 5 ensemble members.</p> <p>This gives us nice behavior across the spectrum:</p> <ul> <li>Ensemble agrees with clear margin \u2192 high SNR \u2192 gamma \u2248 1 \u2192 confidence \u2248 p_top1</li> <li>Ensemble disagrees or margin is thin \u2192 low SNR \u2192 gamma \u2248 0 \u2192 confidence \u2248 0</li> <li>Uniform probabilities (model can't distinguish classes) \u2192 margin = 0, confidence = 0</li> </ul>"},{"location":"blogs/model_confidence/#isotonic-calibration","title":"Isotonic Calibration","text":"<p>Just like raw ensemble std needs conformal calibration for regression, raw VGMU scores need calibration for classification. We use isotonic regression \u2014 a standard technique that fits a monotonically non-decreasing mapping from raw confidence to empirical accuracy on the validation set.</p> <p>During training, we compute VGMU scores for all validation predictions and fit an isotonic regression mapping <code>raw_confidence \u2192 P(correct)</code>. The fitted mapping is stored as a simple piecewise-linear function (just two arrays of thresholds) that can be applied with <code>np.interp</code> at inference time \u2014 no sklearn dependency needed in production.</p> <p>After calibration, a confidence of 0.85 means that among validation predictions with similar VGMU scores, about 85% were correctly classified. This gives the score a direct probabilistic interpretation.</p>"},{"location":"blogs/model_confidence/#training-output","title":"Training output","text":"<p>During training, classification models now print calibration diagnostics showing how raw confidence maps to actual accuracy across bins:</p> <pre><code>==================================================\nCalibrating Classification Confidence (VGMU)\n==================================================\n  Validation samples: 2451\n  Overall accuracy: 0.847\n  Raw confidence  - mean: 0.621, std: 0.284\n  Calibrated conf - mean: 0.847, std: 0.128\n  Bin 1: n=  490, accuracy=0.639, calibrated_conf=0.654\n  Bin 2: n=  490, accuracy=0.794, calibrated_conf=0.805\n  Bin 3: n=  490, accuracy=0.871, calibrated_conf=0.873\n  Bin 4: n=  491, accuracy=0.924, calibrated_conf=0.922\n  Bin 5: n=  490, accuracy=0.998, calibrated_conf=0.982\n</code></pre> <p>This lets you verify that the calibration is working \u2014 accuracy should increase monotonically across bins, and calibrated confidence should track accuracy closely.</p>"},{"location":"blogs/model_confidence/#unified-across-frameworks","title":"Unified Across Frameworks","text":"<p>One design goal we're happy with: the same UQ pipeline runs for all three model types. Each framework trains its ensemble differently, but the uncertainty signal and calibration pipeline are unified \u2014 conformal scaling for regression, VGMU + isotonic calibration for classification.</p> Framework Ensemble Regression Confidence Classification Confidence XGBoost5-fold CVEnsemble std + conformal scalingVGMU + isotonic calibration PyTorch5-fold CVEnsemble std + conformal scalingVGMU + isotonic calibration ChemProp5-fold CVEnsemble std + conformal scalingVGMU + isotonic calibration <p>This consistency means confidence scores have the same interpretation across frameworks, which simplifies things when comparing models on the same dataset.</p>"},{"location":"blogs/model_confidence/#what-confidence-doesnt-tell-you","title":"What Confidence Doesn't Tell You","text":"<p>We want to be upfront about the limitations. Confidence reflects how much the ensemble models agree \u2014 but agreement doesn't guarantee correctness. All 5 models can confidently agree on the wrong answer, especially for compounds that are structurally far from the training data.</p> <ul> <li>High confidence \u2260 correct prediction. It means the models agree, not that they're right. This is a fundamental limitation of ensemble-based UQ noted by Ovadia et al., 2019.</li> <li>Novel chemistry may get falsely high confidence if it happens to fall in a region where the models extrapolate consistently.</li> <li>Confidence is relative to the training set. A confidence of 0.9 on a kinase solubility model doesn't transfer to a PROTAC dataset.</li> <li>Conformal coverage assumes exchangeability. The guarantee holds when test data comes from the same distribution as calibration data. For out-of-distribution compounds, coverage may degrade.</li> </ul> <p>For truly out-of-distribution detection, we'd recommend pairing confidence with applicability domain analysis (e.g., feature-space proximity to training data). This is something we're actively exploring for future Workbench releases.</p>"},{"location":"blogs/model_confidence/#summary","title":"Summary","text":"<p>Here's how Workbench approaches model confidence today:</p> <p>Regression models:</p> <ol> <li>Ensemble disagreement \u2014 Building on Lakshminarayanan et al., the 5-fold CV ensemble provides <code>prediction_std</code> as the raw uncertainty signal</li> <li>Conformal calibration \u2014 Following Angelopoulos &amp; Bates, we scale std into prediction intervals with target coverage (80% CI \u2192 ~80% coverage)</li> <li>Percentile-rank confidence \u2014 Ranks each prediction's std against the training distribution (0.0 \u2013 1.0)</li> </ol> <p>Classification models:</p> <ol> <li>Ensemble probabilities \u2014 Each of the 5 models outputs class probabilities; we average them and also track per-model disagreement</li> <li>VGMU scoring \u2014 Following Weiss et al. (2025), we combine the probability margin between top classes with ensemble disagreement via a signal-to-noise ratio</li> <li>Isotonic calibration \u2014 Maps raw VGMU scores to P(correct) using isotonic regression on validation data, giving confidence a direct probabilistic interpretation</li> </ol> <p>Both approaches share the same philosophy: leverage the ensemble's own disagreement as the uncertainty signal, then calibrate it against held-out data so the numbers are meaningful. We're excited about this unified framework and looking forward to incorporating applicability domain methods and testing on a wider range of datasets.</p>"},{"location":"blogs/model_confidence/#references","title":"References","text":"<ul> <li>Lakshminarayanan et al., \"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\" (2017) \u2014 Foundational work on using ensemble disagreement for uncertainty</li> <li>Vovk et al., \"Algorithmic Learning in a Random World\" \u2014 Foundational text on conformal prediction</li> <li>Angelopoulos &amp; Bates, \"Conformal Prediction: A Gentle Introduction\" (2021) \u2014 Accessible introduction to conformal methods</li> <li>Gneiting et al., \"Probabilistic Forecasts, Calibration and Sharpness\" (2007) \u2014 Calibration vs. discrimination framework</li> <li>Ovadia et al., \"Can You Trust Your Model's Uncertainty?\" (2019) \u2014 Analysis of ensemble UQ under dataset shift</li> <li>Weiss et al., \"Variance-Gated Ensembles: An Epistemic-Aware Framework\" (2025) \u2014 VGMU approach for combining margin and ensemble variance in classification</li> <li>Galil et al., \"What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?\" (2022) \u2014 Analysis showing entropy-based measures outperform max probability for failure detection</li> <li>Wimmer et al., \"Quantifying Aleatoric and Epistemic Uncertainty with Proper Scoring Rules\" (2023) \u2014 Caveats on entropy decomposition for small ensembles</li> <li>OpenADMET Blind Challenge \u2014 ExpansionRx MLM CLint dataset used for examples in this blog</li> </ul>"},{"location":"blogs/model_confidence/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"blogs/molecular_standardization/","title":"Molecular Standardization: Canonicalization, Tautomerization, and Salt Handling","text":"<p>FeatureSets Handle This</p> <p>Workbench FeatureSets run standardization automatically when you create molecular descriptors from SMILES data.</p> <p>In this blog we'll look at why molecular standardization matters for ML pipelines, what Workbench's feature endpoints actually do under the hood, and how the popular AqSol compound solubility dataset illustrates the challenges of working with real-world chemical data.</p>"},{"location":"blogs/molecular_standardization/#why-standardization-matters","title":"Why Standardization Matters","text":"<p>The same molecule can be represented many different ways in SMILES notation. Benzene alone has multiple valid representations: <code>C1=CC=CC=C1</code>, <code>c1ccccc1</code>, <code>C1=CC=C(C=C1)</code> \u2014 all describe the same compound. Drug compounds are even worse: they come as salts, mixtures of tautomers, and with inconsistent stereochemistry annotations.</p> <p>If you feed these raw SMILES into a descriptor computation pipeline, structurally identical compounds produce different feature vectors. Your ML model sees noise where there should be signal. Standardization eliminates this problem.</p>"},{"location":"blogs/molecular_standardization/#workbenchs-standardization-pipeline","title":"Workbench's Standardization Pipeline","text":"<p>Workbench feature endpoints run a four-step standardization pipeline using RDKit's MolStandardize module (following the methodology described in the ChEMBL structure pipeline) before computing any molecular descriptors:</p>"},{"location":"blogs/molecular_standardization/#step-1-cleanup","title":"Step 1: Cleanup","text":"<p>Removes explicit hydrogens, disconnects metal atoms from organic fragments, and normalizes functional group representations (e.g., different ways of drawing nitro groups or sulfoxides).</p>"},{"location":"blogs/molecular_standardization/#step-2-saltfragment-handling","title":"Step 2: Salt/Fragment Handling","text":"<p>Many drug compounds are stored as salt forms (e.g., sodium acetate <code>[Na+].CC(=O)[O-]</code>). Workbench provides two modes for handling these:</p> <ul> <li><code>extract_salts=True</code> (default): Identifies and keeps the largest organic fragment, removes counterions, and records the removed salt for traceability. The pipeline distinguishes true salts from mixtures using a heuristic: small fragments (\u22646 heavy atoms if neutral, \u226410 if charged) are treated as salts, while multiple large neutral organic fragments are flagged as mixtures and logged.</li> <li><code>extract_salts=False</code>: Keeps the full molecule with all fragments intact and preserves ionic charges. Useful when the salt form itself affects the property you're modeling (e.g., solubility, formulation studies).</li> </ul> <pre><code># Default: removes salts (ChEMBL standard)\ndf = standardize(df, extract_salts=True)\n# Input:  [Na+].CC(=O)[O-]  \u2192  smiles: CC(=O)O, salt: [Na+]\n\n# Keep salts for salt-dependent properties\ndf = standardize(df, extract_salts=False)\n# Input:  [Na+].CC(=O)[O-]  \u2192  smiles: [Na+].CC(=O)[O-], salt: None\n</code></pre>"},{"location":"blogs/molecular_standardization/#step-3-charge-neutralization","title":"Step 3: Charge Neutralization","text":"<p>When salts are extracted, charges on the parent molecule are neutralized (e.g., <code>CC(=O)[O-]</code> \u2192 <code>CC(=O)O</code>). This step is skipped when keeping salts to preserve ionic character.</p>"},{"location":"blogs/molecular_standardization/#step-4-tautomer-canonicalization","title":"Step 4: Tautomer Canonicalization","text":"<p>Tautomers are isomers that differ in proton and double-bond positions but exist in rapid equilibrium. The classic example is the keto-enol pair. Workbench uses RDKit's tautomer enumerator to pick a canonical form, ensuring that the same compound always produces the same descriptors regardless of which tautomeric form appeared in the source data. This step is enabled by default (<code>canonicalize_tautomer=True</code>) but can be disabled for workflows where preserving the as-drawn tautomeric form is preferred.</p> <pre><code># 2-hydroxypyridine and 2-pyridone are the same compound\nOc1ccccn1  \u2192  O=c1cccc[nH]1  (canonical tautomer)\n</code></pre>"},{"location":"blogs/molecular_standardization/#what-about-invalid-structures","title":"What About Invalid Structures?","text":"<p>Molecules that fail RDKit parsing (bad valences, unsupported atom types, malformed SMILES) are not silently dropped. The pipeline preserves these rows in the output DataFrame with their original SMILES and fills all descriptor columns with NaN. This keeps row alignment intact so downstream ML pipelines can handle missing values through imputation or filtering as appropriate.</p>"},{"location":"blogs/molecular_standardization/#descriptor-computation","title":"Descriptor Computation","text":"<p>After standardization, Workbench computes ~315 2D molecular descriptors from three sources:</p> Source Count Description RDKit~220Constitutional, topological, electronic, lipophilicity, pharmacophore, and ADMET-specific descriptors (TPSA, QED, Lipinski) Mordred~85Five ADMET-focused modules: AcidBase, Aromatic, Constitutional, Chi connectivity indices, and CarbonTypes Stereochemistry10Custom features: R/S center counts, E/Z bond counts, stereo complexity, and fraction-defined metrics <p>Invalid molecules receive NaN values rather than being dropped, preserving row alignment with the input DataFrame. The <code>Ipc</code> descriptor is excluded due to known overflow issues in RDKit.</p>"},{"location":"blogs/molecular_standardization/#3d-descriptor-computation","title":"3D Descriptor Computation","text":"<p>In addition to the 2D descriptors above, Workbench offers a separate 3D descriptor pipeline that generates conformers and computes 75 shape/pharmacophore features:</p> Source Count Description RDKit 3D Shape10Principal moments of inertia, normalized PMI ratios, asphericity, eccentricity, spherocity, radius of gyration Mordred 3D52Charged partial surface area (CPSA), geometrical indices, gravitational indices, plane of best fit Pharmacophore 3D8Molecular axis/volume, amphiphilic moment, charge centroid distance, intramolecular H-bond potential Conformer Ensemble5MMFF94 energy statistics (min, range, std), conformer count, conformational flexibility index <p>Conformers are generated using RDKit's ETKDGv3 algorithm (with special handling for macrocycles) and optionally optimized with the MMFF94 force field. By default the endpoint generates 10 conformers per molecule and uses the lowest-energy conformer for descriptor calculation. This makes the 3D endpoint significantly slower than 2D (~1-2 molecules/second vs. near-instant for 2D), so batch sizes are kept smaller to stay within serverless timeouts.</p>"},{"location":"blogs/molecular_standardization/#feature-endpoints-deployed-on-aws","title":"Feature Endpoints: Deployed on AWS","text":"<p>These standardization and descriptor computations run inside Workbench feature endpoints \u2014 SageMaker-hosted transformer models that take raw SMILES and return standardized structures plus computed descriptors. Three variants are available:</p> <ul> <li><code>smiles-to-taut-md-stereo-v1</code>: Standard 2D pipeline with salt extraction (ChEMBL default)</li> <li><code>smiles-to-taut-md-stereo-v1-keep-salts</code>: 2D pipeline that preserves salt forms for salt-sensitive modeling</li> <li><code>smiles-to-3d-descriptors-v1</code>: 3D conformer-based descriptors (75 features) with salt extraction</li> </ul> <p>All three endpoints can be deployed as serverless (cost-efficient for intermittent workloads) or on dedicated instances for higher throughput.</p>"},{"location":"blogs/molecular_standardization/#the-aqsol-dataset-a-real-world-example","title":"The AqSol Dataset: A Real-World Example","text":"<p>AqSolDB is a curated reference set of aqueous solubility values containing 9,982 unique compounds from 9 publicly available datasets (Harvard DataVerse).</p> <p>Running this dataset through the full standardization + descriptor pipeline is a good stress test for real-world chemical data \u2014 the dataset includes organometallics, unusual valences, and SMILES notation quirks that exercise every step of the pipeline.</p>"},{"location":"blogs/molecular_standardization/#key-differences-canonicalization-vs-tautomerization","title":"Key Differences: Canonicalization vs Tautomerization","text":"Aspect Canonicalization Tautomerization PurposeStandardizes the entire molecular representationHandles proton/bond-shift equilibria ScopeAtom ordering, bond types, stereochemistryFunctional groups capable of tautomerization OutputUnique, canonical SMILES stringA specific canonical tautomeric form Use CaseDeduplication, consistency, comparisonConsistent descriptors across tautomeric forms"},{"location":"blogs/molecular_standardization/#references","title":"References","text":"<ul> <li>ChEMBL Structure Pipeline: Bento, A.P., et al. \"An open source chemical structure curation pipeline using RDKit.\" Journal of Cheminformatics 12, 51 (2020). DOI: 10.1186/s13321-020-00456-1</li> <li>RDKit Standardization: Landrum, G. \"Standardization and Validation with the RDKit.\" RSC Open Science (2021). GitHub Notebook</li> <li>RDKit: https://github.com/rdkit/rdkit</li> <li>Mordred: https://github.com/mordred-descriptor/mordred</li> <li>AqSolDB: Sorkun, M.C., et al. \"AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds.\" Scientific Data 6, 143 (2019). DOI: 10.1038/s41597-019-0151-1</li> </ul>"},{"location":"blogs/molecular_standardization/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"blogs/storage/compound_etl/","title":"Compound ETL","text":"<p>From Raw SMILES to ML Pipeline Ready Molecules</p> <p>In this blog, we'll walk through the essential steps for preparing molecular data for machine learning pipelines. Starting with raw SMILES strings, we'll demonstrate how to clean, standardize, and preprocess molecules, ensuring they are ready for downstream tasks: feature selection, engineering, and modeling.</p>"},{"location":"blogs/storage/compound_etl/#why-compound-etl","title":"Why Compound ETL?","text":"<p>Raw molecular datasets often contain inconsistencies, salts, and redundant entries. A well-structured ETL (Extract, Transform, Load) pipeline ensures the data is clean, standardized, and reproducible, which is crucial for building reliable ML models. We'll cover the following steps:</p> <ol> <li>Validating SMILES</li> <li>Removing duplicates</li> <li>Handling stereochemistry</li> <li>Selecting the largest fragment</li> <li>Canonicalizing molecules</li> <li>Tautomerizing molecules</li> </ol>"},{"location":"blogs/storage/compound_etl/#data","title":"Data","text":"<p>AqSolDB: A curated reference set of aqueous solubility data, created by the Autonomous Energy Materials Discovery [AMD] research group, consists of aqueous solubility values for 9,982 unique compounds curated from 9 publicly available datasets. Source: Nature Scientific Data </p> <p>Download from Harvard DataVerse: Harvard DataVerse: AqSolDB</p>"},{"location":"blogs/storage/compound_etl/#python-packages","title":"Python Packages","text":"<ul> <li>RDKit: Open-source toolkit for cheminformatics, used for tasks like SMILES validation, fragment selection, and stereochemistry handling.</li> <li>Mordred Community: A community-maintained molecular descriptor calculator for feature extraction and engineering.</li> </ul>"},{"location":"blogs/storage/compound_etl/#etl-steps","title":"ETL Steps","text":"<p>Here are the core steps of our Compound ETL pipeline:</p>"},{"location":"blogs/storage/compound_etl/#1-check-for-invalid-smiles","title":"1. Check for Invalid SMILES","text":"<p>Validating the SMILES strings ensures that downstream processing doesn\u2019t fail due to malformed data. This step identifies and filters out invalid or problematic entries.</p>"},{"location":"blogs/storage/compound_etl/#2-deduplicate","title":"2. Deduplicate","text":"<p>Duplicate molecules can skew analysis and modeling results. Deduplication ensures a clean and minimal dataset.</p>"},{"location":"blogs/storage/compound_etl/#3-handle-stereochemistry","title":"3. Handle Stereochemistry","text":"<p>Stereochemistry affects molecular properties significantly. This step determines whether to retain or relax stereochemical definitions, depending on the use case.</p>"},{"location":"blogs/storage/compound_etl/#4-select-largest-fragment","title":"4. Select Largest Fragment","text":"<p>Many compounds contain salts or counterions. This step extracts the largest fragment with at least one heavy atom and retains any other fragments as metadata.</p>"},{"location":"blogs/storage/compound_etl/#5-canonicalize-molecules","title":"5. Canonicalize Molecules","text":"<p>Canonicalization ensures that each molecule is represented in a unique and consistent format. This step is critical for reproducibility and efficient comparison.</p>"},{"location":"blogs/storage/compound_etl/#6-tautomerize-molecules","title":"6. Tautomerize Molecules","text":"<p>Tautomerization standardizes different tautomeric forms of a compound into a single representation, reducing redundancy and improving consistency.</p>"},{"location":"blogs/storage/compound_etl/#canonicalization-and-tautomerization","title":"Canonicalization and Tautomerization","text":"<p>For an in-depth look at why Canonicalization and Tautomerization are crucial for compound preprocessing, see our blog on Canonicalization and Tautomerization. It covers the importance of standardizing molecular representations to ensure robust and reproducible machine learning workflows.</p>"},{"location":"blogs/storage/compound_etl/#conclusion","title":"Conclusion","text":"<p>By following this Compound ETL pipeline, you can transform raw molecular data into a clean, standardized, and ML-ready format. This foundational preprocessing step sets the stage for effective feature engineering, modeling, and analysis.</p> <p>Stay tuned for the next blog, where we'll dive into feature engineering for chemical compounds!</p>"},{"location":"blogs/storage/compound_explorer/","title":"Compound Explorer","text":"<p>A Workbench based Application</p> <p>In this blog, we'll walk through the steps we used for taking a 'pile of SMILES' and tranforming them into  compound data for processing and display in the Compound Explorer application. </p>"},{"location":"blogs/storage/compound_explorer/#why-compound-explorer","title":"Why Compound Explorer?","text":"<p>The workbench toolkit has a ton of functionality for constructing end-to-end AWS Machine Learning pipelines. We wanted to construct as application that combined components in the toolkit to create an engaging and informative web application.</p>"},{"location":"blogs/storage/compound_explorer/#workbench-pipeline","title":"Workbench Pipeline","text":"<ol> <li>SMILES Processing:</li> <li>Validation: SMILES are syntactically correct.</li> <li>Fragment Selection: Retain the largest fragment (with at least one heavy atom) of each molecule.</li> <li>Canonicalization: Generate a unique representation for each molecule.</li> <li> <p>Tautomerization: Normalize tautomers to standardize inputs.</p> </li> <li> <p>Feature Space Proximity Models:</p> </li> <li> <p>Build KNN-based proximity graphs for:</p> <ul> <li>Descriptor Features: Using molecular descriptors (RDKit, Mordred).</li> <li>Fingerprints: Using chemical fingerprints for structural similarity (ECFP)</li> </ul> </li> <li> <p>2D Projections:</p> </li> <li>LogP vs pKa: Provide a chemically intuitive 2D Space.</li> <li> <p>Projections (t-SNE, UMAP, etc):</p> <ul> <li>Descriptor Space</li> <li>Fingerprint Space</li> </ul> </li> <li> <p>Interactive Visualization:</p> </li> <li>Hover displays Molecular drawing    </li> <li>5 Closest Neighbors (2 sets):<ul> <li>Blue Lines: Descriptor-based neighbors.</li> <li>Green Lines: Fingerprint-based neighbors.</li> </ul> </li> </ol>"},{"location":"blogs/storage/compound_explorer/#interactivity-highlights","title":"Interactivity Highlights:","text":"<ul> <li>Neighbor Connections: Clearly differentiate relationships with color-coded edges.</li> <li>Hover Effects: Enable chemists to interactively explore molecular neighborhoods.</li> <li>Projection Linking: Allow toggling between Descriptor and Fingerprint spaces.</li> </ul>"},{"location":"blogs/storage/compound_explorer/#data","title":"Data","text":"<p>AqSolDB: A curated reference set of aqueous solubility data, created by the Autonomous Energy Materials Discovery [AMD] research group, consists of aqueous solubility values for 9,982 unique compounds curated from 9 publicly available datasets. Source: Nature Scientific Data </p> <p>Download from Harvard DataVerse: Harvard DataVerse: AqSolDB</p>"},{"location":"blogs/storage/compound_explorer/#python-packages","title":"Python Packages","text":"<ul> <li>RDKit: Open-source toolkit for cheminformatics, used for tasks like SMILES validation, fragment selection, and stereochemistry handling.</li> <li>Mordred Community: A community-maintained molecular descriptor calculator for feature extraction and engineering.</li> </ul>"},{"location":"blogs/storage/compound_explorer/#canonicalization-and-tautomerization","title":"Canonicalization and Tautomerization","text":"<p>For an in-depth look at why Canonicalization and Tautomerization are crucial for compound preprocessing, see our blog on Canonicalization and Tautomerization. It covers the importance of standardizing molecular representations to ensure robust and reproducible machine learning workflows.</p>"},{"location":"blogs/storage/compound_explorer/#conclusion","title":"Conclusion","text":"<p>Stay tuned for the next blog, where we'll dive into feature engineering for chemical compounds!</p>"},{"location":"blogs/storage/eda/","title":"Exploratory Data Analysis","text":"<p>Workbench EDA</p> <p>The Workbench toolkit a set of plots that show EDA results, it also has a flexible plugin architecture to expand, enhance, or even replace the current set of web components Dashboard.</p> <p>The Workbench framework has a broad range of Exploratory Data Analysis (EDA) functionality. Each time a DataSource or FeatureSet is created that data is run through a full set of EDA techniques:</p> <ul> <li>TBD</li> <li>TBD2</li> </ul> Workbench Exploratory Data Analysis"},{"location":"blogs/storage/eda/#additional-resources","title":"Additional Resources","text":"<ul> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"blogs/storage/feature_importances/","title":"XGBoost Feature Importance vs. SHAP Values: Understanding the Differences","text":"<p>When working with XGBoost models, there are two primary ways to understand which features are most important to your model: the built-in feature importance metrics and SHAP (SHapley Additive exPlanations) values. While both aim to explain feature impact, they work in fundamentally different ways and can sometimes yield different results.</p>"},{"location":"blogs/storage/feature_importances/#xgboost-built-in-feature-importance","title":"XGBoost Built-in Feature Importance","text":"<p>XGBoost offers several built-in methods to calculate feature importance:</p>"},{"location":"blogs/storage/feature_importances/#1-gain-default","title":"1. Gain (Default)","text":"<ul> <li>What it measures: The average gain of a feature when it is used in trees</li> <li>How it works: It calculates how much each feature improves the model performance when used in splits</li> <li>Strengths: Fast to calculate and directly tied to the model's optimization objective</li> <li>Limitations: Can be biased toward high-cardinality features (features with many unique values)</li> </ul>"},{"location":"blogs/storage/feature_importances/#2-weight","title":"2. Weight","text":"<ul> <li>What it measures: The number of times a feature appears in trees</li> <li>How it works: Simply counts feature occurrences across all trees</li> <li>Limitations: A feature could appear frequently but have minimal impact on predictions</li> </ul>"},{"location":"blogs/storage/feature_importances/#3-cover","title":"3. Cover","text":"<ul> <li>What it measures: The average coverage of splits using the feature</li> <li>How it works: Measures how many samples are affected by splits on this feature</li> <li>Limitations: May not reflect the actual contribution to prediction quality</li> </ul>"},{"location":"blogs/storage/feature_importances/#shap-values","title":"SHAP Values","text":"<p>SHAP values offer an alternative approach based on game theory concepts:</p>"},{"location":"blogs/storage/feature_importances/#how-shap-works","title":"How SHAP Works","text":"<ul> <li>Definition: SHAP values represent how much each feature contributes to the predicted value of the target, given all the other features of that row</li> <li>Mathematical foundation: Based on Shapley values from cooperative game theory</li> <li>Sample-level insights: Calculated for each individual prediction, then typically averaged across all samples</li> </ul>"},{"location":"blogs/storage/feature_importances/#key-differences-from-xgboost-feature-importance","title":"Key Differences from XGBoost Feature Importance","text":"<ol> <li>Individual vs. Global Explanations</li> <li>SHAP provides both individual prediction explanations and global importance</li> <li> <p>XGBoost feature importance only offers global importance metrics</p> </li> <li> <p>Feature Interactions</p> </li> <li>XGBoost's built-in importance (especially Total Gain) only considers one ordering of features, which can lead to biases when there are interactions</li> <li> <p>SHAP accounts for feature interactions by considering all possible feature combinations</p> </li> <li> <p>Model Agnostic</p> </li> <li>SHAP can be applied to any model, not just XGBoost</li> <li>XGBoost's built-in importance is specific to tree-based models</li> </ol>"},{"location":"blogs/storage/feature_importances/#why-they-sometimes-disagree","title":"Why They Sometimes Disagree","text":"<p>It's not uncommon to see different rankings between XGBoost feature importance and SHAP values. This can happen because:</p> <ol> <li> <p>Impurity-based importances (such as XGBoost built-in routines) give more weight to high cardinality features, while gain may be affected by tree structure</p> </li> <li> <p>SHAP considers all possible feature combinations, while XGBoost importance is based on the actual tree structures built during training</p> </li> <li> <p>SHAP evaluates feature importance in terms of how features contribute to predictions, while other methods like Permutation Feature Importance (PFI) measure how features affect model performance</p> </li> </ol>"},{"location":"blogs/storage/feature_importances/#which-should-you-use","title":"Which Should You Use?","text":"<p>Both approaches have their place:</p> <ul> <li>XGBoost Built-in Importance: Faster to compute and directly tied to the model's optimization process</li> <li>SHAP Values: More theoretically sound, consistent across models, and offers both local and global explanations</li> </ul> <p>For critical applications where model interpretability is essential, consider using both methods and investigating discrepancies between them. If they disagree, this might reveal interesting insights about your data and model.</p> <p>A complete explanation system would ideally include both XGBoost's built-in importance (for quick insights) and SHAP values (for deeper, more theoretically robust interpretations).</p>"},{"location":"blogs/storage/fun_with_endpoints/","title":"Fun with Endpoints","text":"<p>This is a blog about timing, combining, and choosing AWS\u00ae endpoints.</p>"},{"location":"blogs/storage/fun_with_endpoints/#endpoints","title":"Endpoints","text":"Endpoint Instance Price/Hour tautomerize-v0-rt ml.t2.medium $0.06 smiles-to-taut-md-stereo-v0-rt ml.t2.medium $0.06 aqsol-mol-class-rt ml.t2.medium $0.06 pipeline-model ml.t2.medium $0.06 pipeline-model-fast ml.c7i.xlarge $0.21"},{"location":"blogs/storage/fun_with_endpoints/#timing-results-fast_inference-direct","title":"Timing results (fast_inference direct)","text":"<pre><code>Timing for 10 rows\nIndividual endpoints Total: 2.083378314971924 seconds\nTaut: 1.0616083145141602 seconds, MD: 0.5406620502471924 seconds, Model: 0.48110198974609375 seconds\nPipeline endpoint: 1.445024013519287 seconds\nPipeline Fast endpoint: 1.2368178367614746 seconds\n\n\nTiming for 100 rows\nIndividual endpoints Total: 3.918509006500244 seconds\nTaut: 2.0423460006713867 seconds, MD: 1.1638188362121582 seconds, Model: 0.7123398780822754 seconds\nPipeline endpoint: 2.8939127922058105 seconds\nPipeline Fast endpoint: 1.852799892425537 seconds\n\n\nTiming for 500 rows\nIndividual endpoints Total: 14.36154294013977 seconds\nTaut: 9.30370306968689 seconds, MD: 4.105616092681885 seconds, Model: 0.9522147178649902 seconds\nPipeline endpoint: 12.265265226364136 seconds\nPipeline Fast endpoint: 8.951914072036743 seconds\n\n\nTiming for 1000 rows\nIndividual endpoints Total: 23.543747663497925 seconds\nTaut: 15.042346715927124 seconds, MD: 6.840778112411499 seconds, Model: 1.6606106758117676 seconds\nPipeline endpoint: 21.389296770095825 seconds\nPipeline Fast endpoint: 12.815913915634155 seconds\n\n\nTiming for 10000 rows\nIndividual endpoints Total: 189.780255317688 seconds\nTaut: 120.399178981781 seconds, MD: 59.34339618682861 seconds, Model: 10.037672281265259 seconds\nPipeline endpoint: 190.11978220939636 seconds\nPipeline Fast endpoint: 110.5272912979126 seconds\n</code></pre>"},{"location":"blogs/storage/fun_with_endpoints/#storage","title":"Storage","text":"<pre><code>Timing for 10 rows\nIndividual endpoints Total: 1.5638809204101562 seconds\nTaut: 0.4830169677734375 seconds, MD: 0.5586769580841064 seconds, Model: 0.5221822261810303 seconds\nPipeline endpoint: 0.9330408573150635 seconds\nPipeline Fast endpoint: 1.0244662761688232 seconds\n\nTiming for 100 rows\nIndividual endpoints Total: 3.009629011154175 seconds\nTaut: 1.0144999027252197 seconds, MD: 1.3848729133605957 seconds, Model: 0.6102499961853027 seconds\nPipeline endpoint: 2.384669065475464 seconds\nPipeline Fast endpoint: 1.4272871017456055 seconds\n\nTiming for 500 rows\nIndividual endpoints Total: 12.45196795463562 seconds\nTaut: 7.562067985534668 seconds, MD: 3.9801008701324463 seconds, Model: 0.9097919464111328 seconds\nPipeline endpoint: 10.858699083328247 seconds\nPipeline Fast endpoint: 6.549439907073975 seconds\n</code></pre>"},{"location":"blogs/storage/fun_with_endpoints/#timing-results-without-instantiation","title":"Timing results (without instantiation)","text":"<pre><code>Timing for 10 rows\nIndividual endpoints Total: 1.4224412441253662 seconds\nTaut: 0.46330809593200684 seconds, MD: 0.47082972526550293 seconds, Model: 0.4882962703704834 seconds\nPipeline endpoint: 0.9333341121673584 seconds\nPipeline Fast endpoint: 0.7804160118103027 seconds\n\nTiming for 100 rows\nIndividual endpoints Total: 4.0739476680755615 seconds\nTaut: 2.0824239253997803 seconds, MD: 1.3490509986877441 seconds, Model: 0.6424648761749268 seconds\nPipeline endpoint: 2.9797439575195312 seconds\nPipeline Fast endpoint: 1.944951057434082 seconds\n\nTiming for 500 rows\nIndividual endpoints Total: 12.776091814041138 seconds\nTaut: 8.085582971572876 seconds, MD: 3.8499021530151367 seconds, Model: 0.8406000137329102 seconds\nPipeline endpoint: 10.98793077468872 seconds\nPipeline Fast endpoint: 6.76329493522644 seconds\n</code></pre>"},{"location":"blogs/storage/fun_with_endpoints/#timing-results-with-instantiation","title":"Timing results (with instantiation)","text":"<p>``` Timing for 10 rows Individual endpoints Total: 6.6 seconds Taut: 2.05 seconds, MD: 2.32 seconds, Model: 2.27 seconds Pipeline endpoint: 2.347784996032715 seconds Pipeline Fast endpoint: 1.9402740001678467 seconds</p> <p>Timing for 100 rows Individual endpoints Total: 7.686334848403931 seconds Taut: 2.3493008613586426 seconds, MD: 2.758458137512207 seconds, Model: 2.578571081161499 seconds Pipeline endpoint: 3.0636699199676514 seconds Pipeline Fast endpoint: 2.204558849334717 seconds</p> <p>Timing for 500 rows Individual endpoints Total: 14.027569770812988 seconds Taut: 6.019566774368286 seconds, MD: 5.3194098472595215 seconds, Model: 2.6885838508605957 seconds Pipeline endpoint: 8.654812097549438 seconds Pipeline Fast endpoint: 6.5653462409973145 seconds</p> <p>\u00ae Amazon Web Services (AWS) is a trademark of Amazon.com, Inc. or its affiliates.</p>"},{"location":"blogs/storage/htg/","title":"EDA: High Target Gradients","text":"<p>Workbench EDS</p> <p>The Workbench toolkit a set of plots that show EDA results, it also has a flexible plugin architecture to expand, enhance, or even replace the current set of web components Dashboard.</p> <p>The Workbench framework has a broad range of Exploratory Data Analysis (EDA) functionality. Each time a DataSource or FeatureSet is created that data is run through a full set of EDA techniques:</p> <ul> <li>TBD</li> <li>TBD2</li> </ul> <p>One of the latest EDA techniques we've added is the addition of a concept called High Target Gradients </p> <ol> <li>Definition: For a given data point (x_i) with target value (y_i), and its neighbor (x_j) with target value (y_j), the target gradient (G_{ij}) can be defined as:</li> </ol> <p>[G_{ij} = \\frac{|y_i - y_j|}{d(x_i, x_j)}]</p> <p>where (d(x_i, x_j)) is the distance between (x_i) and (x_j) in the feature space. This equation gives you the rate of change of the target value with respect to the change in features, similar to a slope in a two-dimensional space.</p> <ol> <li>Max Gradient for Each Point: For each data point (x_i), you can compute the maximum target gradient with respect to all its neighbors:</li> </ol> <p>[G_{i}^{max} = \\max_{j \\neq i} G_{ij}]</p> <p>This gives you a scalar value for each point in your training data that represents the maximum rate of change of the target value in its local neighborhood.</p> <ol> <li> <p>Usage: You can use (G_{i}^{max}) to identify and filter areas in the feature space that have high target gradients, which may indicate potential issues with data quality or feature representation.</p> </li> <li> <p>Visualization: Plotting the distribution of (G_{i}^{max}) values or visualizing them in the context of the feature space can help you identify regions or specific points that warrant further investigation.</p> </li> </ol>"},{"location":"blogs/storage/htg/#additional-resources","title":"Additional Resources","text":"<ul> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"blogs/storage/model_monitoring/","title":"Model Monitoring","text":"<p>Amazon SageMaker Model Monitor currently provides the following types of monitoring:</p> <ul> <li>Monitor Data Quality: Detect drifts in data quality such as deviations from baseline data types.</li> <li>Monitor Model Quality: Monitor drift in model quality metrics, such as accuracy.</li> <li>Monitor Bias Drift for Models in Production: Monitor bias in your model\u2019s predictions.</li> <li>Monitor Feature Attribution Drift for Models in Production: Monitor drift in feature attribution.</li> </ul>"},{"location":"blogs/storage/residual_analysis/","title":"Residual analysis","text":""},{"location":"blogs/storage/residual_analysis/#residual-analysis","title":"Residual Analysis","text":"<p>Overview and Definition Residual analysis involves examining the differences between observed and predicted values, known as residuals, to assess the performance of a predictive model. It is a critical step in model evaluation as it helps identify patterns of errors, diagnose potential problems, and improve model performance. By understanding where and why a model's predictions deviate from actual values, we can make informed adjustments to the model or the data to enhance accuracy and robustness.</p> <p>Sparse Data Regions The observation is in a part of feature space with little or no nearby training observations, leading to poor generalization in these regions and resulting in high prediction errors.</p> <p>Noisy/Inconsistent Data and Preprocessing Issues The observation is in a part of feature space where the training data is noisy, incorrect, or has high variance in the target variable. Additionally, missing values or incorrect data transformations can introduce errors, leading to unreliable predictions and high residuals.</p> <p>Feature Resolution The current feature set may not fully resolve the compounds, leading to \u2018collisions\u2019 where different compounds are assigned identical features. Such unresolved features can result in different compounds exhibiting the same features, causing high residuals due to unaccounted structural or chemical nuances.</p> <p>Activity Cliffs Structurally similar compounds exhibit significantly different activities, making accurate prediction challenging due to steep changes in activity with minor structural modifications.</p> <p>Feature Engineering Issues Irrelevant or redundant features and poor feature scaling can negatively impact the model's performance and accuracy, resulting in higher residuals.</p> <p>Model Overfitting or Underfitting Overfitting occurs when the model is too complex and captures noise, while underfitting happens when the model is too simple and misses underlying patterns, both leading to inaccurate predictions.</p>"},{"location":"cached/cached_data_source/","title":"CachedDataSource","text":"<p>Model Examples</p> <p>Examples of using the Model Class are in the Examples section at the bottom of this page. AWS Model setup and deployment are quite complicated to do manually but the Workbench Model Class makes it a breeze!</p> <p>CachedDataSource: Caches the method results for Workbench DataSources</p>"},{"location":"cached/cached_data_source/#workbench.cached.cached_data_source.CachedDataSource","title":"<code>CachedDataSource</code>","text":"<p>               Bases: <code>CachedArtifactMixin</code>, <code>AthenaSource</code></p> <p>CachedDataSource: Caches the method results for Workbench DataSources</p> <p>Note: Cached method values may lag underlying DataSource changes.</p> Common Usage <pre><code>my_data = CachedDataSource(name)\nmy_data.details()\nmy_data.health_check()\nmy_data.workbench_meta()\n</code></pre> Source code in <code>src/workbench/cached/cached_data_source.py</code> <pre><code>class CachedDataSource(CachedArtifactMixin, AthenaSource):\n    \"\"\"CachedDataSource: Caches the method results for Workbench DataSources\n\n    Note: Cached method values may lag underlying DataSource changes.\n\n    Common Usage:\n        ```python\n        my_data = CachedDataSource(name)\n        my_data.details()\n        my_data.health_check()\n        my_data.workbench_meta()\n        ```\n    \"\"\"\n\n    def __init__(self, data_name: str, database: str = \"workbench\"):\n        \"\"\"CachedDataSource Initialization\"\"\"\n        AthenaSource.__init__(self, data_name=data_name, database=database, use_cached_meta=True)\n\n    @CachedArtifactMixin.cache_result\n    def summary(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the DataSource Details.\n\n        Returns:\n            dict: A dictionary of details about the DataSource\n        \"\"\"\n        return super().summary(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the DataSource Details.\n\n        Returns:\n            dict: A dictionary of details about the DataSource\n        \"\"\"\n        return super().details(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def health_check(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the DataSource Health Check.\n\n        Returns:\n            dict: A dictionary of health check details for the DataSource\n        \"\"\"\n        return super().health_check(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def workbench_meta(self) -&gt; Union[dict, None]:\n        \"\"\"Retrieve the Workbench Metadata for this DataSource.\n\n        Returns:\n            Union[dict, None]: Dictionary of Workbench metadata for this Artifact\n        \"\"\"\n        return super().workbench_meta()\n\n    def smart_sample(self) -&gt; pd.DataFrame:\n        \"\"\"Retrieve the Smart Sample for this DataSource.\n\n        Returns:\n            pd.DataFrame: The Smart Sample DataFrame\n        \"\"\"\n        return super().smart_sample()\n</code></pre>"},{"location":"cached/cached_data_source/#workbench.cached.cached_data_source.CachedDataSource.__init__","title":"<code>__init__(data_name, database='workbench')</code>","text":"<p>CachedDataSource Initialization</p> Source code in <code>src/workbench/cached/cached_data_source.py</code> <pre><code>def __init__(self, data_name: str, database: str = \"workbench\"):\n    \"\"\"CachedDataSource Initialization\"\"\"\n    AthenaSource.__init__(self, data_name=data_name, database=database, use_cached_meta=True)\n</code></pre>"},{"location":"cached/cached_data_source/#workbench.cached.cached_data_source.CachedDataSource.details","title":"<code>details(**kwargs)</code>","text":"<p>Retrieve the DataSource Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the DataSource</p> Source code in <code>src/workbench/cached/cached_data_source.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef details(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the DataSource Details.\n\n    Returns:\n        dict: A dictionary of details about the DataSource\n    \"\"\"\n    return super().details(**kwargs)\n</code></pre>"},{"location":"cached/cached_data_source/#workbench.cached.cached_data_source.CachedDataSource.health_check","title":"<code>health_check(**kwargs)</code>","text":"<p>Retrieve the DataSource Health Check.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of health check details for the DataSource</p> Source code in <code>src/workbench/cached/cached_data_source.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef health_check(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the DataSource Health Check.\n\n    Returns:\n        dict: A dictionary of health check details for the DataSource\n    \"\"\"\n    return super().health_check(**kwargs)\n</code></pre>"},{"location":"cached/cached_data_source/#workbench.cached.cached_data_source.CachedDataSource.smart_sample","title":"<code>smart_sample()</code>","text":"<p>Retrieve the Smart Sample for this DataSource.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The Smart Sample DataFrame</p> Source code in <code>src/workbench/cached/cached_data_source.py</code> <pre><code>def smart_sample(self) -&gt; pd.DataFrame:\n    \"\"\"Retrieve the Smart Sample for this DataSource.\n\n    Returns:\n        pd.DataFrame: The Smart Sample DataFrame\n    \"\"\"\n    return super().smart_sample()\n</code></pre>"},{"location":"cached/cached_data_source/#workbench.cached.cached_data_source.CachedDataSource.summary","title":"<code>summary(**kwargs)</code>","text":"<p>Retrieve the DataSource Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the DataSource</p> Source code in <code>src/workbench/cached/cached_data_source.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef summary(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the DataSource Details.\n\n    Returns:\n        dict: A dictionary of details about the DataSource\n    \"\"\"\n    return super().summary(**kwargs)\n</code></pre>"},{"location":"cached/cached_data_source/#workbench.cached.cached_data_source.CachedDataSource.workbench_meta","title":"<code>workbench_meta()</code>","text":"<p>Retrieve the Workbench Metadata for this DataSource.</p> <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Union[dict, None]: Dictionary of Workbench metadata for this Artifact</p> Source code in <code>src/workbench/cached/cached_data_source.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef workbench_meta(self) -&gt; Union[dict, None]:\n    \"\"\"Retrieve the Workbench Metadata for this DataSource.\n\n    Returns:\n        Union[dict, None]: Dictionary of Workbench metadata for this Artifact\n    \"\"\"\n    return super().workbench_meta()\n</code></pre>"},{"location":"cached/cached_data_source/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Pull DataSource Details</p> <pre><code>from workbench.cached.cached_data_source import CachedDataSource\n\n# Grab a DataSource\nds = CachedDataSource(\"abalone_data\")\n\n# Show the details\nds.details()\n\n&gt; ds.details()\n\n{'name': 'abalone_data',\n 'health_tags': [],\n 'aws_arn': 'arn:aws:glue:x:table/workbench/abalone_data',\n 'size': 0.070272,\n 'created': '2024-11-09T20:42:34.000Z',\n 'modified': '2024-11-10T19:57:52.000Z',\n 'input': 's3://workbench-public-data/common/aBaLone.CSV',\n 'workbench_health_tags': '',\n 'workbench_correlations': {'length': {'diameter': 0.9868115846024996,\n</code></pre>"},{"location":"cached/cached_endpoint/","title":"CachedEndpoint","text":"<p>Model Examples</p> <p>Examples of using the Model Class are in the Examples section at the bottom of this page. AWS Model setup and deployment are quite complicated to do manually but the Workbench Model Class makes it a breeze!</p> <p>CachedEndpoint: Caches the method results for Workbench Endpoints</p>"},{"location":"cached/cached_endpoint/#workbench.cached.cached_endpoint.CachedEndpoint","title":"<code>CachedEndpoint</code>","text":"<p>               Bases: <code>CachedArtifactMixin</code>, <code>EndpointCore</code></p> <p>CachedEndpoint: Caches the method results for Workbench Endpoints</p> <p>Note: Cached method values may lag underlying Endpoint changes.</p> Common Usage <pre><code>my_endpoint = CachedEndpoint(name)\nmy_endpoint.details()\nmy_endpoint.health_check()\nmy_endpoint.workbench_meta()\n</code></pre> Source code in <code>src/workbench/cached/cached_endpoint.py</code> <pre><code>class CachedEndpoint(CachedArtifactMixin, EndpointCore):\n    \"\"\"CachedEndpoint: Caches the method results for Workbench Endpoints\n\n    Note: Cached method values may lag underlying Endpoint changes.\n\n    Common Usage:\n        ```python\n        my_endpoint = CachedEndpoint(name)\n        my_endpoint.details()\n        my_endpoint.health_check()\n        my_endpoint.workbench_meta()\n        ```\n    \"\"\"\n\n    def __init__(self, endpoint_name: str):\n        \"\"\"CachedEndpoint Initialization\"\"\"\n        EndpointCore.__init__(self, endpoint_name=endpoint_name, use_cached_meta=True)\n\n    @CachedArtifactMixin.cache_result\n    def summary(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the CachedEndpoint Details.\n\n        Returns:\n            dict: A dictionary of details about the CachedEndpoint\n        \"\"\"\n        return super().summary(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the CachedEndpoint Details.\n\n        Returns:\n            dict: A dictionary of details about the CachedEndpoint\n        \"\"\"\n        return super().details(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def health_check(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the CachedEndpoint Health Check.\n\n        Returns:\n            dict: A dictionary of health check details for the CachedEndpoint\n        \"\"\"\n        return super().health_check(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def workbench_meta(self) -&gt; Union[str, None]:\n        \"\"\"Retrieve the Enumerated Model Type (REGRESSOR, CLASSIFER, etc).\n\n        Returns:\n            str: The Enumerated Model Type\n        \"\"\"\n        return super().workbench_meta()\n\n    @CachedArtifactMixin.cache_result\n    def endpoint_metrics(self) -&gt; Union[str, None]:\n        \"\"\"Retrieve the Endpoint Metrics\n\n        Returns:\n            str: The Endpoint Metrics\n        \"\"\"\n        return super().endpoint_metrics()\n</code></pre>"},{"location":"cached/cached_endpoint/#workbench.cached.cached_endpoint.CachedEndpoint.__init__","title":"<code>__init__(endpoint_name)</code>","text":"<p>CachedEndpoint Initialization</p> Source code in <code>src/workbench/cached/cached_endpoint.py</code> <pre><code>def __init__(self, endpoint_name: str):\n    \"\"\"CachedEndpoint Initialization\"\"\"\n    EndpointCore.__init__(self, endpoint_name=endpoint_name, use_cached_meta=True)\n</code></pre>"},{"location":"cached/cached_endpoint/#workbench.cached.cached_endpoint.CachedEndpoint.details","title":"<code>details(**kwargs)</code>","text":"<p>Retrieve the CachedEndpoint Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the CachedEndpoint</p> Source code in <code>src/workbench/cached/cached_endpoint.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef details(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the CachedEndpoint Details.\n\n    Returns:\n        dict: A dictionary of details about the CachedEndpoint\n    \"\"\"\n    return super().details(**kwargs)\n</code></pre>"},{"location":"cached/cached_endpoint/#workbench.cached.cached_endpoint.CachedEndpoint.endpoint_metrics","title":"<code>endpoint_metrics()</code>","text":"<p>Retrieve the Endpoint Metrics</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>The Endpoint Metrics</p> Source code in <code>src/workbench/cached/cached_endpoint.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef endpoint_metrics(self) -&gt; Union[str, None]:\n    \"\"\"Retrieve the Endpoint Metrics\n\n    Returns:\n        str: The Endpoint Metrics\n    \"\"\"\n    return super().endpoint_metrics()\n</code></pre>"},{"location":"cached/cached_endpoint/#workbench.cached.cached_endpoint.CachedEndpoint.health_check","title":"<code>health_check(**kwargs)</code>","text":"<p>Retrieve the CachedEndpoint Health Check.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of health check details for the CachedEndpoint</p> Source code in <code>src/workbench/cached/cached_endpoint.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef health_check(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the CachedEndpoint Health Check.\n\n    Returns:\n        dict: A dictionary of health check details for the CachedEndpoint\n    \"\"\"\n    return super().health_check(**kwargs)\n</code></pre>"},{"location":"cached/cached_endpoint/#workbench.cached.cached_endpoint.CachedEndpoint.summary","title":"<code>summary(**kwargs)</code>","text":"<p>Retrieve the CachedEndpoint Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the CachedEndpoint</p> Source code in <code>src/workbench/cached/cached_endpoint.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef summary(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the CachedEndpoint Details.\n\n    Returns:\n        dict: A dictionary of details about the CachedEndpoint\n    \"\"\"\n    return super().summary(**kwargs)\n</code></pre>"},{"location":"cached/cached_endpoint/#workbench.cached.cached_endpoint.CachedEndpoint.workbench_meta","title":"<code>workbench_meta()</code>","text":"<p>Retrieve the Enumerated Model Type (REGRESSOR, CLASSIFER, etc).</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>The Enumerated Model Type</p> Source code in <code>src/workbench/cached/cached_endpoint.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef workbench_meta(self) -&gt; Union[str, None]:\n    \"\"\"Retrieve the Enumerated Model Type (REGRESSOR, CLASSIFER, etc).\n\n    Returns:\n        str: The Enumerated Model Type\n    \"\"\"\n    return super().workbench_meta()\n</code></pre>"},{"location":"cached/cached_endpoint/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Get Endpoint Details</p> <pre><code>from workbench.cached.cached_endpoint import CachedEndpoint\n\n# Grab an Endpoint\nend = CachedEndpoint(\"abalone-regression\")\n\n# Get the Details\n end.details()\n\n{'name': 'abalone-regression-end',\n 'health_tags': [],\n 'status': 'InService',\n 'instance': 'Serverless (2GB/5)',\n 'instance_count': '-',\n 'variant': 'AllTraffic',\n 'model_name': 'abalone-regression',\n 'model_type': 'regressor',\n 'model_metrics':        RMSE     R2    MAPE  MedAE  NumRows\n 1.64  2.246  0.502  16.393  1.209      834,\n 'confusion_matrix': None,\n 'predictions':      class_number_of_rings  prediction    id\n 0                       16   10.516158     7\n 1                        9    9.031365     8\n 2                       10    9.264600    17\n 3                        7    8.578638    18\n 4                       12   10.492446    27\n ..                     ...         ...   ...\n 829                     11   11.915862  4148\n 830                      8    8.210898  4157\n 831                      8    7.693689  4158\n 832                      9    7.542521  4167\n 833                      8    9.060015  4168\n</code></pre>"},{"location":"cached/cached_feature_set/","title":"CachedFeatureSet","text":"<p>Model Examples</p> <p>Examples of using the Model Class are in the Examples section at the bottom of this page. AWS Model setup and deployment are quite complicated to do manually but the Workbench Model Class makes it a breeze!</p> <p>CachedFeatureSet: Caches the method results for Workbench FeatureSets</p>"},{"location":"cached/cached_feature_set/#workbench.cached.cached_feature_set.CachedFeatureSet","title":"<code>CachedFeatureSet</code>","text":"<p>               Bases: <code>CachedArtifactMixin</code>, <code>FeatureSetCore</code></p> <p>CachedFeatureSet: Caches the method results for Workbench FeatureSets</p> <p>Note: Cached method values may lag underlying FeatureSet changes.</p> Common Usage <pre><code>my_features = CachedFeatureSet(name)\nmy_features.details()\nmy_features.health_check()\nmy_features.workbench_meta()\n</code></pre> Source code in <code>src/workbench/cached/cached_feature_set.py</code> <pre><code>class CachedFeatureSet(CachedArtifactMixin, FeatureSetCore):\n    \"\"\"CachedFeatureSet: Caches the method results for Workbench FeatureSets\n\n    Note: Cached method values may lag underlying FeatureSet changes.\n\n    Common Usage:\n        ```python\n        my_features = CachedFeatureSet(name)\n        my_features.details()\n        my_features.health_check()\n        my_features.workbench_meta()\n        ```\n    \"\"\"\n\n    def __init__(self, feature_set_name: str, database: str = \"workbench\"):\n        \"\"\"CachedFeatureSet Initialization\"\"\"\n        FeatureSetCore.__init__(self, feature_set_name=feature_set_name, use_cached_meta=True)\n\n    @CachedArtifactMixin.cache_result\n    def summary(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the FeatureSet Details.\n\n        Returns:\n            dict: A dictionary of details about the FeatureSet\n        \"\"\"\n        return super().summary(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the FeatureSet Details.\n\n        Returns:\n            dict: A dictionary of details about the FeatureSet\n        \"\"\"\n        return super().details(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def health_check(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the FeatureSet Health Check.\n\n        Returns:\n            dict: A dictionary of health check details for the FeatureSet\n        \"\"\"\n        return super().health_check(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def workbench_meta(self) -&gt; Union[str, None]:\n        \"\"\"Retrieve the Workbench Metadata for this DataSource.\n\n        Returns:\n            Union[dict, None]: Dictionary of Workbench metadata for this Artifact\n        \"\"\"\n        return super().workbench_meta()\n\n    @CachedArtifactMixin.cache_result\n    def smart_sample(self) -&gt; pd.DataFrame:\n        \"\"\"Retrieve the Smart Sample for this FeatureSet.\n\n        Returns:\n            pd.DataFrame: The Smart Sample DataFrame\n        \"\"\"\n        return super().smart_sample()\n</code></pre>"},{"location":"cached/cached_feature_set/#workbench.cached.cached_feature_set.CachedFeatureSet.__init__","title":"<code>__init__(feature_set_name, database='workbench')</code>","text":"<p>CachedFeatureSet Initialization</p> Source code in <code>src/workbench/cached/cached_feature_set.py</code> <pre><code>def __init__(self, feature_set_name: str, database: str = \"workbench\"):\n    \"\"\"CachedFeatureSet Initialization\"\"\"\n    FeatureSetCore.__init__(self, feature_set_name=feature_set_name, use_cached_meta=True)\n</code></pre>"},{"location":"cached/cached_feature_set/#workbench.cached.cached_feature_set.CachedFeatureSet.details","title":"<code>details(**kwargs)</code>","text":"<p>Retrieve the FeatureSet Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the FeatureSet</p> Source code in <code>src/workbench/cached/cached_feature_set.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef details(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the FeatureSet Details.\n\n    Returns:\n        dict: A dictionary of details about the FeatureSet\n    \"\"\"\n    return super().details(**kwargs)\n</code></pre>"},{"location":"cached/cached_feature_set/#workbench.cached.cached_feature_set.CachedFeatureSet.health_check","title":"<code>health_check(**kwargs)</code>","text":"<p>Retrieve the FeatureSet Health Check.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of health check details for the FeatureSet</p> Source code in <code>src/workbench/cached/cached_feature_set.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef health_check(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the FeatureSet Health Check.\n\n    Returns:\n        dict: A dictionary of health check details for the FeatureSet\n    \"\"\"\n    return super().health_check(**kwargs)\n</code></pre>"},{"location":"cached/cached_feature_set/#workbench.cached.cached_feature_set.CachedFeatureSet.smart_sample","title":"<code>smart_sample()</code>","text":"<p>Retrieve the Smart Sample for this FeatureSet.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The Smart Sample DataFrame</p> Source code in <code>src/workbench/cached/cached_feature_set.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef smart_sample(self) -&gt; pd.DataFrame:\n    \"\"\"Retrieve the Smart Sample for this FeatureSet.\n\n    Returns:\n        pd.DataFrame: The Smart Sample DataFrame\n    \"\"\"\n    return super().smart_sample()\n</code></pre>"},{"location":"cached/cached_feature_set/#workbench.cached.cached_feature_set.CachedFeatureSet.summary","title":"<code>summary(**kwargs)</code>","text":"<p>Retrieve the FeatureSet Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the FeatureSet</p> Source code in <code>src/workbench/cached/cached_feature_set.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef summary(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the FeatureSet Details.\n\n    Returns:\n        dict: A dictionary of details about the FeatureSet\n    \"\"\"\n    return super().summary(**kwargs)\n</code></pre>"},{"location":"cached/cached_feature_set/#workbench.cached.cached_feature_set.CachedFeatureSet.workbench_meta","title":"<code>workbench_meta()</code>","text":"<p>Retrieve the Workbench Metadata for this DataSource.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>Union[dict, None]: Dictionary of Workbench metadata for this Artifact</p> Source code in <code>src/workbench/cached/cached_feature_set.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef workbench_meta(self) -&gt; Union[str, None]:\n    \"\"\"Retrieve the Workbench Metadata for this DataSource.\n\n    Returns:\n        Union[dict, None]: Dictionary of Workbench metadata for this Artifact\n    \"\"\"\n    return super().workbench_meta()\n</code></pre>"},{"location":"cached/cached_feature_set/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Pull FeatureSet Details</p> <pre><code>from workbench.cached.cached_feature_set import CachedFeatureSet\n\n# Grab a FeatureSet\nfs = CachedFeatureSet(\"abalone_features\")\n\n# Show the details\nfs.details()\n\n&gt; fs.details()\n\n{'name': 'abalone_features',\n 'health_tags': [],\n 'aws_arn': 'arn:aws:glue:x:table/workbench/abalone_data',\n 'size': 0.070272,\n 'created': '2024-11-09T20:42:34.000Z',\n 'modified': '2024-11-10T19:57:52.000Z',\n 'input': 's3://workbench-public-data/common/aBaLone.CSV',\n 'workbench_health_tags': '',\n 'workbench_correlations': {'length': {'diameter': 0.9868115846024996,\n</code></pre>"},{"location":"cached/cached_meta/","title":"CachedMeta","text":"<p>CachedMeta Examples</p> <p>Examples of using the CachedMeta class are listed at the bottom of this page Examples.</p> <p>CachedMeta: A class that provides caching for the Meta() class</p>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta","title":"<code>CachedMeta</code>","text":"<p>               Bases: <code>CloudMeta</code></p> <p>CachedMeta: Singleton class for caching metadata functionality.</p> Common Usage <pre><code>from workbench.cached.cached_meta import CachedMeta\nmeta = CachedMeta()\n\n# Get the AWS Account Info\nmeta.account()\nmeta.config()\n\n# These are 'list' methods\nmeta.etl_jobs()\nmeta.data_sources()\nmeta.feature_sets(details=True/False)\nmeta.models(details=True/False)\nmeta.endpoints(details=True/False)\nmeta.views()\n\n# These are 'describe' methods\nmeta.data_source(\"abalone_data\")\nmeta.feature_set(\"abalone_features\")\nmeta.model(\"abalone-regression\")\nmeta.endpoint(\"abalone-endpoint\")\n</code></pre> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>class CachedMeta(CloudMeta):\n    \"\"\"CachedMeta: Singleton class for caching metadata functionality.\n\n    Common Usage:\n       ```python\n       from workbench.cached.cached_meta import CachedMeta\n       meta = CachedMeta()\n\n       # Get the AWS Account Info\n       meta.account()\n       meta.config()\n\n       # These are 'list' methods\n       meta.etl_jobs()\n       meta.data_sources()\n       meta.feature_sets(details=True/False)\n       meta.models(details=True/False)\n       meta.endpoints(details=True/False)\n       meta.views()\n\n       # These are 'describe' methods\n       meta.data_source(\"abalone_data\")\n       meta.feature_set(\"abalone_features\")\n       meta.model(\"abalone-regression\")\n       meta.endpoint(\"abalone-endpoint\")\n       ```\n    \"\"\"\n\n    _instance = None  # Class attribute to hold the singleton instance\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super(CachedMeta, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self):\n        \"\"\"CachedMeta Initialization\"\"\"\n        if hasattr(self, \"_initialized\") and self._initialized:\n            return  # Prevent reinitialization\n\n        self.log = logging.getLogger(\"workbench\")\n        self.log.important(\"Initializing CachedMeta...\")\n        super().__init__()\n\n        # Create both our Meta Cache and Fresh Cache (tracks if data is stale)\n        self.meta_cache = WorkbenchCache(prefix=\"meta\")\n        self.fresh_cache = WorkbenchCache(prefix=\"meta_fresh\", expire=300)  # 5-minute expiration\n\n        # Create a ThreadPoolExecutor for refreshing stale data\n        self.thread_pool = ThreadPoolExecutor(max_workers=5)\n\n        # Mark the instance as initialized\n        self._initialized = True\n\n    def check(self):\n        \"\"\"Check if our underlying caches are working\"\"\"\n        return self.meta_cache.check()\n\n    def list_meta_cache(self):\n        \"\"\"List the current Meta Cache\"\"\"\n        return self.meta_cache.list_keys()\n\n    def clear_meta_cache(self):\n        \"\"\"Clear the current Meta Cache\"\"\"\n        self.meta_cache.clear()\n\n    @cache_result\n    def account(self) -&gt; dict:\n        \"\"\"Cloud Platform Account Info\n\n        Returns:\n            dict: Cloud Platform Account Info\n        \"\"\"\n        return super().account()\n\n    @cache_result\n    def config(self) -&gt; dict:\n        \"\"\"Return the current Workbench Configuration\n\n        Returns:\n            dict: The current Workbench Configuration\n        \"\"\"\n        return super().config()\n\n    @cache_result\n    def incoming_data(self) -&gt; pd.DataFrame:\n        \"\"\"Get summary data about data in the incoming raw data\n\n        Returns:\n            pd.DataFrame: A summary of the incoming raw data\n        \"\"\"\n        return super().incoming_data()\n\n    @cache_result\n    def etl_jobs(self) -&gt; pd.DataFrame:\n        \"\"\"Get summary data about Extract, Transform, Load (ETL) Jobs\n\n        Returns:\n            pd.DataFrame: A summary of the ETL Jobs deployed in the Cloud Platform\n        \"\"\"\n        return super().etl_jobs()\n\n    @cache_result\n    def data_sources(self) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the Data Sources deployed in the Cloud Platform\n\n        Returns:\n            pd.DataFrame: A summary of the Data Sources deployed in the Cloud Platform\n        \"\"\"\n        return super().data_sources()\n\n    @cache_result\n    def views(self, database: str = \"workbench\") -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the all the Views, for the given database, in AWS\n\n        Args:\n            database (str, optional): Glue database. Defaults to 'workbench'.\n\n        Returns:\n            pd.DataFrame: A summary of all the Views, for the given database, in AWS\n        \"\"\"\n        return super().views(database=database)\n\n    @cache_result\n    def feature_sets(self, details: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the Feature Sets deployed in the Cloud Platform\n\n        Args:\n            details (bool, optional): Include detailed information. Defaults to False.\n\n        Returns:\n            pd.DataFrame: A summary of the Feature Sets deployed in the Cloud Platform\n        \"\"\"\n        return super().feature_sets(details=details)\n\n    @cache_result\n    def models(self, details: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the Models deployed in the Cloud Platform\n\n        Args:\n            details (bool, optional): Include detailed information. Defaults to False.\n\n        Returns:\n            pd.DataFrame: A summary of the Models deployed in the Cloud Platform\n        \"\"\"\n        return super().models(details=details)\n\n    @cache_result\n    def endpoints(self, details: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Get a summary of the Endpoints deployed in the Cloud Platform\n\n        Args:\n            details (bool, optional): Include detailed information. Defaults to False.\n\n        Returns:\n            pd.DataFrame: A summary of the Endpoints in the Cloud Platform\n        \"\"\"\n        return super().endpoints(details=details)\n\n    @cache_result\n    def glue_job(self, job_name: str) -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Glue Job\n\n        Args:\n            job_name (str): The name of the Glue Job\n\n        Returns:\n            dict: The details of the Glue Job (None if not found)\n        \"\"\"\n        return super().glue_job(job_name=job_name)\n\n    @cache_result\n    def data_source(self, data_source_name: str, database: str = \"workbench\") -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Data Source\n\n        Args:\n            data_source_name (str): The name of the Data Source\n            database (str, optional): The Glue database. Defaults to 'workbench'.\n\n        Returns:\n            dict: The details of the Data Source (None if not found)\n        \"\"\"\n        return super().data_source(data_source_name=data_source_name, database=database)\n\n    @cache_result\n    def feature_set(self, feature_set_name: str) -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Feature Set\n\n        Args:\n            feature_set_name (str): The name of the Feature Set\n\n        Returns:\n            dict: The details of the Feature Set (None if not found)\n        \"\"\"\n        return super().feature_set(feature_set_name=feature_set_name)\n\n    @cache_result\n    def model(self, model_name: str) -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Model\n\n        Args:\n            model_name (str): The name of the Model\n\n        Returns:\n            dict: The details of the Model (None if not found)\n        \"\"\"\n        return super().model(model_name=model_name)\n\n    @cache_result\n    def endpoint(self, endpoint_name: str) -&gt; Union[dict, None]:\n        \"\"\"Get the details of a specific Endpoint\n\n        Args:\n            endpoint_name (str): The name of the Endpoint\n\n        Returns:\n            dict: The details of the Endpoint (None if not found)\n        \"\"\"\n        return super().endpoint(endpoint_name=endpoint_name)\n\n    def _refresh_data_in_background(self, cache_key, method, *args, **kwargs):\n        \"\"\"Background task to refresh AWS metadata.\"\"\"\n        result = method(self, *args, **kwargs)\n        self.meta_cache.set(cache_key, result)\n        self.log.important(f\"Updated Metadata for {cache_key}\")\n\n    @staticmethod\n    def _flatten_redis_key(method, *args, **kwargs):\n        \"\"\"Flatten the args and kwargs into a single string\"\"\"\n        arg_str = \"_\".join(str(arg) for arg in args)\n        kwarg_str = \"_\".join(f\"{k}_{v}\" for k, v in sorted(kwargs.items()))\n        return f\"{method.__name__}_{arg_str}_{kwarg_str}\".replace(\" \", \"\").replace(\"'\", \"\")\n\n    def __del__(self):\n        \"\"\"Destructor to shut down the thread pool gracefully.\"\"\"\n        self.close()\n\n    def close(self):\n        \"\"\"Explicitly close the thread pool, if needed.\"\"\"\n        if self.thread_pool:\n            self.log.important(\"Shutting down the ThreadPoolExecutor...\")\n            try:\n                self.thread_pool.shutdown(wait=True)  # Gracefully shutdown\n            except RuntimeError as e:\n                self.log.error(f\"Error during thread pool shutdown: {e}\")\n            finally:\n                self.thread_pool = None\n\n    def __repr__(self):\n        return f\"CachedMeta()\\n\\t{repr(self.meta_cache)}\\n\\t{super().__repr__()}\"\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.__del__","title":"<code>__del__()</code>","text":"<p>Destructor to shut down the thread pool gracefully.</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor to shut down the thread pool gracefully.\"\"\"\n    self.close()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.__init__","title":"<code>__init__()</code>","text":"<p>CachedMeta Initialization</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>def __init__(self):\n    \"\"\"CachedMeta Initialization\"\"\"\n    if hasattr(self, \"_initialized\") and self._initialized:\n        return  # Prevent reinitialization\n\n    self.log = logging.getLogger(\"workbench\")\n    self.log.important(\"Initializing CachedMeta...\")\n    super().__init__()\n\n    # Create both our Meta Cache and Fresh Cache (tracks if data is stale)\n    self.meta_cache = WorkbenchCache(prefix=\"meta\")\n    self.fresh_cache = WorkbenchCache(prefix=\"meta_fresh\", expire=300)  # 5-minute expiration\n\n    # Create a ThreadPoolExecutor for refreshing stale data\n    self.thread_pool = ThreadPoolExecutor(max_workers=5)\n\n    # Mark the instance as initialized\n    self._initialized = True\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.account","title":"<code>account()</code>","text":"<p>Cloud Platform Account Info</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Cloud Platform Account Info</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef account(self) -&gt; dict:\n    \"\"\"Cloud Platform Account Info\n\n    Returns:\n        dict: Cloud Platform Account Info\n    \"\"\"\n    return super().account()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.check","title":"<code>check()</code>","text":"<p>Check if our underlying caches are working</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>def check(self):\n    \"\"\"Check if our underlying caches are working\"\"\"\n    return self.meta_cache.check()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.clear_meta_cache","title":"<code>clear_meta_cache()</code>","text":"<p>Clear the current Meta Cache</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>def clear_meta_cache(self):\n    \"\"\"Clear the current Meta Cache\"\"\"\n    self.meta_cache.clear()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.close","title":"<code>close()</code>","text":"<p>Explicitly close the thread pool, if needed.</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>def close(self):\n    \"\"\"Explicitly close the thread pool, if needed.\"\"\"\n    if self.thread_pool:\n        self.log.important(\"Shutting down the ThreadPoolExecutor...\")\n        try:\n            self.thread_pool.shutdown(wait=True)  # Gracefully shutdown\n        except RuntimeError as e:\n            self.log.error(f\"Error during thread pool shutdown: {e}\")\n        finally:\n            self.thread_pool = None\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.config","title":"<code>config()</code>","text":"<p>Return the current Workbench Configuration</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The current Workbench Configuration</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef config(self) -&gt; dict:\n    \"\"\"Return the current Workbench Configuration\n\n    Returns:\n        dict: The current Workbench Configuration\n    \"\"\"\n    return super().config()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.data_source","title":"<code>data_source(data_source_name, database='workbench')</code>","text":"<p>Get the details of a specific Data Source</p> <p>Parameters:</p> Name Type Description Default <code>data_source_name</code> <code>str</code> <p>The name of the Data Source</p> required <code>database</code> <code>str</code> <p>The Glue database. Defaults to 'workbench'.</p> <code>'workbench'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Data Source (None if not found)</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef data_source(self, data_source_name: str, database: str = \"workbench\") -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Data Source\n\n    Args:\n        data_source_name (str): The name of the Data Source\n        database (str, optional): The Glue database. Defaults to 'workbench'.\n\n    Returns:\n        dict: The details of the Data Source (None if not found)\n    \"\"\"\n    return super().data_source(data_source_name=data_source_name, database=database)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.data_sources","title":"<code>data_sources()</code>","text":"<p>Get a summary of the Data Sources deployed in the Cloud Platform</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Data Sources deployed in the Cloud Platform</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef data_sources(self) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the Data Sources deployed in the Cloud Platform\n\n    Returns:\n        pd.DataFrame: A summary of the Data Sources deployed in the Cloud Platform\n    \"\"\"\n    return super().data_sources()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.endpoint","title":"<code>endpoint(endpoint_name)</code>","text":"<p>Get the details of a specific Endpoint</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the Endpoint</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Endpoint (None if not found)</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef endpoint(self, endpoint_name: str) -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Endpoint\n\n    Args:\n        endpoint_name (str): The name of the Endpoint\n\n    Returns:\n        dict: The details of the Endpoint (None if not found)\n    \"\"\"\n    return super().endpoint(endpoint_name=endpoint_name)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.endpoints","title":"<code>endpoints(details=False)</code>","text":"<p>Get a summary of the Endpoints deployed in the Cloud Platform</p> <p>Parameters:</p> Name Type Description Default <code>details</code> <code>bool</code> <p>Include detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Endpoints in the Cloud Platform</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef endpoints(self, details: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the Endpoints deployed in the Cloud Platform\n\n    Args:\n        details (bool, optional): Include detailed information. Defaults to False.\n\n    Returns:\n        pd.DataFrame: A summary of the Endpoints in the Cloud Platform\n    \"\"\"\n    return super().endpoints(details=details)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.etl_jobs","title":"<code>etl_jobs()</code>","text":"<p>Get summary data about Extract, Transform, Load (ETL) Jobs</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the ETL Jobs deployed in the Cloud Platform</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef etl_jobs(self) -&gt; pd.DataFrame:\n    \"\"\"Get summary data about Extract, Transform, Load (ETL) Jobs\n\n    Returns:\n        pd.DataFrame: A summary of the ETL Jobs deployed in the Cloud Platform\n    \"\"\"\n    return super().etl_jobs()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.feature_set","title":"<code>feature_set(feature_set_name)</code>","text":"<p>Get the details of a specific Feature Set</p> <p>Parameters:</p> Name Type Description Default <code>feature_set_name</code> <code>str</code> <p>The name of the Feature Set</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Feature Set (None if not found)</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef feature_set(self, feature_set_name: str) -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Feature Set\n\n    Args:\n        feature_set_name (str): The name of the Feature Set\n\n    Returns:\n        dict: The details of the Feature Set (None if not found)\n    \"\"\"\n    return super().feature_set(feature_set_name=feature_set_name)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.feature_sets","title":"<code>feature_sets(details=False)</code>","text":"<p>Get a summary of the Feature Sets deployed in the Cloud Platform</p> <p>Parameters:</p> Name Type Description Default <code>details</code> <code>bool</code> <p>Include detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Feature Sets deployed in the Cloud Platform</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef feature_sets(self, details: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the Feature Sets deployed in the Cloud Platform\n\n    Args:\n        details (bool, optional): Include detailed information. Defaults to False.\n\n    Returns:\n        pd.DataFrame: A summary of the Feature Sets deployed in the Cloud Platform\n    \"\"\"\n    return super().feature_sets(details=details)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.glue_job","title":"<code>glue_job(job_name)</code>","text":"<p>Get the details of a specific Glue Job</p> <p>Parameters:</p> Name Type Description Default <code>job_name</code> <code>str</code> <p>The name of the Glue Job</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Glue Job (None if not found)</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef glue_job(self, job_name: str) -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Glue Job\n\n    Args:\n        job_name (str): The name of the Glue Job\n\n    Returns:\n        dict: The details of the Glue Job (None if not found)\n    \"\"\"\n    return super().glue_job(job_name=job_name)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.incoming_data","title":"<code>incoming_data()</code>","text":"<p>Get summary data about data in the incoming raw data</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the incoming raw data</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef incoming_data(self) -&gt; pd.DataFrame:\n    \"\"\"Get summary data about data in the incoming raw data\n\n    Returns:\n        pd.DataFrame: A summary of the incoming raw data\n    \"\"\"\n    return super().incoming_data()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.list_meta_cache","title":"<code>list_meta_cache()</code>","text":"<p>List the current Meta Cache</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>def list_meta_cache(self):\n    \"\"\"List the current Meta Cache\"\"\"\n    return self.meta_cache.list_keys()\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.model","title":"<code>model(model_name)</code>","text":"<p>Get the details of a specific Model</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the Model</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The details of the Model (None if not found)</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef model(self, model_name: str) -&gt; Union[dict, None]:\n    \"\"\"Get the details of a specific Model\n\n    Args:\n        model_name (str): The name of the Model\n\n    Returns:\n        dict: The details of the Model (None if not found)\n    \"\"\"\n    return super().model(model_name=model_name)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.models","title":"<code>models(details=False)</code>","text":"<p>Get a summary of the Models deployed in the Cloud Platform</p> <p>Parameters:</p> Name Type Description Default <code>details</code> <code>bool</code> <p>Include detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of the Models deployed in the Cloud Platform</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef models(self, details: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the Models deployed in the Cloud Platform\n\n    Args:\n        details (bool, optional): Include detailed information. Defaults to False.\n\n    Returns:\n        pd.DataFrame: A summary of the Models deployed in the Cloud Platform\n    \"\"\"\n    return super().models(details=details)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.CachedMeta.views","title":"<code>views(database='workbench')</code>","text":"<p>Get a summary of the all the Views, for the given database, in AWS</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>Glue database. Defaults to 'workbench'.</p> <code>'workbench'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A summary of all the Views, for the given database, in AWS</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>@cache_result\ndef views(self, database: str = \"workbench\") -&gt; pd.DataFrame:\n    \"\"\"Get a summary of the all the Views, for the given database, in AWS\n\n    Args:\n        database (str, optional): Glue database. Defaults to 'workbench'.\n\n    Returns:\n        pd.DataFrame: A summary of all the Views, for the given database, in AWS\n    \"\"\"\n    return super().views(database=database)\n</code></pre>"},{"location":"cached/cached_meta/#workbench.cached.cached_meta.cache_result","title":"<code>cache_result(method)</code>","text":"<p>Decorator to cache method results in meta_cache</p> Source code in <code>src/workbench/cached/cached_meta.py</code> <pre><code>def cache_result(method):\n    \"\"\"Decorator to cache method results in meta_cache\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        # Create a unique cache key based on the method name and arguments\n        cache_key = CachedMeta._flatten_redis_key(method, *args, **kwargs)\n\n        # Check for fresh data, spawn thread to refresh if stale\n        if self.fresh_cache.atomic_set(cache_key, True):\n            self.log.important(f\"Async: Metadata for {cache_key} refresh thread started...\")\n            self.thread_pool.submit(self._refresh_data_in_background, cache_key, method, *args, **kwargs)\n\n        # Return data (fresh or stale) if available\n        cached_value = self.meta_cache.get(cache_key)\n        if cached_value is not None:\n            return cached_value\n\n        # Fall back to calling the method if no cached data found\n        self.log.important(f\"Blocking: Getting Metadata for {cache_key}\")\n        result = method(self, *args, **kwargs)\n        self.meta_cache.set(cache_key, result)\n        return result\n\n    return wrapper\n</code></pre>"},{"location":"cached/cached_meta/#examples","title":"Examples","text":"<p>These example show how to use the <code>CachedMeta()</code> class to pull lists of artifacts from AWS. DataSources, FeatureSets, Models, Endpoints and more. If you're building a web interface plugin, the CachedMeta class is a great place to start.</p> <p>Workbench REPL</p> <p>If you'd like to see exactly what data/details you get back from the <code>CachedMeta()</code> class, you can spin up the Workbench REPL, use the class and test out all the methods. Try it out! Workbench REPL</p> Using Workbench REPL<pre><code>CachedMeta = CachedMeta()\nmodel_df = CachedMeta.models()\nmodel_df\n               Model Group   Health Owner  ...             Input     Status                Description\n0      wine-classification  healthy     -  ...     wine_features  Completed  Wine Classification Model\n1  abalone-regression-full  healthy     -  ...  abalone_features  Completed   Abalone Regression Model\n2       abalone-regression  healthy     -  ...  abalone_features  Completed   Abalone Regression Model\n\n[3 rows x 10 columns]\n</code></pre> <p>List the Models in AWS</p> <pre><code>from workbench.cached.cached_meta import CachedMeta\n\n# Create our CachedMeta Class and get a list of our Models\nCachedMeta = CachedMeta()\nmodel_df = CachedMeta.models()\n\nprint(f\"Number of Models: {len(model_df)}\")\nprint(model_df)\n\n# Get more details data on the Models\nmodel_names = model_df[\"Model Group\"].tolist()\nfor name in model_names:\n    pprint(CachedMeta.model(name))\n</code></pre> <p>Output</p> <pre><code>Number of Models: 3\n               Model Group   Health Owner  ...             Input     Status                Description\n0      wine-classification  healthy     -  ...     wine_features  Completed  Wine Classification Model\n1  abalone-regression-full  healthy     -  ...  abalone_features  Completed   Abalone Regression Model\n2       abalone-regression  healthy     -  ...  abalone_features  Completed   Abalone Regression Model\n\n[3 rows x 10 columns]\nwine-classification\nabalone-regression-full\nabalone-regression\n</code></pre> <p>Getting Model Performance Metrics</p> <pre><code>from workbench.cached.cached_meta import CachedMeta\n\n# Create our CachedMeta Class and get a list of our Models\nCachedMeta = CachedMeta()\nmodel_df = CachedMeta.models()\n\nprint(f\"Number of Models: {len(model_df)}\")\nprint(model_df)\n\n# Get more details data on the Models\nmodel_names = model_df[\"Model Group\"].tolist()\nfor name in model_names[:5]:\n    model_details = CachedMeta.model(name)\n    print(f\"\\n\\nModel: {name}\")\n    performance_metrics = model_details[\"workbench_CachedMeta\"][\"workbench_inference_metrics\"]\n    print(f\"\\tPerformance Metrics: {performance_metrics}\")\n</code></pre> <p>Output</p> <pre><code>wine-classification\n    ARN: arn:aws:sagemaker:us-west-2:507740646243:model-package-group/wine-classification\n    Description: Wine Classification Model\n    Tags: wine::classification\n    Performance Metrics:\n        [{'wine_class': 'TypeA', 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'support': 12}, {'wine_class': 'TypeB', 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'support': 14}, {'wine_class': 'TypeC', 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'support': 9}]\n\nabalone-regression\n    ARN: arn:aws:sagemaker:us-west-2:507740646243:model-package-group/abalone-regression\n    Description: Abalone Regression Model\n    Tags: abalone::regression\n    Performance Metrics:\n        [{'MAE': 1.64, 'RMSE': 2.246, 'R2': 0.502, 'MAPE': 16.393, 'MedAE': 1.209, 'NumRows': 834}]\n</code></pre> <p>List the Endpoints in AWS</p> <pre><code>from pprint import pprint\nfrom workbench.cached.cached_meta import CachedMeta\n\n# Create our CachedMeta Class and get a list of our Endpoints\nCachedMeta = CachedMeta()\nendpoint_df = CachedMeta.endpoints()\nprint(f\"Number of Endpoints: {len(endpoint_df)}\")\nprint(endpoint_df)\n\n# Get more details data on the Endpoints\nendpoint_names = endpoint_df[\"Name\"].tolist()\nfor name in endpoint_names:\n    pprint(CachedMeta.endpoint(name))\n</code></pre> <p>Output</p> <pre><code>Number of Endpoints: 2\n                      Name   Health            Instance           Created  ...     Status     Variant Capture Samp(%)\n0  wine-classification-end  healthy  Serverless (2GB/5)  2024-03-23 23:09  ...  InService  AllTraffic   False       -\n1   abalone-regression-end  healthy  Serverless (2GB/5)  2024-03-23 21:11  ...  InService  AllTraffic   False       -\n\n[2 rows x 10 columns]\nwine-classification-end\n&lt;lots of details about endpoints&gt;\n</code></pre> <p>Not Finding some particular AWS Data?</p> <p>The Workbench CachedMeta API Class also has <code>(details=True)</code> arguments, so make sure to check those out.</p>"},{"location":"cached/cached_model/","title":"CachedModel","text":"<p>Model Examples</p> <p>Examples of using the Model Class are in the Examples section at the bottom of this page. AWS Model setup and deployment are quite complicated to do manually but the Workbench Model Class makes it a breeze!</p> <p>CachedModel: Caches the method results for Workbench Models</p>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel","title":"<code>CachedModel</code>","text":"<p>               Bases: <code>CachedArtifactMixin</code>, <code>ModelCore</code></p> <p>CachedModel: Caches the method results for Workbench Models</p> <p>Note: Cached method values may lag underlying Model changes.</p> Common Usage <pre><code>my_model = CachedModel(name)\nmy_model.details()\nmy_model.health_check()\nmy_model.workbench_meta()\n</code></pre> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>class CachedModel(CachedArtifactMixin, ModelCore):\n    \"\"\"CachedModel: Caches the method results for Workbench Models\n\n    Note: Cached method values may lag underlying Model changes.\n\n    Common Usage:\n        ```python\n        my_model = CachedModel(name)\n        my_model.details()\n        my_model.health_check()\n        my_model.workbench_meta()\n        ```\n    \"\"\"\n\n    def __init__(self, name: str):\n        \"\"\"CachedModel Initialization\"\"\"\n        ModelCore.__init__(self, model_name=name, use_cached_meta=True)\n\n    @CachedArtifactMixin.cache_result\n    def summary(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the CachedModel Details.\n\n        Returns:\n            dict: A dictionary of details about the CachedModel\n        \"\"\"\n        return super().summary(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def details(self, **kwargs) -&gt; dict:\n        \"\"\"Retrieve the CachedModel Details.\n\n        Returns:\n            dict: A dictionary of details about the CachedModel\n        \"\"\"\n        return super().details(**kwargs)\n\n    @CachedArtifactMixin.cache_result\n    def workbench_meta(self) -&gt; Union[str, None]:\n        \"\"\"Retrieve the Enumerated Model Type (REGRESSOR, CLASSIFER, etc).\n\n        Returns:\n            str: The Enumerated Model Type\n        \"\"\"\n        return super().workbench_meta()\n\n    @CachedArtifactMixin.cache_result\n    def get_endpoint_inference_path(self) -&gt; Union[str, None]:\n        \"\"\"Retrieve the Endpoint Inference Path.\n\n        Returns:\n            str: The Endpoint Inference Path\n        \"\"\"\n        return super().get_endpoint_inference_path()\n\n    @CachedArtifactMixin.cache_result\n    def list_inference_runs(self) -&gt; list[str]:\n        \"\"\"Retrieve the captured prediction results for this model\n\n        Returns:\n            list[str]: List of Inference Runs\n        \"\"\"\n        return super().list_inference_runs()\n\n    @CachedArtifactMixin.cache_result\n    def get_inference_metrics(self, capture_name: str = \"auto\") -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the captured prediction results for this model\n\n        Args:\n            capture_name (str, optional): Specific capture_name (default: auto)\n\n        Returns:\n            pd.DataFrame: DataFrame of the Captured Metrics (might be None)\n        \"\"\"\n        return super().get_inference_metrics(capture_name=capture_name)\n\n    @CachedArtifactMixin.cache_result\n    def get_inference_predictions(\n        self, capture_name: str = \"full_cross_fold\", target_rows: int = 5000\n    ) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the captured prediction results for this model\n\n        Args:\n            capture_name (str, optional): Specific capture_name (default: full_cross_fold)\n            target_rows (int, optional): Target number of rows to return (default: 2000)\n\n        Returns:\n            pd.DataFrame: DataFrame of the Captured Predictions (might be None)\n        \"\"\"\n        df = super().get_inference_predictions(capture_name=capture_name)\n        if df is None:\n            return None\n\n        # Compute residual based on model type\n        is_regressor = self.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]\n        is_classifier = self.model_type == ModelType.CLASSIFIER\n\n        if is_regressor:\n            target = self.target()\n            if target and \"prediction\" in df.columns and target in df.columns:\n                df[\"residual\"] = abs(df[\"prediction\"] - df[target])\n\n        elif is_classifier:\n            target = self.target()\n            class_labels = self.class_labels()\n            if target and \"prediction\" in df.columns and target in df.columns and class_labels:\n                # Create a mapping from label to ordinal index\n                label_to_idx = {label: idx for idx, label in enumerate(class_labels)}\n                # Compute residual as distance between predicted and actual class\n                df[\"residual\"] = abs(\n                    df[\"prediction\"].map(label_to_idx).fillna(-1) - df[target].map(label_to_idx).fillna(-1)\n                )\n\n        # Use smart_aggregator to aggregate similar rows if we have too many\n        if len(df) &gt; target_rows:\n            self.log.info(\n                f\"{self.name}:{capture_name} Using smart_aggregator to reduce {len(df)} rows to ~{target_rows}\"\n            )\n            df = smart_aggregator(df, target_rows=target_rows)\n\n        return df\n\n    @CachedArtifactMixin.cache_result\n    def confusion_matrix(self, capture_name: str = \"auto\") -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the confusion matrix for the model\n\n        Args:\n            capture_name (str, optional): Specific capture_name (default: auto)\n\n        Returns:\n            pd.DataFrame: DataFrame of the Confusion Matrix (might be None)\n        \"\"\"\n        return super().confusion_matrix(capture_name=capture_name)\n\n    @CachedArtifactMixin.cache_result\n    def shap_importance(self) -&gt; Optional[List[Tuple[str, float]]]:\n        \"\"\"Retrieve the SHAP Feature Importance for this model.\n\n        Returns:\n            Optional[List[Tuple[str, float]]]: List of tuples containing feature names and their importance scores\n        \"\"\"\n        return super().shap_importance()\n\n    @CachedArtifactMixin.cache_result\n    def shap_values(self) -&gt; Optional[Union[pd.DataFrame, Dict[str, pd.DataFrame]]]:\n        \"\"\"Retrieve the SHAP values (contributions) for this model.\n\n        Returns:\n            Optional[Union[pd.DataFrame, dict]]: SHAP values (DataFrame or dict of DataFrames for multiclass)\n        \"\"\"\n        return super().shap_values()\n\n    @CachedArtifactMixin.cache_result\n    def shap_feature_values(self) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Retrieve the feature values for SHAP sample rows (used for plot coloring).\n\n        Returns:\n            Optional[pd.DataFrame]: Feature values for SHAP sample rows\n        \"\"\"\n        return super().shap_feature_values()\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.__init__","title":"<code>__init__(name)</code>","text":"<p>CachedModel Initialization</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"CachedModel Initialization\"\"\"\n    ModelCore.__init__(self, model_name=name, use_cached_meta=True)\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.confusion_matrix","title":"<code>confusion_matrix(capture_name='auto')</code>","text":"<p>Retrieve the confusion matrix for the model</p> <p>Parameters:</p> Name Type Description Default <code>capture_name</code> <code>str</code> <p>Specific capture_name (default: auto)</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: DataFrame of the Confusion Matrix (might be None)</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef confusion_matrix(self, capture_name: str = \"auto\") -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the confusion matrix for the model\n\n    Args:\n        capture_name (str, optional): Specific capture_name (default: auto)\n\n    Returns:\n        pd.DataFrame: DataFrame of the Confusion Matrix (might be None)\n    \"\"\"\n    return super().confusion_matrix(capture_name=capture_name)\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.details","title":"<code>details(**kwargs)</code>","text":"<p>Retrieve the CachedModel Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the CachedModel</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef details(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the CachedModel Details.\n\n    Returns:\n        dict: A dictionary of details about the CachedModel\n    \"\"\"\n    return super().details(**kwargs)\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.get_endpoint_inference_path","title":"<code>get_endpoint_inference_path()</code>","text":"<p>Retrieve the Endpoint Inference Path.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>The Endpoint Inference Path</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef get_endpoint_inference_path(self) -&gt; Union[str, None]:\n    \"\"\"Retrieve the Endpoint Inference Path.\n\n    Returns:\n        str: The Endpoint Inference Path\n    \"\"\"\n    return super().get_endpoint_inference_path()\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.get_inference_metrics","title":"<code>get_inference_metrics(capture_name='auto')</code>","text":"<p>Retrieve the captured prediction results for this model</p> <p>Parameters:</p> Name Type Description Default <code>capture_name</code> <code>str</code> <p>Specific capture_name (default: auto)</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: DataFrame of the Captured Metrics (might be None)</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef get_inference_metrics(self, capture_name: str = \"auto\") -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the captured prediction results for this model\n\n    Args:\n        capture_name (str, optional): Specific capture_name (default: auto)\n\n    Returns:\n        pd.DataFrame: DataFrame of the Captured Metrics (might be None)\n    \"\"\"\n    return super().get_inference_metrics(capture_name=capture_name)\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.get_inference_predictions","title":"<code>get_inference_predictions(capture_name='full_cross_fold', target_rows=5000)</code>","text":"<p>Retrieve the captured prediction results for this model</p> <p>Parameters:</p> Name Type Description Default <code>capture_name</code> <code>str</code> <p>Specific capture_name (default: full_cross_fold)</p> <code>'full_cross_fold'</code> <code>target_rows</code> <code>int</code> <p>Target number of rows to return (default: 2000)</p> <code>5000</code> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: DataFrame of the Captured Predictions (might be None)</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef get_inference_predictions(\n    self, capture_name: str = \"full_cross_fold\", target_rows: int = 5000\n) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the captured prediction results for this model\n\n    Args:\n        capture_name (str, optional): Specific capture_name (default: full_cross_fold)\n        target_rows (int, optional): Target number of rows to return (default: 2000)\n\n    Returns:\n        pd.DataFrame: DataFrame of the Captured Predictions (might be None)\n    \"\"\"\n    df = super().get_inference_predictions(capture_name=capture_name)\n    if df is None:\n        return None\n\n    # Compute residual based on model type\n    is_regressor = self.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]\n    is_classifier = self.model_type == ModelType.CLASSIFIER\n\n    if is_regressor:\n        target = self.target()\n        if target and \"prediction\" in df.columns and target in df.columns:\n            df[\"residual\"] = abs(df[\"prediction\"] - df[target])\n\n    elif is_classifier:\n        target = self.target()\n        class_labels = self.class_labels()\n        if target and \"prediction\" in df.columns and target in df.columns and class_labels:\n            # Create a mapping from label to ordinal index\n            label_to_idx = {label: idx for idx, label in enumerate(class_labels)}\n            # Compute residual as distance between predicted and actual class\n            df[\"residual\"] = abs(\n                df[\"prediction\"].map(label_to_idx).fillna(-1) - df[target].map(label_to_idx).fillna(-1)\n            )\n\n    # Use smart_aggregator to aggregate similar rows if we have too many\n    if len(df) &gt; target_rows:\n        self.log.info(\n            f\"{self.name}:{capture_name} Using smart_aggregator to reduce {len(df)} rows to ~{target_rows}\"\n        )\n        df = smart_aggregator(df, target_rows=target_rows)\n\n    return df\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.list_inference_runs","title":"<code>list_inference_runs()</code>","text":"<p>Retrieve the captured prediction results for this model</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of Inference Runs</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef list_inference_runs(self) -&gt; list[str]:\n    \"\"\"Retrieve the captured prediction results for this model\n\n    Returns:\n        list[str]: List of Inference Runs\n    \"\"\"\n    return super().list_inference_runs()\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.shap_feature_values","title":"<code>shap_feature_values()</code>","text":"<p>Retrieve the feature values for SHAP sample rows (used for plot coloring).</p> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>Optional[pd.DataFrame]: Feature values for SHAP sample rows</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef shap_feature_values(self) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Retrieve the feature values for SHAP sample rows (used for plot coloring).\n\n    Returns:\n        Optional[pd.DataFrame]: Feature values for SHAP sample rows\n    \"\"\"\n    return super().shap_feature_values()\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.shap_importance","title":"<code>shap_importance()</code>","text":"<p>Retrieve the SHAP Feature Importance for this model.</p> <p>Returns:</p> Type Description <code>Optional[List[Tuple[str, float]]]</code> <p>Optional[List[Tuple[str, float]]]: List of tuples containing feature names and their importance scores</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef shap_importance(self) -&gt; Optional[List[Tuple[str, float]]]:\n    \"\"\"Retrieve the SHAP Feature Importance for this model.\n\n    Returns:\n        Optional[List[Tuple[str, float]]]: List of tuples containing feature names and their importance scores\n    \"\"\"\n    return super().shap_importance()\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.shap_values","title":"<code>shap_values()</code>","text":"<p>Retrieve the SHAP values (contributions) for this model.</p> <p>Returns:</p> Type Description <code>Optional[Union[DataFrame, Dict[str, DataFrame]]]</code> <p>Optional[Union[pd.DataFrame, dict]]: SHAP values (DataFrame or dict of DataFrames for multiclass)</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef shap_values(self) -&gt; Optional[Union[pd.DataFrame, Dict[str, pd.DataFrame]]]:\n    \"\"\"Retrieve the SHAP values (contributions) for this model.\n\n    Returns:\n        Optional[Union[pd.DataFrame, dict]]: SHAP values (DataFrame or dict of DataFrames for multiclass)\n    \"\"\"\n    return super().shap_values()\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.summary","title":"<code>summary(**kwargs)</code>","text":"<p>Retrieve the CachedModel Details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about the CachedModel</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef summary(self, **kwargs) -&gt; dict:\n    \"\"\"Retrieve the CachedModel Details.\n\n    Returns:\n        dict: A dictionary of details about the CachedModel\n    \"\"\"\n    return super().summary(**kwargs)\n</code></pre>"},{"location":"cached/cached_model/#workbench.cached.cached_model.CachedModel.workbench_meta","title":"<code>workbench_meta()</code>","text":"<p>Retrieve the Enumerated Model Type (REGRESSOR, CLASSIFER, etc).</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>The Enumerated Model Type</p> Source code in <code>src/workbench/cached/cached_model.py</code> <pre><code>@CachedArtifactMixin.cache_result\ndef workbench_meta(self) -&gt; Union[str, None]:\n    \"\"\"Retrieve the Enumerated Model Type (REGRESSOR, CLASSIFER, etc).\n\n    Returns:\n        str: The Enumerated Model Type\n    \"\"\"\n    return super().workbench_meta()\n</code></pre>"},{"location":"cached/cached_model/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Pull Inference Run</p> <pre><code>from workbench.cached.cached_model import CachedModel\n\n# Grab a Model\nmodel = CachedModel(\"abalone-regression\")\n\n# List the inference runs\nmodel.list_inference_runs()\n['auto_inference', 'model_training']\n\n# Grab specific inference results\nmodel.get_inference_predictions(\"auto_inference\")\n     class_number_of_rings  prediction    id\n0                       16   10.516158     7\n1                        9    9.031365     8\n..                     ...         ...   ...\n831                      8    7.693689  4158\n832                      9    7.542521  4167\n</code></pre>"},{"location":"cached/overview/","title":"Caching Overview","text":"<p>Caching is Crazy</p> <p>Yes, but it's a necessary evil for Web Interfaces. AWS APIs (boto3, Sagemaker) often takes multiple seconds to respond and will often throttle requests if spammed. So for quicker response and less spamming we're using Cached Classes for any Web Interface work.</p>"},{"location":"cached/overview/#welcome-to-the-workbench-cached-classes","title":"Welcome to the Workbench Cached Classes","text":"<p>These classes provide caching for the for the most used Workbench classes. They transparently handle all the details around retrieving and caching results from the underlying classes.</p> <ul> <li>CachedMeta: Manages lists of Artifacts (get all models, endpoints, etc).</li> <li>CachedDataSource: Caches the method results for Workbench DataSource.</li> <li>CachedFeatureSet: Caches the method results for Workbench FeatureSets.</li> <li>CachedModel: Caches the method results for Workbench Models.</li> <li>CachedEndpoint: Caches the method results for Workbench Endpoints.</li> </ul> <p>Examples</p> <p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p>"},{"location":"chem_utils/","title":"Chemical Utilities","text":"<p>Examples</p> <p>Examples of using the Chemical Utilities are listed at the bottom of this page Examples.</p> <p>The majority of the chemical utilities in Workbench use either RDKIT or Mordred (Community). The inclusion of these utilities allows the use and deployment of this functionality into AWS (FeatureSets, Models, Endpoints). </p>"},{"location":"chem_utils/#examples","title":"Examples","text":""},{"location":"chem_utils/#canonical-smiles","title":"Canonical Smiles","text":"examples/chem_utils/canonicalize_smiles.py<pre><code>\"\"\"Example for computing Canonicalize SMILES strings\"\"\"\nimport pandas as pd\nfrom workbench.utils.chem_utils import canonicalize\n\ntest_data = [\n    {\"id\": \"Acetylacetone\", \"smiles\": \"CC(=O)CC(=O)C\", \"expected\": \"CC(=O)CC(C)=O\"},\n    {\"id\": \"Imidazole\", \"smiles\": \"c1cnc[nH]1\", \"expected\": \"c1c[nH]cn1\"},\n    {\"id\": \"Pyridone\", \"smiles\": \"C1=CC=NC(=O)C=C1\", \"expected\": \"O=c1cccccn1\"},\n    {\"id\": \"Guanidine\", \"smiles\": \"C(=N)N=C(N)N\", \"expected\": \"N=CN=C(N)N\"},\n    {\"id\": \"Catechol\", \"smiles\": \"c1cc(c(cc1)O)O\", \"expected\": \"Oc1ccccc1O\"},\n    {\"id\": \"Formamide\", \"smiles\": \"C(=O)N\", \"expected\": \"NC=O\"},\n    {\"id\": \"Urea\", \"smiles\": \"C(=O)(N)N\", \"expected\": \"NC(N)=O\"},\n    {\"id\": \"Phenol\", \"smiles\": \"c1ccc(cc1)O\", \"expected\": \"Oc1ccccc1\"},\n]\n\n# Convert test data to a DataFrame\ndf = pd.DataFrame(test_data)\n\n# Perform canonicalization\nresult_df = canonicalize(df)\nprint(result_df)\n</code></pre> <p>Output</p> <pre><code>              id            smiles       expected smiles_canonical\n0  Acetylacetone     CC(=O)CC(=O)C  CC(=O)CC(C)=O    CC(=O)CC(C)=O\n1      Imidazole        c1cnc[nH]1     c1c[nH]cn1       c1c[nH]cn1\n2       Pyridone  C1=CC=NC(=O)C=C1    O=c1cccccn1      O=c1cccccn1\n3      Guanidine      C(=N)N=C(N)N     N=CN=C(N)N       N=CN=C(N)N\n4       Catechol    c1cc(c(cc1)O)O     Oc1ccccc1O       Oc1ccccc1O\n5      Formamide            C(=O)N           NC=O             NC=O\n6           Urea         C(=O)(N)N        NC(N)=O          NC(N)=O\n7         Phenol       c1ccc(cc1)O      Oc1ccccc1        Oc1ccccc1\n</code></pre>"},{"location":"chem_utils/#tautomerize-smiles","title":"Tautomerize Smiles","text":"examples/chem_utils/tautomerize_smiles.py<pre><code>\"\"\"Example for Tautomerizing SMILES strings\"\"\"\nimport pandas as pd\nfrom workbench.utils.chem_utils import tautomerize_smiles\n\ntest_data = [\n    # Salicylaldehyde undergoes keto-enol tautomerization.\n    {\"id\": \"Salicylaldehyde (Keto)\", \"smiles\": \"O=Cc1cccc(O)c1\", \"expected\": \"O=Cc1cccc(O)c1\"},\n    {\"id\": \"2-Hydroxybenzaldehyde (Enol)\", \"smiles\": \"Oc1ccc(C=O)cc1\", \"expected\": \"O=Cc1ccc(O)cc1\"},\n    # Acetylacetone undergoes keto-enol tautomerization to favor the enol form.\n    {\"id\": \"Acetylacetone\", \"smiles\": \"CC(=O)CC(=O)C\", \"expected\": \"CC(=O)CC(C)=O\"},\n    # Imidazole undergoes a proton shift in the aromatic ring.\n    {\"id\": \"Imidazole\", \"smiles\": \"c1cnc[nH]1\", \"expected\": \"c1c[nH]cn1\"},\n    # Pyridone prefers the lactam form in RDKit's tautomer enumeration.\n    {\"id\": \"Pyridone\", \"smiles\": \"C1=CC=NC(=O)C=C1\", \"expected\": \"O=c1cccccn1\"},\n    # Guanidine undergoes amine-imine tautomerization.\n    {\"id\": \"Guanidine\", \"smiles\": \"C(=N)N=C(N)N\", \"expected\": \"N=C(N)N=CN\"},\n    # Catechol standardizes hydroxyl group placement in the aromatic system.\n    {\"id\": \"Catechol\", \"smiles\": \"c1cc(c(cc1)O)O\", \"expected\": \"Oc1ccccc1O\"},\n    # Formamide canonicalizes to NC=O, reflecting its stable form.\n    {\"id\": \"Formamide\", \"smiles\": \"C(=O)N\", \"expected\": \"NC=O\"},\n    # Urea undergoes a proton shift between nitrogen atoms.\n    {\"id\": \"Urea\", \"smiles\": \"C(=O)(N)N\", \"expected\": \"NC(N)=O\"},\n    # Phenol standardizes hydroxyl group placement in the aromatic system.\n    {\"id\": \"Phenol\", \"smiles\": \"c1ccc(cc1)O\", \"expected\": \"Oc1ccccc1\"}\n]\n\n# Convert test data to a DataFrame\ndf = pd.DataFrame(test_data)\n\n# Perform tautomerization\nresult_df = tautomerize_smiles(df)\nprint(result_df)\n</code></pre> <p>Output</p> <pre><code>                             id       smiles_orig        expected smiles_canonical          smiles\n0        Salicylaldehyde (Keto)    O=Cc1cccc(O)c1  O=Cc1cccc(O)c1   O=Cc1cccc(O)c1  O=Cc1cccc(O)c1\n1  2-Hydroxybenzaldehyde (Enol)    Oc1ccc(C=O)cc1  O=Cc1ccc(O)cc1   O=Cc1ccc(O)cc1  O=Cc1ccc(O)cc1\n2                 Acetylacetone     CC(=O)CC(=O)C   CC(=O)CC(C)=O    CC(=O)CC(C)=O   CC(=O)CC(C)=O\n3                     Imidazole        c1cnc[nH]1      c1c[nH]cn1       c1c[nH]cn1      c1c[nH]cn1\n4                      Pyridone  C1=CC=NC(=O)C=C1     O=c1cccccn1      O=c1cccccn1     O=c1cccccn1\n5                     Guanidine      C(=N)N=C(N)N      N=C(N)N=CN       N=CN=C(N)N      N=C(N)N=CN\n6                      Catechol    c1cc(c(cc1)O)O      Oc1ccccc1O       Oc1ccccc1O      Oc1ccccc1O\n7                     Formamide            C(=O)N            NC=O             NC=O            NC=O\n8                          Urea         C(=O)(N)N         NC(N)=O          NC(N)=O         NC(N)=O\n9                        Phenol       c1ccc(cc1)O       Oc1ccccc1        Oc1ccccc1       Oc1ccccc1\n</code></pre>"},{"location":"chem_utils/#additional-resources","title":"Additional Resources","text":"<ul> <li>Workbench API Classes: API Classes</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"cloudwatch/","title":"Workbench CloudWatch","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines. As part of this we're including CloudWatch log forwarding/aggregation for any service using the Workbench API (Dashboard, Glue, Lambda, Notebook, Laptop, etc).</p>"},{"location":"cloudwatch/#log-groups-and-streams","title":"Log Groups and Streams","text":"<p>The Workbench logging setup includes the addition of a CloudWatch 'Handler' that forwards all log messages to the <code>WorkbenchLogGroup</code></p> <p>Individual Streams</p> <p>Each process running Workbench will get a unique individual stream.</p> <ul> <li>ecs/DashBoard* (any logs from Workbench Dashboard)</li> <li>glue/* (logs from Glue jobs)</li> <li>lambda/* (logs from Lambda jobs)</li> <li>docker/* (logs from Docker containers)</li> <li>laptop/* (logs from laptop/notebooks)</li> </ul> <p>Since many jobs are run nightly/often, the stream will also have a date on the end... <code>glue/my_job/2024_08_01_17_15</code></p>"},{"location":"cloudwatch/#aws-cloudwatch-made-easy","title":"AWS CloudWatch made Easy","text":"<p>Logs in Easy Mode</p> <p>The Workbench <code>cloud_watch</code> command line tool gives you access to important logs without the hassle. Automatic display of important event and the context around those events.</p> <pre><code>pip install workbench\ncloud_watch\n</code></pre> <p>The <code>cloud_watch</code> script will automatically show the interesting (WARNING and CRITICAL) messages from any source within the last hour. There are lots of options to the script, just use <code>--help</code> to see options and descriptions.</p> <pre><code>cloud_watch --help\n</code></pre> <p>Here are some example options:</p> <pre><code># Show important logs in last 12 hours\ncloud_watch --start-time 720 \n\n# Show a particular stream\ncloud_watch --stream glue/my_job \n\n# Show/search for a message substring\ncloud_watch --search SHAP\n\n# Show a log levels (matching and above)\ncloud_watch --log-level WARNING\ncloud_watch --log-level ERROR\ncloud_watch --log-level CRITICAL\nOR\ncloud_watch --log-level ALL  (for all events)\n\n# Combine flags \ncloud_watch --log-level ERROR --search SHAP\ncloud_watch --log-level ERROR --stream Dashboard\n</code></pre> <p>These options can be used in combination and try out the other options to make the perfect log search :)</p> <p></p>"},{"location":"cloudwatch/#more-information","title":"More Information","text":"<p>Check out our presentation on Workbench CloudWatch</p>"},{"location":"cloudwatch/#access-through-aws-console","title":"Access through AWS Console","text":"<p>Since we're leveraging AWS functionality you can always use the AWS console to look/investigate the logs. In the AWS console go to CloudWatch... Log Groups... WorkbenchLogGroup</p> <p></p>"},{"location":"cloudwatch/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"comparisons/workbench_vs_databricks/","title":"Databricks","text":"<p>Visibility and Control</p> <p>Workbench provides AWS ML Pipeline visibility Workbench Dashboard and control over the creation, modification, and deletion of artifacts through the Python API Workbench REPL.</p>"},{"location":"comparisons/workbench_vs_databricks/#workbench-vs-databricks","title":"Workbench vs Databricks","text":"<p>Databricks is a mature and comprehensive platform offering an extensive suite of functionality for data management, collaborative notebooks, and the full machine learning lifecycle. It\u2019s widely regarded as the \u201cRolls-Royce\u201d of ML pipeline systems\u2014robust, feature-rich, and designed for scalable, enterprise-grade workloads. </p> <p>In contrast, Workbench is a more specialized tool focused on the AWS ecosystem, aimed at simplifying the creation and management of ML pipelines. It\u2019s like a scrappy go-kart\u2014lightweight, fast, and agile\u2014offering a streamlined experience without the bells and whistles of larger platforms.</p> Feature / Aspect Databricks SCP Workbench Purpose Unified cloud-based platform for big data analytics, data engineering, and machine learning. Python API and web interface framework to simplify creation and deployment of AWS SageMaker ML models and pipelines. Primary Focus Data processing at scale using Apache Spark, collaborative notebooks, ML lifecycle, and data lakes. Simplifies building, managing, and monitoring AWS ML pipelines, especially SageMaker models and AWS Glue jobs. Core Technology Built on Apache Spark, native cloud platform integrations (Azure, AWS, GCP). Built as an AWS-focused tool leveraging AWS SageMaker, Glue, Athena, Feature Store, with Python API and dashboards. Deployment Model Cloud-native SaaS platform (AWS, Azure, GCP). Private SaaS (BYOC - Bring Your Own Cloud), deployed within customer's AWS account for full control and compliance. User Interface Collaborative notebooks, dashboard, job scheduler, MLflow integration, SQL analytics UI. Web dashboard with multiple interfaces providing visibility into AWS Glue jobs, SageMaker models, endpoints, data sources, feature sets. API Supports multiple languages (Python, Scala, SQL, R). Comprehensive SDKs and MLflow APIs. A Python API that abstracts and manages AWS ML pipeline components, making AWS ML service usage easier via Python objects. Machine Learning Support End-to-end ML lifecycle management including experiment tracking, model training, deployment, monitoring. Focused on AWS SageMaker model creation, deployment, and pipeline management with monitoring via dashboard. Data Processing Optimized for big data processing using Spark and Delta Lake. Supports streaming and batch. Relies on AWS Glue and Athena for data ETL and querying; no native big data engine like Spark. Ecosystem Integration Supports integration with many data sources and cloud services. Integrates with BI and data science tools. Deeply integrated with AWS services, minimal or no direct support for other cloud platforms or third-party tools. Security &amp; Compliance Cloud provider\u2019s security features, supports enterprise compliance. Private SaaS allows keeping data and services within own AWS environment, facilitating strict control and compliance. Target User Enterprises needing an end-to-end unified analytics and ML platform with collaboration. Organizations heavily invested in AWS who want easier programmatic control and monitoring of SageMaker and AWS ML pipelines. Community and Maturity Established commercial product with large user base and extensive documentation/support. Newer project, actively developed, smaller community, focused on AWS ML pipeline usability. Use Case Example Building scalable data lakes, collaborative data science projects, streaming analytics, enterprise ML workflows. Simplifying deployment and monitoring of AWS SageMaker ML models and Glue pipelines with dashboards."},{"location":"comparisons/workbench_vs_databricks/#additional-resources","title":"Additional Resources","text":"<ul> <li>Using Workbench for ML Pipelines: Workbench API Classes</li> </ul> <ul> <li>Workbench Core Classes: Core Classes</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"compound_explorer/tox21/","title":"Tox21 Dataset Overview","text":"<p>The Tox21 dataset is a public resource developed through the \"Toxicology in the 21st Century\" initiative. It provides qualitative toxicity measurements for a variety of compounds across multiple biological targets, including nuclear receptors and stress response pathways. This dataset is widely used for developing and benchmarking computational models in toxicology.</p>"},{"location":"compound_explorer/tox21/#dataset-access","title":"Dataset Access","text":"<ul> <li>Download Links:</li> <li>Tox21 Data Browser: Access to chemical structures, annotations, and quality control information. oai_citation_attribution:7\u2021Tox21</li> <li> <p>Tox21 Machine Learning Dataset: Contains training and test data with dense and sparse features. oai_citation_attribution:6\u2021Bioinformatics JKU</p> </li> <li> <p>Additional Resources:</p> </li> <li>EPA CompTox Chemicals Dashboard: Provides chemistry, toxicity, and exposure information for a wide range of chemicals. oai_citation_attribution:5\u2021Tox21</li> <li>PubChem: Offers access to large-scale screening data, including Tox21 quantitative high-throughput screening (qHTS) data. oai_citation_attribution:4\u2021Tox21</li> </ul>"},{"location":"compound_explorer/tox21/#dataset-columns","title":"Dataset Columns","text":"<p>The Tox21 dataset includes the following columns:</p> Column Name Description <code>Formula</code> Chemical formula of the compound. <code>FW</code> Molecular weight (Formula Weight) of the compound. <code>DSSTox_CID</code> Unique identifier from the Distributed Structure-Searchable Toxicity (DSSTox) database. <code>SR-HSE</code> Stress response assay for heat shock element. <code>ID</code> Internal identifier for the compound. <code>smiles</code> Simplified Molecular Input Line Entry System representation of the compound's structure. <code>molecule</code> Molecular structure information (format may vary). <code>NR-AR</code> Nuclear receptor assay for androgen receptor. <code>SR-ARE</code> Stress response assay for antioxidant response element. <code>NR-Aromatase</code> Nuclear receptor assay for aromatase enzyme. <code>NR-ER-LBD</code> Nuclear receptor assay for estrogen receptor ligand-binding domain. <code>NR-AhR</code> Nuclear receptor assay for aryl hydrocarbon receptor. <code>SR-MMP</code> Stress response assay for mitochondrial membrane potential. <code>NR-ER</code> Nuclear receptor assay for estrogen receptor. <code>NR-PPAR-gamma</code> Nuclear receptor assay for peroxisome proliferator-activated receptor gamma. <code>SR-p53</code> Stress response assay for p53 tumor suppressor protein. <code>SR-ATAD5</code> Stress response assay for ATAD5 (ATPase family AAA domain-containing protein 5). <code>NR-AR-LBD</code> Nuclear receptor assay for androgen receptor ligand-binding domain. <p>Note: Descriptions for some columns are based on standard assay targets; specific details may vary.</p>"},{"location":"compound_explorer/tox21/#references","title":"References","text":"<ul> <li>Tox21 Data and Tools oai_citation_attribution:3\u2021Tox21</li> <li>Tox21 Machine Learning Data Set oai_citation_attribution:2\u2021Bioinformatics JKU</li> <li>EPA CompTox Chemicals Dashboard oai_citation_attribution:1\u2021Tox21</li> <li>PubChem oai_citation_attribution:0\u2021Tox21</li> </ul>"},{"location":"compound_explorer/toxicity_modeling/","title":"Toxicity Modeling Approach","text":""},{"location":"compound_explorer/toxicity_modeling/#overview","title":"Overview","text":"<p>The composite modeling approach integrates multiple methodologies to assess compound toxicity. It combines heuristic methods, a global model, and individual nuclear receptor (NR) models, providing a comprehensive and flexible framework for toxicity prediction and confidence scoring.</p>"},{"location":"compound_explorer/toxicity_modeling/#components","title":"Components","text":""},{"location":"compound_explorer/toxicity_modeling/#1-heuristic","title":"1. Heuristic","text":"<ul> <li>Description: Uses known toxicity data for compounds.</li> <li>Purpose: Bootstrap UI/tags and acts as a known baseline leveraging knowledge from existing toxicity databases.</li> </ul>"},{"location":"compound_explorer/toxicity_modeling/#2-global-model","title":"2. Global Model","text":"<ul> <li>Description: A unified model encompassing all nuclear receptor reaction sites.</li> <li>Purpose: Offers a broad toxicity prediction across diverse receptor interactions.</li> </ul>"},{"location":"compound_explorer/toxicity_modeling/#3-individual-nr-models","title":"3. Individual NR Models","text":"<ul> <li>Description: A set of <code>N</code> specialized models, each targeting a specific nuclear receptor (NR).</li> <li>Purpose: Delivers fine-grained predictions for receptor-specific toxicity.</li> <li>Strengths:</li> <li>Pinpoints receptor-specific interactions.</li> <li>Enhances interpretability of toxicity mechanisms.</li> </ul>"},{"location":"compound_explorer/toxicity_modeling/#confidence-metric","title":"Confidence Metric","text":"<ul> <li>Approach:</li> <li>Combine outputs from heuristic, global, and NR models.</li> <li>Assess consistency and agreement across models.</li> <li>Purpose: Quantifies the reliability of toxicity predictions.</li> <li>Advantages:</li> <li>Increases trust in predictions.</li> <li>Highlights regions of feature space with conflicting or sparse data.</li> </ul>"},{"location":"compound_explorer/toxicity_modeling/#applications","title":"Applications","text":"<ul> <li>Toxicity screening for drug discovery and chemical safety assessments.</li> <li>Identification of high-risk compounds and prioritization for experimental validation.</li> <li>Confidence-based decision-making for regulatory and research contexts.</li> </ul>"},{"location":"compound_explorer/toxicity_modeling/#future-directions","title":"Future Directions","text":"<ul> <li>Integrate ensemble techniques to further refine model combinations.</li> <li>Enhance confidence metrics using additional layers of evidence (e.g., structural similarity, external validation).</li> <li>Expand NR model coverage for broader receptor families.</li> </ul>"},{"location":"confidence/","title":"Confidence Scores in Workbench","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord</p> <p>Workbench provides confidence scores for every model prediction, giving users a measure of how much to trust each prediction. Higher confidence means the ensemble models agree closely; lower confidence means they disagree.</p>"},{"location":"confidence/#overview","title":"Overview","text":"<p>Every Workbench model \u2014 XGBoost, PyTorch, or ChemProp \u2014 is a 5-model ensemble trained via cross-validation. The same uncertainty quantification pipeline runs for all three frameworks:</p> Framework Ensemble Std Source Calibration XGBoost5-fold CV, XGBRegressor per fold<code>np.std</code> across 5 predictionsConformal scaling PyTorch5-fold CV, TabularMLP per fold<code>np.std</code> across 5 predictionsConformal scaling ChemProp5-fold CV, MPNN per fold<code>np.std</code> across 5 predictionsConformal scaling"},{"location":"confidence/#three-step-pipeline","title":"Three-Step Pipeline","text":""},{"location":"confidence/#1-ensemble-disagreement","title":"1. Ensemble Disagreement","text":"<p>Each fold of the 5-fold cross-validation produces a model trained on a different slice of the data. At inference time, all 5 models make a prediction and we take the average. The standard deviation across the 5 predictions (<code>prediction_std</code>) is the raw uncertainty signal.</p> <p>When the models agree closely (low std), the prediction is more reliable. When they disagree (high std), something about that compound is tricky.</p>"},{"location":"confidence/#2-conformal-calibration","title":"2. Conformal Calibration","text":"<p>Raw ensemble std tells you which predictions to trust more, but the numbers aren't calibrated \u2014 a std of 0.3 doesn't map to a meaningful interval. Workbench uses conformal prediction to fix this:</p> <ol> <li>Compute nonconformity scores on held-out validation data: <code>score = |actual - predicted| / std</code></li> <li>For each confidence level (50%, 68%, 80%, 90%, 95%), find the quantile of scores that achieves the target coverage</li> <li>Build intervals: <code>prediction \u00b1 scale_factor \u00d7 std</code></li> </ol> <p>The scaling factors are computed once during training and stored as metadata. At inference, building intervals is a simple multiply.</p> <p>The result: prediction intervals that vary per-compound (based on ensemble disagreement) but are calibrated to achieve correct coverage. An 80% interval really does contain ~80% of true values.</p>"},{"location":"confidence/#3-percentile-rank-confidence","title":"3. Percentile-Rank Confidence","text":"<p>Confidence is the percentile rank of each prediction's <code>prediction_std</code> within the training set's std distribution:</p> <pre><code>confidence = 1 - percentile_rank(prediction_std)\n</code></pre> <ul> <li>Confidence 0.7 means this prediction's ensemble disagreement is lower than 70% of the training set \u2014 a relatively tight prediction.</li> <li>Confidence 0.1 means 90% of training predictions had lower uncertainty \u2014 this compound is an outlier.</li> </ul> <p>This approach gives scores that spread across the full 0\u20131 range, are directly interpretable, and require no arbitrary parameters.</p>"},{"location":"confidence/#interpreting-confidence-scores","title":"Interpreting Confidence Scores","text":""},{"location":"confidence/#high-confidence-07","title":"High Confidence (&gt; 0.7)","text":"<ul> <li>Ensemble models agree closely on the prediction</li> <li>Prediction intervals are narrower than most training predictions</li> <li>Good candidates for prioritization</li> </ul>"},{"location":"confidence/#medium-confidence-03-07","title":"Medium Confidence (0.3 \u2013 0.7)","text":"<ul> <li>Typical level of ensemble disagreement</li> <li>Predictions are likely reasonable but verify important decisions</li> </ul>"},{"location":"confidence/#low-confidence-03","title":"Low Confidence (&lt; 0.3)","text":"<ul> <li>Ensemble models disagree significantly</li> <li>Prediction intervals are wider than most training predictions</li> <li>May indicate out-of-distribution compounds or regions where the model is uncertain</li> </ul>"},{"location":"confidence/#what-confidence-doesnt-tell-you","title":"What Confidence Doesn't Tell You","text":"<p>Confidence reflects how much the ensemble models agree \u2014 but agreement doesn't guarantee correctness:</p> <ul> <li>High confidence \u2260 correct prediction. It means the models agree, not that they're right.</li> <li>Novel chemistry may get falsely high confidence if it happens to fall in a region where models extrapolate consistently.</li> <li>Confidence is relative to the training set. A confidence of 0.9 from a kinase solubility model doesn't transfer to a PROTAC dataset.</li> </ul> <p>For truly out-of-distribution detection, consider pairing confidence with applicability domain analysis.</p>"},{"location":"confidence/#metrics-for-evaluating-confidence","title":"Metrics for Evaluating Confidence","text":"<p>Workbench computes several metrics to evaluate how well confidence correlates with actual prediction quality:</p>"},{"location":"confidence/#confidence_to_error_corr","title":"confidence_to_error_corr","text":"<p>Spearman correlation between confidence and absolute error. Should be negative (high confidence = low error). Target: &lt; -0.5</p>"},{"location":"confidence/#interval_to_error_corr","title":"interval_to_error_corr","text":"<p>Spearman correlation between interval width and absolute error. Should be positive (wide intervals = high error). Target: &gt; 0.5</p>"},{"location":"confidence/#coverage-metrics","title":"Coverage Metrics","text":"<p>For each confidence level (50%, 68%, 80%, 90%, 95%), the percentage of true values that fall within the prediction interval. Should match the target coverage.</p>"},{"location":"confidence/#deep-dive","title":"Deep Dive","text":"<p>For more details on the approach, including code walkthrough and validation results, see the Model Confidence Blog.</p>"},{"location":"confidence/#additional-resources","title":"Additional Resources","text":"<p>Need help with confidence scores or uncertainty quantification? Want to develop a customized application tailored to your business needs?</p> <ul> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"core_classes/overview/","title":"Core Classes","text":"<p>Workbench Core Classes</p> <p>These classes interact with many of the Cloud Platform services and are therefore more complex. They provide additional control and refinement over the AWS ML Pipline. For most use cases the API Classes should be used</p> <p>Welcome to the Workbench Core Classes</p> <p>The Core Classes provide low-level APIs for the Workbench package, these classes directly interface with the AWS Sagemaker Pipeline interfaces and have a large number of methods with reasonable complexity.</p> <p>The API Classes have method pass-through so just call the method on the API Class and voil\u00e0 it works the same.</p> <p></p>"},{"location":"core_classes/overview/#artifacts","title":"Artifacts","text":"<ul> <li>AthenaSource: Manages AWS Data Catalog and Athena</li> <li>FeatureSetCore: Manages AWS Feature Store and Feature Groups</li> <li>ModelCore: Manages the training and deployment of AWS Model Groups and Packages</li> <li>EndpointCore: Manages the deployment and invocations/inference on AWS Endpoints</li> </ul>"},{"location":"core_classes/overview/#transforms","title":"Transforms","text":"<p>Transforms are a set of classes that transform one type of <code>Artifact</code> to another type. For instance <code>DataToFeatureSet</code> takes a <code>DataSource</code> artifact and creates a <code>FeatureSet</code> artifact.</p> <ul> <li>DataLoaders Light: Loads various light/smaller data into AWS Data Catalog and Athena</li> <li>DataLoaders Heavy: Loads heavy/larger data (via Glue) into AWS Data Catalog and Athena</li> <li>DataToFeatures: Transforms a DataSource into a FeatureSet (AWS Feature Store/Group)</li> <li>FeaturesToModel: Trains and deploys an AWS Model Package/Group from a FeatureSet</li> <li>ModelToEndpoint: Manages the provisioning and deployment of a Model Endpoint</li> <li>PandasTransforms:Pandas DataFrame transforms and helper methods.</li> </ul>"},{"location":"core_classes/artifacts/artifact/","title":"Artifact","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the any class that inherits from the Artifact Class and voil\u00e0 it works the same.</p> <p>The Workbench Artifact class is a base/abstract class that defines API implemented by all the child classes (DataSource, FeatureSet, Model, Endpoint).</p> <p>Artifact: Abstract Base Class for all Artifact classes in Workbench. Artifacts simply reflect and aggregate one or more AWS Services</p>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact","title":"<code>Artifact</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Artifact: Abstract Base Class for all Artifact classes in Workbench</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>class Artifact(ABC):\n    \"\"\"Artifact: Abstract Base Class for all Artifact classes in Workbench\"\"\"\n\n    # Class-level shared resources\n    log = logging.getLogger(\"workbench\")\n\n    # Config Manager\n    cm = ConfigManager()\n    if not cm.config_okay():\n        log = logging.getLogger(\"workbench\")\n        log.critical(\"Workbench Configuration Incomplete...\")\n        log.critical(\"Run the 'workbench' command and follow the prompts...\")\n        raise FatalConfigError()\n\n    # AWS Account Clamp\n    aws_account_clamp = AWSAccountClamp()\n    boto3_session = aws_account_clamp.boto3_session\n    sm_session = aws_account_clamp.sagemaker_session()\n    sm_client = aws_account_clamp.sagemaker_client()\n    aws_region = aws_account_clamp.region\n\n    # Setup Bucket Paths\n    workbench_bucket = cm.get_config(\"WORKBENCH_BUCKET\")\n    data_sources_s3_path = f\"s3://{workbench_bucket}/data-sources\"\n    feature_sets_s3_path = f\"s3://{workbench_bucket}/feature-sets\"\n    models_s3_path = f\"s3://{workbench_bucket}/models\"\n    endpoints_s3_path = f\"s3://{workbench_bucket}/endpoints\"\n\n    # Delimiter for storing lists in AWS Tags\n    tag_delimiter = \"::\"\n\n    # Grab our Dataframe Cache Storage\n    df_cache = DFStoreCore(path_prefix=\"/workbench/dataframe_cache\")\n\n    # Artifact may want to use the Parameter Store or Dataframe Store\n    param_store = ParameterStoreCore()\n    df_store = DFStoreCore()\n\n    def __init__(self, name: str, use_cached_meta: bool = False):\n        \"\"\"Initialize the Artifact Base Class\n\n        Args:\n            name (str): The Name of this artifact\n            use_cached_meta (bool): Should we use cached metadata? (default: False)\n        \"\"\"\n        self.name = name\n        if use_cached_meta:\n            self.log.info(f\"Using Cached Metadata for {self.name}\")\n            self.meta = CachedMeta()\n        else:\n            self.meta = CloudMeta()\n\n    def __post_init__(self):\n        \"\"\"Artifact Post Initialization\"\"\"\n\n        # Do I exist? (very metaphysical)\n        if not self.exists():\n            self.log.debug(f\"Artifact {self.name} does not exist\")\n            return\n\n        # Conduct a Health Check on this Artifact\n        health_issues = self.health_check()\n        if health_issues:\n            if \"needs_onboard\" in health_issues:\n                self.log.important(f\"Artifact {self.name} needs to be onboarded\")\n            elif health_issues == [\"no_activity\"]:\n                self.log.debug(f\"Artifact {self.name} has no activity, which is fine\")\n            else:\n                self.log.warning(f\"Health Check Failed {self.name}: {health_issues}\")\n            for issue in health_issues:\n                self.add_health_tag(issue)\n        else:\n            self.log.info(f\"Health Check Passed {self.name}\")\n\n    @classmethod\n    def is_name_valid(cls, name: str, delimiter: str = \"_\", lower_case: bool = True) -&gt; bool:\n        \"\"\"Check if the name adheres to the naming conventions for this Artifact.\n\n        Args:\n            name (str): The name/id to check.\n            delimiter (str): The delimiter to use in the name/id string (default: \"_\")\n            lower_case (bool): Should the name be lowercased? (default: True)\n\n        Returns:\n            bool: True if the name is valid, False otherwise.\n        \"\"\"\n        valid_name = cls.generate_valid_name(name, delimiter=delimiter, lower_case=lower_case)\n        if name != valid_name:\n            cls.log.warning(f\"Artifact name: '{name}' is not valid. Convert it to something like: '{valid_name}'\")\n            return False\n        return True\n\n    @staticmethod\n    def generate_valid_name(name: str, delimiter: str = \"_\", lower_case: bool = True) -&gt; str:\n        \"\"\"Only allow letters and the specified delimiter, also lowercase the string.\n\n        Args:\n            name (str): The name/id string to check.\n            delimiter (str): The delimiter to use in the name/id string (default: \"_\")\n            lower_case (bool): Should the name be lowercased? (default: True)\n\n        Returns:\n            str: A generated valid name/id.\n        \"\"\"\n        valid_name = \"\".join(c for c in name if c.isalnum() or c in [\"_\", \"-\"])\n        if lower_case:\n            valid_name = valid_name.lower()\n\n        # Replace with the chosen delimiter\n        return valid_name.replace(\"_\", delimiter).replace(\"-\", delimiter)\n\n    @abstractmethod\n    def exists(self) -&gt; bool:\n        \"\"\"Does the Artifact exist? Can we connect to it?\"\"\"\n        pass\n\n    def workbench_meta(self) -&gt; Union[dict, None]:\n        \"\"\"Get the Workbench specific metadata for this Artifact\n\n        Returns:\n            Union[dict, None]: Dictionary of Workbench metadata for this Artifact\n\n        Note: This functionality will work for FeatureSets, Models, and Endpoints\n              but not for DataSources and Graphs, those classes need to override this method.\n        \"\"\"\n        return self.meta.get_aws_tags(self.arn())\n\n    def expected_meta(self) -&gt; list[str]:\n        \"\"\"Metadata we expect to see for this Artifact when it's ready\n        Returns:\n            list[str]: List of expected metadata keys\n        \"\"\"\n\n        # If an artifact has additional expected metadata override this method\n        return [\"workbench_status\"]\n\n    @abstractmethod\n    def refresh_meta(self):\n        \"\"\"Refresh the Artifact's metadata\"\"\"\n        pass\n\n    def ready(self) -&gt; bool:\n        \"\"\"Is the Artifact ready? Is initial setup complete and expected metadata populated?\"\"\"\n\n        # If anything goes wrong, assume the artifact is not ready\n        try:\n            # Check for the expected metadata\n            expected_meta = self.expected_meta()\n            existing_meta = self.workbench_meta()\n            ready = set(existing_meta.keys()).issuperset(expected_meta)\n            if ready:\n                return True\n            else:\n                self.log.info(\"Artifact is not ready!\")\n                return False\n        except Exception as e:\n            self.log.error(f\"Artifact malformed: {e}\")\n            return False\n\n    @abstractmethod\n    def onboard(self) -&gt; bool:\n        \"\"\"Onboard this Artifact into Workbench\n        Returns:\n            bool: True if the Artifact was successfully onboarded, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def details(self) -&gt; dict:\n        \"\"\"Additional Details about this Artifact\"\"\"\n        pass\n\n    @abstractmethod\n    def size(self) -&gt; float:\n        \"\"\"Return the size of this artifact in MegaBytes\"\"\"\n        pass\n\n    @abstractmethod\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        pass\n\n    @abstractmethod\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        pass\n\n    @abstractmethod\n    def hash(self) -&gt; str:\n        \"\"\"Return the hash of this artifact, useful for content validation\"\"\"\n        pass\n\n    @abstractmethod\n    def arn(self):\n        \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n        pass\n\n    @abstractmethod\n    def aws_url(self):\n        \"\"\"AWS console/web interface for this artifact\"\"\"\n        pass\n\n    @abstractmethod\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get the full AWS metadata for this artifact\"\"\"\n        pass\n\n    @abstractmethod\n    def delete(self):\n        \"\"\"Delete this artifact including all related AWS objects\"\"\"\n        pass\n\n    def upsert_workbench_meta(self, new_meta: dict):\n        \"\"\"Add Workbench specific metadata to this Artifact\n        Args:\n            new_meta (dict): Dictionary of NEW metadata to add\n        Note:\n            This functionality will work for FeatureSets, Models, and Endpoints\n            but not for DataSources. The DataSource class overrides this method.\n        \"\"\"\n\n        # Check for ReadOnly Role\n        if self.aws_account_clamp.read_only:\n            self.log.info(\"Cannot add metadata with a ReadOnly Permissions...\")\n            return\n\n        # Sanity check\n        aws_arn = self.arn()\n        if aws_arn is None:\n            self.log.error(f\"ARN is None for {self.name}!\")\n            return\n\n        # Add the new metadata to the existing metadata\n        self.log.info(f\"Adding Tags to {self.name}:{str(new_meta)[:50]}...\")\n        aws_tags = dict_to_aws_tags(new_meta)\n        try:\n            self.sm_client.add_tags(ResourceArn=aws_arn, Tags=aws_tags)\n        except Exception as e:\n            self.log.error(f\"Error adding metadata to {aws_arn}: {e}\")\n\n    def get_tags(self, tag_type=\"user\") -&gt; list:\n        \"\"\"Get the tags for this artifact\n        Args:\n            tag_type (str): Type of tags to return (user or health)\n        Returns:\n            list[str]: List of tags for this artifact\n        \"\"\"\n        if tag_type == \"user\":\n            user_tags = self.workbench_meta().get(\"workbench_tags\")\n            return user_tags.split(self.tag_delimiter) if user_tags else []\n\n        # Grab our health tags\n        health_tags = self.workbench_meta().get(\"workbench_health_tags\")\n\n        # If we don't have health tags, create the storage and return an empty list\n        if health_tags is None:\n            self.log.important(f\"{self.name} creating workbench_health_tags storage...\")\n            self.upsert_workbench_meta({\"workbench_health_tags\": \"\"})\n            return []\n\n        # Otherwise, return the health tags\n        return health_tags.split(self.tag_delimiter) if health_tags else []\n\n    def set_tags(self, tags):\n        self.upsert_workbench_meta({\"workbench_tags\": self.tag_delimiter.join(tags)})\n\n    def add_tag(self, tag, tag_type=\"user\"):\n        \"\"\"Add a tag for this artifact, ensuring no duplicates and maintaining order.\n        Args:\n            tag (str): Tag to add for this artifact\n            tag_type (str): Type of tag to add (user or health)\n        \"\"\"\n        current_tags = self.get_tags(tag_type) if tag_type == \"user\" else self.get_health_tags()\n        if tag not in current_tags:\n            current_tags.append(tag)\n            combined_tags = self.tag_delimiter.join(current_tags)\n            if tag_type == \"user\":\n                self.upsert_workbench_meta({\"workbench_tags\": combined_tags})\n            else:\n                self.upsert_workbench_meta({\"workbench_health_tags\": combined_tags})\n\n    def remove_workbench_tag(self, tag, tag_type=\"user\"):\n        \"\"\"Remove a tag from this artifact if it exists.\n        Args:\n            tag (str): Tag to remove from this artifact\n            tag_type (str): Type of tag to remove (user or health)\n        \"\"\"\n        current_tags = self.get_tags(tag_type) if tag_type == \"user\" else self.get_health_tags()\n        if tag in current_tags:\n            current_tags.remove(tag)\n            combined_tags = self.tag_delimiter.join(current_tags)\n            if tag_type == \"user\":\n                self.upsert_workbench_meta({\"workbench_tags\": combined_tags})\n            elif tag_type == \"health\":\n                self.upsert_workbench_meta({\"workbench_health_tags\": combined_tags})\n\n    # Syntactic sugar for health tags\n    def get_health_tags(self):\n        return self.get_tags(tag_type=\"health\")\n\n    def set_health_tags(self, tags):\n        self.upsert_workbench_meta({\"workbench_health_tags\": self.tag_delimiter.join(tags)})\n\n    def add_health_tag(self, tag):\n        self.add_tag(tag, tag_type=\"health\")\n\n    def remove_health_tag(self, tag):\n        self.remove_workbench_tag(tag, tag_type=\"health\")\n\n    # Owner of this artifact\n    def get_owner(self) -&gt; str:\n        \"\"\"Get the owner of this artifact\"\"\"\n        return self.workbench_meta().get(\"workbench_owner\", \"unknown\")\n\n    def set_owner(self, owner: str):\n        \"\"\"Set the owner of this artifact\n\n        Args:\n            owner (str): Owner to set for this artifact\n        \"\"\"\n        self.upsert_workbench_meta({\"workbench_owner\": owner})\n\n    def get_input(self) -&gt; str:\n        \"\"\"Get the input data for this artifact\"\"\"\n        return self.workbench_meta().get(\"workbench_input\", \"unknown\")\n\n    def set_input(self, input_data: str):\n        \"\"\"Set the input data for this artifact\n\n        Args:\n            input_data (str): Name of input data for this artifact\n        Note:\n            This breaks the official provenance of the artifact, so use with caution.\n        \"\"\"\n        self.log.important(f\"{self.name}: Setting input to {input_data}...\")\n        self.log.important(\"Be careful with this! It breaks automatic provenance of the artifact!\")\n        self.upsert_workbench_meta({\"workbench_input\": input_data})\n\n    def get_status(self) -&gt; str:\n        \"\"\"Get the status for this artifact\"\"\"\n        return self.workbench_meta().get(\"workbench_status\", \"unknown\")\n\n    def set_status(self, status: str):\n        \"\"\"Set the status for this artifact\n        Args:\n            status (str): Status to set for this artifact\n        \"\"\"\n        self.upsert_workbench_meta({\"workbench_status\": status})\n\n    def health_check(self, deep: bool = False) -&gt; list[str]:\n        \"\"\"Perform a health check on this artifact\n\n        Args:\n            deep (bool): If True, perform more extensive (expensive) health checks (default: False)\n\n        Returns:\n            list[str]: List of health issues\n        \"\"\"\n        health_issues = []\n        if not self.ready():\n            return [\"needs_onboard\"]\n        # FIXME: Revisit AWS URL check\n        # if \"unknown\" in self.aws_url():\n        #    health_issues.append(\"aws_url_unknown\")\n        return health_issues\n\n    def summary(self) -&gt; dict:\n        \"\"\"This is generic summary information for all Artifacts. If you\n        want to get more detailed information, call the details() method\n        which is implemented by the specific Artifact class\"\"\"\n        basic = {\n            \"name\": self.name,\n            \"health_tags\": self.get_health_tags(),\n            \"aws_arn\": self.arn(),\n            \"size\": self.size(),\n            \"created\": self.created(),\n            \"modified\": self.modified(),\n            \"input\": self.get_input(),\n        }\n        # Combine the workbench metadata with the basic metadata\n        return {**basic, **self.workbench_meta()}\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of this artifact\n\n        Returns:\n            str: String representation of this artifact\n        \"\"\"\n\n        # If the artifact does not exist, return a message\n        if not self.exists():\n            return f\"{self.__class__.__name__}: {self.name} does not exist\"\n\n        summary_dict = self.summary()\n        display_keys = [\n            \"aws_arn\",\n            \"health_tags\",\n            \"size\",\n            \"created\",\n            \"modified\",\n            \"input\",\n            \"workbench_status\",\n            \"workbench_tags\",\n        ]\n        summary_items = [f\"  {repr(key)}: {repr(value)}\" for key, value in summary_dict.items() if key in display_keys]\n        summary_str = f\"{self.__class__.__name__}: {self.name}\\n\" + \",\\n\".join(summary_items)\n        return summary_str\n\n    def delete_metadata(self, key_to_delete: str):\n        \"\"\"Delete specific metadata from this artifact\n        Args:\n            key_to_delete (str): Metadata key to delete\n        \"\"\"\n\n        aws_arn = self.arn()\n        self.log.important(f\"Deleting Metadata {key_to_delete} for Artifact: {aws_arn}...\")\n\n        # First, fetch all the existing tags\n        existing_tags = self.sm_session.list_tags(aws_arn)\n\n        # Convert existing AWS tags to a dictionary for easy manipulation\n        existing_tags_dict = {item[\"Key\"]: item[\"Value\"] for item in existing_tags}\n\n        # Identify tags to delete\n        tag_list_to_delete = []\n        for key in existing_tags_dict.keys():\n            if key == key_to_delete or key.startswith(f\"{key_to_delete}_chunk_\"):\n                tag_list_to_delete.append(key)\n\n        # Delete the identified tags\n        if tag_list_to_delete:\n            self.sm_client.delete_tags(ResourceArn=aws_arn, TagKeys=tag_list_to_delete)\n        else:\n            self.log.info(f\"No Metadata found: {key_to_delete}...\")\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.__init__","title":"<code>__init__(name, use_cached_meta=False)</code>","text":"<p>Initialize the Artifact Base Class</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The Name of this artifact</p> required <code>use_cached_meta</code> <code>bool</code> <p>Should we use cached metadata? (default: False)</p> <code>False</code> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def __init__(self, name: str, use_cached_meta: bool = False):\n    \"\"\"Initialize the Artifact Base Class\n\n    Args:\n        name (str): The Name of this artifact\n        use_cached_meta (bool): Should we use cached metadata? (default: False)\n    \"\"\"\n    self.name = name\n    if use_cached_meta:\n        self.log.info(f\"Using Cached Metadata for {self.name}\")\n        self.meta = CachedMeta()\n    else:\n        self.meta = CloudMeta()\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Artifact Post Initialization</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Artifact Post Initialization\"\"\"\n\n    # Do I exist? (very metaphysical)\n    if not self.exists():\n        self.log.debug(f\"Artifact {self.name} does not exist\")\n        return\n\n    # Conduct a Health Check on this Artifact\n    health_issues = self.health_check()\n    if health_issues:\n        if \"needs_onboard\" in health_issues:\n            self.log.important(f\"Artifact {self.name} needs to be onboarded\")\n        elif health_issues == [\"no_activity\"]:\n            self.log.debug(f\"Artifact {self.name} has no activity, which is fine\")\n        else:\n            self.log.warning(f\"Health Check Failed {self.name}: {health_issues}\")\n        for issue in health_issues:\n            self.add_health_tag(issue)\n    else:\n        self.log.info(f\"Health Check Passed {self.name}\")\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of this artifact</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of this artifact\n\n    Returns:\n        str: String representation of this artifact\n    \"\"\"\n\n    # If the artifact does not exist, return a message\n    if not self.exists():\n        return f\"{self.__class__.__name__}: {self.name} does not exist\"\n\n    summary_dict = self.summary()\n    display_keys = [\n        \"aws_arn\",\n        \"health_tags\",\n        \"size\",\n        \"created\",\n        \"modified\",\n        \"input\",\n        \"workbench_status\",\n        \"workbench_tags\",\n    ]\n    summary_items = [f\"  {repr(key)}: {repr(value)}\" for key, value in summary_dict.items() if key in display_keys]\n    summary_str = f\"{self.__class__.__name__}: {self.name}\\n\" + \",\\n\".join(summary_items)\n    return summary_str\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.add_tag","title":"<code>add_tag(tag, tag_type='user')</code>","text":"<p>Add a tag for this artifact, ensuring no duplicates and maintaining order. Args:     tag (str): Tag to add for this artifact     tag_type (str): Type of tag to add (user or health)</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def add_tag(self, tag, tag_type=\"user\"):\n    \"\"\"Add a tag for this artifact, ensuring no duplicates and maintaining order.\n    Args:\n        tag (str): Tag to add for this artifact\n        tag_type (str): Type of tag to add (user or health)\n    \"\"\"\n    current_tags = self.get_tags(tag_type) if tag_type == \"user\" else self.get_health_tags()\n    if tag not in current_tags:\n        current_tags.append(tag)\n        combined_tags = self.tag_delimiter.join(current_tags)\n        if tag_type == \"user\":\n            self.upsert_workbench_meta({\"workbench_tags\": combined_tags})\n        else:\n            self.upsert_workbench_meta({\"workbench_health_tags\": combined_tags})\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.arn","title":"<code>arn()</code>  <code>abstractmethod</code>","text":"<p>AWS ARN (Amazon Resource Name) for this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef arn(self):\n    \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.aws_meta","title":"<code>aws_meta()</code>  <code>abstractmethod</code>","text":"<p>Get the full AWS metadata for this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef aws_meta(self) -&gt; dict:\n    \"\"\"Get the full AWS metadata for this artifact\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.aws_url","title":"<code>aws_url()</code>  <code>abstractmethod</code>","text":"<p>AWS console/web interface for this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef aws_url(self):\n    \"\"\"AWS console/web interface for this artifact\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.created","title":"<code>created()</code>  <code>abstractmethod</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.delete","title":"<code>delete()</code>  <code>abstractmethod</code>","text":"<p>Delete this artifact including all related AWS objects</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef delete(self):\n    \"\"\"Delete this artifact including all related AWS objects\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.delete_metadata","title":"<code>delete_metadata(key_to_delete)</code>","text":"<p>Delete specific metadata from this artifact Args:     key_to_delete (str): Metadata key to delete</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def delete_metadata(self, key_to_delete: str):\n    \"\"\"Delete specific metadata from this artifact\n    Args:\n        key_to_delete (str): Metadata key to delete\n    \"\"\"\n\n    aws_arn = self.arn()\n    self.log.important(f\"Deleting Metadata {key_to_delete} for Artifact: {aws_arn}...\")\n\n    # First, fetch all the existing tags\n    existing_tags = self.sm_session.list_tags(aws_arn)\n\n    # Convert existing AWS tags to a dictionary for easy manipulation\n    existing_tags_dict = {item[\"Key\"]: item[\"Value\"] for item in existing_tags}\n\n    # Identify tags to delete\n    tag_list_to_delete = []\n    for key in existing_tags_dict.keys():\n        if key == key_to_delete or key.startswith(f\"{key_to_delete}_chunk_\"):\n            tag_list_to_delete.append(key)\n\n    # Delete the identified tags\n    if tag_list_to_delete:\n        self.sm_client.delete_tags(ResourceArn=aws_arn, TagKeys=tag_list_to_delete)\n    else:\n        self.log.info(f\"No Metadata found: {key_to_delete}...\")\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.details","title":"<code>details()</code>  <code>abstractmethod</code>","text":"<p>Additional Details about this Artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef details(self) -&gt; dict:\n    \"\"\"Additional Details about this Artifact\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.exists","title":"<code>exists()</code>  <code>abstractmethod</code>","text":"<p>Does the Artifact exist? Can we connect to it?</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef exists(self) -&gt; bool:\n    \"\"\"Does the Artifact exist? Can we connect to it?\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.expected_meta","title":"<code>expected_meta()</code>","text":"<p>Metadata we expect to see for this Artifact when it's ready Returns:     list[str]: List of expected metadata keys</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def expected_meta(self) -&gt; list[str]:\n    \"\"\"Metadata we expect to see for this Artifact when it's ready\n    Returns:\n        list[str]: List of expected metadata keys\n    \"\"\"\n\n    # If an artifact has additional expected metadata override this method\n    return [\"workbench_status\"]\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.generate_valid_name","title":"<code>generate_valid_name(name, delimiter='_', lower_case=True)</code>  <code>staticmethod</code>","text":"<p>Only allow letters and the specified delimiter, also lowercase the string.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name/id string to check.</p> required <code>delimiter</code> <code>str</code> <p>The delimiter to use in the name/id string (default: \"_\")</p> <code>'_'</code> <code>lower_case</code> <code>bool</code> <p>Should the name be lowercased? (default: True)</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A generated valid name/id.</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@staticmethod\ndef generate_valid_name(name: str, delimiter: str = \"_\", lower_case: bool = True) -&gt; str:\n    \"\"\"Only allow letters and the specified delimiter, also lowercase the string.\n\n    Args:\n        name (str): The name/id string to check.\n        delimiter (str): The delimiter to use in the name/id string (default: \"_\")\n        lower_case (bool): Should the name be lowercased? (default: True)\n\n    Returns:\n        str: A generated valid name/id.\n    \"\"\"\n    valid_name = \"\".join(c for c in name if c.isalnum() or c in [\"_\", \"-\"])\n    if lower_case:\n        valid_name = valid_name.lower()\n\n    # Replace with the chosen delimiter\n    return valid_name.replace(\"_\", delimiter).replace(\"-\", delimiter)\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.get_input","title":"<code>get_input()</code>","text":"<p>Get the input data for this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def get_input(self) -&gt; str:\n    \"\"\"Get the input data for this artifact\"\"\"\n    return self.workbench_meta().get(\"workbench_input\", \"unknown\")\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.get_owner","title":"<code>get_owner()</code>","text":"<p>Get the owner of this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def get_owner(self) -&gt; str:\n    \"\"\"Get the owner of this artifact\"\"\"\n    return self.workbench_meta().get(\"workbench_owner\", \"unknown\")\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.get_status","title":"<code>get_status()</code>","text":"<p>Get the status for this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def get_status(self) -&gt; str:\n    \"\"\"Get the status for this artifact\"\"\"\n    return self.workbench_meta().get(\"workbench_status\", \"unknown\")\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.get_tags","title":"<code>get_tags(tag_type='user')</code>","text":"<p>Get the tags for this artifact Args:     tag_type (str): Type of tags to return (user or health) Returns:     list[str]: List of tags for this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def get_tags(self, tag_type=\"user\") -&gt; list:\n    \"\"\"Get the tags for this artifact\n    Args:\n        tag_type (str): Type of tags to return (user or health)\n    Returns:\n        list[str]: List of tags for this artifact\n    \"\"\"\n    if tag_type == \"user\":\n        user_tags = self.workbench_meta().get(\"workbench_tags\")\n        return user_tags.split(self.tag_delimiter) if user_tags else []\n\n    # Grab our health tags\n    health_tags = self.workbench_meta().get(\"workbench_health_tags\")\n\n    # If we don't have health tags, create the storage and return an empty list\n    if health_tags is None:\n        self.log.important(f\"{self.name} creating workbench_health_tags storage...\")\n        self.upsert_workbench_meta({\"workbench_health_tags\": \"\"})\n        return []\n\n    # Otherwise, return the health tags\n    return health_tags.split(self.tag_delimiter) if health_tags else []\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.hash","title":"<code>hash()</code>  <code>abstractmethod</code>","text":"<p>Return the hash of this artifact, useful for content validation</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef hash(self) -&gt; str:\n    \"\"\"Return the hash of this artifact, useful for content validation\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.health_check","title":"<code>health_check(deep=False)</code>","text":"<p>Perform a health check on this artifact</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, perform more extensive (expensive) health checks (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of health issues</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def health_check(self, deep: bool = False) -&gt; list[str]:\n    \"\"\"Perform a health check on this artifact\n\n    Args:\n        deep (bool): If True, perform more extensive (expensive) health checks (default: False)\n\n    Returns:\n        list[str]: List of health issues\n    \"\"\"\n    health_issues = []\n    if not self.ready():\n        return [\"needs_onboard\"]\n    # FIXME: Revisit AWS URL check\n    # if \"unknown\" in self.aws_url():\n    #    health_issues.append(\"aws_url_unknown\")\n    return health_issues\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.is_name_valid","title":"<code>is_name_valid(name, delimiter='_', lower_case=True)</code>  <code>classmethod</code>","text":"<p>Check if the name adheres to the naming conventions for this Artifact.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name/id to check.</p> required <code>delimiter</code> <code>str</code> <p>The delimiter to use in the name/id string (default: \"_\")</p> <code>'_'</code> <code>lower_case</code> <code>bool</code> <p>Should the name be lowercased? (default: True)</p> <code>True</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the name is valid, False otherwise.</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@classmethod\ndef is_name_valid(cls, name: str, delimiter: str = \"_\", lower_case: bool = True) -&gt; bool:\n    \"\"\"Check if the name adheres to the naming conventions for this Artifact.\n\n    Args:\n        name (str): The name/id to check.\n        delimiter (str): The delimiter to use in the name/id string (default: \"_\")\n        lower_case (bool): Should the name be lowercased? (default: True)\n\n    Returns:\n        bool: True if the name is valid, False otherwise.\n    \"\"\"\n    valid_name = cls.generate_valid_name(name, delimiter=delimiter, lower_case=lower_case)\n    if name != valid_name:\n        cls.log.warning(f\"Artifact name: '{name}' is not valid. Convert it to something like: '{valid_name}'\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.modified","title":"<code>modified()</code>  <code>abstractmethod</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.onboard","title":"<code>onboard()</code>  <code>abstractmethod</code>","text":"<p>Onboard this Artifact into Workbench Returns:     bool: True if the Artifact was successfully onboarded, False otherwise</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef onboard(self) -&gt; bool:\n    \"\"\"Onboard this Artifact into Workbench\n    Returns:\n        bool: True if the Artifact was successfully onboarded, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.ready","title":"<code>ready()</code>","text":"<p>Is the Artifact ready? Is initial setup complete and expected metadata populated?</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def ready(self) -&gt; bool:\n    \"\"\"Is the Artifact ready? Is initial setup complete and expected metadata populated?\"\"\"\n\n    # If anything goes wrong, assume the artifact is not ready\n    try:\n        # Check for the expected metadata\n        expected_meta = self.expected_meta()\n        existing_meta = self.workbench_meta()\n        ready = set(existing_meta.keys()).issuperset(expected_meta)\n        if ready:\n            return True\n        else:\n            self.log.info(\"Artifact is not ready!\")\n            return False\n    except Exception as e:\n        self.log.error(f\"Artifact malformed: {e}\")\n        return False\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.refresh_meta","title":"<code>refresh_meta()</code>  <code>abstractmethod</code>","text":"<p>Refresh the Artifact's metadata</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef refresh_meta(self):\n    \"\"\"Refresh the Artifact's metadata\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.remove_workbench_tag","title":"<code>remove_workbench_tag(tag, tag_type='user')</code>","text":"<p>Remove a tag from this artifact if it exists. Args:     tag (str): Tag to remove from this artifact     tag_type (str): Type of tag to remove (user or health)</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def remove_workbench_tag(self, tag, tag_type=\"user\"):\n    \"\"\"Remove a tag from this artifact if it exists.\n    Args:\n        tag (str): Tag to remove from this artifact\n        tag_type (str): Type of tag to remove (user or health)\n    \"\"\"\n    current_tags = self.get_tags(tag_type) if tag_type == \"user\" else self.get_health_tags()\n    if tag in current_tags:\n        current_tags.remove(tag)\n        combined_tags = self.tag_delimiter.join(current_tags)\n        if tag_type == \"user\":\n            self.upsert_workbench_meta({\"workbench_tags\": combined_tags})\n        elif tag_type == \"health\":\n            self.upsert_workbench_meta({\"workbench_health_tags\": combined_tags})\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.set_input","title":"<code>set_input(input_data)</code>","text":"<p>Set the input data for this artifact</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>str</code> <p>Name of input data for this artifact</p> required <p>Note:     This breaks the official provenance of the artifact, so use with caution.</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def set_input(self, input_data: str):\n    \"\"\"Set the input data for this artifact\n\n    Args:\n        input_data (str): Name of input data for this artifact\n    Note:\n        This breaks the official provenance of the artifact, so use with caution.\n    \"\"\"\n    self.log.important(f\"{self.name}: Setting input to {input_data}...\")\n    self.log.important(\"Be careful with this! It breaks automatic provenance of the artifact!\")\n    self.upsert_workbench_meta({\"workbench_input\": input_data})\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.set_owner","title":"<code>set_owner(owner)</code>","text":"<p>Set the owner of this artifact</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str</code> <p>Owner to set for this artifact</p> required Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def set_owner(self, owner: str):\n    \"\"\"Set the owner of this artifact\n\n    Args:\n        owner (str): Owner to set for this artifact\n    \"\"\"\n    self.upsert_workbench_meta({\"workbench_owner\": owner})\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.set_status","title":"<code>set_status(status)</code>","text":"<p>Set the status for this artifact Args:     status (str): Status to set for this artifact</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def set_status(self, status: str):\n    \"\"\"Set the status for this artifact\n    Args:\n        status (str): Status to set for this artifact\n    \"\"\"\n    self.upsert_workbench_meta({\"workbench_status\": status})\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.size","title":"<code>size()</code>  <code>abstractmethod</code>","text":"<p>Return the size of this artifact in MegaBytes</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>@abstractmethod\ndef size(self) -&gt; float:\n    \"\"\"Return the size of this artifact in MegaBytes\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.summary","title":"<code>summary()</code>","text":"<p>This is generic summary information for all Artifacts. If you want to get more detailed information, call the details() method which is implemented by the specific Artifact class</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def summary(self) -&gt; dict:\n    \"\"\"This is generic summary information for all Artifacts. If you\n    want to get more detailed information, call the details() method\n    which is implemented by the specific Artifact class\"\"\"\n    basic = {\n        \"name\": self.name,\n        \"health_tags\": self.get_health_tags(),\n        \"aws_arn\": self.arn(),\n        \"size\": self.size(),\n        \"created\": self.created(),\n        \"modified\": self.modified(),\n        \"input\": self.get_input(),\n    }\n    # Combine the workbench metadata with the basic metadata\n    return {**basic, **self.workbench_meta()}\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.upsert_workbench_meta","title":"<code>upsert_workbench_meta(new_meta)</code>","text":"<p>Add Workbench specific metadata to this Artifact Args:     new_meta (dict): Dictionary of NEW metadata to add Note:     This functionality will work for FeatureSets, Models, and Endpoints     but not for DataSources. The DataSource class overrides this method.</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def upsert_workbench_meta(self, new_meta: dict):\n    \"\"\"Add Workbench specific metadata to this Artifact\n    Args:\n        new_meta (dict): Dictionary of NEW metadata to add\n    Note:\n        This functionality will work for FeatureSets, Models, and Endpoints\n        but not for DataSources. The DataSource class overrides this method.\n    \"\"\"\n\n    # Check for ReadOnly Role\n    if self.aws_account_clamp.read_only:\n        self.log.info(\"Cannot add metadata with a ReadOnly Permissions...\")\n        return\n\n    # Sanity check\n    aws_arn = self.arn()\n    if aws_arn is None:\n        self.log.error(f\"ARN is None for {self.name}!\")\n        return\n\n    # Add the new metadata to the existing metadata\n    self.log.info(f\"Adding Tags to {self.name}:{str(new_meta)[:50]}...\")\n    aws_tags = dict_to_aws_tags(new_meta)\n    try:\n        self.sm_client.add_tags(ResourceArn=aws_arn, Tags=aws_tags)\n    except Exception as e:\n        self.log.error(f\"Error adding metadata to {aws_arn}: {e}\")\n</code></pre>"},{"location":"core_classes/artifacts/artifact/#workbench.core.artifacts.artifact.Artifact.workbench_meta","title":"<code>workbench_meta()</code>","text":"<p>Get the Workbench specific metadata for this Artifact</p> <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Union[dict, None]: Dictionary of Workbench metadata for this Artifact</p> This functionality will work for FeatureSets, Models, and Endpoints <p>but not for DataSources and Graphs, those classes need to override this method.</p> Source code in <code>src/workbench/core/artifacts/artifact.py</code> <pre><code>def workbench_meta(self) -&gt; Union[dict, None]:\n    \"\"\"Get the Workbench specific metadata for this Artifact\n\n    Returns:\n        Union[dict, None]: Dictionary of Workbench metadata for this Artifact\n\n    Note: This functionality will work for FeatureSets, Models, and Endpoints\n          but not for DataSources and Graphs, those classes need to override this method.\n    \"\"\"\n    return self.meta.get_aws_tags(self.arn())\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/","title":"AthenaSource","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the DataSource API Class and voil\u00e0 it works the same.</p> <p>AthenaSource: Workbench Data Source accessible through Athena</p>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource","title":"<code>AthenaSource</code>","text":"<p>               Bases: <code>DataSourceAbstract</code></p> <p>AthenaSource: Workbench Data Source accessible through Athena</p> Common Usage <pre><code>my_data = AthenaSource(data_name, database=\"workbench\")\nmy_data.summary()\nmy_data.details()\ndf = my_data.query(f\"select * from {data_name} limit 5\")\n</code></pre> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>class AthenaSource(DataSourceAbstract):\n    \"\"\"AthenaSource: Workbench Data Source accessible through Athena\n\n    Common Usage:\n        ```python\n        my_data = AthenaSource(data_name, database=\"workbench\")\n        my_data.summary()\n        my_data.details()\n        df = my_data.query(f\"select * from {data_name} limit 5\")\n        ```\n    \"\"\"\n\n    def __init__(self, data_name, database=\"workbench\", **kwargs):\n        \"\"\"AthenaSource Initialization\n\n        Args:\n            data_name (str): Name of Athena Table\n            database (str): Athena Database Name (default: workbench)\n        \"\"\"\n        # Ensure the data_name is a valid name/id\n        self.is_name_valid(data_name)\n\n        # Call superclass init\n        super().__init__(data_name, database, **kwargs)\n\n        # Grab our metadata from the Meta class\n        self.log.info(f\"Retrieving metadata for: {self.name}...\")\n        self.data_source_meta = self.meta.data_source(data_name, database=database)\n        if self.data_source_meta is None:\n            self.log.error(f\"Unable to find {database}:{self.table} in Glue Catalogs...\")\n            return\n\n        # Call superclass post init\n        super().__post_init__()\n\n        # All done\n        self.log.debug(f\"AthenaSource Initialized: {database}.{self.table}\")\n\n    def refresh_meta(self):\n        \"\"\"Refresh our internal AWS Broker catalog metadata\"\"\"\n        self.data_source_meta = self.meta.data_source(self.name, database=self.database)\n\n    def exists(self) -&gt; bool:\n        \"\"\"Validation Checks for this Data Source\"\"\"\n\n        # Are we able to pull AWS Metadata for this table_name?\"\"\"\n        # Do we have a valid data_source_meta?\n        if getattr(self, \"data_source_meta\", None) is None:\n            self.log.debug(f\"AthenaSource {self.table} not found in Workbench Metadata...\")\n            return False\n        return True\n\n    def arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n        # Grab our Workbench Role Manager, get our AWS account id, and region for ARN creation\n        account_id = self.aws_account_clamp.account_id\n        region = self.aws_account_clamp.region\n        arn = f\"arn:aws:glue:{region}:{account_id}:table/{self.database}/{self.table}\"\n        return arn\n\n    def workbench_meta(self) -&gt; dict:\n        \"\"\"Get the Workbench specific metadata for this Artifact\"\"\"\n\n        # Sanity Check if we have invalid AWS Metadata\n        if self.data_source_meta is None:\n            if not self.exists():\n                self.log.error(f\"DataSource {self.name} doesn't appear to exist...\")\n            else:\n                self.log.critical(f\"Unable to get AWS Metadata for {self.table}\")\n                self.log.critical(\"Malformed Artifact! Delete this Artifact and recreate it!\")\n            return {}\n\n        # Get the Workbench Metadata from the 'Parameters' section of the DataSource Metadata\n        params = self.data_source_meta.get(\"Parameters\", {})\n        return {key: decode_value(value) for key, value in params.items() if \"workbench\" in key}\n\n    def upsert_workbench_meta(self, new_meta: dict):\n        \"\"\"Add Workbench specific metadata to this Artifact\n\n        Args:\n            new_meta (dict): Dictionary of new metadata to add\n        \"\"\"\n        self.log.important(f\"Upserting Workbench Metadata {self.name}:{str(new_meta)[:50]}...\")\n\n        # Give a warning message for keys that don't start with workbench_\n        for key in new_meta.keys():\n            if not key.startswith(\"workbench_\"):\n                self.log.error(\"NOT STORING META: Append 'workbench_' to key names to avoid overwriting AWS meta data\")\n\n        # Now convert any non-string values to JSON strings\n        for key, value in new_meta.items():\n            if not isinstance(value, str):\n                new_meta[key] = json.dumps(value, cls=CustomEncoder)\n\n        # Store our updated metadata\n        try:\n            wr.catalog.upsert_table_parameters(\n                parameters=new_meta,\n                database=self.database,\n                table=self.table,\n                boto3_session=self.boto3_session,\n            )\n        except botocore.exceptions.ClientError as e:\n            error_code = e.response[\"Error\"][\"Code\"]\n            if error_code == \"InvalidInputException\":\n                self.log.error(f\"Unable to upsert metadata for {self.table}\")\n                self.log.error(\"Probably because the metadata is too large\")\n                self.log.error(new_meta)\n            elif error_code == \"ConcurrentModificationException\":\n                self.log.warning(\"ConcurrentModificationException... trying again...\")\n                time.sleep(5)\n                wr.catalog.upsert_table_parameters(\n                    parameters=new_meta,\n                    database=self.database,\n                    table=self.table,\n                    boto3_session=self.boto3_session,\n                )\n            else:\n                self.log.critical(f\"Failed to upsert metadata: {e}\")\n                self.log.critical(f\"{self.name} is Malformed! Delete this Artifact and recreate it!\")\n        except Exception as e:\n            self.log.critical(f\"Failed to upsert metadata: {e}\")\n            self.log.critical(f\"{self.name} is Malformed! Delete this Artifact and recreate it!\")\n\n    def size(self) -&gt; float:\n        \"\"\"Return the size of this data in MegaBytes\"\"\"\n        size_in_bytes = sum(wr.s3.size_objects(self.s3_storage_location(), boto3_session=self.boto3_session).values())\n        size_in_mb = size_in_bytes / 1_000_000\n        return size_in_mb\n\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get the FULL AWS metadata for this artifact\"\"\"\n        return self.data_source_meta\n\n    def aws_url(self):\n        \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n        workbench_details = self.workbench_meta().get(\"workbench_details\", {})\n        return workbench_details.get(\"aws_url\", \"unknown\")\n\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        return self.data_source_meta[\"CreateTime\"]\n\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        return self.data_source_meta[\"UpdateTime\"]\n\n    def hash(self) -&gt; str:\n        \"\"\"Get the hash for the set of Parquet files used for this Artifact\"\"\"\n        s3_uri = self.s3_storage_location()\n        return compute_parquet_hash(s3_uri, self.boto3_session)\n\n    def table_hash(self) -&gt; str:\n        \"\"\"Get the table hash for this AthenaSource\"\"\"\n        s3_scratch = f\"s3://{self.workbench_bucket}/temp/athena_output\"\n        return compute_athena_table_hash(self.database, self.table, self.boto3_session, s3_scratch)\n\n    def num_rows(self) -&gt; int:\n        \"\"\"Return the number of rows for this Data Source\"\"\"\n        count_df = self.query(f'select count(*) AS workbench_count from \"{self.database}\".\"{self.table}\"')\n        return count_df[\"workbench_count\"][0] if count_df is not None else 0\n\n    def num_columns(self) -&gt; int:\n        \"\"\"Return the number of columns for this Data Source\"\"\"\n        return len(self.columns)\n\n    @property\n    def columns(self) -&gt; list[str]:\n        \"\"\"Return the column names for this Athena Table\"\"\"\n        return [item[\"Name\"] for item in self.data_source_meta[\"StorageDescriptor\"][\"Columns\"]]\n\n    @property\n    def column_types(self) -&gt; list[str]:\n        \"\"\"Return the column types of the internal AthenaSource\"\"\"\n        return [item[\"Type\"] for item in self.data_source_meta[\"StorageDescriptor\"][\"Columns\"]]\n\n    def query(self, query: str) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Query the AthenaSource\n\n        Args:\n            query (str): The query to run against the AthenaSource\n\n        Returns:\n            pd.DataFrame: The results of the query\n        \"\"\"\n\n        # Call internal class _query method\n        return self.database_query(self.database, query)\n\n    @classmethod\n    def database_query(cls, database: str, query: str) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Specify the Database and Query the Athena Service\n\n        Args:\n            database (str): The Athena Database to query\n            query (str): The query to run against the AthenaSource\n\n        Returns:\n            pd.DataFrame: The results of the query\n        \"\"\"\n        cls.log.debug(f\"Executing Query: {query}...\")\n        try:\n            df = wr.athena.read_sql_query(\n                sql=query,\n                database=database,\n                ctas_approach=False,\n                boto3_session=cls.boto3_session,\n            )\n            scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n            if scanned_bytes &gt; 0:\n                cls.log.debug(f\"Athena Query successful (scanned bytes: {scanned_bytes})\")\n            return df\n        except wr.exceptions.QueryFailed as e:\n            cls.log.critical(f\"Failed to execute query: {e}\")\n            return None\n\n    def execute_statement(self, query: str, silence_errors: bool = False):\n        \"\"\"Execute a non-returning SQL statement in Athena with retries.\n\n        Args:\n            query (str): The query to run against the AthenaSource\n            silence_errors (bool): Silence errors (default: False)\n        \"\"\"\n        attempt = 0\n        max_retries = 5\n        retry_delay = 30\n        while attempt &lt; max_retries:\n            try:\n                # Start the query execution\n                query_execution_id = wr.athena.start_query_execution(\n                    sql=query,\n                    database=self.database,\n                    boto3_session=self.boto3_session,\n                )\n                self.log.debug(f\"QueryExecutionId: {query_execution_id}\")\n\n                # Wait for the query to complete\n                wr.athena.wait_query(query_execution_id=query_execution_id, boto3_session=self.boto3_session)\n                self.log.debug(f\"Query executed successfully: {query_execution_id}\")\n                break  # If successful, exit the retry loop\n            except wr.exceptions.QueryFailed as e:\n                if \"AlreadyExistsException\" in str(e):\n                    self.log.warning(f\"Table already exists: {e} \\nIgnoring...\")\n                    break  # No need to retry for this error\n                elif \"ConcurrentModificationException\" in str(e) or \"OperationTimeoutException\" in str(e):\n                    self.log.warning(f\"Exception {e}\\nRetrying...\")\n                    attempt += 1\n                    if attempt &lt; max_retries:\n                        time.sleep(retry_delay)\n                    else:\n                        if not silence_errors:\n                            self.log.critical(f\"Failed to execute query after {max_retries} attempts: {query}\")\n                            self.log.critical(f\"Error: {e}\")\n                        raise\n                else:\n                    if not silence_errors:\n                        self.log.critical(f\"Failed to execute query: {query}\")\n                        self.log.critical(f\"Error: {e}\")\n                    raise\n\n    def s3_storage_location(self) -&gt; str:\n        \"\"\"Get the S3 Storage Location for this Data Source\"\"\"\n        return self.data_source_meta[\"StorageDescriptor\"][\"Location\"]\n\n    def athena_test_query(self):\n        \"\"\"Validate that Athena Queries are working\"\"\"\n        query = f'select count(*) as workbench_count from \"{self.table}\"'\n        df = wr.athena.read_sql_query(\n            sql=query,\n            database=self.database,\n            ctas_approach=False,\n            boto3_session=self.boto3_session,\n        )\n        scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n        self.log.info(f\"Athena TEST Query successful (scanned bytes: {scanned_bytes})\")\n\n    def descriptive_stats(self) -&gt; dict[dict]:\n        \"\"\"Compute Descriptive Stats for all the numeric columns in a DataSource\n\n        Returns:\n            dict(dict): A dictionary of descriptive stats for each column in the form\n                 {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},\n                  'col2': ...}\n        \"\"\"\n\n        # First check if we have already computed the descriptive stats\n        stat_dict = self.workbench_meta().get(\"workbench_descriptive_stats\")\n        if stat_dict:\n            self.log.info(\"Returning precomputed meta(workbench_descriptive_stats)\")\n            return stat_dict\n\n        # Call the SQL function to compute descriptive stats\n        stat_dict = sql.descriptive_stats(self)\n\n        # Push the descriptive stat data into our DataSource Metadata\n        self.upsert_workbench_meta({\"workbench_descriptive_stats\": stat_dict})\n\n        # Return the descriptive stats\n        return stat_dict\n\n    @cache_dataframe(\"sample\")\n    def sample(self, rows: int = 100) -&gt; pd.DataFrame:\n        \"\"\"Pull a sample of rows from the DataSource\n\n        Args:\n            rows (int): Number of rows to sample (default: 100)\n\n        Returns:\n            pd.DataFrame: A sample DataFrame for an Athena DataSource\n        \"\"\"\n\n        # Call the SQL function to pull a sample of the rows\n        return sql.sample_rows(self, rows=rows)\n\n    @cache_dataframe(\"outliers\")\n    def outliers(self, scale: float = 1.5, use_stddev=False) -&gt; pd.DataFrame:\n        \"\"\"Compute outliers for all the numeric columns in a DataSource\n\n        Args:\n            scale (float): The scale to use for the IQR (default: 1.5)\n            use_stddev (bool): Use Standard Deviation instead of IQR (default: False)\n\n        Returns:\n            pd.DataFrame: A DataFrame of outliers from this DataSource\n\n        Notes:\n            Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma)\n            The scale parameter can be adjusted to change the IQR multiplier\n        \"\"\"\n\n        # Compute outliers using the SQL Outliers class\n        sql_outliers = sql.outliers.Outliers()\n        return sql_outliers.compute_outliers(self, scale=scale, use_stddev=use_stddev)\n\n    @cache_dataframe(\"smart_sample\")\n    def smart_sample(self, rows: int = 100) -&gt; pd.DataFrame:\n        \"\"\"Get a smart sample dataframe for this DataSource\n\n        Args:\n            rows (int): Number of rows to sample (default: 100)\n\n        Returns:\n            pd.DataFrame: A combined DataFrame of sample data + outliers\n        \"\"\"\n\n        # Compute/recompute the smart sample\n        self.log.important(f\"Computing Smart Sample {self.name}...\")\n\n        # Outliers DataFrame\n        outlier_rows = self.outliers()\n\n        # Sample DataFrame\n        sample_rows = self.sample(rows=rows)\n        sample_rows[\"outlier_group\"] = \"sample\"\n\n        # Combine the sample rows with the outlier rows\n        all_rows = pd.concat([outlier_rows, sample_rows]).reset_index(drop=True)\n\n        # Drop duplicates\n        all_except_outlier_group = [col for col in all_rows.columns if col != \"outlier_group\"]\n        all_rows = all_rows.drop_duplicates(subset=all_except_outlier_group, ignore_index=True)\n\n        # Return the smart_sample data\n        return all_rows\n\n    def correlations(self) -&gt; dict[dict]:\n        \"\"\"Compute Correlations for all the numeric columns in a DataSource\n\n        Returns:\n            dict(dict): A dictionary of correlations for each column in this format\n                 {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},\n                  'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}\n        \"\"\"\n\n        # First check if we have already computed the correlations\n        correlations_dict = self.workbench_meta().get(\"workbench_correlations\")\n        if correlations_dict:\n            return correlations_dict\n\n        # Call the SQL function to compute correlations\n        correlations_dict = sql.correlations(self)\n\n        # Push the correlation data into our DataSource Metadata\n        self.upsert_workbench_meta({\"workbench_correlations\": correlations_dict})\n\n        # Return the correlation data\n        return correlations_dict\n\n    def column_stats(self) -&gt; dict[dict]:\n        \"\"\"Compute Column Stats for all the columns in a DataSource\n\n        Returns:\n            dict(dict): A dictionary of stats for each column this format\n            NB: String columns will NOT have num_zeros, descriptive_stats or correlation data\n                {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n                 'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100,\n                          'descriptive_stats': {...}, 'correlations': {...}},\n                 ...}\n        \"\"\"\n\n        # First check if we have already computed the column stats\n        columns_stats_dict = self.workbench_meta().get(\"workbench_column_stats\")\n        if columns_stats_dict:\n            return columns_stats_dict\n\n        # Call the SQL function to compute column stats\n        column_stats_dict = sql.column_stats(self)\n\n        # Push the column stats data into our DataSource Metadata\n        self.upsert_workbench_meta({\"workbench_column_stats\": column_stats_dict})\n\n        # Return the column stats data\n        return column_stats_dict\n\n    def value_counts(self) -&gt; dict[dict]:\n        \"\"\"Compute 'value_counts' for all the string columns in a DataSource\n\n        Returns:\n            dict(dict): A dictionary of value counts for each column in the form\n                 {'col1': {'value_1': 42, 'value_2': 16, 'value_3': 9,...},\n                  'col2': ...}\n        \"\"\"\n\n        # First check if we have already computed the value counts\n        value_counts_dict = self.workbench_meta().get(\"workbench_value_counts\")\n        if value_counts_dict:\n            return value_counts_dict\n\n        # Call the SQL function to compute value_counts\n        value_count_dict = sql.value_counts(self)\n\n        # Push the value_count data into our DataSource Metadata\n        self.upsert_workbench_meta({\"workbench_value_counts\": value_count_dict})\n\n        # Return the value_count data\n        return value_count_dict\n\n    def details(self) -&gt; dict[dict]:\n        \"\"\"Additional Details about this AthenaSource Artifact\n\n        Returns:\n            dict(dict): A dictionary of details about this AthenaSource\n        \"\"\"\n        self.log.info(f\"Computing DataSource Details ({self.name})...\")\n\n        # Get the details from the base class\n        details = super().details()\n\n        # Compute additional details\n        details[\"s3_storage_location\"] = self.s3_storage_location()\n        details[\"storage_type\"] = \"athena\"\n\n        # Compute our AWS URL\n        query = f'select * from \"{self.database}.{self.table}\" limit 10'\n        query_exec_id = wr.athena.start_query_execution(\n            sql=query, database=self.database, boto3_session=self.boto3_session\n        )\n        base_url = \"https://console.aws.amazon.com/athena/home\"\n        details[\"aws_url\"] = f\"{base_url}?region={self.aws_region}#query/history/{query_exec_id}\"\n\n        # Push the aws_url data into our DataSource Metadata\n        # FIXME: We need to revisit this but doing an upsert just for aws_url is silly\n        # self.upsert_workbench_meta({\"workbench_details\": {\"aws_url\": details[\"aws_url\"]}})\n\n        # Convert any datetime fields to ISO-8601 strings\n        details = convert_all_to_iso8601(details)\n\n        # Add the column stats\n        details[\"column_stats\"] = self.column_stats()\n\n        # Return the details data\n        return details\n\n    def delete(self):\n        \"\"\"Instance Method: Delete the AWS Data Catalog Table and S3 Storage Objects\"\"\"\n\n        # Make sure the AthenaSource exists\n        if not self.exists():\n            self.log.warning(f\"Trying to delete an AthenaSource that doesn't exist: {self.name}\")\n\n        # Call the Class Method to delete the AthenaSource\n        AthenaSource.managed_delete(self.name, database=self.database)\n\n    @classmethod\n    def managed_delete(cls, data_source_name: str, database: str = \"workbench\"):\n        \"\"\"Class Method: Delete the AWS Data Catalog Table and S3 Storage Objects\n\n        Args:\n            data_source_name (str): Name of DataSource (AthenaSource)\n            database (str): Athena Database Name (default: workbench)\n        \"\"\"\n        table = data_source_name  # The table name is the same as the data_source_name\n\n        # Check if the Glue Catalog Table exists\n        if not wr.catalog.does_table_exist(database, table, boto3_session=cls.boto3_session):\n            cls.log.info(f\"DataSource {table} not found in database {database}.\")\n            return\n\n        # Delete any views associated with this AthenaSource\n        cls.delete_views(table, database)\n\n        # Delete S3 Storage Objects (if they exist)\n        try:\n            # Make an AWS Query to get the S3 storage location\n            s3_path = wr.catalog.get_table_location(database, table, boto3_session=cls.boto3_session)\n\n            # Delete Data Catalog Table\n            cls.log.info(f\"Deleting DataCatalog Table: {database}.{table}...\")\n            wr.catalog.delete_table_if_exists(database, table, boto3_session=cls.boto3_session)\n\n            # Make sure we add the trailing slash\n            s3_path = s3_path if s3_path.endswith(\"/\") else f\"{s3_path}/\"\n            cls.log.info(f\"Deleting S3 Storage Objects: {s3_path}...\")\n            wr.s3.delete_objects(s3_path, boto3_session=cls.boto3_session)\n        except Exception as e:\n            cls.log.error(f\"Failure when trying to delete {data_source_name}: {e}\")\n\n        # Delete any dataframes that were stored in the Dataframe Cache\n        cls.log.info(\"Deleting Dataframe Cache...\")\n        cls.df_cache.delete_recursive(data_source_name)\n\n    @classmethod\n    def delete_views(cls, table: str, database: str):\n        \"\"\"Delete any views associated with this FeatureSet\n\n        Args:\n            table (str): Name of Athena Table\n            database (str): Athena Database Name\n        \"\"\"\n        from workbench.core.views.view_utils import delete_views_and_supplemental_data\n\n        delete_views_and_supplemental_data(table, database, cls.boto3_session)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.column_types","title":"<code>column_types</code>  <code>property</code>","text":"<p>Return the column types of the internal AthenaSource</p>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Return the column names for this Athena Table</p>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.__init__","title":"<code>__init__(data_name, database='workbench', **kwargs)</code>","text":"<p>AthenaSource Initialization</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>Name of Athena Table</p> required <code>database</code> <code>str</code> <p>Athena Database Name (default: workbench)</p> <code>'workbench'</code> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def __init__(self, data_name, database=\"workbench\", **kwargs):\n    \"\"\"AthenaSource Initialization\n\n    Args:\n        data_name (str): Name of Athena Table\n        database (str): Athena Database Name (default: workbench)\n    \"\"\"\n    # Ensure the data_name is a valid name/id\n    self.is_name_valid(data_name)\n\n    # Call superclass init\n    super().__init__(data_name, database, **kwargs)\n\n    # Grab our metadata from the Meta class\n    self.log.info(f\"Retrieving metadata for: {self.name}...\")\n    self.data_source_meta = self.meta.data_source(data_name, database=database)\n    if self.data_source_meta is None:\n        self.log.error(f\"Unable to find {database}:{self.table} in Glue Catalogs...\")\n        return\n\n    # Call superclass post init\n    super().__post_init__()\n\n    # All done\n    self.log.debug(f\"AthenaSource Initialized: {database}.{self.table}\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.arn","title":"<code>arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for this artifact</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n    # Grab our Workbench Role Manager, get our AWS account id, and region for ARN creation\n    account_id = self.aws_account_clamp.account_id\n    region = self.aws_account_clamp.region\n    arn = f\"arn:aws:glue:{region}:{account_id}:table/{self.database}/{self.table}\"\n    return arn\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.athena_test_query","title":"<code>athena_test_query()</code>","text":"<p>Validate that Athena Queries are working</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def athena_test_query(self):\n    \"\"\"Validate that Athena Queries are working\"\"\"\n    query = f'select count(*) as workbench_count from \"{self.table}\"'\n    df = wr.athena.read_sql_query(\n        sql=query,\n        database=self.database,\n        ctas_approach=False,\n        boto3_session=self.boto3_session,\n    )\n    scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n    self.log.info(f\"Athena TEST Query successful (scanned bytes: {scanned_bytes})\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.aws_meta","title":"<code>aws_meta()</code>","text":"<p>Get the FULL AWS metadata for this artifact</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def aws_meta(self) -&gt; dict:\n    \"\"\"Get the FULL AWS metadata for this artifact\"\"\"\n    return self.data_source_meta\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.aws_url","title":"<code>aws_url()</code>","text":"<p>The AWS URL for looking at/querying this data source</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def aws_url(self):\n    \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n    workbench_details = self.workbench_meta().get(\"workbench_details\", {})\n    return workbench_details.get(\"aws_url\", \"unknown\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.column_stats","title":"<code>column_stats()</code>","text":"<p>Compute Column Stats for all the columns in a DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of stats for each column this format</p> <code>NB</code> <code>dict[dict]</code> <p>String columns will NOT have num_zeros, descriptive_stats or correlation data {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},  'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100,           'descriptive_stats': {...}, 'correlations': {...}},  ...}</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def column_stats(self) -&gt; dict[dict]:\n    \"\"\"Compute Column Stats for all the columns in a DataSource\n\n    Returns:\n        dict(dict): A dictionary of stats for each column this format\n        NB: String columns will NOT have num_zeros, descriptive_stats or correlation data\n            {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n             'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100,\n                      'descriptive_stats': {...}, 'correlations': {...}},\n             ...}\n    \"\"\"\n\n    # First check if we have already computed the column stats\n    columns_stats_dict = self.workbench_meta().get(\"workbench_column_stats\")\n    if columns_stats_dict:\n        return columns_stats_dict\n\n    # Call the SQL function to compute column stats\n    column_stats_dict = sql.column_stats(self)\n\n    # Push the column stats data into our DataSource Metadata\n    self.upsert_workbench_meta({\"workbench_column_stats\": column_stats_dict})\n\n    # Return the column stats data\n    return column_stats_dict\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.correlations","title":"<code>correlations()</code>","text":"<p>Compute Correlations for all the numeric columns in a DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of correlations for each column in this format  {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},   'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def correlations(self) -&gt; dict[dict]:\n    \"\"\"Compute Correlations for all the numeric columns in a DataSource\n\n    Returns:\n        dict(dict): A dictionary of correlations for each column in this format\n             {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},\n              'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}\n    \"\"\"\n\n    # First check if we have already computed the correlations\n    correlations_dict = self.workbench_meta().get(\"workbench_correlations\")\n    if correlations_dict:\n        return correlations_dict\n\n    # Call the SQL function to compute correlations\n    correlations_dict = sql.correlations(self)\n\n    # Push the correlation data into our DataSource Metadata\n    self.upsert_workbench_meta({\"workbench_correlations\": correlations_dict})\n\n    # Return the correlation data\n    return correlations_dict\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.created","title":"<code>created()</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    return self.data_source_meta[\"CreateTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.database_query","title":"<code>database_query(database, query)</code>  <code>classmethod</code>","text":"<p>Specify the Database and Query the Athena Service</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The Athena Database to query</p> required <code>query</code> <code>str</code> <p>The query to run against the AthenaSource</p> required <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: The results of the query</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>@classmethod\ndef database_query(cls, database: str, query: str) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Specify the Database and Query the Athena Service\n\n    Args:\n        database (str): The Athena Database to query\n        query (str): The query to run against the AthenaSource\n\n    Returns:\n        pd.DataFrame: The results of the query\n    \"\"\"\n    cls.log.debug(f\"Executing Query: {query}...\")\n    try:\n        df = wr.athena.read_sql_query(\n            sql=query,\n            database=database,\n            ctas_approach=False,\n            boto3_session=cls.boto3_session,\n        )\n        scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n        if scanned_bytes &gt; 0:\n            cls.log.debug(f\"Athena Query successful (scanned bytes: {scanned_bytes})\")\n        return df\n    except wr.exceptions.QueryFailed as e:\n        cls.log.critical(f\"Failed to execute query: {e}\")\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.delete","title":"<code>delete()</code>","text":"<p>Instance Method: Delete the AWS Data Catalog Table and S3 Storage Objects</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def delete(self):\n    \"\"\"Instance Method: Delete the AWS Data Catalog Table and S3 Storage Objects\"\"\"\n\n    # Make sure the AthenaSource exists\n    if not self.exists():\n        self.log.warning(f\"Trying to delete an AthenaSource that doesn't exist: {self.name}\")\n\n    # Call the Class Method to delete the AthenaSource\n    AthenaSource.managed_delete(self.name, database=self.database)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.delete_views","title":"<code>delete_views(table, database)</code>  <code>classmethod</code>","text":"<p>Delete any views associated with this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Name of Athena Table</p> required <code>database</code> <code>str</code> <p>Athena Database Name</p> required Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>@classmethod\ndef delete_views(cls, table: str, database: str):\n    \"\"\"Delete any views associated with this FeatureSet\n\n    Args:\n        table (str): Name of Athena Table\n        database (str): Athena Database Name\n    \"\"\"\n    from workbench.core.views.view_utils import delete_views_and_supplemental_data\n\n    delete_views_and_supplemental_data(table, database, cls.boto3_session)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.descriptive_stats","title":"<code>descriptive_stats()</code>","text":"<p>Compute Descriptive Stats for all the numeric columns in a DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of descriptive stats for each column in the form  {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},   'col2': ...}</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def descriptive_stats(self) -&gt; dict[dict]:\n    \"\"\"Compute Descriptive Stats for all the numeric columns in a DataSource\n\n    Returns:\n        dict(dict): A dictionary of descriptive stats for each column in the form\n             {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},\n              'col2': ...}\n    \"\"\"\n\n    # First check if we have already computed the descriptive stats\n    stat_dict = self.workbench_meta().get(\"workbench_descriptive_stats\")\n    if stat_dict:\n        self.log.info(\"Returning precomputed meta(workbench_descriptive_stats)\")\n        return stat_dict\n\n    # Call the SQL function to compute descriptive stats\n    stat_dict = sql.descriptive_stats(self)\n\n    # Push the descriptive stat data into our DataSource Metadata\n    self.upsert_workbench_meta({\"workbench_descriptive_stats\": stat_dict})\n\n    # Return the descriptive stats\n    return stat_dict\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.details","title":"<code>details()</code>","text":"<p>Additional Details about this AthenaSource Artifact</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about this AthenaSource</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def details(self) -&gt; dict[dict]:\n    \"\"\"Additional Details about this AthenaSource Artifact\n\n    Returns:\n        dict(dict): A dictionary of details about this AthenaSource\n    \"\"\"\n    self.log.info(f\"Computing DataSource Details ({self.name})...\")\n\n    # Get the details from the base class\n    details = super().details()\n\n    # Compute additional details\n    details[\"s3_storage_location\"] = self.s3_storage_location()\n    details[\"storage_type\"] = \"athena\"\n\n    # Compute our AWS URL\n    query = f'select * from \"{self.database}.{self.table}\" limit 10'\n    query_exec_id = wr.athena.start_query_execution(\n        sql=query, database=self.database, boto3_session=self.boto3_session\n    )\n    base_url = \"https://console.aws.amazon.com/athena/home\"\n    details[\"aws_url\"] = f\"{base_url}?region={self.aws_region}#query/history/{query_exec_id}\"\n\n    # Push the aws_url data into our DataSource Metadata\n    # FIXME: We need to revisit this but doing an upsert just for aws_url is silly\n    # self.upsert_workbench_meta({\"workbench_details\": {\"aws_url\": details[\"aws_url\"]}})\n\n    # Convert any datetime fields to ISO-8601 strings\n    details = convert_all_to_iso8601(details)\n\n    # Add the column stats\n    details[\"column_stats\"] = self.column_stats()\n\n    # Return the details data\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.execute_statement","title":"<code>execute_statement(query, silence_errors=False)</code>","text":"<p>Execute a non-returning SQL statement in Athena with retries.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to run against the AthenaSource</p> required <code>silence_errors</code> <code>bool</code> <p>Silence errors (default: False)</p> <code>False</code> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def execute_statement(self, query: str, silence_errors: bool = False):\n    \"\"\"Execute a non-returning SQL statement in Athena with retries.\n\n    Args:\n        query (str): The query to run against the AthenaSource\n        silence_errors (bool): Silence errors (default: False)\n    \"\"\"\n    attempt = 0\n    max_retries = 5\n    retry_delay = 30\n    while attempt &lt; max_retries:\n        try:\n            # Start the query execution\n            query_execution_id = wr.athena.start_query_execution(\n                sql=query,\n                database=self.database,\n                boto3_session=self.boto3_session,\n            )\n            self.log.debug(f\"QueryExecutionId: {query_execution_id}\")\n\n            # Wait for the query to complete\n            wr.athena.wait_query(query_execution_id=query_execution_id, boto3_session=self.boto3_session)\n            self.log.debug(f\"Query executed successfully: {query_execution_id}\")\n            break  # If successful, exit the retry loop\n        except wr.exceptions.QueryFailed as e:\n            if \"AlreadyExistsException\" in str(e):\n                self.log.warning(f\"Table already exists: {e} \\nIgnoring...\")\n                break  # No need to retry for this error\n            elif \"ConcurrentModificationException\" in str(e) or \"OperationTimeoutException\" in str(e):\n                self.log.warning(f\"Exception {e}\\nRetrying...\")\n                attempt += 1\n                if attempt &lt; max_retries:\n                    time.sleep(retry_delay)\n                else:\n                    if not silence_errors:\n                        self.log.critical(f\"Failed to execute query after {max_retries} attempts: {query}\")\n                        self.log.critical(f\"Error: {e}\")\n                    raise\n            else:\n                if not silence_errors:\n                    self.log.critical(f\"Failed to execute query: {query}\")\n                    self.log.critical(f\"Error: {e}\")\n                raise\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.exists","title":"<code>exists()</code>","text":"<p>Validation Checks for this Data Source</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Validation Checks for this Data Source\"\"\"\n\n    # Are we able to pull AWS Metadata for this table_name?\"\"\"\n    # Do we have a valid data_source_meta?\n    if getattr(self, \"data_source_meta\", None) is None:\n        self.log.debug(f\"AthenaSource {self.table} not found in Workbench Metadata...\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.hash","title":"<code>hash()</code>","text":"<p>Get the hash for the set of Parquet files used for this Artifact</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def hash(self) -&gt; str:\n    \"\"\"Get the hash for the set of Parquet files used for this Artifact\"\"\"\n    s3_uri = self.s3_storage_location()\n    return compute_parquet_hash(s3_uri, self.boto3_session)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.managed_delete","title":"<code>managed_delete(data_source_name, database='workbench')</code>  <code>classmethod</code>","text":"<p>Class Method: Delete the AWS Data Catalog Table and S3 Storage Objects</p> <p>Parameters:</p> Name Type Description Default <code>data_source_name</code> <code>str</code> <p>Name of DataSource (AthenaSource)</p> required <code>database</code> <code>str</code> <p>Athena Database Name (default: workbench)</p> <code>'workbench'</code> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>@classmethod\ndef managed_delete(cls, data_source_name: str, database: str = \"workbench\"):\n    \"\"\"Class Method: Delete the AWS Data Catalog Table and S3 Storage Objects\n\n    Args:\n        data_source_name (str): Name of DataSource (AthenaSource)\n        database (str): Athena Database Name (default: workbench)\n    \"\"\"\n    table = data_source_name  # The table name is the same as the data_source_name\n\n    # Check if the Glue Catalog Table exists\n    if not wr.catalog.does_table_exist(database, table, boto3_session=cls.boto3_session):\n        cls.log.info(f\"DataSource {table} not found in database {database}.\")\n        return\n\n    # Delete any views associated with this AthenaSource\n    cls.delete_views(table, database)\n\n    # Delete S3 Storage Objects (if they exist)\n    try:\n        # Make an AWS Query to get the S3 storage location\n        s3_path = wr.catalog.get_table_location(database, table, boto3_session=cls.boto3_session)\n\n        # Delete Data Catalog Table\n        cls.log.info(f\"Deleting DataCatalog Table: {database}.{table}...\")\n        wr.catalog.delete_table_if_exists(database, table, boto3_session=cls.boto3_session)\n\n        # Make sure we add the trailing slash\n        s3_path = s3_path if s3_path.endswith(\"/\") else f\"{s3_path}/\"\n        cls.log.info(f\"Deleting S3 Storage Objects: {s3_path}...\")\n        wr.s3.delete_objects(s3_path, boto3_session=cls.boto3_session)\n    except Exception as e:\n        cls.log.error(f\"Failure when trying to delete {data_source_name}: {e}\")\n\n    # Delete any dataframes that were stored in the Dataframe Cache\n    cls.log.info(\"Deleting Dataframe Cache...\")\n    cls.df_cache.delete_recursive(data_source_name)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.modified","title":"<code>modified()</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    return self.data_source_meta[\"UpdateTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.num_columns","title":"<code>num_columns()</code>","text":"<p>Return the number of columns for this Data Source</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def num_columns(self) -&gt; int:\n    \"\"\"Return the number of columns for this Data Source\"\"\"\n    return len(self.columns)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.num_rows","title":"<code>num_rows()</code>","text":"<p>Return the number of rows for this Data Source</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def num_rows(self) -&gt; int:\n    \"\"\"Return the number of rows for this Data Source\"\"\"\n    count_df = self.query(f'select count(*) AS workbench_count from \"{self.database}\".\"{self.table}\"')\n    return count_df[\"workbench_count\"][0] if count_df is not None else 0\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.outliers","title":"<code>outliers(scale=1.5, use_stddev=False)</code>","text":"<p>Compute outliers for all the numeric columns in a DataSource</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>The scale to use for the IQR (default: 1.5)</p> <code>1.5</code> <code>use_stddev</code> <code>bool</code> <p>Use Standard Deviation instead of IQR (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame of outliers from this DataSource</p> Notes <p>Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma) The scale parameter can be adjusted to change the IQR multiplier</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>@cache_dataframe(\"outliers\")\ndef outliers(self, scale: float = 1.5, use_stddev=False) -&gt; pd.DataFrame:\n    \"\"\"Compute outliers for all the numeric columns in a DataSource\n\n    Args:\n        scale (float): The scale to use for the IQR (default: 1.5)\n        use_stddev (bool): Use Standard Deviation instead of IQR (default: False)\n\n    Returns:\n        pd.DataFrame: A DataFrame of outliers from this DataSource\n\n    Notes:\n        Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma)\n        The scale parameter can be adjusted to change the IQR multiplier\n    \"\"\"\n\n    # Compute outliers using the SQL Outliers class\n    sql_outliers = sql.outliers.Outliers()\n    return sql_outliers.compute_outliers(self, scale=scale, use_stddev=use_stddev)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.query","title":"<code>query(query)</code>","text":"<p>Query the AthenaSource</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to run against the AthenaSource</p> required <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: The results of the query</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def query(self, query: str) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Query the AthenaSource\n\n    Args:\n        query (str): The query to run against the AthenaSource\n\n    Returns:\n        pd.DataFrame: The results of the query\n    \"\"\"\n\n    # Call internal class _query method\n    return self.database_query(self.database, query)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.refresh_meta","title":"<code>refresh_meta()</code>","text":"<p>Refresh our internal AWS Broker catalog metadata</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def refresh_meta(self):\n    \"\"\"Refresh our internal AWS Broker catalog metadata\"\"\"\n    self.data_source_meta = self.meta.data_source(self.name, database=self.database)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.s3_storage_location","title":"<code>s3_storage_location()</code>","text":"<p>Get the S3 Storage Location for this Data Source</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def s3_storage_location(self) -&gt; str:\n    \"\"\"Get the S3 Storage Location for this Data Source\"\"\"\n    return self.data_source_meta[\"StorageDescriptor\"][\"Location\"]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.sample","title":"<code>sample(rows=100)</code>","text":"<p>Pull a sample of rows from the DataSource</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Number of rows to sample (default: 100)</p> <code>100</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A sample DataFrame for an Athena DataSource</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>@cache_dataframe(\"sample\")\ndef sample(self, rows: int = 100) -&gt; pd.DataFrame:\n    \"\"\"Pull a sample of rows from the DataSource\n\n    Args:\n        rows (int): Number of rows to sample (default: 100)\n\n    Returns:\n        pd.DataFrame: A sample DataFrame for an Athena DataSource\n    \"\"\"\n\n    # Call the SQL function to pull a sample of the rows\n    return sql.sample_rows(self, rows=rows)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.size","title":"<code>size()</code>","text":"<p>Return the size of this data in MegaBytes</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def size(self) -&gt; float:\n    \"\"\"Return the size of this data in MegaBytes\"\"\"\n    size_in_bytes = sum(wr.s3.size_objects(self.s3_storage_location(), boto3_session=self.boto3_session).values())\n    size_in_mb = size_in_bytes / 1_000_000\n    return size_in_mb\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.smart_sample","title":"<code>smart_sample(rows=100)</code>","text":"<p>Get a smart sample dataframe for this DataSource</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Number of rows to sample (default: 100)</p> <code>100</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A combined DataFrame of sample data + outliers</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>@cache_dataframe(\"smart_sample\")\ndef smart_sample(self, rows: int = 100) -&gt; pd.DataFrame:\n    \"\"\"Get a smart sample dataframe for this DataSource\n\n    Args:\n        rows (int): Number of rows to sample (default: 100)\n\n    Returns:\n        pd.DataFrame: A combined DataFrame of sample data + outliers\n    \"\"\"\n\n    # Compute/recompute the smart sample\n    self.log.important(f\"Computing Smart Sample {self.name}...\")\n\n    # Outliers DataFrame\n    outlier_rows = self.outliers()\n\n    # Sample DataFrame\n    sample_rows = self.sample(rows=rows)\n    sample_rows[\"outlier_group\"] = \"sample\"\n\n    # Combine the sample rows with the outlier rows\n    all_rows = pd.concat([outlier_rows, sample_rows]).reset_index(drop=True)\n\n    # Drop duplicates\n    all_except_outlier_group = [col for col in all_rows.columns if col != \"outlier_group\"]\n    all_rows = all_rows.drop_duplicates(subset=all_except_outlier_group, ignore_index=True)\n\n    # Return the smart_sample data\n    return all_rows\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.table_hash","title":"<code>table_hash()</code>","text":"<p>Get the table hash for this AthenaSource</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def table_hash(self) -&gt; str:\n    \"\"\"Get the table hash for this AthenaSource\"\"\"\n    s3_scratch = f\"s3://{self.workbench_bucket}/temp/athena_output\"\n    return compute_athena_table_hash(self.database, self.table, self.boto3_session, s3_scratch)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.upsert_workbench_meta","title":"<code>upsert_workbench_meta(new_meta)</code>","text":"<p>Add Workbench specific metadata to this Artifact</p> <p>Parameters:</p> Name Type Description Default <code>new_meta</code> <code>dict</code> <p>Dictionary of new metadata to add</p> required Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def upsert_workbench_meta(self, new_meta: dict):\n    \"\"\"Add Workbench specific metadata to this Artifact\n\n    Args:\n        new_meta (dict): Dictionary of new metadata to add\n    \"\"\"\n    self.log.important(f\"Upserting Workbench Metadata {self.name}:{str(new_meta)[:50]}...\")\n\n    # Give a warning message for keys that don't start with workbench_\n    for key in new_meta.keys():\n        if not key.startswith(\"workbench_\"):\n            self.log.error(\"NOT STORING META: Append 'workbench_' to key names to avoid overwriting AWS meta data\")\n\n    # Now convert any non-string values to JSON strings\n    for key, value in new_meta.items():\n        if not isinstance(value, str):\n            new_meta[key] = json.dumps(value, cls=CustomEncoder)\n\n    # Store our updated metadata\n    try:\n        wr.catalog.upsert_table_parameters(\n            parameters=new_meta,\n            database=self.database,\n            table=self.table,\n            boto3_session=self.boto3_session,\n        )\n    except botocore.exceptions.ClientError as e:\n        error_code = e.response[\"Error\"][\"Code\"]\n        if error_code == \"InvalidInputException\":\n            self.log.error(f\"Unable to upsert metadata for {self.table}\")\n            self.log.error(\"Probably because the metadata is too large\")\n            self.log.error(new_meta)\n        elif error_code == \"ConcurrentModificationException\":\n            self.log.warning(\"ConcurrentModificationException... trying again...\")\n            time.sleep(5)\n            wr.catalog.upsert_table_parameters(\n                parameters=new_meta,\n                database=self.database,\n                table=self.table,\n                boto3_session=self.boto3_session,\n            )\n        else:\n            self.log.critical(f\"Failed to upsert metadata: {e}\")\n            self.log.critical(f\"{self.name} is Malformed! Delete this Artifact and recreate it!\")\n    except Exception as e:\n        self.log.critical(f\"Failed to upsert metadata: {e}\")\n        self.log.critical(f\"{self.name} is Malformed! Delete this Artifact and recreate it!\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.value_counts","title":"<code>value_counts()</code>","text":"<p>Compute 'value_counts' for all the string columns in a DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of value counts for each column in the form  {'col1': {'value_1': 42, 'value_2': 16, 'value_3': 9,...},   'col2': ...}</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def value_counts(self) -&gt; dict[dict]:\n    \"\"\"Compute 'value_counts' for all the string columns in a DataSource\n\n    Returns:\n        dict(dict): A dictionary of value counts for each column in the form\n             {'col1': {'value_1': 42, 'value_2': 16, 'value_3': 9,...},\n              'col2': ...}\n    \"\"\"\n\n    # First check if we have already computed the value counts\n    value_counts_dict = self.workbench_meta().get(\"workbench_value_counts\")\n    if value_counts_dict:\n        return value_counts_dict\n\n    # Call the SQL function to compute value_counts\n    value_count_dict = sql.value_counts(self)\n\n    # Push the value_count data into our DataSource Metadata\n    self.upsert_workbench_meta({\"workbench_value_counts\": value_count_dict})\n\n    # Return the value_count data\n    return value_count_dict\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#workbench.core.artifacts.athena_source.AthenaSource.workbench_meta","title":"<code>workbench_meta()</code>","text":"<p>Get the Workbench specific metadata for this Artifact</p> Source code in <code>src/workbench/core/artifacts/athena_source.py</code> <pre><code>def workbench_meta(self) -&gt; dict:\n    \"\"\"Get the Workbench specific metadata for this Artifact\"\"\"\n\n    # Sanity Check if we have invalid AWS Metadata\n    if self.data_source_meta is None:\n        if not self.exists():\n            self.log.error(f\"DataSource {self.name} doesn't appear to exist...\")\n        else:\n            self.log.critical(f\"Unable to get AWS Metadata for {self.table}\")\n            self.log.critical(\"Malformed Artifact! Delete this Artifact and recreate it!\")\n        return {}\n\n    # Get the Workbench Metadata from the 'Parameters' section of the DataSource Metadata\n    params = self.data_source_meta.get(\"Parameters\", {})\n    return {key: decode_value(value) for key, value in params.items() if \"workbench\" in key}\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/","title":"DataSource Abstract","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the DataSource API Class and voil\u00e0 it works the same.</p> <p>The DataSource Abstract class is a base/abstract class that defines API implemented by all the child classes (currently just AthenaSource but later RDSSource, FutureThing ).</p> <p>DataSourceAbstract: Abstract Base Class for all data sources (S3: CSV, JSONL, Parquet, RDS, etc)</p>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract","title":"<code>DataSourceAbstract</code>","text":"<p>               Bases: <code>Artifact</code></p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>class DataSourceAbstract(Artifact):\n    def __init__(self, data_name: str, database: str = \"workbench\", **kwargs):\n        \"\"\"DataSourceAbstract: Abstract Base Class for all data sources\n        Args:\n            data_name(str): The Name for this Data Source\n            database(str): The database to use for this Data Source (default: workbench)\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(data_name, **kwargs)\n\n        # Set up our instance attributes\n        self._database = database\n        self._table_name = data_name\n\n    def __post_init__(self):\n        # Call superclass post_init\n        super().__post_init__()\n\n    @deprecated(version=\"0.9\")\n    def get_database(self) -&gt; str:\n        \"\"\"Get the database for this Data Source\"\"\"\n        return self._database\n\n    @property\n    def database(self) -&gt; str:\n        \"\"\"Get the database for this Data Source\"\"\"\n        return self._database\n\n    @property\n    def table(self) -&gt; str:\n        \"\"\"Get the base table name for this Data Source\"\"\"\n        return self._table_name\n\n    @abstractmethod\n    def num_rows(self) -&gt; int:\n        \"\"\"Return the number of rows for this Data Source\"\"\"\n        pass\n\n    @abstractmethod\n    def num_columns(self) -&gt; int:\n        \"\"\"Return the number of columns for this Data Source\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def columns(self) -&gt; list[str]:\n        \"\"\"Return the column names for this Data Source\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def column_types(self) -&gt; list[str]:\n        \"\"\"Return the column types for this Data Source\"\"\"\n        pass\n\n    def column_details(self) -&gt; dict:\n        \"\"\"Return the column details for this Data Source\n\n        Returns:\n            dict: The column details for this Data Source\n        \"\"\"\n        return dict(zip(self.columns, self.column_types))\n\n    def views(self) -&gt; list[str]:\n        \"\"\"Return the views for this Data Source\"\"\"\n        from workbench.core.views.view_utils import list_views\n\n        return list_views(self)\n\n    def supplemental_data(self) -&gt; list[str]:\n        \"\"\"Return the supplemental data for this Data Source\"\"\"\n        from workbench.core.views.view_utils import list_supplemental_data\n\n        return list_supplemental_data(self)\n\n    def view(self, view_name: str) -&gt; \"View\":\n        \"\"\"Return a DataFrame for a specific view\n        Args:\n            view_name (str): The name of the view to return\n        Returns:\n            pd.DataFrame: A DataFrame for the specified view\n        \"\"\"\n        from workbench.core.views import View\n\n        return View(self, view_name)\n\n    def set_display_columns(self, diplay_columns: list[str]):\n        \"\"\"Set the display columns for this Data Source\n\n        Args:\n            diplay_columns (list[str]): The display columns for this Data Source\n        \"\"\"\n        # Check mismatch of display columns to computation columns\n        c_view = self.view(\"computation\")\n        computation_columns = c_view.columns\n        mismatch_columns = [col for col in diplay_columns if col not in computation_columns]\n        if mismatch_columns:\n            self.log.monitor(f\"Display View/Computation mismatch: {mismatch_columns}\")\n\n        self.log.important(f\"Setting Display Columns...{diplay_columns}\")\n        from workbench.core.views import DisplayView\n\n        # Create a NEW display view\n        DisplayView.create(self, source_table=c_view.table, column_list=diplay_columns)\n\n    def set_computation_columns(self, computation_columns: list[str]):\n        \"\"\"Set the computation columns for this Data Source\n\n        Args:\n            computation_columns (list[str]): The computation columns for this Data Source\n        \"\"\"\n        self.log.important(f\"Setting Computation Columns...{computation_columns}\")\n        from workbench.core.views import ComputationView\n\n        # Create a NEW computation view\n        ComputationView.create(self, column_list=computation_columns)\n        self.recompute_stats()\n\n    def _create_display_view(self):\n        \"\"\"Internal: Create the Display View for this DataSource\"\"\"\n        from workbench.core.views import View\n\n        View(self, \"display\")\n\n    @abstractmethod\n    def query(self, query: str) -&gt; pd.DataFrame:\n        \"\"\"Query the DataSourceAbstract\n        Args:\n            query(str): The SQL query to execute\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_statement(self, query: str):\n        \"\"\"Execute an SQL statement that doesn't return a result\n        Args:\n            query(str): The SQL statement to execute\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def sample(self) -&gt; pd.DataFrame:\n        \"\"\"Return a sample DataFrame from this DataSourceAbstract\n\n        Returns:\n            pd.DataFrame: A sample DataFrame from this DataSource\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def descriptive_stats(self) -&gt; dict[dict]:\n        \"\"\"Compute Descriptive Stats for all the numeric columns in a DataSource\n\n        Returns:\n            dict(dict): A dictionary of descriptive stats for each column in the form\n                 {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},\n                  'col2': ...}\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def outliers(self, scale: float = 1.5) -&gt; pd.DataFrame:\n        \"\"\"Return a DataFrame of outliers from this DataSource\n\n        Args:\n            scale (float): The scale to use for the IQR (default: 1.5)\n\n        Returns:\n            pd.DataFrame: A DataFrame of outliers from this DataSource\n\n        Notes:\n            Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers\n            The scale parameter can be adjusted to change the IQR multiplier\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def smart_sample(self) -&gt; pd.DataFrame:\n        \"\"\"Get a SMART sample dataframe from this DataSource\n        Returns:\n            pd.DataFrame: A combined DataFrame of sample data + outliers\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def value_counts(self) -&gt; dict[dict]:\n        \"\"\"Compute 'value_counts' for all the string columns in a DataSource\n\n        Returns:\n            dict(dict): A dictionary of value counts for each column in the form\n                 {'col1': {'value_1': X, 'value_2': Y, 'value_3': Z,...},\n                  'col2': ...}\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def column_stats(self) -&gt; dict[dict]:\n        \"\"\"Compute Column Stats for all the columns in a DataSource\n\n        Returns:\n            dict(dict): A dictionary of stats for each column this format\n            NB: String columns will NOT have num_zeros and descriptive stats\n                {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n                 'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},\n                 ...}\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def correlations(self) -&gt; dict[dict]:\n        \"\"\"Compute Correlations for all the numeric columns in a DataSource\n\n        Returns:\n            dict(dict): A dictionary of correlations for each column in this format\n                 {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},\n                  'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}\n        \"\"\"\n        pass\n\n    def details(self) -&gt; dict:\n        \"\"\"Additional Details about this DataSourceAbstract Artifact\"\"\"\n        details = self.summary()\n        details[\"num_rows\"] = self.num_rows()\n        details[\"num_columns\"] = self.num_columns()\n        details[\"column_details\"] = self.column_details()\n        return details\n\n    def expected_meta(self) -&gt; list[str]:\n        \"\"\"DataSources have quite a bit of expected Metadata for EDA displays\"\"\"\n\n        # For DataSources, we expect to see the following metadata\n        expected_meta = [\n            # FIXME: Revisit this\n            # \"workbench_details\",\n            \"workbench_descriptive_stats\",\n            \"workbench_value_counts\",\n            \"workbench_correlations\",\n            \"workbench_column_stats\",\n        ]\n        return expected_meta\n\n    def ready(self) -&gt; bool:\n        \"\"\"Is the DataSource ready?\"\"\"\n\n        # Check if our parent class (Artifact) is ready\n        if not super().ready():\n            return False\n\n        # If we don't have a smart_sample we're probably not ready\n        if not self.df_cache.check(f\"{self.name}/smart_sample\"):\n            self.log.warning(f\"DataSource {self.name} not ready...\")\n            return False\n\n        # Okay so we have sample, outliers, and smart_sample so we are ready\n        return True\n\n    def onboard(self) -&gt; bool:\n        \"\"\"This is a BLOCKING method that will onboard the data source (make it ready)\n\n        Returns:\n            bool: True if the DataSource was onboarded successfully\n        \"\"\"\n        self.log.important(f\"Onboarding {self.name}...\")\n        self.set_status(\"onboarding\")\n        self.remove_health_tag(\"needs_onboard\")\n\n        # Make sure our display view actually exists\n        self.view(\"display\").ensure_exists()\n\n        # Recompute the stats\n        self.recompute_stats()\n\n        # Run a health check and refresh the meta\n        time.sleep(2)  # Give the AWS Metadata a chance to update\n        self.health_check(deep=True)\n        self.refresh_meta()\n        self.details()\n        self.set_status(\"ready\")\n        return True\n\n    def recompute_stats(self) -&gt; bool:\n        \"\"\"This is a BLOCKING method that will recompute the stats for the data source\n\n        Returns:\n            bool: True if the DataSource stats were recomputed successfully\n        \"\"\"\n        self.log.important(f\"Recomputing Stats {self.name}...\")\n\n        # Make sure our computation view actually exists\n        self.view(\"computation\").ensure_exists()\n\n        # Compute the sample, column stats, outliers, and smart_sample\n        self.df_cache.delete(f\"{self.name}/sample\")\n        self.sample()\n        self.column_stats()\n        self.refresh_meta()  # Refresh the meta since outliers needs descriptive_stats and value_counts\n        self.df_cache.delete(f\"{self.name}/outliers\")\n        self.outliers()\n        self.df_cache.delete(f\"{self.name}/smart_sample\")\n        self.smart_sample()\n        return True\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.column_types","title":"<code>column_types</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the column types for this Data Source</p>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.columns","title":"<code>columns</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the column names for this Data Source</p>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.database","title":"<code>database</code>  <code>property</code>","text":"<p>Get the database for this Data Source</p>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.table","title":"<code>table</code>  <code>property</code>","text":"<p>Get the base table name for this Data Source</p>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.__init__","title":"<code>__init__(data_name, database='workbench', **kwargs)</code>","text":"<p>DataSourceAbstract: Abstract Base Class for all data sources Args:     data_name(str): The Name for this Data Source     database(str): The database to use for this Data Source (default: workbench)</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def __init__(self, data_name: str, database: str = \"workbench\", **kwargs):\n    \"\"\"DataSourceAbstract: Abstract Base Class for all data sources\n    Args:\n        data_name(str): The Name for this Data Source\n        database(str): The database to use for this Data Source (default: workbench)\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(data_name, **kwargs)\n\n    # Set up our instance attributes\n    self._database = database\n    self._table_name = data_name\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.column_details","title":"<code>column_details()</code>","text":"<p>Return the column details for this Data Source</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The column details for this Data Source</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def column_details(self) -&gt; dict:\n    \"\"\"Return the column details for this Data Source\n\n    Returns:\n        dict: The column details for this Data Source\n    \"\"\"\n    return dict(zip(self.columns, self.column_types))\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.column_stats","title":"<code>column_stats()</code>  <code>abstractmethod</code>","text":"<p>Compute Column Stats for all the columns in a DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of stats for each column this format</p> <code>NB</code> <code>dict[dict]</code> <p>String columns will NOT have num_zeros and descriptive stats {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},  'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},  ...}</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef column_stats(self) -&gt; dict[dict]:\n    \"\"\"Compute Column Stats for all the columns in a DataSource\n\n    Returns:\n        dict(dict): A dictionary of stats for each column this format\n        NB: String columns will NOT have num_zeros and descriptive stats\n            {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n             'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},\n             ...}\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.correlations","title":"<code>correlations()</code>  <code>abstractmethod</code>","text":"<p>Compute Correlations for all the numeric columns in a DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of correlations for each column in this format  {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},   'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef correlations(self) -&gt; dict[dict]:\n    \"\"\"Compute Correlations for all the numeric columns in a DataSource\n\n    Returns:\n        dict(dict): A dictionary of correlations for each column in this format\n             {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},\n              'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.descriptive_stats","title":"<code>descriptive_stats()</code>  <code>abstractmethod</code>","text":"<p>Compute Descriptive Stats for all the numeric columns in a DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of descriptive stats for each column in the form  {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},   'col2': ...}</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef descriptive_stats(self) -&gt; dict[dict]:\n    \"\"\"Compute Descriptive Stats for all the numeric columns in a DataSource\n\n    Returns:\n        dict(dict): A dictionary of descriptive stats for each column in the form\n             {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},\n              'col2': ...}\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.details","title":"<code>details()</code>","text":"<p>Additional Details about this DataSourceAbstract Artifact</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def details(self) -&gt; dict:\n    \"\"\"Additional Details about this DataSourceAbstract Artifact\"\"\"\n    details = self.summary()\n    details[\"num_rows\"] = self.num_rows()\n    details[\"num_columns\"] = self.num_columns()\n    details[\"column_details\"] = self.column_details()\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.execute_statement","title":"<code>execute_statement(query)</code>  <code>abstractmethod</code>","text":"<p>Execute an SQL statement that doesn't return a result Args:     query(str): The SQL statement to execute</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef execute_statement(self, query: str):\n    \"\"\"Execute an SQL statement that doesn't return a result\n    Args:\n        query(str): The SQL statement to execute\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.expected_meta","title":"<code>expected_meta()</code>","text":"<p>DataSources have quite a bit of expected Metadata for EDA displays</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def expected_meta(self) -&gt; list[str]:\n    \"\"\"DataSources have quite a bit of expected Metadata for EDA displays\"\"\"\n\n    # For DataSources, we expect to see the following metadata\n    expected_meta = [\n        # FIXME: Revisit this\n        # \"workbench_details\",\n        \"workbench_descriptive_stats\",\n        \"workbench_value_counts\",\n        \"workbench_correlations\",\n        \"workbench_column_stats\",\n    ]\n    return expected_meta\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.get_database","title":"<code>get_database()</code>","text":"<p>Get the database for this Data Source</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@deprecated(version=\"0.9\")\ndef get_database(self) -&gt; str:\n    \"\"\"Get the database for this Data Source\"\"\"\n    return self._database\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.num_columns","title":"<code>num_columns()</code>  <code>abstractmethod</code>","text":"<p>Return the number of columns for this Data Source</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef num_columns(self) -&gt; int:\n    \"\"\"Return the number of columns for this Data Source\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.num_rows","title":"<code>num_rows()</code>  <code>abstractmethod</code>","text":"<p>Return the number of rows for this Data Source</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef num_rows(self) -&gt; int:\n    \"\"\"Return the number of rows for this Data Source\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.onboard","title":"<code>onboard()</code>","text":"<p>This is a BLOCKING method that will onboard the data source (make it ready)</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the DataSource was onboarded successfully</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def onboard(self) -&gt; bool:\n    \"\"\"This is a BLOCKING method that will onboard the data source (make it ready)\n\n    Returns:\n        bool: True if the DataSource was onboarded successfully\n    \"\"\"\n    self.log.important(f\"Onboarding {self.name}...\")\n    self.set_status(\"onboarding\")\n    self.remove_health_tag(\"needs_onboard\")\n\n    # Make sure our display view actually exists\n    self.view(\"display\").ensure_exists()\n\n    # Recompute the stats\n    self.recompute_stats()\n\n    # Run a health check and refresh the meta\n    time.sleep(2)  # Give the AWS Metadata a chance to update\n    self.health_check(deep=True)\n    self.refresh_meta()\n    self.details()\n    self.set_status(\"ready\")\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.outliers","title":"<code>outliers(scale=1.5)</code>  <code>abstractmethod</code>","text":"<p>Return a DataFrame of outliers from this DataSource</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>The scale to use for the IQR (default: 1.5)</p> <code>1.5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame of outliers from this DataSource</p> Notes <p>Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers The scale parameter can be adjusted to change the IQR multiplier</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef outliers(self, scale: float = 1.5) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame of outliers from this DataSource\n\n    Args:\n        scale (float): The scale to use for the IQR (default: 1.5)\n\n    Returns:\n        pd.DataFrame: A DataFrame of outliers from this DataSource\n\n    Notes:\n        Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers\n        The scale parameter can be adjusted to change the IQR multiplier\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.query","title":"<code>query(query)</code>  <code>abstractmethod</code>","text":"<p>Query the DataSourceAbstract Args:     query(str): The SQL query to execute</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef query(self, query: str) -&gt; pd.DataFrame:\n    \"\"\"Query the DataSourceAbstract\n    Args:\n        query(str): The SQL query to execute\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.ready","title":"<code>ready()</code>","text":"<p>Is the DataSource ready?</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def ready(self) -&gt; bool:\n    \"\"\"Is the DataSource ready?\"\"\"\n\n    # Check if our parent class (Artifact) is ready\n    if not super().ready():\n        return False\n\n    # If we don't have a smart_sample we're probably not ready\n    if not self.df_cache.check(f\"{self.name}/smart_sample\"):\n        self.log.warning(f\"DataSource {self.name} not ready...\")\n        return False\n\n    # Okay so we have sample, outliers, and smart_sample so we are ready\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.recompute_stats","title":"<code>recompute_stats()</code>","text":"<p>This is a BLOCKING method that will recompute the stats for the data source</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the DataSource stats were recomputed successfully</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def recompute_stats(self) -&gt; bool:\n    \"\"\"This is a BLOCKING method that will recompute the stats for the data source\n\n    Returns:\n        bool: True if the DataSource stats were recomputed successfully\n    \"\"\"\n    self.log.important(f\"Recomputing Stats {self.name}...\")\n\n    # Make sure our computation view actually exists\n    self.view(\"computation\").ensure_exists()\n\n    # Compute the sample, column stats, outliers, and smart_sample\n    self.df_cache.delete(f\"{self.name}/sample\")\n    self.sample()\n    self.column_stats()\n    self.refresh_meta()  # Refresh the meta since outliers needs descriptive_stats and value_counts\n    self.df_cache.delete(f\"{self.name}/outliers\")\n    self.outliers()\n    self.df_cache.delete(f\"{self.name}/smart_sample\")\n    self.smart_sample()\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.sample","title":"<code>sample()</code>  <code>abstractmethod</code>","text":"<p>Return a sample DataFrame from this DataSourceAbstract</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A sample DataFrame from this DataSource</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef sample(self) -&gt; pd.DataFrame:\n    \"\"\"Return a sample DataFrame from this DataSourceAbstract\n\n    Returns:\n        pd.DataFrame: A sample DataFrame from this DataSource\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.set_computation_columns","title":"<code>set_computation_columns(computation_columns)</code>","text":"<p>Set the computation columns for this Data Source</p> <p>Parameters:</p> Name Type Description Default <code>computation_columns</code> <code>list[str]</code> <p>The computation columns for this Data Source</p> required Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def set_computation_columns(self, computation_columns: list[str]):\n    \"\"\"Set the computation columns for this Data Source\n\n    Args:\n        computation_columns (list[str]): The computation columns for this Data Source\n    \"\"\"\n    self.log.important(f\"Setting Computation Columns...{computation_columns}\")\n    from workbench.core.views import ComputationView\n\n    # Create a NEW computation view\n    ComputationView.create(self, column_list=computation_columns)\n    self.recompute_stats()\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.set_display_columns","title":"<code>set_display_columns(diplay_columns)</code>","text":"<p>Set the display columns for this Data Source</p> <p>Parameters:</p> Name Type Description Default <code>diplay_columns</code> <code>list[str]</code> <p>The display columns for this Data Source</p> required Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def set_display_columns(self, diplay_columns: list[str]):\n    \"\"\"Set the display columns for this Data Source\n\n    Args:\n        diplay_columns (list[str]): The display columns for this Data Source\n    \"\"\"\n    # Check mismatch of display columns to computation columns\n    c_view = self.view(\"computation\")\n    computation_columns = c_view.columns\n    mismatch_columns = [col for col in diplay_columns if col not in computation_columns]\n    if mismatch_columns:\n        self.log.monitor(f\"Display View/Computation mismatch: {mismatch_columns}\")\n\n    self.log.important(f\"Setting Display Columns...{diplay_columns}\")\n    from workbench.core.views import DisplayView\n\n    # Create a NEW display view\n    DisplayView.create(self, source_table=c_view.table, column_list=diplay_columns)\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.smart_sample","title":"<code>smart_sample()</code>  <code>abstractmethod</code>","text":"<p>Get a SMART sample dataframe from this DataSource Returns:     pd.DataFrame: A combined DataFrame of sample data + outliers</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef smart_sample(self) -&gt; pd.DataFrame:\n    \"\"\"Get a SMART sample dataframe from this DataSource\n    Returns:\n        pd.DataFrame: A combined DataFrame of sample data + outliers\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.supplemental_data","title":"<code>supplemental_data()</code>","text":"<p>Return the supplemental data for this Data Source</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def supplemental_data(self) -&gt; list[str]:\n    \"\"\"Return the supplemental data for this Data Source\"\"\"\n    from workbench.core.views.view_utils import list_supplemental_data\n\n    return list_supplemental_data(self)\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.value_counts","title":"<code>value_counts()</code>  <code>abstractmethod</code>","text":"<p>Compute 'value_counts' for all the string columns in a DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of value counts for each column in the form  {'col1': {'value_1': X, 'value_2': Y, 'value_3': Z,...},   'col2': ...}</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>@abstractmethod\ndef value_counts(self) -&gt; dict[dict]:\n    \"\"\"Compute 'value_counts' for all the string columns in a DataSource\n\n    Returns:\n        dict(dict): A dictionary of value counts for each column in the form\n             {'col1': {'value_1': X, 'value_2': Y, 'value_3': Z,...},\n              'col2': ...}\n    \"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.view","title":"<code>view(view_name)</code>","text":"<p>Return a DataFrame for a specific view Args:     view_name (str): The name of the view to return Returns:     pd.DataFrame: A DataFrame for the specified view</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def view(self, view_name: str) -&gt; \"View\":\n    \"\"\"Return a DataFrame for a specific view\n    Args:\n        view_name (str): The name of the view to return\n    Returns:\n        pd.DataFrame: A DataFrame for the specified view\n    \"\"\"\n    from workbench.core.views import View\n\n    return View(self, view_name)\n</code></pre>"},{"location":"core_classes/artifacts/data_source_abstract/#workbench.core.artifacts.data_source_abstract.DataSourceAbstract.views","title":"<code>views()</code>","text":"<p>Return the views for this Data Source</p> Source code in <code>src/workbench/core/artifacts/data_source_abstract.py</code> <pre><code>def views(self) -&gt; list[str]:\n    \"\"\"Return the views for this Data Source\"\"\"\n    from workbench.core.views.view_utils import list_views\n\n    return list_views(self)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/","title":"EndpointCore","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the Endpoint API Class and voil\u00e0 it works the same.</p> <p>EndpointCore: Workbench EndpointCore Class</p>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore","title":"<code>EndpointCore</code>","text":"<p>               Bases: <code>Artifact</code></p> <p>EndpointCore: Workbench EndpointCore Class</p> Common Usage <pre><code>my_endpoint = EndpointCore(endpoint_name)\nprediction_df = my_endpoint.predict(test_df)\nmetrics = my_endpoint.regression_metrics(target_column, prediction_df)\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value:0.3f}\")\n</code></pre> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>class EndpointCore(Artifact):\n    \"\"\"EndpointCore: Workbench EndpointCore Class\n\n    Common Usage:\n        ```python\n        my_endpoint = EndpointCore(endpoint_name)\n        prediction_df = my_endpoint.predict(test_df)\n        metrics = my_endpoint.regression_metrics(target_column, prediction_df)\n        for metric, value in metrics.items():\n            print(f\"{metric}: {value:0.3f}\")\n        ```\n    \"\"\"\n\n    def __init__(self, endpoint_name, **kwargs):\n        \"\"\"EndpointCore Initialization\n\n        Args:\n            endpoint_name (str): Name of Endpoint in Workbench\n        \"\"\"\n\n        # Make sure the endpoint_name is a valid name\n        self.is_name_valid(endpoint_name, delimiter=\"-\", lower_case=False)\n\n        # Call SuperClass Initialization\n        super().__init__(endpoint_name, **kwargs)\n\n        # Grab an Cloud Metadata object and pull information for Endpoints\n        self.endpoint_name = endpoint_name\n        self.endpoint_meta = self.meta.endpoint(self.endpoint_name)\n\n        # Sanity check that we found the endpoint\n        if self.endpoint_meta is None:\n            self.log.important(f\"Could not find endpoint {self.name} within current visibility scope\")\n            return\n\n        # Sanity check the Endpoint state\n        if self.endpoint_meta[\"EndpointStatus\"] == \"Failed\":\n            self.log.critical(f\"Endpoint {self.name} is in a failed state\")\n            reason = self.endpoint_meta[\"FailureReason\"]\n            self.log.critical(f\"Failure Reason: {reason}\")\n            self.log.critical(\"Please delete this endpoint and re-deploy...\")\n\n        # Set the Inference, Capture, and Monitoring S3 Paths\n        base_endpoint_path = f\"{self.endpoints_s3_path}/{self.name}\"\n        self.endpoint_inference_path = f\"{base_endpoint_path}/inference\"\n        self.endpoint_data_capture_path = f\"{base_endpoint_path}/data_capture\"\n        self.endpoint_monitoring_path = f\"{base_endpoint_path}/monitoring\"\n\n        # Set the Model Name\n        self.model_name = self.get_input()\n\n        # This is for endpoint error handling later\n        self.endpoint_return_columns = None\n\n        # We temporary cache the endpoint metrics\n        self.temp_storage = Cache(prefix=\"temp_storage\", expire=300)  # 5 minutes\n\n        # Call SuperClass Post Initialization\n        super().__post_init__()\n\n        # All done\n        self.log.info(f\"EndpointCore Initialized: {self.endpoint_name}\")\n\n    def refresh_meta(self):\n        \"\"\"Refresh the Artifact's metadata\"\"\"\n        self.endpoint_meta = self.meta.endpoint(self.endpoint_name)\n\n    def exists(self) -&gt; bool:\n        \"\"\"Does the feature_set_name exist in the AWS Metadata?\"\"\"\n        if self.endpoint_meta is None:\n            self.log.debug(f\"Endpoint {self.endpoint_name} not found in AWS Metadata\")\n            return False\n        return True\n\n    def health_check(self, deep: bool = False) -&gt; list[str]:\n        \"\"\"Perform a health check on this endpoint\n\n        Args:\n            deep (bool): If True, perform more extensive (expensive) health checks (default: False)\n\n        Returns:\n            list[str]: List of health issues\n        \"\"\"\n        if not self.ready():\n            return [\"needs_onboard\"]\n\n        # Call the base class health check\n        health_issues = super().health_check(deep=deep)\n\n        # Does this endpoint have a config?\n        # Note: This is not an authoritative check, so improve later\n        if self.endpoint_meta.get(\"ProductionVariants\") is None:\n            health_issues.append(\"no_config\")\n\n        # Deep checks (expensive CloudWatch metrics call)\n        if deep:\n            # We're going to check for 5xx errors and no activity\n            endpoint_metrics = self.endpoint_metrics()\n\n            # Check if we have metrics\n            if endpoint_metrics is None:\n                health_issues.append(\"unknown_error\")\n                return health_issues\n\n            # Check for 5xx errors\n            num_errors = endpoint_metrics[\"Invocation5XXErrors\"].sum()\n            if num_errors &gt; 5:\n                health_issues.append(\"5xx_errors\")\n            elif num_errors &gt; 0:\n                health_issues.append(\"5xx_errors_min\")\n            else:\n                self.remove_health_tag(\"5xx_errors\")\n                self.remove_health_tag(\"5xx_errors_min\")\n\n            # Check for Endpoint activity\n            num_invocations = endpoint_metrics[\"Invocations\"].sum()\n            if num_invocations == 0:\n                health_issues.append(\"no_activity\")\n            else:\n                self.remove_health_tag(\"no_activity\")\n\n        return health_issues\n\n    def is_serverless(self) -&gt; bool:\n        \"\"\"Check if the current endpoint is serverless.\n\n        Returns:\n            bool: True if the endpoint is serverless, False otherwise.\n        \"\"\"\n        return \"Serverless\" in self.endpoint_meta[\"InstanceType\"]\n\n    def data_capture(self):\n        \"\"\"Get the MonitorCore class for this endpoint\"\"\"\n        from workbench.core.artifacts.data_capture_core import DataCaptureCore\n\n        return DataCaptureCore(self.endpoint_name)\n\n    def enable_data_capture(self):\n        \"\"\"Add data capture to the endpoint\"\"\"\n        self.data_capture().enable()\n\n    def monitor(self):\n        \"\"\"Get the MonitorCore class for this endpoint\"\"\"\n        from workbench.core.artifacts.monitor_core import MonitorCore\n\n        return MonitorCore(self.endpoint_name)\n\n    def size(self) -&gt; float:\n        \"\"\"Return the size of this data in MegaBytes\"\"\"\n        return 0.0\n\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n        return self.endpoint_meta\n\n    def arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n        return self.endpoint_meta[\"EndpointArn\"]\n\n    def aws_url(self):\n        \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n        return f\"https://{self.aws_region}.console.aws.amazon.com/athena/home\"\n\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        return self.endpoint_meta[\"CreationTime\"]\n\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        return self.endpoint_meta[\"LastModifiedTime\"]\n\n    def model_data_url(self) -&gt; Optional[str]:\n        \"\"\"Return the model data URL for this endpoint\n\n        Returns:\n            Optional[str]: The model data URL for this endpoint\n        \"\"\"\n        from workbench.utils.endpoint_utils import internal_model_data_url  # Avoid circular import\n\n        return internal_model_data_url(self.endpoint_config_name(), self.boto3_session)\n\n    def hash(self) -&gt; Optional[str]:\n        \"\"\"Return the hash for the internal model used by this endpoint\n\n        Returns:\n            Optional[str]: The hash for the internal model used by this endpoint\n        \"\"\"\n        model_url = self.model_data_url()\n        return compute_s3_object_hash(model_url, self.boto3_session)\n\n    @property\n    def instance_type(self) -&gt; str:\n        \"\"\"Return the instance type for this endpoint\"\"\"\n        return self.endpoint_meta[\"InstanceType\"]\n\n    def endpoint_metrics(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Return the metrics for this endpoint\n\n        Returns:\n            pd.DataFrame: DataFrame with the metrics for this endpoint (or None if no metrics)\n        \"\"\"\n\n        # Do we have it cached?\n        metrics_key = f\"endpoint:{self.name}:endpoint_metrics\"\n        endpoint_metrics = self.temp_storage.get(metrics_key)\n        if endpoint_metrics is not None:\n            return endpoint_metrics\n\n        # We don't have it cached so let's get it from CloudWatch\n        if \"ProductionVariants\" not in self.endpoint_meta:\n            return None\n        self.log.important(\"Updating endpoint metrics...\")\n        variant = self.endpoint_meta[\"ProductionVariants\"][0][\"VariantName\"]\n        endpoint_metrics = EndpointMetrics().get_metrics(self.name, variant=variant)\n        self.temp_storage.set(metrics_key, endpoint_metrics)\n        return endpoint_metrics\n\n    def details(self) -&gt; dict:\n        \"\"\"Additional Details about this Endpoint\n\n        Returns:\n            dict(dict): A dictionary of details about this Endpoint\n        \"\"\"\n\n        # Fill in all the details about this Endpoint\n        details = self.summary()\n\n        # Get details from our AWS Metadata\n        details[\"status\"] = self.endpoint_meta[\"EndpointStatus\"]\n        details[\"instance\"] = self.endpoint_meta[\"InstanceType\"]\n        try:\n            details[\"instance_count\"] = self.endpoint_meta[\"ProductionVariants\"][0][\"CurrentInstanceCount\"] or \"-\"\n        except KeyError:\n            details[\"instance_count\"] = \"-\"\n        if \"ProductionVariants\" in self.endpoint_meta:\n            details[\"variant\"] = self.endpoint_meta[\"ProductionVariants\"][0][\"VariantName\"]\n        else:\n            details[\"variant\"] = \"-\"\n\n        # Add endpoint metrics from CloudWatch\n        details[\"endpoint_metrics\"] = self.endpoint_metrics()\n\n        # Return the details\n        return details\n\n    def is_monitored(self) -&gt; bool:\n        \"\"\"Is monitoring enabled for this Endpoint?\n\n        Returns:\n            True if monitoring is enabled, False otherwise.\n        \"\"\"\n        try:\n            response = self.sm_client.list_monitoring_schedules(EndpointName=self.name)\n            return bool(response.get(\"MonitoringScheduleSummaries\", []))\n        except ClientError:\n            return False\n\n    def onboard(self, interactive: bool = False) -&gt; bool:\n        \"\"\"This is a BLOCKING method that will onboard the Endpoint (make it ready)\n        Args:\n            interactive (bool, optional): If True, will prompt the user for information. (default: False)\n        Returns:\n            bool: True if the Endpoint is successfully onboarded, False otherwise\n        \"\"\"\n\n        # Make sure our input is defined\n        if self.get_input() == \"unknown\":\n            if interactive:\n                input_model = input(\"Input Model?: \")\n            else:\n                self.log.critical(\"Input Model is not defined!\")\n                return False\n        else:\n            input_model = self.get_input()\n\n        # Now that we have the details, let's onboard the Endpoint with args\n        return self.onboard_with_args(input_model)\n\n    def onboard_with_args(self, input_model: str) -&gt; bool:\n        \"\"\"Onboard the Endpoint with the given arguments\n\n        Args:\n            input_model (str): The input model for this endpoint\n        Returns:\n            bool: True if the Endpoint is successfully onboarded, False otherwise\n        \"\"\"\n        # Set the status to onboarding\n        self.set_status(\"onboarding\")\n\n        self.upsert_workbench_meta({\"workbench_input\": input_model})\n        self.model_name = input_model\n\n        # Remove the needs_onboard tag\n        self.remove_health_tag(\"needs_onboard\")\n        self.set_status(\"ready\")\n\n        # Run a health check and refresh the meta\n        time.sleep(2)  # Give the AWS Metadata a chance to update\n        self.health_check(deep=True)\n        self.refresh_meta()\n        self.details()\n        return True\n\n    def auto_inference(self) -&gt; pd.DataFrame:\n        \"\"\"Run inference on the endpoint using the test data from the model training view\"\"\"\n\n        # Sanity Check that we have a model\n        model = ModelCore(self.get_input())\n        if not model.exists():\n            self.log.error(\"No model found for this endpoint. Returning empty DataFrame.\")\n            return pd.DataFrame()\n\n        # Grab the evaluation data from the Model's training view\n        all_df = model.training_view().pull_dataframe()\n        eval_df = all_df[~all_df[\"training\"]]\n\n        # Remove AWS created columns\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n        eval_df = eval_df.drop(columns=aws_cols, errors=\"ignore\")\n\n        # Run inference\n        return self.inference(eval_df, \"auto_inference\")\n\n    def full_inference(self) -&gt; pd.DataFrame:\n        \"\"\"Run inference on the endpoint using all the data from the model training view\"\"\"\n\n        # Sanity Check that we have a model\n        model = ModelCore(self.get_input())\n        if not model.exists():\n            self.log.error(\"No model found for this endpoint. Returning empty DataFrame.\")\n            return pd.DataFrame()\n\n        # Grab the full data from the Model's training view\n        eval_df = model.training_view().pull_dataframe()\n\n        # Remove AWS created columns\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n        eval_df = eval_df.drop(columns=aws_cols, errors=\"ignore\")\n\n        # Run inference\n        return self.inference(eval_df, \"full_inference\")\n\n    def inference(\n        self,\n        eval_df: pd.DataFrame,\n        capture_name: str = None,\n        id_column: str = None,\n        drop_error_rows: bool = False,\n        include_quantiles: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Run inference on the Endpoint using the provided DataFrame\n\n        Args:\n            eval_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features)\n            capture_name (str, optional): Name of the inference capture (default=None)\n            id_column (str, optional): Name of the ID column (default=None)\n            drop_error_rows (bool, optional): If True, drop rows that had endpoint errors/issues (default=False)\n            include_quantiles (bool): Include q_* quantile columns in saved output (default: False)\n\n        Returns:\n            pd.DataFrame: DataFrame with the inference results\n\n        Note:\n            If capture=True inference/performance metrics are written to S3 Endpoint Inference Folder\n        \"\"\"\n\n        # Check if this is a 'floating endpoint' (no model)\n        if self.get_input() == \"unknown\":\n            self.log.important(\"No model associated with this endpoint, running 'no frills' inference...\")\n            return self.fast_inference(eval_df)\n\n        # Grab the model features and target column\n        model = ModelCore(self.model_name)\n        features = model.features()\n        targets = model.target()  # Note: We have multi-target models (so this could be a list)\n\n        # Run predictions on the evaluation data\n        prediction_df = self._predict(eval_df, features, drop_error_rows)\n        if prediction_df.empty:\n            self.log.warning(\"No predictions were made. Returning empty DataFrame.\")\n            return prediction_df\n\n        # Normalize targets to handle both string and list formats\n        if isinstance(targets, list):\n            primary_target = targets[0] if targets else None\n        else:\n            primary_target = targets\n\n        # Sanity Check that the target column is present\n        if primary_target not in prediction_df.columns:\n            self.log.important(f\"Target Column {primary_target} not found in prediction_df!\")\n            self.log.important(\"In order to compute metrics, the target column must be present!\")\n            metrics = pd.DataFrame()\n\n        # Compute the standard performance metrics for this model\n        else:\n            if model.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n                prediction_df = self.residuals(primary_target, prediction_df)\n                metrics = self.regression_metrics(primary_target, prediction_df)\n            elif model.model_type == ModelType.CLASSIFIER:\n                metrics = self.classification_metrics(primary_target, prediction_df)\n            else:\n                # For other model types, we don't compute metrics\n                self.log.info(f\"Model Type: {model.model_type} doesn't have metrics...\")\n                metrics = pd.DataFrame()\n\n        # Print out the metrics\n        print(f\"Performance Metrics for {self.model_name} on {self.name}\")\n        print(metrics.head())\n\n        # Capture the inference results and metrics\n        if primary_target and capture_name:\n\n            # If we don't have an id_column, we'll pull it from the model's FeatureSet\n            if id_column is None:\n                fs = FeatureSetCore(model.get_input())\n                id_column = fs.id_column\n\n            # Normalize targets to a list for iteration\n            target_list = targets if isinstance(targets, list) else [targets]\n            primary_target = target_list[0]\n\n            # For single-target models (99% of cases), just save with capture_name\n            # For multi-target models, save each as {prefix}_{target} plus primary as capture_name\n            is_multi_target = len(target_list) &gt; 1\n\n            if is_multi_target:\n                prefix = \"auto\" if capture_name == \"auto_inference\" else capture_name\n\n            for target in target_list:\n                # Drop rows with NaN target values for metrics/plots\n                target_df = prediction_df.dropna(subset=[target])\n\n                # For multi-target models, prediction column is {target}_pred, otherwise \"prediction\"\n                pred_col = f\"{target}_pred\" if is_multi_target else \"prediction\"\n\n                # Compute per-target metrics\n                if model.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n                    target_metrics = self.regression_metrics(target, target_df, prediction_col=pred_col)\n                elif model.model_type == ModelType.CLASSIFIER:\n                    target_metrics = self.classification_metrics(target, target_df, prediction_col=pred_col)\n                else:\n                    target_metrics = pd.DataFrame()\n\n                if is_multi_target:\n                    # Multi-target: save as {prefix}_{target}\n                    target_capture_name = f\"{prefix}_{target}\"\n                    description = target_capture_name.replace(\"_\", \" \").title()\n                    self._capture_inference_results(\n                        target_capture_name,\n                        target_df,\n                        target,\n                        model.model_type,\n                        target_metrics,\n                        description,\n                        features,\n                        id_column,\n                        include_quantiles,\n                    )\n\n                # Save primary target (or single target) with original capture_name\n                if target == primary_target:\n                    self._capture_inference_results(\n                        capture_name,\n                        target_df,\n                        target,\n                        model.model_type,\n                        target_metrics,\n                        capture_name.replace(\"_\", \" \").title(),\n                        features,\n                        id_column,\n                        include_quantiles,\n                    )\n\n            # Capture uncertainty metrics if prediction_std is available (UQ, ChemProp, etc.)\n            if \"prediction_std\" in prediction_df.columns:\n                metrics = uq_metrics(prediction_df, primary_target)\n                self.param_store.upsert(f\"/workbench/models/{model.name}/inference/{capture_name}\", metrics)\n\n        # Return the prediction DataFrame\n        return prediction_df\n\n    def cross_fold_inference(self, include_quantiles: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Pull cross-fold inference training results for this Endpoint's model\n\n        Args:\n            include_quantiles (bool): Include q_* quantile columns in saved output (default: False)\n\n        Returns:\n            pd.DataFrame: A DataFrame with cross fold predictions\n        \"\"\"\n\n        # Grab our model\n        model = ModelCore(self.model_name)\n\n        # Compute CrossFold (Metrics and Prediction Dataframe)\n        # For PyTorch and ChemProp, pull pre-computed CV results from training\n        if model.model_framework in [ModelFramework.UNKNOWN, ModelFramework.XGBOOST]:\n            cross_fold_metrics, out_of_fold_df = xgboost_pull_cv(model)\n        elif model.model_framework == ModelFramework.PYTORCH:\n            cross_fold_metrics, out_of_fold_df = pytorch_pull_cv(model)\n        elif model.model_framework == ModelFramework.CHEMPROP:\n            cross_fold_metrics, out_of_fold_df = chemprop_pull_cv(model)\n        else:\n            self.log.error(f\"Cross-Fold Inference not supported for Model Framework: {model.model_framework}.\")\n            return pd.DataFrame()\n\n        # If the metrics dataframe isn't empty save to the param store\n        if not cross_fold_metrics.empty:\n            # Convert to list of dictionaries\n            metrics = cross_fold_metrics.to_dict(orient=\"records\")\n            self.param_store.upsert(f\"/workbench/models/{model.name}/inference/cross_fold\", metrics)\n\n        # If the out_of_fold_df is empty return it\n        if out_of_fold_df.empty:\n            self.log.warning(\"No out-of-fold predictions were made. Returning empty DataFrame.\")\n            return out_of_fold_df\n\n        # Capture the results\n        targets = model.target()  # Note: We have multi-target models (so this could be a list)\n        model_type = model.model_type\n\n        # Get the id_column from the model's FeatureSet\n        fs = FeatureSetCore(model.get_input())\n        id_column = fs.id_column\n\n        # Normalize targets to a list for iteration\n        target_list = targets if isinstance(targets, list) else [targets]\n        primary_target = target_list[0]\n\n        # If we don't have a smiles column, try to merge it from the FeatureSet\n        if \"smiles\" not in out_of_fold_df.columns:\n            fs_df = fs.query(f'SELECT {fs.id_column}, \"smiles\" FROM \"{fs.athena_table}\"')\n            if \"smiles\" in fs_df.columns:\n                self.log.info(\"Merging 'smiles' column from FeatureSet into out-of-fold predictions.\")\n                out_of_fold_df = out_of_fold_df.merge(fs_df, on=fs.id_column, how=\"left\")\n\n        # Collect UQ columns (q_*, confidence) for additional tracking (used for hashing)\n        additional_columns = [col for col in out_of_fold_df.columns if col.startswith(\"q_\") or col == \"confidence\"]\n        if additional_columns:\n            self.log.info(f\"UQ columns from training: {', '.join(additional_columns)}\")\n\n        # Capture uncertainty metrics if prediction_std is available (UQ, ChemProp, etc.)\n        if \"prediction_std\" in out_of_fold_df.columns:\n            metrics = uq_metrics(out_of_fold_df, primary_target)\n            self.param_store.upsert(f\"/workbench/models/{model.name}/inference/full_cross_fold\", metrics)\n\n        # For single-target models (99% of cases), just save as \"full_cross_fold\"\n        # For multi-target models, save each as cv_{target} plus primary as \"full_cross_fold\"\n        is_multi_target = len(target_list) &gt; 1\n        for target in target_list:\n            # Drop rows with NaN target values for metrics/plots\n            target_df = out_of_fold_df.dropna(subset=[target])\n\n            # For multi-target models, prediction column is {target}_pred, otherwise \"prediction\"\n            pred_col = f\"{target}_pred\" if is_multi_target else \"prediction\"\n\n            # Compute per-target metrics\n            if model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n                target_metrics = self.regression_metrics(target, target_df, prediction_col=pred_col)\n            elif model_type == ModelType.CLASSIFIER:\n                target_metrics = self.classification_metrics(target, target_df, prediction_col=pred_col)\n            else:\n                target_metrics = pd.DataFrame()\n\n            if is_multi_target:\n                # Multi-target: save as cv_{target}\n                capture_name = f\"cv_{target}\"\n                description = capture_name.replace(\"_\", \" \").title()\n                self._capture_inference_results(\n                    capture_name,\n                    target_df,\n                    target,\n                    model_type,\n                    target_metrics,\n                    description,\n                    features=additional_columns,\n                    id_column=id_column,\n                    include_quantiles=include_quantiles,\n                )\n\n            # Save primary target (or single target) as \"full_cross_fold\"\n            if target == primary_target:\n                self._capture_inference_results(\n                    \"full_cross_fold\",\n                    target_df,\n                    target,\n                    model_type,\n                    target_metrics,\n                    \"Full Cross Fold\",\n                    features=additional_columns,\n                    id_column=id_column,\n                    include_quantiles=include_quantiles,\n                )\n\n        return out_of_fold_df\n\n    def fast_inference(self, eval_df: pd.DataFrame, threads: int = 4) -&gt; pd.DataFrame:\n        \"\"\"Run inference on the Endpoint using the provided DataFrame\n\n        Args:\n            eval_df (pd.DataFrame): The DataFrame to run predictions on\n            threads (int): The number of threads to use (default: 4)\n\n        Returns:\n            pd.DataFrame: The DataFrame with predictions\n\n        Note:\n            There's no sanity checks or error handling... just FAST Inference!\n        \"\"\"\n        return fast_inference(self.name, eval_df, self.sm_session, threads=threads)\n\n    def _predict(self, eval_df: pd.DataFrame, features: list[str], drop_error_rows: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Internal: Run prediction on observations in the given DataFrame\n\n        Args:\n            eval_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features)\n            features (list[str]): List of feature column names needed for prediction\n            drop_error_rows (bool): If True, drop rows that had endpoint errors/issues (default=False)\n        Returns:\n            pd.DataFrame: Return the DataFrame with additional columns, prediction and any _proba columns\n        \"\"\"\n\n        # Sanity check: Does the DataFrame have 0 rows?\n        if eval_df.empty:\n            self.log.warning(\"Evaluation DataFrame has 0 rows. No predictions to run.\")\n            return pd.DataFrame(columns=eval_df.columns)  # Return empty DataFrame with same structure\n\n        # Sanity check: Does the DataFrame have the required features?\n        df_columns_lower = set(col.lower() for col in eval_df.columns)\n        features_lower = set(feature.lower() for feature in features)\n        if not features_lower.issubset(df_columns_lower):\n            missing_features = features_lower - df_columns_lower\n            raise ValueError(f\"DataFrame does not contain required features: {missing_features}\")\n\n        # Create our Endpoint Predictor Class\n        predictor = Predictor(\n            self.endpoint_name,\n            sagemaker_session=self.sm_session,\n            serializer=CSVSerializer(),\n            deserializer=CSVDeserializer(),\n        )\n\n        # Get batch size from endpoint metadata (default 100)\n        # Some endpoints (e.g., 3D descriptor generation) need smaller batches due to processing time\n        batch_size = self.workbench_meta().get(\"inference_batch_size\", 100)\n\n        # Now split up the dataframe into chunks, send those chunks to our\n        # endpoint (with error handling) and stitch all the chunks back together\n        df_list = []\n        total_rows = len(eval_df)\n        for index in range(0, len(eval_df), batch_size):\n            self.log.info(f\"Processing {index}:{min(index+batch_size, total_rows)} out of {total_rows} rows...\")\n\n            # Compute partial DataFrames, add them to a list, and concatenate at the end\n            partial_df = self._endpoint_error_handling(predictor, eval_df[index : index + batch_size], drop_error_rows)\n            df_list.append(partial_df)\n\n        # Concatenate the dataframes\n        combined_df = pd.concat(df_list, ignore_index=True)\n\n        # Some endpoints will put in \"N/A\" values (for CSV serialization)\n        # We need to convert these to NaN and the run the conversions below\n        # Report on the number of N/A values in each column in the DataFrame\n        # For any cound above 0 list the column name and the number of N/A values\n        na_counts = combined_df.isin([\"N/A\"]).sum()\n        for column, count in na_counts.items():\n            if count &gt; 0:\n                self.log.warning(f\"{column} has {count} N/A values, converting to NaN\")\n        pd.set_option(\"future.no_silent_downcasting\", True)\n        combined_df = combined_df.replace(\"N/A\", float(\"nan\"))\n\n        # Convert data to numeric\n        # Note: Since we're using CSV serializers numeric columns often get changed to generic 'object' types\n\n        # Hard Conversion\n        # Note: We explicitly catch exceptions for columns that cannot be converted to numeric\n        converted_df = combined_df.copy()\n        for column in combined_df.columns:\n            try:\n                converted_df[column] = pd.to_numeric(combined_df[column])\n            except ValueError:\n                # If a ValueError is raised, the column cannot be converted to numeric, so we keep it as is\n                pass\n            except TypeError:\n                # This typically means a duplicated column name, so confirm duplicate (more than 1) and log it\n                column_count = (converted_df.columns == column).sum()\n                self.log.critical(f\"{column} occurs {column_count} times in the DataFrame.\")\n                pass\n\n        # Soft Conversion\n        # Convert columns to the best possible dtype that supports the pd.NA missing value.\n        converted_df = converted_df.convert_dtypes()\n\n        # Convert pd.NA placeholders to pd.NA\n        # Note: CSV serialization converts pd.NA to blank strings, so we have to put in placeholders\n        converted_df.replace(\"__NA__\", pd.NA, inplace=True)\n\n        # Check for True/False values in the string columns\n        for column in converted_df.select_dtypes(include=[\"string\"]).columns:\n            if converted_df[column].str.lower().isin([\"true\", \"false\"]).all():\n                converted_df[column] = converted_df[column].str.lower().map({\"true\": True, \"false\": False})\n\n        # Return the Dataframe\n        return converted_df\n\n    def _endpoint_error_handling(self, predictor, feature_df, drop_error_rows: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Internal: Handles errors, retries, and binary search for problematic rows.\n\n        Args:\n            predictor (Predictor): The SageMaker Predictor object\n            feature_df (pd.DataFrame): DataFrame to run predictions on\n            drop_error_rows (bool): If True, drop rows that had endpoint errors/issues (default=False)\n        Returns:\n            pd.DataFrame: DataFrame with predictions (NaNs for problematic rows or dropped rows if specified)\n        \"\"\"\n\n        # Sanity check: Does the DataFrame have 0 rows?\n        if feature_df.empty:\n            self.log.warning(\"DataFrame has 0 rows. No predictions to run.\")\n            return pd.DataFrame(columns=feature_df.columns)\n\n        # Convert DataFrame into a CSV buffer\n        csv_buffer = StringIO()\n        feature_df.to_csv(csv_buffer, index=False)\n\n        try:\n            # Send CSV buffer to the predictor and process results\n            results = predictor.predict(csv_buffer.getvalue())\n            results_df = pd.DataFrame.from_records(results[1:], columns=results[0])\n            self.endpoint_return_columns = results_df.columns.tolist()\n            return results_df\n\n        except botocore.exceptions.ClientError as err:\n            error_code = err.response[\"Error\"][\"Code\"]\n            if error_code == \"ModelNotReadyException\":\n                self.log.error(f\"Error {error_code}\")\n                self.log.error(err.response)\n                self.log.error(\"Model not ready. Sleeping and retrying...\")\n                time.sleep(60)\n                return self._endpoint_error_handling(predictor, feature_df)\n\n            elif error_code == \"ModelError\":\n                # Log full error response to capture all available debugging info\n                self.log.error(f\"Error {error_code}\")\n                self.log.error(err.response)\n                self.log.warning(\"Bisecting the DataFrame and retrying...\")\n\n                # Base case: single row handling\n                if len(feature_df) == 1:\n                    if not self.endpoint_return_columns:\n                        raise\n                    self.log.warning(f\"Endpoint Inference failed on: {feature_df}\")\n                    if drop_error_rows:\n                        self.log.warning(\"Dropping rows with endpoint errors...\")\n                        return pd.DataFrame(columns=feature_df.columns)\n                    # Fill with NaNs for inference columns, keeping original feature data\n                    self.log.warning(\"Filling with NaNs for inference columns...\")\n                    return self._fill_with_nans(feature_df)\n\n                # Binary search for problematic rows\n                mid_point = len(feature_df) // 2\n                self.log.info(f\"Bisect DataFrame: 0 -&gt; {mid_point} and {mid_point} -&gt; {len(feature_df)}\")\n                first_half = self._endpoint_error_handling(predictor, feature_df.iloc[:mid_point], drop_error_rows)\n                second_half = self._endpoint_error_handling(predictor, feature_df.iloc[mid_point:], drop_error_rows)\n                return pd.concat([first_half, second_half], ignore_index=True)\n\n            else:\n                self.log.critical(f\"Unexpected ClientError: {error_code}\")\n                self.log.critical(err.response)\n                raise\n\n        except Exception as err:\n            self.log.critical(f\"Unexpected general error: {err}\")\n            raise\n\n    def _fill_with_nans(self, feature_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Internal: Fill a single-row DataFrame with NaNs for inference columns, keeping original feature data.\"\"\"\n\n        # Create a single-row DataFrame with NaNs, ensuring dtype=object to prevent type downcasting\n        one_row_df_with_nans = pd.DataFrame({col: [np.nan] for col in self.endpoint_return_columns}, dtype=object)\n\n        # Check if feature_df is not empty and has at least one row\n        if not feature_df.empty:\n            # Copy values from the input DataFrame for overlapping columns\n            for column in set(feature_df.columns).intersection(self.endpoint_return_columns):\n                # Use .iloc[0] to access the first row by position, regardless of the index\n                one_row_df_with_nans.at[0, column] = feature_df.iloc[0][column]\n\n        return one_row_df_with_nans\n\n    @staticmethod\n    def _hash_dataframe(df: pd.DataFrame, hash_length: int = 8):\n        # Internal: Compute a data hash for the dataframe\n        if df.empty:\n            return \"--hash--\"\n\n        # Sort the dataframe by columns to ensure consistent ordering\n        df = df.copy()\n        df = df.sort_values(by=sorted(df.columns.tolist()))\n        row_hashes = pd.util.hash_pandas_object(df, index=False)\n        combined = row_hashes.values.tobytes()\n        return hashlib.md5(combined).hexdigest()[:hash_length]\n\n    def _capture_inference_results(\n        self,\n        capture_name: str,\n        pred_results_df: pd.DataFrame,\n        target: str,\n        model_type: ModelType,\n        metrics: pd.DataFrame,\n        description: str,\n        features: list,\n        id_column: str = None,\n        include_quantiles: bool = False,\n    ):\n        \"\"\"Internal: Capture the inference results and metrics to S3 for a single target\n\n        Args:\n            capture_name (str): Name of the inference capture\n            pred_results_df (pd.DataFrame): DataFrame with the prediction results\n            target (str): Target column name\n            model_type (ModelType): Type of the model (e.g. REGRESSOR, CLASSIFIER)\n            metrics (pd.DataFrame): DataFrame with the performance metrics\n            description (str): Description of the inference results\n            features (list): List of features to include in the inference results\n            id_column (str, optional): Name of the ID column (default=None)\n            include_quantiles (bool): Include q_* quantile columns in output (default: False)\n        \"\"\"\n\n        # Compute a dataframe hash (just use the last 8)\n        data_hash = self._hash_dataframe(pred_results_df[features])\n\n        # Metadata for the model inference\n        inference_meta = {\n            \"name\": capture_name,\n            \"data_hash\": data_hash,\n            \"num_rows\": len(pred_results_df),\n            \"description\": description,\n        }\n\n        # Create the S3 Path for the Inference Capture\n        inference_capture_path = f\"{self.endpoint_inference_path}/{capture_name}\"\n\n        # Write the metadata dictionary and metrics to our S3 Model Inference Folder\n        wr.s3.to_json(\n            pd.DataFrame([inference_meta]),\n            f\"{inference_capture_path}/inference_meta.json\",\n            index=False,\n        )\n        self.log.info(f\"Writing metrics to {inference_capture_path}/inference_metrics.csv\")\n        wr.s3.to_csv(metrics, f\"{inference_capture_path}/inference_metrics.csv\", index=False)\n\n        # Save the inference predictions for this target\n        self._save_target_inference(inference_capture_path, pred_results_df, target, id_column, include_quantiles)\n\n        # CLASSIFIER: Write the confusion matrix to our S3 Model Inference Folder\n        if model_type == ModelType.CLASSIFIER:\n            conf_mtx = self.generate_confusion_matrix(target, pred_results_df)\n            self.log.info(f\"Writing confusion matrix to {inference_capture_path}/inference_cm.csv\")\n            # Note: Unlike other dataframes here, we want to write the index (labels) to the CSV\n            wr.s3.to_csv(conf_mtx, f\"{inference_capture_path}/inference_cm.csv\", index=True)\n\n        # Now recompute the details for our Model\n        self.log.important(f\"Loading inference metrics for {self.model_name}...\")\n        model = ModelCore(self.model_name)\n        model._load_inference_metrics(capture_name)\n\n    def _save_target_inference(\n        self,\n        inference_capture_path: str,\n        pred_results_df: pd.DataFrame,\n        target: str,\n        id_column: str = None,\n        include_quantiles: bool = False,\n    ):\n        \"\"\"Save inference results for a single target.\n\n        Args:\n            inference_capture_path (str): S3 path for inference capture\n            pred_results_df (pd.DataFrame): DataFrame with prediction results\n            target (str): Target column name\n            id_column (str, optional): Name of the ID column\n            include_quantiles (bool): Include q_* quantile columns in output (default: False)\n        \"\"\"\n        cols = pred_results_df.columns\n\n        # Build output columns: id, target, prediction, prediction_std, UQ columns, proba columns\n        output_columns = []\n        if id_column and id_column in cols:\n            output_columns.append(id_column)\n        if target and target in cols:\n            output_columns.append(target)\n\n        output_columns += [c for c in [\"prediction\", \"prediction_std\"] if c in cols]\n\n        # Add confidence column (always include if present)\n        if \"confidence\" in cols:\n            output_columns.append(\"confidence\")\n\n        # Add quantile columns (q_*) only if requested\n        if include_quantiles:\n            output_columns += [c for c in cols if c.startswith(\"q_\")]\n\n        # Add proba columns for classifiers\n        output_columns += [c for c in cols if c.endswith(\"_proba\")]\n\n        # Add smiles column if present\n        if \"smiles\" in cols:\n            output_columns.append(\"smiles\")\n\n        # Write the predictions to S3\n        output_file = f\"{inference_capture_path}/inference_predictions.csv\"\n        self.log.info(f\"Writing predictions to {output_file}\")\n        wr.s3.to_csv(pred_results_df[output_columns], output_file, index=False)\n\n    def regression_metrics(\n        self, target_column: str, prediction_df: pd.DataFrame, prediction_col: str = \"prediction\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Compute the performance metrics for this Endpoint\n\n        Args:\n            target_column (str): Name of the target column\n            prediction_df (pd.DataFrame): DataFrame with the prediction results\n            prediction_col (str): Name of the prediction column (default: \"prediction\")\n\n        Returns:\n            pd.DataFrame: DataFrame with the performance metrics\n        \"\"\"\n        return compute_regression_metrics(prediction_df, target_column, prediction_col)\n\n    def residuals(self, target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Add the residuals to the prediction DataFrame\n        Args:\n            target_column (str): Name of the target column\n            prediction_df (pd.DataFrame): DataFrame with the prediction results\n        Returns:\n            pd.DataFrame: DataFrame with two new columns called 'residuals' and 'residuals_abs'\n        \"\"\"\n        # Check for prediction column\n        if \"prediction\" not in prediction_df.columns:\n            self.log.warning(\"No 'prediction' column found. Cannot compute residuals.\")\n            return prediction_df\n\n        y_true = prediction_df[target_column]\n        y_pred = prediction_df[\"prediction\"]\n\n        # Check for classification scenario\n        if not pd.api.types.is_numeric_dtype(y_true) or not pd.api.types.is_numeric_dtype(y_pred):\n            self.log.warning(\"Target and Prediction columns are not numeric. Computing 'diffs'...\")\n            prediction_df[\"residuals\"] = (y_true != y_pred).astype(int)\n            prediction_df[\"residuals_abs\"] = prediction_df[\"residuals\"]\n        else:\n            # Compute numeric residuals for regression\n            prediction_df[\"residuals\"] = y_true - y_pred\n            prediction_df[\"residuals_abs\"] = np.abs(prediction_df[\"residuals\"])\n\n        return prediction_df\n\n    def classification_metrics(\n        self, target_column: str, prediction_df: pd.DataFrame, prediction_col: str = \"prediction\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Compute the performance metrics for this Endpoint\n\n        Args:\n            target_column (str): Name of the target column\n            prediction_df (pd.DataFrame): DataFrame with the prediction results\n            prediction_col (str): Name of the prediction column (default: \"prediction\")\n\n        Returns:\n            pd.DataFrame: DataFrame with the performance metrics\n        \"\"\"\n        # Get class labels from the model (metrics_utils will infer if None)\n        class_labels = ModelCore(self.model_name).class_labels()\n        return compute_classification_metrics(prediction_df, target_column, class_labels, prediction_col)\n\n    def generate_confusion_matrix(self, target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Compute the confusion matrix for this Endpoint\n\n        Args:\n            target_column (str): Name of the target column\n            prediction_df (pd.DataFrame): DataFrame with the prediction results\n\n        Returns:\n            pd.DataFrame: DataFrame with the confusion matrix\n        \"\"\"\n        # Check for prediction column\n        if \"prediction\" not in prediction_df.columns:\n            self.log.warning(\"No 'prediction' column found in DataFrame\")\n            return pd.DataFrame()\n\n        # Drop rows with NaN predictions (can't include in confusion matrix)\n        nan_mask = prediction_df[\"prediction\"].isna()\n        if nan_mask.any():\n            n_nan = nan_mask.sum()\n            self.log.warning(f\"Dropping {n_nan} rows with NaN predictions for confusion matrix\")\n            prediction_df = prediction_df[~nan_mask].copy()\n\n        y_true = prediction_df[target_column]\n        y_pred = prediction_df[\"prediction\"]\n\n        # Get model class labels\n        model_class_labels = ModelCore(self.model_name).class_labels()\n\n        # Use model labels if available, otherwise infer from data\n        if model_class_labels:\n            self.log.important(\"Using model class labels for confusion matrix ordering...\")\n            labels = model_class_labels\n        else:\n            labels = sorted(list(set(y_true) | set(y_pred)))\n\n        # Compute confusion matrix and create DataFrame\n        conf_mtx = confusion_matrix(y_true, y_pred, labels=labels)\n        conf_mtx_df = pd.DataFrame(conf_mtx, index=labels, columns=labels)\n        conf_mtx_df.index.name = \"labels\"\n        return conf_mtx_df\n\n    def endpoint_config_name(self) -&gt; str:\n        # Grab the Endpoint Config Name from the AWS\n        details = self.sm_client.describe_endpoint(EndpointName=self.endpoint_name)\n        return details[\"EndpointConfigName\"]\n\n    def set_input(self, input: str, force=False):\n        \"\"\"Override: Set the input data for this artifact\n\n        Args:\n            input (str): Name of input for this artifact\n            force (bool, optional): Force the input to be set. Defaults to False.\n        Note:\n            We're going to not allow this to be used for Models\n        \"\"\"\n        if not force:\n            self.log.warning(f\"Endpoint {self.name}: Does not allow manual override of the input!\")\n            return\n\n        # Okay we're going to allow this to be set\n        self.log.important(f\"{self.name}: Setting input to {input}...\")\n        self.log.important(\"Be careful with this! It breaks automatic provenance of the artifact!\")\n        self.upsert_workbench_meta({\"workbench_input\": input})\n\n    def delete(self):\n        \"\"\"Delete an existing Endpoint: Underlying Models, Configuration, and Endpoint\"\"\"\n        if not self.exists():\n            self.log.warning(f\"Trying to delete an Endpoint that doesn't exist: {self.name}\")\n\n        # Remove this endpoint from the list of registered endpoints\n        self.log.info(f\"Removing {self.name} from the list of registered endpoints...\")\n        ModelCore(self.model_name).remove_endpoint(self.name)\n\n        # Call the Class Method to delete the Endpoint\n        EndpointCore.managed_delete(endpoint_name=self.name)\n\n    @classmethod\n    def managed_delete(cls, endpoint_name: str):\n        \"\"\"Delete the Endpoint and associated resources if it exists\"\"\"\n\n        # Check if the endpoint exists\n        try:\n            endpoint_info = cls.sm_client.describe_endpoint(EndpointName=endpoint_name)\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] in [\"ValidationException\", \"ResourceNotFound\"]:\n                cls.log.info(f\"Endpoint {endpoint_name} not found!\")\n                return\n            raise  # Re-raise unexpected errors\n\n        # Delete underlying models (Endpoints store/use models internally)\n        cls.delete_endpoint_models(endpoint_name)\n\n        # Get Endpoint Config Name and delete if exists\n        endpoint_config_name = endpoint_info[\"EndpointConfigName\"]\n        try:\n            cls.log.info(f\"Deleting Endpoint Config {endpoint_config_name}...\")\n            cls.sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n        except ClientError:\n            cls.log.info(f\"Endpoint Config {endpoint_config_name} not found...\")\n\n        # Delete any monitoring schedules associated with the endpoint\n        monitoring_schedules = cls.sm_client.list_monitoring_schedules(EndpointName=endpoint_name)[\n            \"MonitoringScheduleSummaries\"\n        ]\n        for schedule in monitoring_schedules:\n            cls.log.info(f\"Deleting Monitoring Schedule {schedule['MonitoringScheduleName']}...\")\n            cls.sm_client.delete_monitoring_schedule(MonitoringScheduleName=schedule[\"MonitoringScheduleName\"])\n\n        # Recursively delete all endpoint S3 artifacts (inference, etc)\n        # Note: We do not want to delete the data_capture/ files since these\n        #       might be used for collection and data drift analysis\n        base_endpoint_path = f\"{cls.endpoints_s3_path}/{endpoint_name}/\"\n        all_s3_objects = wr.s3.list_objects(base_endpoint_path, boto3_session=cls.boto3_session)\n\n        # Filter out objects that contain 'data_capture/' in their path\n        s3_objects_to_delete = [obj for obj in all_s3_objects if \"/data_capture/\" not in obj]\n        cls.log.info(f\"Found {len(all_s3_objects)} total objects at {base_endpoint_path}\")\n        cls.log.info(f\"Filtering out data_capture files, will delete {len(s3_objects_to_delete)} objects...\")\n        cls.log.info(f\"Objects to delete: {s3_objects_to_delete}\")\n\n        if s3_objects_to_delete:\n            wr.s3.delete_objects(s3_objects_to_delete, boto3_session=cls.boto3_session)\n            cls.log.info(f\"Successfully deleted {len(s3_objects_to_delete)} objects\")\n        else:\n            cls.log.info(\"No objects to delete (only data_capture files found)\")\n\n        # Delete any dataframes that were stored in the Dataframe Cache\n        cls.log.info(\"Deleting Dataframe Cache...\")\n        cls.df_cache.delete_recursive(endpoint_name)\n\n        # Delete the endpoint\n        cls.log.info(f\"Deleting Endpoint {endpoint_name}...\")\n        time.sleep(10)  # Allow AWS to catch up\n        try:\n            cls.sm_client.delete_endpoint(EndpointName=endpoint_name)\n        except ClientError as e:\n            cls.log.error(\"Error deleting endpoint.\")\n            raise e\n\n        time.sleep(10)  # Final sleep for AWS to fully register deletions\n\n    @classmethod\n    def delete_endpoint_models(cls, endpoint_name: str):\n        \"\"\"Delete the underlying Model for an Endpoint\n\n        Args:\n            endpoint_name (str): The name of the endpoint to delete\n        \"\"\"\n\n        # Grab the Endpoint Config Name from AWS\n        endpoint_config_name = cls.sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointConfigName\"]\n\n        # Retrieve the Model Names from the Endpoint Config\n        try:\n            endpoint_config = cls.sm_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n        except botocore.exceptions.ClientError:\n            cls.log.info(f\"Endpoint Config {endpoint_config_name} doesn't exist...\")\n            return\n        model_names = [variant[\"ModelName\"] for variant in endpoint_config[\"ProductionVariants\"]]\n        for model_name in model_names:\n            cls.log.info(f\"Deleting Internal Model {model_name}...\")\n            try:\n                cls.sm_client.delete_model(ModelName=model_name)\n            except botocore.exceptions.ClientError as error:\n                error_code = error.response[\"Error\"][\"Code\"]\n                error_message = error.response[\"Error\"][\"Message\"]\n                if error_code == \"ResourceInUse\":\n                    cls.log.warning(f\"Model {model_name} is still in use...\")\n                else:\n                    cls.log.warning(f\"Error: {error_code} - {error_message}\")\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.instance_type","title":"<code>instance_type</code>  <code>property</code>","text":"<p>Return the instance type for this endpoint</p>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.__init__","title":"<code>__init__(endpoint_name, **kwargs)</code>","text":"<p>EndpointCore Initialization</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>Name of Endpoint in Workbench</p> required Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def __init__(self, endpoint_name, **kwargs):\n    \"\"\"EndpointCore Initialization\n\n    Args:\n        endpoint_name (str): Name of Endpoint in Workbench\n    \"\"\"\n\n    # Make sure the endpoint_name is a valid name\n    self.is_name_valid(endpoint_name, delimiter=\"-\", lower_case=False)\n\n    # Call SuperClass Initialization\n    super().__init__(endpoint_name, **kwargs)\n\n    # Grab an Cloud Metadata object and pull information for Endpoints\n    self.endpoint_name = endpoint_name\n    self.endpoint_meta = self.meta.endpoint(self.endpoint_name)\n\n    # Sanity check that we found the endpoint\n    if self.endpoint_meta is None:\n        self.log.important(f\"Could not find endpoint {self.name} within current visibility scope\")\n        return\n\n    # Sanity check the Endpoint state\n    if self.endpoint_meta[\"EndpointStatus\"] == \"Failed\":\n        self.log.critical(f\"Endpoint {self.name} is in a failed state\")\n        reason = self.endpoint_meta[\"FailureReason\"]\n        self.log.critical(f\"Failure Reason: {reason}\")\n        self.log.critical(\"Please delete this endpoint and re-deploy...\")\n\n    # Set the Inference, Capture, and Monitoring S3 Paths\n    base_endpoint_path = f\"{self.endpoints_s3_path}/{self.name}\"\n    self.endpoint_inference_path = f\"{base_endpoint_path}/inference\"\n    self.endpoint_data_capture_path = f\"{base_endpoint_path}/data_capture\"\n    self.endpoint_monitoring_path = f\"{base_endpoint_path}/monitoring\"\n\n    # Set the Model Name\n    self.model_name = self.get_input()\n\n    # This is for endpoint error handling later\n    self.endpoint_return_columns = None\n\n    # We temporary cache the endpoint metrics\n    self.temp_storage = Cache(prefix=\"temp_storage\", expire=300)  # 5 minutes\n\n    # Call SuperClass Post Initialization\n    super().__post_init__()\n\n    # All done\n    self.log.info(f\"EndpointCore Initialized: {self.endpoint_name}\")\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.arn","title":"<code>arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for this artifact</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n    return self.endpoint_meta[\"EndpointArn\"]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.auto_inference","title":"<code>auto_inference()</code>","text":"<p>Run inference on the endpoint using the test data from the model training view</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def auto_inference(self) -&gt; pd.DataFrame:\n    \"\"\"Run inference on the endpoint using the test data from the model training view\"\"\"\n\n    # Sanity Check that we have a model\n    model = ModelCore(self.get_input())\n    if not model.exists():\n        self.log.error(\"No model found for this endpoint. Returning empty DataFrame.\")\n        return pd.DataFrame()\n\n    # Grab the evaluation data from the Model's training view\n    all_df = model.training_view().pull_dataframe()\n    eval_df = all_df[~all_df[\"training\"]]\n\n    # Remove AWS created columns\n    aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n    eval_df = eval_df.drop(columns=aws_cols, errors=\"ignore\")\n\n    # Run inference\n    return self.inference(eval_df, \"auto_inference\")\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.aws_meta","title":"<code>aws_meta()</code>","text":"<p>Get ALL the AWS metadata for this artifact</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def aws_meta(self) -&gt; dict:\n    \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n    return self.endpoint_meta\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.aws_url","title":"<code>aws_url()</code>","text":"<p>The AWS URL for looking at/querying this data source</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def aws_url(self):\n    \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n    return f\"https://{self.aws_region}.console.aws.amazon.com/athena/home\"\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.classification_metrics","title":"<code>classification_metrics(target_column, prediction_df, prediction_col='prediction')</code>","text":"<p>Compute the performance metrics for this Endpoint</p> <p>Parameters:</p> Name Type Description Default <code>target_column</code> <code>str</code> <p>Name of the target column</p> required <code>prediction_df</code> <code>DataFrame</code> <p>DataFrame with the prediction results</p> required <code>prediction_col</code> <code>str</code> <p>Name of the prediction column (default: \"prediction\")</p> <code>'prediction'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with the performance metrics</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def classification_metrics(\n    self, target_column: str, prediction_df: pd.DataFrame, prediction_col: str = \"prediction\"\n) -&gt; pd.DataFrame:\n    \"\"\"Compute the performance metrics for this Endpoint\n\n    Args:\n        target_column (str): Name of the target column\n        prediction_df (pd.DataFrame): DataFrame with the prediction results\n        prediction_col (str): Name of the prediction column (default: \"prediction\")\n\n    Returns:\n        pd.DataFrame: DataFrame with the performance metrics\n    \"\"\"\n    # Get class labels from the model (metrics_utils will infer if None)\n    class_labels = ModelCore(self.model_name).class_labels()\n    return compute_classification_metrics(prediction_df, target_column, class_labels, prediction_col)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.created","title":"<code>created()</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    return self.endpoint_meta[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.cross_fold_inference","title":"<code>cross_fold_inference(include_quantiles=False)</code>","text":"<p>Pull cross-fold inference training results for this Endpoint's model</p> <p>Parameters:</p> Name Type Description Default <code>include_quantiles</code> <code>bool</code> <p>Include q_* quantile columns in saved output (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with cross fold predictions</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def cross_fold_inference(self, include_quantiles: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Pull cross-fold inference training results for this Endpoint's model\n\n    Args:\n        include_quantiles (bool): Include q_* quantile columns in saved output (default: False)\n\n    Returns:\n        pd.DataFrame: A DataFrame with cross fold predictions\n    \"\"\"\n\n    # Grab our model\n    model = ModelCore(self.model_name)\n\n    # Compute CrossFold (Metrics and Prediction Dataframe)\n    # For PyTorch and ChemProp, pull pre-computed CV results from training\n    if model.model_framework in [ModelFramework.UNKNOWN, ModelFramework.XGBOOST]:\n        cross_fold_metrics, out_of_fold_df = xgboost_pull_cv(model)\n    elif model.model_framework == ModelFramework.PYTORCH:\n        cross_fold_metrics, out_of_fold_df = pytorch_pull_cv(model)\n    elif model.model_framework == ModelFramework.CHEMPROP:\n        cross_fold_metrics, out_of_fold_df = chemprop_pull_cv(model)\n    else:\n        self.log.error(f\"Cross-Fold Inference not supported for Model Framework: {model.model_framework}.\")\n        return pd.DataFrame()\n\n    # If the metrics dataframe isn't empty save to the param store\n    if not cross_fold_metrics.empty:\n        # Convert to list of dictionaries\n        metrics = cross_fold_metrics.to_dict(orient=\"records\")\n        self.param_store.upsert(f\"/workbench/models/{model.name}/inference/cross_fold\", metrics)\n\n    # If the out_of_fold_df is empty return it\n    if out_of_fold_df.empty:\n        self.log.warning(\"No out-of-fold predictions were made. Returning empty DataFrame.\")\n        return out_of_fold_df\n\n    # Capture the results\n    targets = model.target()  # Note: We have multi-target models (so this could be a list)\n    model_type = model.model_type\n\n    # Get the id_column from the model's FeatureSet\n    fs = FeatureSetCore(model.get_input())\n    id_column = fs.id_column\n\n    # Normalize targets to a list for iteration\n    target_list = targets if isinstance(targets, list) else [targets]\n    primary_target = target_list[0]\n\n    # If we don't have a smiles column, try to merge it from the FeatureSet\n    if \"smiles\" not in out_of_fold_df.columns:\n        fs_df = fs.query(f'SELECT {fs.id_column}, \"smiles\" FROM \"{fs.athena_table}\"')\n        if \"smiles\" in fs_df.columns:\n            self.log.info(\"Merging 'smiles' column from FeatureSet into out-of-fold predictions.\")\n            out_of_fold_df = out_of_fold_df.merge(fs_df, on=fs.id_column, how=\"left\")\n\n    # Collect UQ columns (q_*, confidence) for additional tracking (used for hashing)\n    additional_columns = [col for col in out_of_fold_df.columns if col.startswith(\"q_\") or col == \"confidence\"]\n    if additional_columns:\n        self.log.info(f\"UQ columns from training: {', '.join(additional_columns)}\")\n\n    # Capture uncertainty metrics if prediction_std is available (UQ, ChemProp, etc.)\n    if \"prediction_std\" in out_of_fold_df.columns:\n        metrics = uq_metrics(out_of_fold_df, primary_target)\n        self.param_store.upsert(f\"/workbench/models/{model.name}/inference/full_cross_fold\", metrics)\n\n    # For single-target models (99% of cases), just save as \"full_cross_fold\"\n    # For multi-target models, save each as cv_{target} plus primary as \"full_cross_fold\"\n    is_multi_target = len(target_list) &gt; 1\n    for target in target_list:\n        # Drop rows with NaN target values for metrics/plots\n        target_df = out_of_fold_df.dropna(subset=[target])\n\n        # For multi-target models, prediction column is {target}_pred, otherwise \"prediction\"\n        pred_col = f\"{target}_pred\" if is_multi_target else \"prediction\"\n\n        # Compute per-target metrics\n        if model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n            target_metrics = self.regression_metrics(target, target_df, prediction_col=pred_col)\n        elif model_type == ModelType.CLASSIFIER:\n            target_metrics = self.classification_metrics(target, target_df, prediction_col=pred_col)\n        else:\n            target_metrics = pd.DataFrame()\n\n        if is_multi_target:\n            # Multi-target: save as cv_{target}\n            capture_name = f\"cv_{target}\"\n            description = capture_name.replace(\"_\", \" \").title()\n            self._capture_inference_results(\n                capture_name,\n                target_df,\n                target,\n                model_type,\n                target_metrics,\n                description,\n                features=additional_columns,\n                id_column=id_column,\n                include_quantiles=include_quantiles,\n            )\n\n        # Save primary target (or single target) as \"full_cross_fold\"\n        if target == primary_target:\n            self._capture_inference_results(\n                \"full_cross_fold\",\n                target_df,\n                target,\n                model_type,\n                target_metrics,\n                \"Full Cross Fold\",\n                features=additional_columns,\n                id_column=id_column,\n                include_quantiles=include_quantiles,\n            )\n\n    return out_of_fold_df\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.data_capture","title":"<code>data_capture()</code>","text":"<p>Get the MonitorCore class for this endpoint</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def data_capture(self):\n    \"\"\"Get the MonitorCore class for this endpoint\"\"\"\n    from workbench.core.artifacts.data_capture_core import DataCaptureCore\n\n    return DataCaptureCore(self.endpoint_name)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.delete","title":"<code>delete()</code>","text":"<p>Delete an existing Endpoint: Underlying Models, Configuration, and Endpoint</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def delete(self):\n    \"\"\"Delete an existing Endpoint: Underlying Models, Configuration, and Endpoint\"\"\"\n    if not self.exists():\n        self.log.warning(f\"Trying to delete an Endpoint that doesn't exist: {self.name}\")\n\n    # Remove this endpoint from the list of registered endpoints\n    self.log.info(f\"Removing {self.name} from the list of registered endpoints...\")\n    ModelCore(self.model_name).remove_endpoint(self.name)\n\n    # Call the Class Method to delete the Endpoint\n    EndpointCore.managed_delete(endpoint_name=self.name)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.delete_endpoint_models","title":"<code>delete_endpoint_models(endpoint_name)</code>  <code>classmethod</code>","text":"<p>Delete the underlying Model for an Endpoint</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the endpoint to delete</p> required Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>@classmethod\ndef delete_endpoint_models(cls, endpoint_name: str):\n    \"\"\"Delete the underlying Model for an Endpoint\n\n    Args:\n        endpoint_name (str): The name of the endpoint to delete\n    \"\"\"\n\n    # Grab the Endpoint Config Name from AWS\n    endpoint_config_name = cls.sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointConfigName\"]\n\n    # Retrieve the Model Names from the Endpoint Config\n    try:\n        endpoint_config = cls.sm_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n    except botocore.exceptions.ClientError:\n        cls.log.info(f\"Endpoint Config {endpoint_config_name} doesn't exist...\")\n        return\n    model_names = [variant[\"ModelName\"] for variant in endpoint_config[\"ProductionVariants\"]]\n    for model_name in model_names:\n        cls.log.info(f\"Deleting Internal Model {model_name}...\")\n        try:\n            cls.sm_client.delete_model(ModelName=model_name)\n        except botocore.exceptions.ClientError as error:\n            error_code = error.response[\"Error\"][\"Code\"]\n            error_message = error.response[\"Error\"][\"Message\"]\n            if error_code == \"ResourceInUse\":\n                cls.log.warning(f\"Model {model_name} is still in use...\")\n            else:\n                cls.log.warning(f\"Error: {error_code} - {error_message}\")\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.details","title":"<code>details()</code>","text":"<p>Additional Details about this Endpoint</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about this Endpoint</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def details(self) -&gt; dict:\n    \"\"\"Additional Details about this Endpoint\n\n    Returns:\n        dict(dict): A dictionary of details about this Endpoint\n    \"\"\"\n\n    # Fill in all the details about this Endpoint\n    details = self.summary()\n\n    # Get details from our AWS Metadata\n    details[\"status\"] = self.endpoint_meta[\"EndpointStatus\"]\n    details[\"instance\"] = self.endpoint_meta[\"InstanceType\"]\n    try:\n        details[\"instance_count\"] = self.endpoint_meta[\"ProductionVariants\"][0][\"CurrentInstanceCount\"] or \"-\"\n    except KeyError:\n        details[\"instance_count\"] = \"-\"\n    if \"ProductionVariants\" in self.endpoint_meta:\n        details[\"variant\"] = self.endpoint_meta[\"ProductionVariants\"][0][\"VariantName\"]\n    else:\n        details[\"variant\"] = \"-\"\n\n    # Add endpoint metrics from CloudWatch\n    details[\"endpoint_metrics\"] = self.endpoint_metrics()\n\n    # Return the details\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.enable_data_capture","title":"<code>enable_data_capture()</code>","text":"<p>Add data capture to the endpoint</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def enable_data_capture(self):\n    \"\"\"Add data capture to the endpoint\"\"\"\n    self.data_capture().enable()\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.endpoint_metrics","title":"<code>endpoint_metrics()</code>","text":"<p>Return the metrics for this endpoint</p> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: DataFrame with the metrics for this endpoint (or None if no metrics)</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def endpoint_metrics(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Return the metrics for this endpoint\n\n    Returns:\n        pd.DataFrame: DataFrame with the metrics for this endpoint (or None if no metrics)\n    \"\"\"\n\n    # Do we have it cached?\n    metrics_key = f\"endpoint:{self.name}:endpoint_metrics\"\n    endpoint_metrics = self.temp_storage.get(metrics_key)\n    if endpoint_metrics is not None:\n        return endpoint_metrics\n\n    # We don't have it cached so let's get it from CloudWatch\n    if \"ProductionVariants\" not in self.endpoint_meta:\n        return None\n    self.log.important(\"Updating endpoint metrics...\")\n    variant = self.endpoint_meta[\"ProductionVariants\"][0][\"VariantName\"]\n    endpoint_metrics = EndpointMetrics().get_metrics(self.name, variant=variant)\n    self.temp_storage.set(metrics_key, endpoint_metrics)\n    return endpoint_metrics\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.exists","title":"<code>exists()</code>","text":"<p>Does the feature_set_name exist in the AWS Metadata?</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Does the feature_set_name exist in the AWS Metadata?\"\"\"\n    if self.endpoint_meta is None:\n        self.log.debug(f\"Endpoint {self.endpoint_name} not found in AWS Metadata\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.fast_inference","title":"<code>fast_inference(eval_df, threads=4)</code>","text":"<p>Run inference on the Endpoint using the provided DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>eval_df</code> <code>DataFrame</code> <p>The DataFrame to run predictions on</p> required <code>threads</code> <code>int</code> <p>The number of threads to use (default: 4)</p> <code>4</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with predictions</p> Note <p>There's no sanity checks or error handling... just FAST Inference!</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def fast_inference(self, eval_df: pd.DataFrame, threads: int = 4) -&gt; pd.DataFrame:\n    \"\"\"Run inference on the Endpoint using the provided DataFrame\n\n    Args:\n        eval_df (pd.DataFrame): The DataFrame to run predictions on\n        threads (int): The number of threads to use (default: 4)\n\n    Returns:\n        pd.DataFrame: The DataFrame with predictions\n\n    Note:\n        There's no sanity checks or error handling... just FAST Inference!\n    \"\"\"\n    return fast_inference(self.name, eval_df, self.sm_session, threads=threads)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.full_inference","title":"<code>full_inference()</code>","text":"<p>Run inference on the endpoint using all the data from the model training view</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def full_inference(self) -&gt; pd.DataFrame:\n    \"\"\"Run inference on the endpoint using all the data from the model training view\"\"\"\n\n    # Sanity Check that we have a model\n    model = ModelCore(self.get_input())\n    if not model.exists():\n        self.log.error(\"No model found for this endpoint. Returning empty DataFrame.\")\n        return pd.DataFrame()\n\n    # Grab the full data from the Model's training view\n    eval_df = model.training_view().pull_dataframe()\n\n    # Remove AWS created columns\n    aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n    eval_df = eval_df.drop(columns=aws_cols, errors=\"ignore\")\n\n    # Run inference\n    return self.inference(eval_df, \"full_inference\")\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.generate_confusion_matrix","title":"<code>generate_confusion_matrix(target_column, prediction_df)</code>","text":"<p>Compute the confusion matrix for this Endpoint</p> <p>Parameters:</p> Name Type Description Default <code>target_column</code> <code>str</code> <p>Name of the target column</p> required <code>prediction_df</code> <code>DataFrame</code> <p>DataFrame with the prediction results</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with the confusion matrix</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def generate_confusion_matrix(self, target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute the confusion matrix for this Endpoint\n\n    Args:\n        target_column (str): Name of the target column\n        prediction_df (pd.DataFrame): DataFrame with the prediction results\n\n    Returns:\n        pd.DataFrame: DataFrame with the confusion matrix\n    \"\"\"\n    # Check for prediction column\n    if \"prediction\" not in prediction_df.columns:\n        self.log.warning(\"No 'prediction' column found in DataFrame\")\n        return pd.DataFrame()\n\n    # Drop rows with NaN predictions (can't include in confusion matrix)\n    nan_mask = prediction_df[\"prediction\"].isna()\n    if nan_mask.any():\n        n_nan = nan_mask.sum()\n        self.log.warning(f\"Dropping {n_nan} rows with NaN predictions for confusion matrix\")\n        prediction_df = prediction_df[~nan_mask].copy()\n\n    y_true = prediction_df[target_column]\n    y_pred = prediction_df[\"prediction\"]\n\n    # Get model class labels\n    model_class_labels = ModelCore(self.model_name).class_labels()\n\n    # Use model labels if available, otherwise infer from data\n    if model_class_labels:\n        self.log.important(\"Using model class labels for confusion matrix ordering...\")\n        labels = model_class_labels\n    else:\n        labels = sorted(list(set(y_true) | set(y_pred)))\n\n    # Compute confusion matrix and create DataFrame\n    conf_mtx = confusion_matrix(y_true, y_pred, labels=labels)\n    conf_mtx_df = pd.DataFrame(conf_mtx, index=labels, columns=labels)\n    conf_mtx_df.index.name = \"labels\"\n    return conf_mtx_df\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.hash","title":"<code>hash()</code>","text":"<p>Return the hash for the internal model used by this endpoint</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The hash for the internal model used by this endpoint</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def hash(self) -&gt; Optional[str]:\n    \"\"\"Return the hash for the internal model used by this endpoint\n\n    Returns:\n        Optional[str]: The hash for the internal model used by this endpoint\n    \"\"\"\n    model_url = self.model_data_url()\n    return compute_s3_object_hash(model_url, self.boto3_session)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.health_check","title":"<code>health_check(deep=False)</code>","text":"<p>Perform a health check on this endpoint</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, perform more extensive (expensive) health checks (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of health issues</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def health_check(self, deep: bool = False) -&gt; list[str]:\n    \"\"\"Perform a health check on this endpoint\n\n    Args:\n        deep (bool): If True, perform more extensive (expensive) health checks (default: False)\n\n    Returns:\n        list[str]: List of health issues\n    \"\"\"\n    if not self.ready():\n        return [\"needs_onboard\"]\n\n    # Call the base class health check\n    health_issues = super().health_check(deep=deep)\n\n    # Does this endpoint have a config?\n    # Note: This is not an authoritative check, so improve later\n    if self.endpoint_meta.get(\"ProductionVariants\") is None:\n        health_issues.append(\"no_config\")\n\n    # Deep checks (expensive CloudWatch metrics call)\n    if deep:\n        # We're going to check for 5xx errors and no activity\n        endpoint_metrics = self.endpoint_metrics()\n\n        # Check if we have metrics\n        if endpoint_metrics is None:\n            health_issues.append(\"unknown_error\")\n            return health_issues\n\n        # Check for 5xx errors\n        num_errors = endpoint_metrics[\"Invocation5XXErrors\"].sum()\n        if num_errors &gt; 5:\n            health_issues.append(\"5xx_errors\")\n        elif num_errors &gt; 0:\n            health_issues.append(\"5xx_errors_min\")\n        else:\n            self.remove_health_tag(\"5xx_errors\")\n            self.remove_health_tag(\"5xx_errors_min\")\n\n        # Check for Endpoint activity\n        num_invocations = endpoint_metrics[\"Invocations\"].sum()\n        if num_invocations == 0:\n            health_issues.append(\"no_activity\")\n        else:\n            self.remove_health_tag(\"no_activity\")\n\n    return health_issues\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.inference","title":"<code>inference(eval_df, capture_name=None, id_column=None, drop_error_rows=False, include_quantiles=False)</code>","text":"<p>Run inference on the Endpoint using the provided DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>eval_df</code> <code>DataFrame</code> <p>DataFrame to run predictions on (must have superset of features)</p> required <code>capture_name</code> <code>str</code> <p>Name of the inference capture (default=None)</p> <code>None</code> <code>id_column</code> <code>str</code> <p>Name of the ID column (default=None)</p> <code>None</code> <code>drop_error_rows</code> <code>bool</code> <p>If True, drop rows that had endpoint errors/issues (default=False)</p> <code>False</code> <code>include_quantiles</code> <code>bool</code> <p>Include q_* quantile columns in saved output (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with the inference results</p> Note <p>If capture=True inference/performance metrics are written to S3 Endpoint Inference Folder</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def inference(\n    self,\n    eval_df: pd.DataFrame,\n    capture_name: str = None,\n    id_column: str = None,\n    drop_error_rows: bool = False,\n    include_quantiles: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Run inference on the Endpoint using the provided DataFrame\n\n    Args:\n        eval_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features)\n        capture_name (str, optional): Name of the inference capture (default=None)\n        id_column (str, optional): Name of the ID column (default=None)\n        drop_error_rows (bool, optional): If True, drop rows that had endpoint errors/issues (default=False)\n        include_quantiles (bool): Include q_* quantile columns in saved output (default: False)\n\n    Returns:\n        pd.DataFrame: DataFrame with the inference results\n\n    Note:\n        If capture=True inference/performance metrics are written to S3 Endpoint Inference Folder\n    \"\"\"\n\n    # Check if this is a 'floating endpoint' (no model)\n    if self.get_input() == \"unknown\":\n        self.log.important(\"No model associated with this endpoint, running 'no frills' inference...\")\n        return self.fast_inference(eval_df)\n\n    # Grab the model features and target column\n    model = ModelCore(self.model_name)\n    features = model.features()\n    targets = model.target()  # Note: We have multi-target models (so this could be a list)\n\n    # Run predictions on the evaluation data\n    prediction_df = self._predict(eval_df, features, drop_error_rows)\n    if prediction_df.empty:\n        self.log.warning(\"No predictions were made. Returning empty DataFrame.\")\n        return prediction_df\n\n    # Normalize targets to handle both string and list formats\n    if isinstance(targets, list):\n        primary_target = targets[0] if targets else None\n    else:\n        primary_target = targets\n\n    # Sanity Check that the target column is present\n    if primary_target not in prediction_df.columns:\n        self.log.important(f\"Target Column {primary_target} not found in prediction_df!\")\n        self.log.important(\"In order to compute metrics, the target column must be present!\")\n        metrics = pd.DataFrame()\n\n    # Compute the standard performance metrics for this model\n    else:\n        if model.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n            prediction_df = self.residuals(primary_target, prediction_df)\n            metrics = self.regression_metrics(primary_target, prediction_df)\n        elif model.model_type == ModelType.CLASSIFIER:\n            metrics = self.classification_metrics(primary_target, prediction_df)\n        else:\n            # For other model types, we don't compute metrics\n            self.log.info(f\"Model Type: {model.model_type} doesn't have metrics...\")\n            metrics = pd.DataFrame()\n\n    # Print out the metrics\n    print(f\"Performance Metrics for {self.model_name} on {self.name}\")\n    print(metrics.head())\n\n    # Capture the inference results and metrics\n    if primary_target and capture_name:\n\n        # If we don't have an id_column, we'll pull it from the model's FeatureSet\n        if id_column is None:\n            fs = FeatureSetCore(model.get_input())\n            id_column = fs.id_column\n\n        # Normalize targets to a list for iteration\n        target_list = targets if isinstance(targets, list) else [targets]\n        primary_target = target_list[0]\n\n        # For single-target models (99% of cases), just save with capture_name\n        # For multi-target models, save each as {prefix}_{target} plus primary as capture_name\n        is_multi_target = len(target_list) &gt; 1\n\n        if is_multi_target:\n            prefix = \"auto\" if capture_name == \"auto_inference\" else capture_name\n\n        for target in target_list:\n            # Drop rows with NaN target values for metrics/plots\n            target_df = prediction_df.dropna(subset=[target])\n\n            # For multi-target models, prediction column is {target}_pred, otherwise \"prediction\"\n            pred_col = f\"{target}_pred\" if is_multi_target else \"prediction\"\n\n            # Compute per-target metrics\n            if model.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n                target_metrics = self.regression_metrics(target, target_df, prediction_col=pred_col)\n            elif model.model_type == ModelType.CLASSIFIER:\n                target_metrics = self.classification_metrics(target, target_df, prediction_col=pred_col)\n            else:\n                target_metrics = pd.DataFrame()\n\n            if is_multi_target:\n                # Multi-target: save as {prefix}_{target}\n                target_capture_name = f\"{prefix}_{target}\"\n                description = target_capture_name.replace(\"_\", \" \").title()\n                self._capture_inference_results(\n                    target_capture_name,\n                    target_df,\n                    target,\n                    model.model_type,\n                    target_metrics,\n                    description,\n                    features,\n                    id_column,\n                    include_quantiles,\n                )\n\n            # Save primary target (or single target) with original capture_name\n            if target == primary_target:\n                self._capture_inference_results(\n                    capture_name,\n                    target_df,\n                    target,\n                    model.model_type,\n                    target_metrics,\n                    capture_name.replace(\"_\", \" \").title(),\n                    features,\n                    id_column,\n                    include_quantiles,\n                )\n\n        # Capture uncertainty metrics if prediction_std is available (UQ, ChemProp, etc.)\n        if \"prediction_std\" in prediction_df.columns:\n            metrics = uq_metrics(prediction_df, primary_target)\n            self.param_store.upsert(f\"/workbench/models/{model.name}/inference/{capture_name}\", metrics)\n\n    # Return the prediction DataFrame\n    return prediction_df\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.is_monitored","title":"<code>is_monitored()</code>","text":"<p>Is monitoring enabled for this Endpoint?</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if monitoring is enabled, False otherwise.</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def is_monitored(self) -&gt; bool:\n    \"\"\"Is monitoring enabled for this Endpoint?\n\n    Returns:\n        True if monitoring is enabled, False otherwise.\n    \"\"\"\n    try:\n        response = self.sm_client.list_monitoring_schedules(EndpointName=self.name)\n        return bool(response.get(\"MonitoringScheduleSummaries\", []))\n    except ClientError:\n        return False\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.is_serverless","title":"<code>is_serverless()</code>","text":"<p>Check if the current endpoint is serverless.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the endpoint is serverless, False otherwise.</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def is_serverless(self) -&gt; bool:\n    \"\"\"Check if the current endpoint is serverless.\n\n    Returns:\n        bool: True if the endpoint is serverless, False otherwise.\n    \"\"\"\n    return \"Serverless\" in self.endpoint_meta[\"InstanceType\"]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.managed_delete","title":"<code>managed_delete(endpoint_name)</code>  <code>classmethod</code>","text":"<p>Delete the Endpoint and associated resources if it exists</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>@classmethod\ndef managed_delete(cls, endpoint_name: str):\n    \"\"\"Delete the Endpoint and associated resources if it exists\"\"\"\n\n    # Check if the endpoint exists\n    try:\n        endpoint_info = cls.sm_client.describe_endpoint(EndpointName=endpoint_name)\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] in [\"ValidationException\", \"ResourceNotFound\"]:\n            cls.log.info(f\"Endpoint {endpoint_name} not found!\")\n            return\n        raise  # Re-raise unexpected errors\n\n    # Delete underlying models (Endpoints store/use models internally)\n    cls.delete_endpoint_models(endpoint_name)\n\n    # Get Endpoint Config Name and delete if exists\n    endpoint_config_name = endpoint_info[\"EndpointConfigName\"]\n    try:\n        cls.log.info(f\"Deleting Endpoint Config {endpoint_config_name}...\")\n        cls.sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n    except ClientError:\n        cls.log.info(f\"Endpoint Config {endpoint_config_name} not found...\")\n\n    # Delete any monitoring schedules associated with the endpoint\n    monitoring_schedules = cls.sm_client.list_monitoring_schedules(EndpointName=endpoint_name)[\n        \"MonitoringScheduleSummaries\"\n    ]\n    for schedule in monitoring_schedules:\n        cls.log.info(f\"Deleting Monitoring Schedule {schedule['MonitoringScheduleName']}...\")\n        cls.sm_client.delete_monitoring_schedule(MonitoringScheduleName=schedule[\"MonitoringScheduleName\"])\n\n    # Recursively delete all endpoint S3 artifacts (inference, etc)\n    # Note: We do not want to delete the data_capture/ files since these\n    #       might be used for collection and data drift analysis\n    base_endpoint_path = f\"{cls.endpoints_s3_path}/{endpoint_name}/\"\n    all_s3_objects = wr.s3.list_objects(base_endpoint_path, boto3_session=cls.boto3_session)\n\n    # Filter out objects that contain 'data_capture/' in their path\n    s3_objects_to_delete = [obj for obj in all_s3_objects if \"/data_capture/\" not in obj]\n    cls.log.info(f\"Found {len(all_s3_objects)} total objects at {base_endpoint_path}\")\n    cls.log.info(f\"Filtering out data_capture files, will delete {len(s3_objects_to_delete)} objects...\")\n    cls.log.info(f\"Objects to delete: {s3_objects_to_delete}\")\n\n    if s3_objects_to_delete:\n        wr.s3.delete_objects(s3_objects_to_delete, boto3_session=cls.boto3_session)\n        cls.log.info(f\"Successfully deleted {len(s3_objects_to_delete)} objects\")\n    else:\n        cls.log.info(\"No objects to delete (only data_capture files found)\")\n\n    # Delete any dataframes that were stored in the Dataframe Cache\n    cls.log.info(\"Deleting Dataframe Cache...\")\n    cls.df_cache.delete_recursive(endpoint_name)\n\n    # Delete the endpoint\n    cls.log.info(f\"Deleting Endpoint {endpoint_name}...\")\n    time.sleep(10)  # Allow AWS to catch up\n    try:\n        cls.sm_client.delete_endpoint(EndpointName=endpoint_name)\n    except ClientError as e:\n        cls.log.error(\"Error deleting endpoint.\")\n        raise e\n\n    time.sleep(10)  # Final sleep for AWS to fully register deletions\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.model_data_url","title":"<code>model_data_url()</code>","text":"<p>Return the model data URL for this endpoint</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The model data URL for this endpoint</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def model_data_url(self) -&gt; Optional[str]:\n    \"\"\"Return the model data URL for this endpoint\n\n    Returns:\n        Optional[str]: The model data URL for this endpoint\n    \"\"\"\n    from workbench.utils.endpoint_utils import internal_model_data_url  # Avoid circular import\n\n    return internal_model_data_url(self.endpoint_config_name(), self.boto3_session)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.modified","title":"<code>modified()</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    return self.endpoint_meta[\"LastModifiedTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.monitor","title":"<code>monitor()</code>","text":"<p>Get the MonitorCore class for this endpoint</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def monitor(self):\n    \"\"\"Get the MonitorCore class for this endpoint\"\"\"\n    from workbench.core.artifacts.monitor_core import MonitorCore\n\n    return MonitorCore(self.endpoint_name)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.onboard","title":"<code>onboard(interactive=False)</code>","text":"<p>This is a BLOCKING method that will onboard the Endpoint (make it ready) Args:     interactive (bool, optional): If True, will prompt the user for information. (default: False) Returns:     bool: True if the Endpoint is successfully onboarded, False otherwise</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def onboard(self, interactive: bool = False) -&gt; bool:\n    \"\"\"This is a BLOCKING method that will onboard the Endpoint (make it ready)\n    Args:\n        interactive (bool, optional): If True, will prompt the user for information. (default: False)\n    Returns:\n        bool: True if the Endpoint is successfully onboarded, False otherwise\n    \"\"\"\n\n    # Make sure our input is defined\n    if self.get_input() == \"unknown\":\n        if interactive:\n            input_model = input(\"Input Model?: \")\n        else:\n            self.log.critical(\"Input Model is not defined!\")\n            return False\n    else:\n        input_model = self.get_input()\n\n    # Now that we have the details, let's onboard the Endpoint with args\n    return self.onboard_with_args(input_model)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.onboard_with_args","title":"<code>onboard_with_args(input_model)</code>","text":"<p>Onboard the Endpoint with the given arguments</p> <p>Parameters:</p> Name Type Description Default <code>input_model</code> <code>str</code> <p>The input model for this endpoint</p> required <p>Returns:     bool: True if the Endpoint is successfully onboarded, False otherwise</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def onboard_with_args(self, input_model: str) -&gt; bool:\n    \"\"\"Onboard the Endpoint with the given arguments\n\n    Args:\n        input_model (str): The input model for this endpoint\n    Returns:\n        bool: True if the Endpoint is successfully onboarded, False otherwise\n    \"\"\"\n    # Set the status to onboarding\n    self.set_status(\"onboarding\")\n\n    self.upsert_workbench_meta({\"workbench_input\": input_model})\n    self.model_name = input_model\n\n    # Remove the needs_onboard tag\n    self.remove_health_tag(\"needs_onboard\")\n    self.set_status(\"ready\")\n\n    # Run a health check and refresh the meta\n    time.sleep(2)  # Give the AWS Metadata a chance to update\n    self.health_check(deep=True)\n    self.refresh_meta()\n    self.details()\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.refresh_meta","title":"<code>refresh_meta()</code>","text":"<p>Refresh the Artifact's metadata</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def refresh_meta(self):\n    \"\"\"Refresh the Artifact's metadata\"\"\"\n    self.endpoint_meta = self.meta.endpoint(self.endpoint_name)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.regression_metrics","title":"<code>regression_metrics(target_column, prediction_df, prediction_col='prediction')</code>","text":"<p>Compute the performance metrics for this Endpoint</p> <p>Parameters:</p> Name Type Description Default <code>target_column</code> <code>str</code> <p>Name of the target column</p> required <code>prediction_df</code> <code>DataFrame</code> <p>DataFrame with the prediction results</p> required <code>prediction_col</code> <code>str</code> <p>Name of the prediction column (default: \"prediction\")</p> <code>'prediction'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with the performance metrics</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def regression_metrics(\n    self, target_column: str, prediction_df: pd.DataFrame, prediction_col: str = \"prediction\"\n) -&gt; pd.DataFrame:\n    \"\"\"Compute the performance metrics for this Endpoint\n\n    Args:\n        target_column (str): Name of the target column\n        prediction_df (pd.DataFrame): DataFrame with the prediction results\n        prediction_col (str): Name of the prediction column (default: \"prediction\")\n\n    Returns:\n        pd.DataFrame: DataFrame with the performance metrics\n    \"\"\"\n    return compute_regression_metrics(prediction_df, target_column, prediction_col)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.residuals","title":"<code>residuals(target_column, prediction_df)</code>","text":"<p>Add the residuals to the prediction DataFrame Args:     target_column (str): Name of the target column     prediction_df (pd.DataFrame): DataFrame with the prediction results Returns:     pd.DataFrame: DataFrame with two new columns called 'residuals' and 'residuals_abs'</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def residuals(self, target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Add the residuals to the prediction DataFrame\n    Args:\n        target_column (str): Name of the target column\n        prediction_df (pd.DataFrame): DataFrame with the prediction results\n    Returns:\n        pd.DataFrame: DataFrame with two new columns called 'residuals' and 'residuals_abs'\n    \"\"\"\n    # Check for prediction column\n    if \"prediction\" not in prediction_df.columns:\n        self.log.warning(\"No 'prediction' column found. Cannot compute residuals.\")\n        return prediction_df\n\n    y_true = prediction_df[target_column]\n    y_pred = prediction_df[\"prediction\"]\n\n    # Check for classification scenario\n    if not pd.api.types.is_numeric_dtype(y_true) or not pd.api.types.is_numeric_dtype(y_pred):\n        self.log.warning(\"Target and Prediction columns are not numeric. Computing 'diffs'...\")\n        prediction_df[\"residuals\"] = (y_true != y_pred).astype(int)\n        prediction_df[\"residuals_abs\"] = prediction_df[\"residuals\"]\n    else:\n        # Compute numeric residuals for regression\n        prediction_df[\"residuals\"] = y_true - y_pred\n        prediction_df[\"residuals_abs\"] = np.abs(prediction_df[\"residuals\"])\n\n    return prediction_df\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.set_input","title":"<code>set_input(input, force=False)</code>","text":"<p>Override: Set the input data for this artifact</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>Name of input for this artifact</p> required <code>force</code> <code>bool</code> <p>Force the input to be set. Defaults to False.</p> <code>False</code> <p>Note:     We're going to not allow this to be used for Models</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def set_input(self, input: str, force=False):\n    \"\"\"Override: Set the input data for this artifact\n\n    Args:\n        input (str): Name of input for this artifact\n        force (bool, optional): Force the input to be set. Defaults to False.\n    Note:\n        We're going to not allow this to be used for Models\n    \"\"\"\n    if not force:\n        self.log.warning(f\"Endpoint {self.name}: Does not allow manual override of the input!\")\n        return\n\n    # Okay we're going to allow this to be set\n    self.log.important(f\"{self.name}: Setting input to {input}...\")\n    self.log.important(\"Be careful with this! It breaks automatic provenance of the artifact!\")\n    self.upsert_workbench_meta({\"workbench_input\": input})\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#workbench.core.artifacts.endpoint_core.EndpointCore.size","title":"<code>size()</code>","text":"<p>Return the size of this data in MegaBytes</p> Source code in <code>src/workbench/core/artifacts/endpoint_core.py</code> <pre><code>def size(self) -&gt; float:\n    \"\"\"Return the size of this data in MegaBytes\"\"\"\n    return 0.0\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/","title":"FeatureSetCore","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the FeatureSet API Class and voil\u00e0 it works the same.</p> <p>FeatureSet: Workbench Feature Set accessible through Athena</p>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore","title":"<code>FeatureSetCore</code>","text":"<p>               Bases: <code>Artifact</code></p> <p>FeatureSetCore: Workbench FeatureSetCore Class</p> Common Usage <pre><code>my_features = FeatureSetCore(feature_name)\nmy_features.summary()\nmy_features.details()\n</code></pre> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>class FeatureSetCore(Artifact):\n    \"\"\"FeatureSetCore: Workbench FeatureSetCore Class\n\n    Common Usage:\n        ```python\n        my_features = FeatureSetCore(feature_name)\n        my_features.summary()\n        my_features.details()\n        ```\n    \"\"\"\n\n    def __init__(self, feature_set_name: str, **kwargs):\n        \"\"\"FeatureSetCore Initialization\n\n        Args:\n            feature_set_name (str): Name of Feature Set\n        \"\"\"\n\n        # Make sure the feature_set name is valid\n        self.is_name_valid(feature_set_name)\n\n        # Call superclass init\n        super().__init__(feature_set_name, **kwargs)\n\n        # Get our FeatureSet metadata\n        self.feature_meta = self.meta.feature_set(self.name)\n\n        # Sanity check and then set up our FeatureSet attributes\n        if self.feature_meta is None:\n            self.log.warning(f\"Could not find feature set {self.name} within current visibility scope\")\n            self.data_source = None\n            return\n        else:\n            self.id_column = self.feature_meta[\"RecordIdentifierFeatureName\"]\n            self.event_time = self.feature_meta[\"EventTimeFeatureName\"]\n\n            # Pull Athena and S3 Storage information from metadata\n            self.athena_table = self.feature_meta[\"workbench_meta\"][\"athena_table\"]\n            self.athena_database = self.feature_meta[\"workbench_meta\"][\"athena_database\"]\n            self.s3_storage = self.feature_meta[\"workbench_meta\"].get(\"s3_storage\")\n\n            # Create our internal DataSource (hardcoded to Athena for now)\n            self.data_source = AthenaSource(self.athena_table, self.athena_database)\n\n            # Check our DataSource (AWS Metadata refresh can fix)\n            if not self.data_source.exists():\n                self.log.warning(\n                    f\"FS: Data Source {self.athena_table} not found, sleeping and refreshing AWS Metadata...\"\n                )\n                time.sleep(3)\n                self.refresh_meta()\n\n        # Spin up our Feature Store\n        self.feature_store = FeatureStore(self.sm_session)\n\n        # Call superclass post_init\n        super().__post_init__()\n\n        # All done\n        self.log.info(f\"FeatureSet Initialized: {self.name}...\")\n\n    @property\n    def table(self) -&gt; str:\n        \"\"\"Get the base table name for this FeatureSet\"\"\"\n        return self.data_source.table\n\n    def refresh_meta(self):\n        \"\"\"Internal: Refresh our internal AWS Feature Store metadata\"\"\"\n        self.log.info(f\"Calling refresh_meta() on the FeatureSet {self.name}\")\n        self.feature_meta = self.meta.feature_set(self.name)\n        self.id_column = self.feature_meta[\"RecordIdentifierFeatureName\"]\n        self.event_time = self.feature_meta[\"EventTimeFeatureName\"]\n        self.athena_table = self.feature_meta[\"workbench_meta\"][\"athena_table\"]\n        self.athena_database = self.feature_meta[\"workbench_meta\"][\"athena_database\"]\n        self.s3_storage = self.feature_meta[\"workbench_meta\"].get(\"s3_storage\")\n        self.data_source = AthenaSource(self.athena_table, self.athena_database)\n        self.log.info(f\"Calling refresh_meta() on the DataSource {self.data_source.name}\")\n        self.data_source.refresh_meta()\n\n    def exists(self) -&gt; bool:\n        \"\"\"Does the feature_set_name exist in the AWS Metadata?\"\"\"\n        if self.feature_meta is None:\n            self.log.debug(f\"FeatureSet {self.name} not found in AWS Metadata!\")\n            return False\n        return True\n\n    def health_check(self, deep: bool = False) -&gt; list[str]:\n        \"\"\"Perform a health check on this FeatureSet\n\n        Args:\n            deep (bool): If True, perform more extensive (expensive) health checks (default: False)\n\n        Returns:\n            list[str]: List of health issues\n        \"\"\"\n        # Call the base class health check\n        health_issues = super().health_check(deep=deep)\n\n        # If we have a 'needs_onboard' in the health check then just return\n        if \"needs_onboard\" in health_issues:\n            return health_issues\n\n        # Check our DataSource\n        if not self.data_source.exists():\n            self.log.critical(f\"Data Source check failed for {self.name}\")\n            self.log.critical(\"Delete this Feature Set and recreate it to fix this issue\")\n            health_issues.append(\"data_source_missing\")\n        return health_issues\n\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n        return self.feature_meta\n\n    def arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n        return self.feature_meta[\"FeatureGroupArn\"]\n\n    def size(self) -&gt; float:\n        \"\"\"Return the size of the internal DataSource in MegaBytes\"\"\"\n        return self.data_source.size()\n\n    @property\n    def columns(self) -&gt; list[str]:\n        \"\"\"Return the column names of the Feature Set\"\"\"\n        return list(self.column_details().keys())\n\n    @property\n    def column_types(self) -&gt; list[str]:\n        \"\"\"Return the column types of the Feature Set\"\"\"\n        return list(self.column_details().values())\n\n    def column_details(self) -&gt; dict:\n        \"\"\"Return the column details of the Feature Set\n\n        Returns:\n            dict: The column details of the Feature Set\n\n        Notes:\n            We can't call just call self.data_source.column_details() because FeatureSets have different\n            types, so we need to overlay that type information on top of the DataSource type information\n        \"\"\"\n        fs_details = {item[\"FeatureName\"]: item[\"FeatureType\"] for item in self.feature_meta[\"FeatureDefinitions\"]}\n        ds_details = self.data_source.column_details()\n\n        # Overlay the FeatureSet type information on top of the DataSource type information\n        for col, dtype in ds_details.items():\n            ds_details[col] = fs_details.get(col, dtype)\n        return ds_details\n\n    def views(self) -&gt; list[str]:\n        \"\"\"Return the views for this Data Source\"\"\"\n        from workbench.core.views.view_utils import list_views\n\n        return list_views(self.data_source)\n\n    def supplemental_data(self) -&gt; list[str]:\n        \"\"\"Return the supplemental data for this Data Source\"\"\"\n        from workbench.core.views.view_utils import list_supplemental_data\n\n        return list_supplemental_data(self.data_source)\n\n    def view(self, view_name: str) -&gt; \"View\":\n        \"\"\"Return a DataFrame for a specific view\n        Args:\n            view_name (str): The name of the view to return\n        Returns:\n            pd.DataFrame: A DataFrame for the specified view\n        \"\"\"\n        from workbench.core.views import View\n\n        return View(self, view_name)\n\n    def set_display_columns(self, display_columns: list[str]):\n        \"\"\"Set the display columns for this Data Source\n\n        Args:\n            display_columns (list[str]): The display columns for this Data Source\n        \"\"\"\n        # Check mismatch of display columns to computation columns\n        c_view = self.view(\"computation\")\n        computation_columns = c_view.columns\n        mismatch_columns = [col for col in display_columns if col not in computation_columns]\n        if mismatch_columns:\n            self.log.monitor(f\"Display View/Computation mismatch: {mismatch_columns}\")\n\n        self.log.important(f\"Setting Display Columns...{display_columns}\")\n        from workbench.core.views import DisplayView\n\n        # Create a NEW display view\n        DisplayView.create(self, source_table=c_view.table, column_list=display_columns)\n\n    def set_computation_columns(self, computation_columns: list[str], reset_display: bool = True):\n        \"\"\"Set the computation columns for this FeatureSet\n\n        Args:\n            computation_columns (list[str]): The computation columns for this FeatureSet\n            reset_display (bool): Also reset the display columns to match (default: True)\n        \"\"\"\n        self.log.important(f\"Setting Computation Columns...{computation_columns}\")\n        from workbench.core.views import ComputationView\n\n        # Create a NEW computation view\n        ComputationView.create(self, column_list=computation_columns)\n        self.recompute_stats()\n\n        # Reset the display columns to match the computation columns\n        if reset_display:\n            self.set_display_columns(computation_columns)\n\n    def set_compressed_features(self, compressed_columns: list[str]):\n        \"\"\"Set the compressed features for this FeatureSet\n\n        Args:\n            compressed_columns (list[str]): The compressed columns for this FeatureSet\n        \"\"\"\n        # Ensure that the compressed features are a subset of the columns\n        if not set(compressed_columns).issubset(set(self.columns)):\n            self.log.warning(\n                f\"Compressed columns {compressed_columns} are not a subset of the columns {self.columns}. \"\n            )\n            return\n\n        # Set the compressed features in our FeatureSet metadata\n        self.log.important(f\"Setting Compressed Columns...{compressed_columns}\")\n        self.upsert_workbench_meta({\"compressed_features\": compressed_columns})\n\n    def get_compressed_features(self) -&gt; list[str]:\n        \"\"\"Get the compressed features for this FeatureSet\n\n        Returns:\n            list[str]: The compressed columns for this FeatureSet\n        \"\"\"\n        # Get the compressed features from our FeatureSet metadata\n        return self.workbench_meta().get(\"compressed_features\", [])\n\n    def num_columns(self) -&gt; int:\n        \"\"\"Return the number of columns of the Feature Set\"\"\"\n        return len(self.columns)\n\n    def num_rows(self) -&gt; int:\n        \"\"\"Return the number of rows of the internal DataSource\"\"\"\n        return self.data_source.num_rows()\n\n    def query(self, query: str, overwrite: bool = True) -&gt; pd.DataFrame:\n        \"\"\"Query the internal DataSource\n\n        Args:\n            query (str): The query to run against the DataSource\n            overwrite (bool): Overwrite the table name in the query (default: True)\n\n        Returns:\n            pd.DataFrame: The results of the query\n        \"\"\"\n        if overwrite:\n            query = query.replace(\" \" + self.name + \" \", \" \" + self.athena_table + \" \")\n        return self.data_source.query(query)\n\n    def aws_url(self):\n        \"\"\"The AWS URL for looking at/querying the underlying data source\"\"\"\n        workbench_details = self.data_source.workbench_meta().get(\"workbench_details\", {})\n        return workbench_details.get(\"aws_url\", \"unknown\")\n\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        return self.feature_meta[\"CreationTime\"]\n\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        # Note: We can't currently figure out how to this from AWS Metadata\n        return self.feature_meta[\"CreationTime\"]\n\n    def hash(self) -&gt; str:\n        \"\"\"Return the hash for the set of Parquet files for this artifact\"\"\"\n        return self.data_source.hash()\n\n    def table_hash(self) -&gt; str:\n        \"\"\"Return the hash for the Athena table\"\"\"\n        return self.data_source.table_hash()\n\n    def get_data_source(self) -&gt; DataSourceFactory:\n        \"\"\"Return the underlying DataSource object\"\"\"\n        return self.data_source\n\n    def get_feature_store(self) -&gt; FeatureStore:\n        \"\"\"Return the underlying AWS FeatureStore object. This can be useful for more advanced usage\n        with create_dataset() such as Joins and time ranges and a host of other options\n        See: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html\n        \"\"\"\n        return self.feature_store\n\n    def create_s3_training_data(self) -&gt; str:\n        \"\"\"Create some Training Data (S3 CSV) from a Feature Set using standard options. If you want\n        additional options/features use the get_feature_store() method and see AWS docs for all\n        the details: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html\n        Returns:\n            str: The full path/file for the CSV file created by Feature Store create_dataset()\n        \"\"\"\n\n        # Set up the S3 Query results path\n        date_time = datetime.now(timezone.utc).strftime(\"%Y-%m-%d_%H-%M-%S\")\n        s3_output_path = self.feature_sets_s3_path + f\"/{self.name}/datasets/all_{date_time}\"\n\n        # Make the query\n        table = self.view(\"training\").table\n        query = f'SELECT * FROM \"{table}\"'\n        athena_query = FeatureGroup(name=self.name, sagemaker_session=self.sm_session).athena_query()\n        athena_query.run(query, output_location=s3_output_path)\n        athena_query.wait()\n        query_execution = athena_query.get_query_execution()\n\n        # Get the full path to the S3 files with the results\n        full_s3_path = s3_output_path + f\"/{query_execution['QueryExecution']['QueryExecutionId']}.csv\"\n        return full_s3_path\n\n    def get_training_data(self) -&gt; pd.DataFrame:\n        \"\"\"Get the training data for this FeatureSet\n\n        Returns:\n            pd.DataFrame: The training data for this FeatureSet\n        \"\"\"\n        from workbench.core.views.view import View\n\n        return View(self, \"training\").pull_dataframe(limit=1_000_000)\n\n    def snapshot_query(self, table_name: str = None) -&gt; str:\n        \"\"\"An Athena query to get the latest snapshot of features\n\n        Args:\n            table_name (str): The name of the table to query (default: None)\n\n        Returns:\n            str: The Athena query to get the latest snapshot of features\n        \"\"\"\n        # Remove FeatureGroup metadata columns that might have gotten added\n        columns = self.columns\n        filter_columns = [\"write_time\", \"api_invocation_time\", \"is_deleted\"]\n        columns = \", \".join(['\"' + x + '\"' for x in columns if x not in filter_columns])\n\n        query = (\n            f\"SELECT {columns} \"\n            f\"    FROM (SELECT *, row_number() OVER (PARTITION BY {self.id_column} \"\n            f\"        ORDER BY {self.event_time} desc, api_invocation_time DESC, write_time DESC) AS row_num \"\n            f'        FROM \"{table_name}\") '\n            \"    WHERE row_num = 1 and NOT is_deleted;\"\n        )\n        return query\n\n    def details(self) -&gt; dict[dict]:\n        \"\"\"Additional Details about this FeatureSet Artifact\n\n        Returns:\n            dict(dict): A dictionary of details about this FeatureSet\n        \"\"\"\n\n        self.log.info(f\"Computing FeatureSet Details ({self.name})...\")\n        details = self.summary()\n        details[\"aws_url\"] = self.aws_url()\n\n        # Store the AWS URL in the Workbench Metadata\n        # FIXME: We need to revisit this but doing an upsert just for aws_url is silly\n        # self.upsert_workbench_meta({\"aws_url\": details[\"aws_url\"]})\n\n        # Now get a summary of the underlying DataSource\n        details[\"storage_summary\"] = self.data_source.summary()\n\n        # Number of Columns\n        details[\"num_columns\"] = self.num_columns()\n\n        # Number of Rows\n        details[\"num_rows\"] = self.num_rows()\n\n        # Additional Details\n        details[\"workbench_status\"] = self.get_status()\n        details[\"workbench_input\"] = self.get_input()\n        details[\"workbench_tags\"] = self.tag_delimiter.join(self.get_tags())\n\n        # Underlying Storage Details\n        details[\"storage_type\"] = \"athena\"  # TODO: Add RDS support\n        details[\"storage_name\"] = self.data_source.name\n\n        # Add the column details and column stats\n        details[\"column_details\"] = self.column_details()\n        details[\"column_stats\"] = self.column_stats()\n\n        # Return the details data\n        return details\n\n    def delete(self):\n        \"\"\"Instance Method: Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects\"\"\"\n        # Make sure the AthenaSource exists\n        if not self.exists():\n            self.log.warning(f\"Trying to delete an FeatureSet that doesn't exist: {self.name}\")\n\n        # Call the Class Method to delete the FeatureSet\n        FeatureSetCore.managed_delete(self.name)\n\n    @classmethod\n    def managed_delete(cls, feature_set_name: str):\n        \"\"\"Class Method: Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects\n\n        Args:\n            feature_set_name (str): The Name of the FeatureSet to delete\n        \"\"\"\n\n        # See if the FeatureSet exists\n        try:\n            response = cls.sm_client.describe_feature_group(FeatureGroupName=feature_set_name)\n        except cls.sm_client.exceptions.ResourceNotFound:\n            cls.log.info(f\"FeatureSet {feature_set_name} not found!\")\n            return\n\n        # Extract database and table information from the response\n        offline_config = response.get(\"OfflineStoreConfig\", {})\n        database = offline_config.get(\"DataCatalogConfig\", {}).get(\"Database\")\n        offline_table = offline_config.get(\"DataCatalogConfig\", {}).get(\"TableName\")\n        data_source_name = offline_table  # Our offline storage IS a DataSource\n\n        # Delete the Feature Group and ensure that it gets deleted\n        cls.log.important(f\"Deleting FeatureSet {feature_set_name}...\")\n        remove_fg = cls.aws_feature_group_delete(feature_set_name)\n        cls.ensure_feature_group_deleted(remove_fg)\n\n        # Delete our underlying DataSource (Data Catalog Table and S3 Storage Objects)\n        AthenaSource.managed_delete(data_source_name, database=database)\n\n        # Delete any views associated with this FeatureSet\n        cls.delete_views(offline_table, database)\n\n        # Feature Sets can often have a lot of cruft so delete the entire bucket/prefix\n        s3_delete_path = cls.feature_sets_s3_path + f\"/{feature_set_name}/\"\n        cls.log.info(f\"Deleting All FeatureSet S3 Storage Objects {s3_delete_path}\")\n        wr.s3.delete_objects(s3_delete_path, boto3_session=cls.boto3_session)\n\n        # Delete any dataframes that were stored in the Dataframe Cache\n        cls.log.info(\"Deleting Dataframe Cache...\")\n        cls.df_cache.delete_recursive(feature_set_name)\n\n    @classmethod\n    @aws_throttle\n    def aws_feature_group_delete(cls, feature_set_name):\n        remove_fg = FeatureGroup(name=feature_set_name, sagemaker_session=cls.sm_session)\n        remove_fg.delete()\n        return remove_fg\n\n    @classmethod\n    def ensure_feature_group_deleted(cls, feature_group):\n        status = \"Deleting\"\n        while status == \"Deleting\":\n            cls.log.debug(\"FeatureSet being Deleted...\")\n            try:\n                status = feature_group.describe().get(\"FeatureGroupStatus\")\n            except botocore.exceptions.ClientError as error:\n                # For ResourceNotFound/ValidationException, this is fine, otherwise raise all other exceptions\n                if error.response[\"Error\"][\"Code\"] in [\"ResourceNotFound\", \"ValidationException\"]:\n                    break\n                else:\n                    raise error\n            time.sleep(1)\n        cls.log.info(f\"FeatureSet {feature_group.name} successfully deleted\")\n\n    def get_training_holdouts(self) -&gt; list[str]:\n        \"\"\"Get the hold out ids for the training view for this FeatureSet\n\n        Returns:\n            list[str]: The list of holdout ids.\n        \"\"\"\n\n        # Create a NEW training view\n        self.log.important(\"Getting Training Holdouts...\")\n        table = self.view(\"training\").table\n        hold_out_ids = self.query(f'SELECT {self.id_column} FROM \"{table}\" where training = FALSE')[\n            self.id_column\n        ].tolist()\n        return hold_out_ids\n\n    # ---- Public methods for training configuration ----\n    def set_training_config(\n        self,\n        holdout_ids: List[Union[str, int]] = None,\n        weight_dict: Dict[Union[str, int], float] = None,\n        default_weight: float = 1.0,\n        exclude_zero_weights: bool = True,\n    ):\n        \"\"\"Configure training view with holdout IDs and/or sample weights.\n\n        This method creates a training view that can include both:\n        - A 'training' column (True/False) based on holdout IDs\n        - A 'sample_weight' column for weighted training\n\n        Args:\n            holdout_ids: List of IDs to mark as training=False (validation/holdout set)\n            weight_dict: Mapping of ID to sample weight\n                - weight &gt; 1.0: oversample/emphasize\n                - weight = 1.0: normal (default)\n                - 0 &lt; weight &lt; 1.0: downweight/de-emphasize\n                - weight = 0.0: exclude from training (filtered out if exclude_zero_weights=True)\n            default_weight: Weight for IDs not in weight_dict (default: 1.0)\n            exclude_zero_weights: If True, filter out rows with sample_weight=0 (default: True)\n\n        Example:\n            # Temporal split with sample weights\n            fs.set_training_config(\n                holdout_ids=temporal_hold_out_ids,  # IDs after cutoff date\n                weight_dict={'compound_42': 0.0, 'compound_99': 2.0},  # exclude/upweight\n            )\n        \"\"\"\n        from workbench.core.views.training_view import TrainingView\n\n        # If neither is provided, create a standard training view\n        if not holdout_ids and not weight_dict:\n            self.log.important(\"No holdouts or weights specified, creating standard training view\")\n            TrainingView.create(self, id_column=self.id_column)\n            return\n\n        # If only holdout_ids, delegate to set_training_holdouts\n        if holdout_ids and not weight_dict:\n            self.set_training_holdouts(holdout_ids)\n            return\n\n        # If only weight_dict, delegate to set_sample_weights\n        if weight_dict and not holdout_ids:\n            self.set_sample_weights(weight_dict, default_weight, exclude_zero_weights)\n            return\n\n        # Both holdout_ids and weight_dict provided - build combined view\n        self.log.important(f\"Setting training config: {len(holdout_ids)} holdouts, {len(weight_dict)} weights\")\n\n        # Get column list (excluding AWS-generated columns)\n        from workbench.core.views.view_utils import get_column_list\n\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n        source_columns = get_column_list(self.data_source, self.table)\n        column_list = [col for col in source_columns if col not in aws_cols]\n\n        # Build the training column CASE statement\n        training_case = self._build_holdout_case(holdout_ids)\n\n        # For large weight_dict, use supplemental table + JOIN\n        if len(weight_dict) &gt;= 100:\n            self.log.info(\"Using supplemental table approach for large weight_dict\")\n            weights_table = self._create_weights_table(weight_dict)\n\n            # Build column selection with table alias\n            sql_columns = \", \".join([f't.\"{col}\"' for col in column_list])\n\n            # Build JOIN query with training CASE and weight from joined table\n            training_case_aliased = training_case.replace(f\"WHEN {self.id_column} IN\", f\"WHEN t.{self.id_column} IN\")\n            inner_sql = f\"\"\"SELECT {sql_columns}, {training_case_aliased},\n                COALESCE(w.sample_weight, {default_weight}) AS sample_weight\n                FROM {self.table} t\n                LEFT JOIN {weights_table} w ON t.{self.id_column} = w.{self.id_column}\"\"\"\n        else:\n            # For small weight_dict, use CASE statement\n            sql_columns = \", \".join([f'\"{column}\"' for column in column_list])\n            weight_case = self._build_weight_case(weight_dict, default_weight)\n            inner_sql = f\"SELECT {sql_columns}, {training_case}, {weight_case} FROM {self.table}\"\n\n        # Optionally filter out zero weights\n        if exclude_zero_weights:\n            zero_count = sum(1 for w in weight_dict.values() if w == 0.0)\n            if zero_count:\n                self.log.important(f\"Filtering out {zero_count} rows with sample_weight = 0\")\n            sql_query = f\"SELECT * FROM ({inner_sql}) WHERE sample_weight &gt; 0\"\n        else:\n            sql_query = inner_sql\n\n        self._create_training_view(sql_query)\n\n    def set_training_holdouts(self, holdout_ids: list[str]):\n        \"\"\"Set the hold out ids for the training view for this FeatureSet\n\n        Args:\n            holdout_ids (list[str]): The list of holdout ids.\n        \"\"\"\n        from workbench.core.views import TrainingView\n\n        self.log.important(f\"Setting Training Holdouts: {len(holdout_ids)} ids...\")\n        TrainingView.create(self, id_column=self.id_column, holdout_ids=holdout_ids)\n\n    def set_sample_weights(\n        self,\n        weight_dict: Dict[Union[str, int], float],\n        default_weight: float = 1.0,\n        exclude_zero_weights: bool = True,\n    ):\n        \"\"\"Configure training view with sample weights for each ID.\n\n        Args:\n            weight_dict: Mapping of ID to sample weight\n                - weight &gt; 1.0: oversample/emphasize\n                - weight = 1.0: normal (default)\n                - 0 &lt; weight &lt; 1.0: downweight/de-emphasize\n                - weight = 0.0: exclude from training\n            default_weight: Weight for IDs not in weight_dict (default: 1.0)\n            exclude_zero_weights: If True, filter out rows with sample_weight=0 (default: True)\n\n        Example:\n            weights = {\n                'compound_42': 3.0,  # oversample 3x\n                'compound_99': 0.1,  # noisy, downweight\n                'compound_123': 0.0, # exclude from training\n            }\n            fs.set_sample_weights(weights)  # zeros automatically excluded\n            fs.set_sample_weights(weights, exclude_zero_weights=False)  # keep zeros\n\n        Note:\n            For large weight_dict (100+ entries), weights are stored as a supplemental\n            table and joined to avoid Athena query size limits.\n        \"\"\"\n        from workbench.core.views import TrainingView\n\n        if not weight_dict:\n            self.log.important(\"Empty weight_dict, creating standard training view\")\n            TrainingView.create(self, id_column=self.id_column)\n            return\n\n        self.log.important(f\"Setting sample weights for {len(weight_dict)} IDs\")\n\n        # For large weight_dict, use supplemental table + JOIN to avoid query size limits\n        if len(weight_dict) &gt;= 100:\n            self.log.info(\"Using supplemental table approach for large weight_dict\")\n            weights_table = self._create_weights_table(weight_dict)\n\n            # Build JOIN query with COALESCE for default weight\n            inner_sql = f\"\"\"SELECT t.*, COALESCE(w.sample_weight, {default_weight}) AS sample_weight\n                FROM {self.table} t\n                LEFT JOIN {weights_table} w ON t.{self.id_column} = w.{self.id_column}\"\"\"\n        else:\n            # For small weight_dict, use CASE statement (simpler, no extra table)\n            weight_case = self._build_weight_case(weight_dict, default_weight)\n            inner_sql = f\"SELECT *, {weight_case} FROM {self.table}\"\n\n        # Optionally filter out zero weights\n        if exclude_zero_weights:\n            zero_count = sum(1 for w in weight_dict.values() if w == 0.0)\n            if zero_count:\n                self.log.important(f\"Filtering out {zero_count} rows with sample_weight = 0\")\n            sql_query = f\"SELECT * FROM ({inner_sql}) WHERE sample_weight &gt; 0\"\n        else:\n            sql_query = inner_sql\n\n        TrainingView.create_with_sql(self, sql_query=sql_query, id_column=self.id_column)\n\n    # ---- Internal helpers for training view SQL generation ----\n    @staticmethod\n    def _format_id_for_sql(id_val: Union[str, int]) -&gt; str:\n        \"\"\"Format an ID value for use in SQL.\"\"\"\n        return repr(id_val)\n\n    def _build_holdout_case(self, holdout_ids: List[Union[str, int]]) -&gt; str:\n        \"\"\"Build SQL CASE statement for training column based on holdout IDs.\"\"\"\n        if all(isinstance(id_val, str) for id_val in holdout_ids):\n            formatted_ids = \", \".join(f\"'{id_val}'\" for id_val in holdout_ids)\n        else:\n            formatted_ids = \", \".join(map(str, holdout_ids))\n        return f\"\"\"CASE\n            WHEN {self.id_column} IN ({formatted_ids}) THEN False\n            ELSE True\n        END AS training\"\"\"\n\n    def _build_weight_case(self, weight_dict: Dict[Union[str, int], float], default_weight: float) -&gt; str:\n        \"\"\"Build SQL CASE statement for sample_weight column.\"\"\"\n        conditions = [\n            f\"WHEN {self.id_column} = {self._format_id_for_sql(id_val)} THEN {weight}\"\n            for id_val, weight in weight_dict.items()\n        ]\n        case_body = \"\\n            \".join(conditions)\n        return f\"\"\"CASE\n            {case_body}\n            ELSE {default_weight}\n        END AS sample_weight\"\"\"\n\n    def _create_training_view(self, sql_query: str):\n        \"\"\"Create the training view directly from a SQL query.\"\"\"\n        view_table = f\"{self.table}___training\"\n        create_view_query = f\"CREATE OR REPLACE VIEW {view_table} AS\\n{sql_query}\"\n        self.data_source.execute_statement(create_view_query)\n\n    def _create_weights_table(self, weight_dict: Dict[Union[str, int], float]) -&gt; str:\n        \"\"\"Store sample weights as a supplemental data table.\n\n        Args:\n            weight_dict: Mapping of ID to sample weight\n\n        Returns:\n            str: The name of the created supplemental table\n        \"\"\"\n        from workbench.core.views.view_utils import dataframe_to_table\n\n        # Create DataFrame from weight_dict\n        df = pd.DataFrame(\n            [(id_val, weight) for id_val, weight in weight_dict.items()],\n            columns=[self.id_column, \"sample_weight\"],\n        )\n\n        # Supplemental table name follows convention: _{base_table}___sample_weights\n        weights_table = f\"_{self.table}___sample_weights\"\n\n        # Store as supplemental data table\n        self.log.info(f\"Creating supplemental weights table: {weights_table}\")\n        dataframe_to_table(self.data_source, df, weights_table)\n\n        return weights_table\n\n    @classmethod\n    def delete_views(cls, table: str, database: str):\n        \"\"\"Delete any views associated with this FeatureSet\n\n        Args:\n            table (str): Name of Athena Table\n            database (str): Athena Database Name\n        \"\"\"\n        from workbench.core.views.view_utils import delete_views_and_supplemental_data\n\n        delete_views_and_supplemental_data(table, database, cls.boto3_session)\n\n    def descriptive_stats(self) -&gt; dict:\n        \"\"\"Get the descriptive stats for the numeric columns of the underlying DataSource\n\n        Returns:\n            dict: A dictionary of descriptive stats for the numeric columns\n        \"\"\"\n        return self.data_source.descriptive_stats()\n\n    def sample(self) -&gt; pd.DataFrame:\n        \"\"\"Get a sample of the data from the underlying DataSource\n\n        Returns:\n            pd.DataFrame: A sample of the data from the underlying DataSource\n        \"\"\"\n        return self.data_source.sample()\n\n    def outliers(self, scale: float = 1.5) -&gt; pd.DataFrame:\n        \"\"\"Compute outliers for all the numeric columns in a DataSource\n\n        Args:\n            scale (float): The scale to use for the IQR (default: 1.5)\n        Returns:\n            pd.DataFrame: A DataFrame of outliers from this DataSource\n        Notes:\n            Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers\n            The scale parameter can be adjusted to change the IQR multiplier\n        \"\"\"\n        return self.data_source.outliers(scale=scale)\n\n    def smart_sample(self) -&gt; pd.DataFrame:\n        \"\"\"Get a SMART sample dataframe from this FeatureSet\n\n        Returns:\n            pd.DataFrame: A combined DataFrame of sample data + outliers\n        \"\"\"\n        return self.data_source.smart_sample()\n\n    def value_counts(self) -&gt; dict:\n        \"\"\"Get the value counts for the string columns of the underlying DataSource\n\n        Returns:\n            dict: A dictionary of value counts for the string columns\n        \"\"\"\n        return self.data_source.value_counts()\n\n    def correlations(self) -&gt; dict:\n        \"\"\"Get the correlations for the numeric columns of the underlying DataSource\n\n        Returns:\n            dict: A dictionary of correlations for the numeric columns\n        \"\"\"\n        return self.data_source.correlations()\n\n    def column_stats(self) -&gt; dict[dict]:\n        \"\"\"Compute Column Stats for all the columns in the FeatureSets underlying DataSource\n\n        Returns:\n            dict(dict): A dictionary of stats for each column this format\n            NB: String columns will NOT have num_zeros and descriptive_stats\n                {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n                 'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},\n                 ...}\n        \"\"\"\n\n        # Grab the column stats from our DataSource\n        ds_column_stats = self.data_source.column_stats()\n\n        # Map the types from our DataSource to the FeatureSet types\n        fs_type_mapper = self.column_details()\n        for col, details in ds_column_stats.items():\n            details[\"fs_dtype\"] = fs_type_mapper.get(col, \"unknown\")\n\n        return ds_column_stats\n\n    def ready(self) -&gt; bool:\n        \"\"\"Is the FeatureSet ready? Is initial setup complete and expected metadata populated?\n        Note: Since FeatureSet is a composite of DataSource and FeatureGroup, we need to\n           check both to see if the FeatureSet is ready.\"\"\"\n\n        # Check if our parent class (Artifact) is ready\n        if not super().ready():\n            return False\n\n        # Okay now call/return the DataSource ready() method\n        return self.data_source.ready()\n\n    def onboard(self) -&gt; bool:\n        \"\"\"This is a BLOCKING method that will onboard the FeatureSet (make it ready)\"\"\"\n\n        # Set our status to onboarding\n        self.log.important(f\"Onboarding {self.name}...\")\n        self.set_status(\"onboarding\")\n        self.remove_health_tag(\"needs_onboard\")\n\n        # Call our underlying DataSource onboard method\n        self.data_source.refresh_meta()\n        if not self.data_source.exists():\n            self.log.critical(f\"Data Source check failed for {self.name}\")\n            self.log.critical(\"Delete this Feature Set and recreate it to fix this issue\")\n            return False\n        if not self.data_source.ready():\n            self.data_source.onboard()\n\n        # Run a health check and refresh the meta\n        time.sleep(2)  # Give the AWS Metadata a chance to update\n        self.health_check(deep=True)\n        self.refresh_meta()\n        self.details()\n        self.set_status(\"ready\")\n        return True\n\n    def recompute_stats(self) -&gt; bool:\n        \"\"\"This is a BLOCKING method that will recompute the stats for the FeatureSet\"\"\"\n\n        # Call our underlying DataSource recompute stats method\n        self.log.important(f\"Recomputing Stats {self.name}...\")\n        self.data_source.recompute_stats()\n        self.details()\n        return True\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.column_types","title":"<code>column_types</code>  <code>property</code>","text":"<p>Return the column types of the Feature Set</p>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Return the column names of the Feature Set</p>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.table","title":"<code>table</code>  <code>property</code>","text":"<p>Get the base table name for this FeatureSet</p>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.__init__","title":"<code>__init__(feature_set_name, **kwargs)</code>","text":"<p>FeatureSetCore Initialization</p> <p>Parameters:</p> Name Type Description Default <code>feature_set_name</code> <code>str</code> <p>Name of Feature Set</p> required Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def __init__(self, feature_set_name: str, **kwargs):\n    \"\"\"FeatureSetCore Initialization\n\n    Args:\n        feature_set_name (str): Name of Feature Set\n    \"\"\"\n\n    # Make sure the feature_set name is valid\n    self.is_name_valid(feature_set_name)\n\n    # Call superclass init\n    super().__init__(feature_set_name, **kwargs)\n\n    # Get our FeatureSet metadata\n    self.feature_meta = self.meta.feature_set(self.name)\n\n    # Sanity check and then set up our FeatureSet attributes\n    if self.feature_meta is None:\n        self.log.warning(f\"Could not find feature set {self.name} within current visibility scope\")\n        self.data_source = None\n        return\n    else:\n        self.id_column = self.feature_meta[\"RecordIdentifierFeatureName\"]\n        self.event_time = self.feature_meta[\"EventTimeFeatureName\"]\n\n        # Pull Athena and S3 Storage information from metadata\n        self.athena_table = self.feature_meta[\"workbench_meta\"][\"athena_table\"]\n        self.athena_database = self.feature_meta[\"workbench_meta\"][\"athena_database\"]\n        self.s3_storage = self.feature_meta[\"workbench_meta\"].get(\"s3_storage\")\n\n        # Create our internal DataSource (hardcoded to Athena for now)\n        self.data_source = AthenaSource(self.athena_table, self.athena_database)\n\n        # Check our DataSource (AWS Metadata refresh can fix)\n        if not self.data_source.exists():\n            self.log.warning(\n                f\"FS: Data Source {self.athena_table} not found, sleeping and refreshing AWS Metadata...\"\n            )\n            time.sleep(3)\n            self.refresh_meta()\n\n    # Spin up our Feature Store\n    self.feature_store = FeatureStore(self.sm_session)\n\n    # Call superclass post_init\n    super().__post_init__()\n\n    # All done\n    self.log.info(f\"FeatureSet Initialized: {self.name}...\")\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.arn","title":"<code>arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for this artifact</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n    return self.feature_meta[\"FeatureGroupArn\"]\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.aws_meta","title":"<code>aws_meta()</code>","text":"<p>Get ALL the AWS metadata for this artifact</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def aws_meta(self) -&gt; dict:\n    \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n    return self.feature_meta\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.aws_url","title":"<code>aws_url()</code>","text":"<p>The AWS URL for looking at/querying the underlying data source</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def aws_url(self):\n    \"\"\"The AWS URL for looking at/querying the underlying data source\"\"\"\n    workbench_details = self.data_source.workbench_meta().get(\"workbench_details\", {})\n    return workbench_details.get(\"aws_url\", \"unknown\")\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.column_details","title":"<code>column_details()</code>","text":"<p>Return the column details of the Feature Set</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The column details of the Feature Set</p> Notes <p>We can't call just call self.data_source.column_details() because FeatureSets have different types, so we need to overlay that type information on top of the DataSource type information</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def column_details(self) -&gt; dict:\n    \"\"\"Return the column details of the Feature Set\n\n    Returns:\n        dict: The column details of the Feature Set\n\n    Notes:\n        We can't call just call self.data_source.column_details() because FeatureSets have different\n        types, so we need to overlay that type information on top of the DataSource type information\n    \"\"\"\n    fs_details = {item[\"FeatureName\"]: item[\"FeatureType\"] for item in self.feature_meta[\"FeatureDefinitions\"]}\n    ds_details = self.data_source.column_details()\n\n    # Overlay the FeatureSet type information on top of the DataSource type information\n    for col, dtype in ds_details.items():\n        ds_details[col] = fs_details.get(col, dtype)\n    return ds_details\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.column_stats","title":"<code>column_stats()</code>","text":"<p>Compute Column Stats for all the columns in the FeatureSets underlying DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of stats for each column this format</p> <code>NB</code> <code>dict[dict]</code> <p>String columns will NOT have num_zeros and descriptive_stats {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},  'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},  ...}</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def column_stats(self) -&gt; dict[dict]:\n    \"\"\"Compute Column Stats for all the columns in the FeatureSets underlying DataSource\n\n    Returns:\n        dict(dict): A dictionary of stats for each column this format\n        NB: String columns will NOT have num_zeros and descriptive_stats\n            {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n             'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},\n             ...}\n    \"\"\"\n\n    # Grab the column stats from our DataSource\n    ds_column_stats = self.data_source.column_stats()\n\n    # Map the types from our DataSource to the FeatureSet types\n    fs_type_mapper = self.column_details()\n    for col, details in ds_column_stats.items():\n        details[\"fs_dtype\"] = fs_type_mapper.get(col, \"unknown\")\n\n    return ds_column_stats\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.correlations","title":"<code>correlations()</code>","text":"<p>Get the correlations for the numeric columns of the underlying DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of correlations for the numeric columns</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def correlations(self) -&gt; dict:\n    \"\"\"Get the correlations for the numeric columns of the underlying DataSource\n\n    Returns:\n        dict: A dictionary of correlations for the numeric columns\n    \"\"\"\n    return self.data_source.correlations()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.create_s3_training_data","title":"<code>create_s3_training_data()</code>","text":"<p>Create some Training Data (S3 CSV) from a Feature Set using standard options. If you want additional options/features use the get_feature_store() method and see AWS docs for all the details: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html Returns:     str: The full path/file for the CSV file created by Feature Store create_dataset()</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def create_s3_training_data(self) -&gt; str:\n    \"\"\"Create some Training Data (S3 CSV) from a Feature Set using standard options. If you want\n    additional options/features use the get_feature_store() method and see AWS docs for all\n    the details: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html\n    Returns:\n        str: The full path/file for the CSV file created by Feature Store create_dataset()\n    \"\"\"\n\n    # Set up the S3 Query results path\n    date_time = datetime.now(timezone.utc).strftime(\"%Y-%m-%d_%H-%M-%S\")\n    s3_output_path = self.feature_sets_s3_path + f\"/{self.name}/datasets/all_{date_time}\"\n\n    # Make the query\n    table = self.view(\"training\").table\n    query = f'SELECT * FROM \"{table}\"'\n    athena_query = FeatureGroup(name=self.name, sagemaker_session=self.sm_session).athena_query()\n    athena_query.run(query, output_location=s3_output_path)\n    athena_query.wait()\n    query_execution = athena_query.get_query_execution()\n\n    # Get the full path to the S3 files with the results\n    full_s3_path = s3_output_path + f\"/{query_execution['QueryExecution']['QueryExecutionId']}.csv\"\n    return full_s3_path\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.created","title":"<code>created()</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    return self.feature_meta[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.delete","title":"<code>delete()</code>","text":"<p>Instance Method: Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def delete(self):\n    \"\"\"Instance Method: Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects\"\"\"\n    # Make sure the AthenaSource exists\n    if not self.exists():\n        self.log.warning(f\"Trying to delete an FeatureSet that doesn't exist: {self.name}\")\n\n    # Call the Class Method to delete the FeatureSet\n    FeatureSetCore.managed_delete(self.name)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.delete_views","title":"<code>delete_views(table, database)</code>  <code>classmethod</code>","text":"<p>Delete any views associated with this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Name of Athena Table</p> required <code>database</code> <code>str</code> <p>Athena Database Name</p> required Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>@classmethod\ndef delete_views(cls, table: str, database: str):\n    \"\"\"Delete any views associated with this FeatureSet\n\n    Args:\n        table (str): Name of Athena Table\n        database (str): Athena Database Name\n    \"\"\"\n    from workbench.core.views.view_utils import delete_views_and_supplemental_data\n\n    delete_views_and_supplemental_data(table, database, cls.boto3_session)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.descriptive_stats","title":"<code>descriptive_stats()</code>","text":"<p>Get the descriptive stats for the numeric columns of the underlying DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of descriptive stats for the numeric columns</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def descriptive_stats(self) -&gt; dict:\n    \"\"\"Get the descriptive stats for the numeric columns of the underlying DataSource\n\n    Returns:\n        dict: A dictionary of descriptive stats for the numeric columns\n    \"\"\"\n    return self.data_source.descriptive_stats()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.details","title":"<code>details()</code>","text":"<p>Additional Details about this FeatureSet Artifact</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about this FeatureSet</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def details(self) -&gt; dict[dict]:\n    \"\"\"Additional Details about this FeatureSet Artifact\n\n    Returns:\n        dict(dict): A dictionary of details about this FeatureSet\n    \"\"\"\n\n    self.log.info(f\"Computing FeatureSet Details ({self.name})...\")\n    details = self.summary()\n    details[\"aws_url\"] = self.aws_url()\n\n    # Store the AWS URL in the Workbench Metadata\n    # FIXME: We need to revisit this but doing an upsert just for aws_url is silly\n    # self.upsert_workbench_meta({\"aws_url\": details[\"aws_url\"]})\n\n    # Now get a summary of the underlying DataSource\n    details[\"storage_summary\"] = self.data_source.summary()\n\n    # Number of Columns\n    details[\"num_columns\"] = self.num_columns()\n\n    # Number of Rows\n    details[\"num_rows\"] = self.num_rows()\n\n    # Additional Details\n    details[\"workbench_status\"] = self.get_status()\n    details[\"workbench_input\"] = self.get_input()\n    details[\"workbench_tags\"] = self.tag_delimiter.join(self.get_tags())\n\n    # Underlying Storage Details\n    details[\"storage_type\"] = \"athena\"  # TODO: Add RDS support\n    details[\"storage_name\"] = self.data_source.name\n\n    # Add the column details and column stats\n    details[\"column_details\"] = self.column_details()\n    details[\"column_stats\"] = self.column_stats()\n\n    # Return the details data\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.exists","title":"<code>exists()</code>","text":"<p>Does the feature_set_name exist in the AWS Metadata?</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Does the feature_set_name exist in the AWS Metadata?\"\"\"\n    if self.feature_meta is None:\n        self.log.debug(f\"FeatureSet {self.name} not found in AWS Metadata!\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.get_compressed_features","title":"<code>get_compressed_features()</code>","text":"<p>Get the compressed features for this FeatureSet</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The compressed columns for this FeatureSet</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def get_compressed_features(self) -&gt; list[str]:\n    \"\"\"Get the compressed features for this FeatureSet\n\n    Returns:\n        list[str]: The compressed columns for this FeatureSet\n    \"\"\"\n    # Get the compressed features from our FeatureSet metadata\n    return self.workbench_meta().get(\"compressed_features\", [])\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.get_data_source","title":"<code>get_data_source()</code>","text":"<p>Return the underlying DataSource object</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def get_data_source(self) -&gt; DataSourceFactory:\n    \"\"\"Return the underlying DataSource object\"\"\"\n    return self.data_source\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.get_feature_store","title":"<code>get_feature_store()</code>","text":"<p>Return the underlying AWS FeatureStore object. This can be useful for more advanced usage with create_dataset() such as Joins and time ranges and a host of other options See: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def get_feature_store(self) -&gt; FeatureStore:\n    \"\"\"Return the underlying AWS FeatureStore object. This can be useful for more advanced usage\n    with create_dataset() such as Joins and time ranges and a host of other options\n    See: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html\n    \"\"\"\n    return self.feature_store\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.get_training_data","title":"<code>get_training_data()</code>","text":"<p>Get the training data for this FeatureSet</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The training data for this FeatureSet</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def get_training_data(self) -&gt; pd.DataFrame:\n    \"\"\"Get the training data for this FeatureSet\n\n    Returns:\n        pd.DataFrame: The training data for this FeatureSet\n    \"\"\"\n    from workbench.core.views.view import View\n\n    return View(self, \"training\").pull_dataframe(limit=1_000_000)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.get_training_holdouts","title":"<code>get_training_holdouts()</code>","text":"<p>Get the hold out ids for the training view for this FeatureSet</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The list of holdout ids.</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def get_training_holdouts(self) -&gt; list[str]:\n    \"\"\"Get the hold out ids for the training view for this FeatureSet\n\n    Returns:\n        list[str]: The list of holdout ids.\n    \"\"\"\n\n    # Create a NEW training view\n    self.log.important(\"Getting Training Holdouts...\")\n    table = self.view(\"training\").table\n    hold_out_ids = self.query(f'SELECT {self.id_column} FROM \"{table}\" where training = FALSE')[\n        self.id_column\n    ].tolist()\n    return hold_out_ids\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.hash","title":"<code>hash()</code>","text":"<p>Return the hash for the set of Parquet files for this artifact</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def hash(self) -&gt; str:\n    \"\"\"Return the hash for the set of Parquet files for this artifact\"\"\"\n    return self.data_source.hash()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.health_check","title":"<code>health_check(deep=False)</code>","text":"<p>Perform a health check on this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, perform more extensive (expensive) health checks (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of health issues</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def health_check(self, deep: bool = False) -&gt; list[str]:\n    \"\"\"Perform a health check on this FeatureSet\n\n    Args:\n        deep (bool): If True, perform more extensive (expensive) health checks (default: False)\n\n    Returns:\n        list[str]: List of health issues\n    \"\"\"\n    # Call the base class health check\n    health_issues = super().health_check(deep=deep)\n\n    # If we have a 'needs_onboard' in the health check then just return\n    if \"needs_onboard\" in health_issues:\n        return health_issues\n\n    # Check our DataSource\n    if not self.data_source.exists():\n        self.log.critical(f\"Data Source check failed for {self.name}\")\n        self.log.critical(\"Delete this Feature Set and recreate it to fix this issue\")\n        health_issues.append(\"data_source_missing\")\n    return health_issues\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.managed_delete","title":"<code>managed_delete(feature_set_name)</code>  <code>classmethod</code>","text":"<p>Class Method: Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects</p> <p>Parameters:</p> Name Type Description Default <code>feature_set_name</code> <code>str</code> <p>The Name of the FeatureSet to delete</p> required Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>@classmethod\ndef managed_delete(cls, feature_set_name: str):\n    \"\"\"Class Method: Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects\n\n    Args:\n        feature_set_name (str): The Name of the FeatureSet to delete\n    \"\"\"\n\n    # See if the FeatureSet exists\n    try:\n        response = cls.sm_client.describe_feature_group(FeatureGroupName=feature_set_name)\n    except cls.sm_client.exceptions.ResourceNotFound:\n        cls.log.info(f\"FeatureSet {feature_set_name} not found!\")\n        return\n\n    # Extract database and table information from the response\n    offline_config = response.get(\"OfflineStoreConfig\", {})\n    database = offline_config.get(\"DataCatalogConfig\", {}).get(\"Database\")\n    offline_table = offline_config.get(\"DataCatalogConfig\", {}).get(\"TableName\")\n    data_source_name = offline_table  # Our offline storage IS a DataSource\n\n    # Delete the Feature Group and ensure that it gets deleted\n    cls.log.important(f\"Deleting FeatureSet {feature_set_name}...\")\n    remove_fg = cls.aws_feature_group_delete(feature_set_name)\n    cls.ensure_feature_group_deleted(remove_fg)\n\n    # Delete our underlying DataSource (Data Catalog Table and S3 Storage Objects)\n    AthenaSource.managed_delete(data_source_name, database=database)\n\n    # Delete any views associated with this FeatureSet\n    cls.delete_views(offline_table, database)\n\n    # Feature Sets can often have a lot of cruft so delete the entire bucket/prefix\n    s3_delete_path = cls.feature_sets_s3_path + f\"/{feature_set_name}/\"\n    cls.log.info(f\"Deleting All FeatureSet S3 Storage Objects {s3_delete_path}\")\n    wr.s3.delete_objects(s3_delete_path, boto3_session=cls.boto3_session)\n\n    # Delete any dataframes that were stored in the Dataframe Cache\n    cls.log.info(\"Deleting Dataframe Cache...\")\n    cls.df_cache.delete_recursive(feature_set_name)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.modified","title":"<code>modified()</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    # Note: We can't currently figure out how to this from AWS Metadata\n    return self.feature_meta[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.num_columns","title":"<code>num_columns()</code>","text":"<p>Return the number of columns of the Feature Set</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def num_columns(self) -&gt; int:\n    \"\"\"Return the number of columns of the Feature Set\"\"\"\n    return len(self.columns)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.num_rows","title":"<code>num_rows()</code>","text":"<p>Return the number of rows of the internal DataSource</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def num_rows(self) -&gt; int:\n    \"\"\"Return the number of rows of the internal DataSource\"\"\"\n    return self.data_source.num_rows()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.onboard","title":"<code>onboard()</code>","text":"<p>This is a BLOCKING method that will onboard the FeatureSet (make it ready)</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def onboard(self) -&gt; bool:\n    \"\"\"This is a BLOCKING method that will onboard the FeatureSet (make it ready)\"\"\"\n\n    # Set our status to onboarding\n    self.log.important(f\"Onboarding {self.name}...\")\n    self.set_status(\"onboarding\")\n    self.remove_health_tag(\"needs_onboard\")\n\n    # Call our underlying DataSource onboard method\n    self.data_source.refresh_meta()\n    if not self.data_source.exists():\n        self.log.critical(f\"Data Source check failed for {self.name}\")\n        self.log.critical(\"Delete this Feature Set and recreate it to fix this issue\")\n        return False\n    if not self.data_source.ready():\n        self.data_source.onboard()\n\n    # Run a health check and refresh the meta\n    time.sleep(2)  # Give the AWS Metadata a chance to update\n    self.health_check(deep=True)\n    self.refresh_meta()\n    self.details()\n    self.set_status(\"ready\")\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.outliers","title":"<code>outliers(scale=1.5)</code>","text":"<p>Compute outliers for all the numeric columns in a DataSource</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>The scale to use for the IQR (default: 1.5)</p> <code>1.5</code> <p>Returns:     pd.DataFrame: A DataFrame of outliers from this DataSource Notes:     Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers     The scale parameter can be adjusted to change the IQR multiplier</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def outliers(self, scale: float = 1.5) -&gt; pd.DataFrame:\n    \"\"\"Compute outliers for all the numeric columns in a DataSource\n\n    Args:\n        scale (float): The scale to use for the IQR (default: 1.5)\n    Returns:\n        pd.DataFrame: A DataFrame of outliers from this DataSource\n    Notes:\n        Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers\n        The scale parameter can be adjusted to change the IQR multiplier\n    \"\"\"\n    return self.data_source.outliers(scale=scale)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.query","title":"<code>query(query, overwrite=True)</code>","text":"<p>Query the internal DataSource</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to run against the DataSource</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table name in the query (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The results of the query</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def query(self, query: str, overwrite: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Query the internal DataSource\n\n    Args:\n        query (str): The query to run against the DataSource\n        overwrite (bool): Overwrite the table name in the query (default: True)\n\n    Returns:\n        pd.DataFrame: The results of the query\n    \"\"\"\n    if overwrite:\n        query = query.replace(\" \" + self.name + \" \", \" \" + self.athena_table + \" \")\n    return self.data_source.query(query)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.ready","title":"<code>ready()</code>","text":"<p>Is the FeatureSet ready? Is initial setup complete and expected metadata populated? Note: Since FeatureSet is a composite of DataSource and FeatureGroup, we need to    check both to see if the FeatureSet is ready.</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def ready(self) -&gt; bool:\n    \"\"\"Is the FeatureSet ready? Is initial setup complete and expected metadata populated?\n    Note: Since FeatureSet is a composite of DataSource and FeatureGroup, we need to\n       check both to see if the FeatureSet is ready.\"\"\"\n\n    # Check if our parent class (Artifact) is ready\n    if not super().ready():\n        return False\n\n    # Okay now call/return the DataSource ready() method\n    return self.data_source.ready()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.recompute_stats","title":"<code>recompute_stats()</code>","text":"<p>This is a BLOCKING method that will recompute the stats for the FeatureSet</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def recompute_stats(self) -&gt; bool:\n    \"\"\"This is a BLOCKING method that will recompute the stats for the FeatureSet\"\"\"\n\n    # Call our underlying DataSource recompute stats method\n    self.log.important(f\"Recomputing Stats {self.name}...\")\n    self.data_source.recompute_stats()\n    self.details()\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.refresh_meta","title":"<code>refresh_meta()</code>","text":"<p>Internal: Refresh our internal AWS Feature Store metadata</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def refresh_meta(self):\n    \"\"\"Internal: Refresh our internal AWS Feature Store metadata\"\"\"\n    self.log.info(f\"Calling refresh_meta() on the FeatureSet {self.name}\")\n    self.feature_meta = self.meta.feature_set(self.name)\n    self.id_column = self.feature_meta[\"RecordIdentifierFeatureName\"]\n    self.event_time = self.feature_meta[\"EventTimeFeatureName\"]\n    self.athena_table = self.feature_meta[\"workbench_meta\"][\"athena_table\"]\n    self.athena_database = self.feature_meta[\"workbench_meta\"][\"athena_database\"]\n    self.s3_storage = self.feature_meta[\"workbench_meta\"].get(\"s3_storage\")\n    self.data_source = AthenaSource(self.athena_table, self.athena_database)\n    self.log.info(f\"Calling refresh_meta() on the DataSource {self.data_source.name}\")\n    self.data_source.refresh_meta()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.sample","title":"<code>sample()</code>","text":"<p>Get a sample of the data from the underlying DataSource</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A sample of the data from the underlying DataSource</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def sample(self) -&gt; pd.DataFrame:\n    \"\"\"Get a sample of the data from the underlying DataSource\n\n    Returns:\n        pd.DataFrame: A sample of the data from the underlying DataSource\n    \"\"\"\n    return self.data_source.sample()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.set_compressed_features","title":"<code>set_compressed_features(compressed_columns)</code>","text":"<p>Set the compressed features for this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>compressed_columns</code> <code>list[str]</code> <p>The compressed columns for this FeatureSet</p> required Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def set_compressed_features(self, compressed_columns: list[str]):\n    \"\"\"Set the compressed features for this FeatureSet\n\n    Args:\n        compressed_columns (list[str]): The compressed columns for this FeatureSet\n    \"\"\"\n    # Ensure that the compressed features are a subset of the columns\n    if not set(compressed_columns).issubset(set(self.columns)):\n        self.log.warning(\n            f\"Compressed columns {compressed_columns} are not a subset of the columns {self.columns}. \"\n        )\n        return\n\n    # Set the compressed features in our FeatureSet metadata\n    self.log.important(f\"Setting Compressed Columns...{compressed_columns}\")\n    self.upsert_workbench_meta({\"compressed_features\": compressed_columns})\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.set_computation_columns","title":"<code>set_computation_columns(computation_columns, reset_display=True)</code>","text":"<p>Set the computation columns for this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>computation_columns</code> <code>list[str]</code> <p>The computation columns for this FeatureSet</p> required <code>reset_display</code> <code>bool</code> <p>Also reset the display columns to match (default: True)</p> <code>True</code> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def set_computation_columns(self, computation_columns: list[str], reset_display: bool = True):\n    \"\"\"Set the computation columns for this FeatureSet\n\n    Args:\n        computation_columns (list[str]): The computation columns for this FeatureSet\n        reset_display (bool): Also reset the display columns to match (default: True)\n    \"\"\"\n    self.log.important(f\"Setting Computation Columns...{computation_columns}\")\n    from workbench.core.views import ComputationView\n\n    # Create a NEW computation view\n    ComputationView.create(self, column_list=computation_columns)\n    self.recompute_stats()\n\n    # Reset the display columns to match the computation columns\n    if reset_display:\n        self.set_display_columns(computation_columns)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.set_display_columns","title":"<code>set_display_columns(display_columns)</code>","text":"<p>Set the display columns for this Data Source</p> <p>Parameters:</p> Name Type Description Default <code>display_columns</code> <code>list[str]</code> <p>The display columns for this Data Source</p> required Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def set_display_columns(self, display_columns: list[str]):\n    \"\"\"Set the display columns for this Data Source\n\n    Args:\n        display_columns (list[str]): The display columns for this Data Source\n    \"\"\"\n    # Check mismatch of display columns to computation columns\n    c_view = self.view(\"computation\")\n    computation_columns = c_view.columns\n    mismatch_columns = [col for col in display_columns if col not in computation_columns]\n    if mismatch_columns:\n        self.log.monitor(f\"Display View/Computation mismatch: {mismatch_columns}\")\n\n    self.log.important(f\"Setting Display Columns...{display_columns}\")\n    from workbench.core.views import DisplayView\n\n    # Create a NEW display view\n    DisplayView.create(self, source_table=c_view.table, column_list=display_columns)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.set_sample_weights","title":"<code>set_sample_weights(weight_dict, default_weight=1.0, exclude_zero_weights=True)</code>","text":"<p>Configure training view with sample weights for each ID.</p> <p>Parameters:</p> Name Type Description Default <code>weight_dict</code> <code>Dict[Union[str, int], float]</code> <p>Mapping of ID to sample weight - weight &gt; 1.0: oversample/emphasize - weight = 1.0: normal (default) - 0 &lt; weight &lt; 1.0: downweight/de-emphasize - weight = 0.0: exclude from training</p> required <code>default_weight</code> <code>float</code> <p>Weight for IDs not in weight_dict (default: 1.0)</p> <code>1.0</code> <code>exclude_zero_weights</code> <code>bool</code> <p>If True, filter out rows with sample_weight=0 (default: True)</p> <code>True</code> Example <p>weights = {     'compound_42': 3.0,  # oversample 3x     'compound_99': 0.1,  # noisy, downweight     'compound_123': 0.0, # exclude from training } fs.set_sample_weights(weights)  # zeros automatically excluded fs.set_sample_weights(weights, exclude_zero_weights=False)  # keep zeros</p> Note <p>For large weight_dict (100+ entries), weights are stored as a supplemental table and joined to avoid Athena query size limits.</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def set_sample_weights(\n    self,\n    weight_dict: Dict[Union[str, int], float],\n    default_weight: float = 1.0,\n    exclude_zero_weights: bool = True,\n):\n    \"\"\"Configure training view with sample weights for each ID.\n\n    Args:\n        weight_dict: Mapping of ID to sample weight\n            - weight &gt; 1.0: oversample/emphasize\n            - weight = 1.0: normal (default)\n            - 0 &lt; weight &lt; 1.0: downweight/de-emphasize\n            - weight = 0.0: exclude from training\n        default_weight: Weight for IDs not in weight_dict (default: 1.0)\n        exclude_zero_weights: If True, filter out rows with sample_weight=0 (default: True)\n\n    Example:\n        weights = {\n            'compound_42': 3.0,  # oversample 3x\n            'compound_99': 0.1,  # noisy, downweight\n            'compound_123': 0.0, # exclude from training\n        }\n        fs.set_sample_weights(weights)  # zeros automatically excluded\n        fs.set_sample_weights(weights, exclude_zero_weights=False)  # keep zeros\n\n    Note:\n        For large weight_dict (100+ entries), weights are stored as a supplemental\n        table and joined to avoid Athena query size limits.\n    \"\"\"\n    from workbench.core.views import TrainingView\n\n    if not weight_dict:\n        self.log.important(\"Empty weight_dict, creating standard training view\")\n        TrainingView.create(self, id_column=self.id_column)\n        return\n\n    self.log.important(f\"Setting sample weights for {len(weight_dict)} IDs\")\n\n    # For large weight_dict, use supplemental table + JOIN to avoid query size limits\n    if len(weight_dict) &gt;= 100:\n        self.log.info(\"Using supplemental table approach for large weight_dict\")\n        weights_table = self._create_weights_table(weight_dict)\n\n        # Build JOIN query with COALESCE for default weight\n        inner_sql = f\"\"\"SELECT t.*, COALESCE(w.sample_weight, {default_weight}) AS sample_weight\n            FROM {self.table} t\n            LEFT JOIN {weights_table} w ON t.{self.id_column} = w.{self.id_column}\"\"\"\n    else:\n        # For small weight_dict, use CASE statement (simpler, no extra table)\n        weight_case = self._build_weight_case(weight_dict, default_weight)\n        inner_sql = f\"SELECT *, {weight_case} FROM {self.table}\"\n\n    # Optionally filter out zero weights\n    if exclude_zero_weights:\n        zero_count = sum(1 for w in weight_dict.values() if w == 0.0)\n        if zero_count:\n            self.log.important(f\"Filtering out {zero_count} rows with sample_weight = 0\")\n        sql_query = f\"SELECT * FROM ({inner_sql}) WHERE sample_weight &gt; 0\"\n    else:\n        sql_query = inner_sql\n\n    TrainingView.create_with_sql(self, sql_query=sql_query, id_column=self.id_column)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.set_training_config","title":"<code>set_training_config(holdout_ids=None, weight_dict=None, default_weight=1.0, exclude_zero_weights=True)</code>","text":"<p>Configure training view with holdout IDs and/or sample weights.</p> <p>This method creates a training view that can include both: - A 'training' column (True/False) based on holdout IDs - A 'sample_weight' column for weighted training</p> <p>Parameters:</p> Name Type Description Default <code>holdout_ids</code> <code>List[Union[str, int]]</code> <p>List of IDs to mark as training=False (validation/holdout set)</p> <code>None</code> <code>weight_dict</code> <code>Dict[Union[str, int], float]</code> <p>Mapping of ID to sample weight - weight &gt; 1.0: oversample/emphasize - weight = 1.0: normal (default) - 0 &lt; weight &lt; 1.0: downweight/de-emphasize - weight = 0.0: exclude from training (filtered out if exclude_zero_weights=True)</p> <code>None</code> <code>default_weight</code> <code>float</code> <p>Weight for IDs not in weight_dict (default: 1.0)</p> <code>1.0</code> <code>exclude_zero_weights</code> <code>bool</code> <p>If True, filter out rows with sample_weight=0 (default: True)</p> <code>True</code> Example Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def set_training_config(\n    self,\n    holdout_ids: List[Union[str, int]] = None,\n    weight_dict: Dict[Union[str, int], float] = None,\n    default_weight: float = 1.0,\n    exclude_zero_weights: bool = True,\n):\n    \"\"\"Configure training view with holdout IDs and/or sample weights.\n\n    This method creates a training view that can include both:\n    - A 'training' column (True/False) based on holdout IDs\n    - A 'sample_weight' column for weighted training\n\n    Args:\n        holdout_ids: List of IDs to mark as training=False (validation/holdout set)\n        weight_dict: Mapping of ID to sample weight\n            - weight &gt; 1.0: oversample/emphasize\n            - weight = 1.0: normal (default)\n            - 0 &lt; weight &lt; 1.0: downweight/de-emphasize\n            - weight = 0.0: exclude from training (filtered out if exclude_zero_weights=True)\n        default_weight: Weight for IDs not in weight_dict (default: 1.0)\n        exclude_zero_weights: If True, filter out rows with sample_weight=0 (default: True)\n\n    Example:\n        # Temporal split with sample weights\n        fs.set_training_config(\n            holdout_ids=temporal_hold_out_ids,  # IDs after cutoff date\n            weight_dict={'compound_42': 0.0, 'compound_99': 2.0},  # exclude/upweight\n        )\n    \"\"\"\n    from workbench.core.views.training_view import TrainingView\n\n    # If neither is provided, create a standard training view\n    if not holdout_ids and not weight_dict:\n        self.log.important(\"No holdouts or weights specified, creating standard training view\")\n        TrainingView.create(self, id_column=self.id_column)\n        return\n\n    # If only holdout_ids, delegate to set_training_holdouts\n    if holdout_ids and not weight_dict:\n        self.set_training_holdouts(holdout_ids)\n        return\n\n    # If only weight_dict, delegate to set_sample_weights\n    if weight_dict and not holdout_ids:\n        self.set_sample_weights(weight_dict, default_weight, exclude_zero_weights)\n        return\n\n    # Both holdout_ids and weight_dict provided - build combined view\n    self.log.important(f\"Setting training config: {len(holdout_ids)} holdouts, {len(weight_dict)} weights\")\n\n    # Get column list (excluding AWS-generated columns)\n    from workbench.core.views.view_utils import get_column_list\n\n    aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n    source_columns = get_column_list(self.data_source, self.table)\n    column_list = [col for col in source_columns if col not in aws_cols]\n\n    # Build the training column CASE statement\n    training_case = self._build_holdout_case(holdout_ids)\n\n    # For large weight_dict, use supplemental table + JOIN\n    if len(weight_dict) &gt;= 100:\n        self.log.info(\"Using supplemental table approach for large weight_dict\")\n        weights_table = self._create_weights_table(weight_dict)\n\n        # Build column selection with table alias\n        sql_columns = \", \".join([f't.\"{col}\"' for col in column_list])\n\n        # Build JOIN query with training CASE and weight from joined table\n        training_case_aliased = training_case.replace(f\"WHEN {self.id_column} IN\", f\"WHEN t.{self.id_column} IN\")\n        inner_sql = f\"\"\"SELECT {sql_columns}, {training_case_aliased},\n            COALESCE(w.sample_weight, {default_weight}) AS sample_weight\n            FROM {self.table} t\n            LEFT JOIN {weights_table} w ON t.{self.id_column} = w.{self.id_column}\"\"\"\n    else:\n        # For small weight_dict, use CASE statement\n        sql_columns = \", \".join([f'\"{column}\"' for column in column_list])\n        weight_case = self._build_weight_case(weight_dict, default_weight)\n        inner_sql = f\"SELECT {sql_columns}, {training_case}, {weight_case} FROM {self.table}\"\n\n    # Optionally filter out zero weights\n    if exclude_zero_weights:\n        zero_count = sum(1 for w in weight_dict.values() if w == 0.0)\n        if zero_count:\n            self.log.important(f\"Filtering out {zero_count} rows with sample_weight = 0\")\n        sql_query = f\"SELECT * FROM ({inner_sql}) WHERE sample_weight &gt; 0\"\n    else:\n        sql_query = inner_sql\n\n    self._create_training_view(sql_query)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.set_training_config--temporal-split-with-sample-weights","title":"Temporal split with sample weights","text":"<p>fs.set_training_config(     holdout_ids=temporal_hold_out_ids,  # IDs after cutoff date     weight_dict={'compound_42': 0.0, 'compound_99': 2.0},  # exclude/upweight )</p>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.set_training_holdouts","title":"<code>set_training_holdouts(holdout_ids)</code>","text":"<p>Set the hold out ids for the training view for this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>holdout_ids</code> <code>list[str]</code> <p>The list of holdout ids.</p> required Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def set_training_holdouts(self, holdout_ids: list[str]):\n    \"\"\"Set the hold out ids for the training view for this FeatureSet\n\n    Args:\n        holdout_ids (list[str]): The list of holdout ids.\n    \"\"\"\n    from workbench.core.views import TrainingView\n\n    self.log.important(f\"Setting Training Holdouts: {len(holdout_ids)} ids...\")\n    TrainingView.create(self, id_column=self.id_column, holdout_ids=holdout_ids)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.size","title":"<code>size()</code>","text":"<p>Return the size of the internal DataSource in MegaBytes</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def size(self) -&gt; float:\n    \"\"\"Return the size of the internal DataSource in MegaBytes\"\"\"\n    return self.data_source.size()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.smart_sample","title":"<code>smart_sample()</code>","text":"<p>Get a SMART sample dataframe from this FeatureSet</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A combined DataFrame of sample data + outliers</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def smart_sample(self) -&gt; pd.DataFrame:\n    \"\"\"Get a SMART sample dataframe from this FeatureSet\n\n    Returns:\n        pd.DataFrame: A combined DataFrame of sample data + outliers\n    \"\"\"\n    return self.data_source.smart_sample()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.snapshot_query","title":"<code>snapshot_query(table_name=None)</code>","text":"<p>An Athena query to get the latest snapshot of features</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to query (default: None)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Athena query to get the latest snapshot of features</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def snapshot_query(self, table_name: str = None) -&gt; str:\n    \"\"\"An Athena query to get the latest snapshot of features\n\n    Args:\n        table_name (str): The name of the table to query (default: None)\n\n    Returns:\n        str: The Athena query to get the latest snapshot of features\n    \"\"\"\n    # Remove FeatureGroup metadata columns that might have gotten added\n    columns = self.columns\n    filter_columns = [\"write_time\", \"api_invocation_time\", \"is_deleted\"]\n    columns = \", \".join(['\"' + x + '\"' for x in columns if x not in filter_columns])\n\n    query = (\n        f\"SELECT {columns} \"\n        f\"    FROM (SELECT *, row_number() OVER (PARTITION BY {self.id_column} \"\n        f\"        ORDER BY {self.event_time} desc, api_invocation_time DESC, write_time DESC) AS row_num \"\n        f'        FROM \"{table_name}\") '\n        \"    WHERE row_num = 1 and NOT is_deleted;\"\n    )\n    return query\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.supplemental_data","title":"<code>supplemental_data()</code>","text":"<p>Return the supplemental data for this Data Source</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def supplemental_data(self) -&gt; list[str]:\n    \"\"\"Return the supplemental data for this Data Source\"\"\"\n    from workbench.core.views.view_utils import list_supplemental_data\n\n    return list_supplemental_data(self.data_source)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.table_hash","title":"<code>table_hash()</code>","text":"<p>Return the hash for the Athena table</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def table_hash(self) -&gt; str:\n    \"\"\"Return the hash for the Athena table\"\"\"\n    return self.data_source.table_hash()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.value_counts","title":"<code>value_counts()</code>","text":"<p>Get the value counts for the string columns of the underlying DataSource</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of value counts for the string columns</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def value_counts(self) -&gt; dict:\n    \"\"\"Get the value counts for the string columns of the underlying DataSource\n\n    Returns:\n        dict: A dictionary of value counts for the string columns\n    \"\"\"\n    return self.data_source.value_counts()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.view","title":"<code>view(view_name)</code>","text":"<p>Return a DataFrame for a specific view Args:     view_name (str): The name of the view to return Returns:     pd.DataFrame: A DataFrame for the specified view</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def view(self, view_name: str) -&gt; \"View\":\n    \"\"\"Return a DataFrame for a specific view\n    Args:\n        view_name (str): The name of the view to return\n    Returns:\n        pd.DataFrame: A DataFrame for the specified view\n    \"\"\"\n    from workbench.core.views import View\n\n    return View(self, view_name)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#workbench.core.artifacts.feature_set_core.FeatureSetCore.views","title":"<code>views()</code>","text":"<p>Return the views for this Data Source</p> Source code in <code>src/workbench/core/artifacts/feature_set_core.py</code> <pre><code>def views(self) -&gt; list[str]:\n    \"\"\"Return the views for this Data Source\"\"\"\n    from workbench.core.views.view_utils import list_views\n\n    return list_views(self.data_source)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/","title":"ModelCore","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the Model API Class and voil\u00e0 it works the same.</p> <p>ModelCore: Workbench ModelCore Class</p>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore","title":"<code>ModelCore</code>","text":"<p>               Bases: <code>Artifact</code></p> <p>ModelCore: Workbench ModelCore Class</p> Common Usage <pre><code>my_model = ModelCore(model_name)\nmy_model.summary()\nmy_model.details()\n</code></pre> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>class ModelCore(Artifact):\n    \"\"\"ModelCore: Workbench ModelCore Class\n\n    Common Usage:\n        ```python\n        my_model = ModelCore(model_name)\n        my_model.summary()\n        my_model.details()\n        ```\n    \"\"\"\n\n    def __init__(self, model_name: str, **kwargs):\n        \"\"\"ModelCore Initialization\n        Args:\n            model_name (str): Name of Model in Workbench.\n            **kwargs: Additional keyword arguments\n        \"\"\"\n\n        # Make sure the model name is valid\n        self.is_name_valid(model_name, delimiter=\"-\", lower_case=False)\n\n        # Call SuperClass Initialization\n        super().__init__(model_name, **kwargs)\n\n        # Initialize our class attributes\n        self.latest_model = None\n        self.model_type = ModelType.UNKNOWN\n        self.model_training_path = None\n        self.endpoint_inference_path = None\n\n        # Grab an Cloud Platform Meta object and pull information for this Model\n        self.model_name = model_name\n        self.model_meta = self.meta.model(self.model_name)\n        if self.model_meta is None:\n            self.log.warning(f\"Could not find model {self.model_name} within current visibility scope\")\n            return\n        else:\n            # Is this a model package group without any models?\n            if len(self.model_meta[\"ModelPackageList\"]) == 0:\n                self.log.critical(f\"Model Group {self.model_name} has no Model Packages!\")\n                self.latest_model = None\n                self.training_job_name = None\n                self.add_health_tag(\"model_not_found\")\n                return\n            try:\n                self.latest_model = self.model_meta[\"ModelPackageList\"][0]\n                self.description = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n                self.training_job_name = self._extract_training_job_name()\n                self.model_type = self._get_model_type()\n                self.model_framework = self._get_model_framework()\n            except (IndexError, KeyError):\n                self.log.critical(f\"Model {self.model_name} appears to be malformed. Delete and recreate it!\")\n                return\n\n        # Set the Model Training S3 Path\n        self.model_training_path = f\"{self.models_s3_path}/{self.model_name}/training\"\n\n        # Get our Endpoint Inference Path (might be None)\n        self.endpoint_inference_path = self.get_endpoint_inference_path()\n\n        # Call SuperClass Post Initialization\n        super().__post_init__()\n\n        # All done\n        self.log.info(f\"Model Initialized: {self.model_name}\")\n\n    def refresh_meta(self):\n        \"\"\"Refresh the Artifact's metadata\"\"\"\n        self.model_meta = self.meta.model(self.model_name)\n        self.latest_model = self.model_meta[\"ModelPackageList\"][0]\n        self.description = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n        self.training_job_name = self._extract_training_job_name()\n\n    def exists(self) -&gt; bool:\n        \"\"\"Does the model metadata exist in the AWS Metadata?\"\"\"\n        if self.model_meta is None:\n            self.log.info(f\"Model {self.model_name} not found in AWS Metadata!\")\n            return False\n        return True\n\n    def health_check(self, deep: bool = False) -&gt; list[str]:\n        \"\"\"Perform a health check on this model\n\n        Args:\n            deep (bool): If True, perform more extensive (expensive) health checks (default: False)\n\n        Returns:\n            list[str]: List of health issues\n        \"\"\"\n        # Call the base class health check\n        health_issues = super().health_check(deep=deep)\n\n        # Check if the model exists\n        if self.latest_model is None:\n            health_issues.append(\"model_not_found\")\n\n        # Model Type\n        if self._get_model_type() == ModelType.UNKNOWN:\n            health_issues.append(\"model_type_unknown\")\n        else:\n            self.remove_health_tag(\"model_type_unknown\")\n\n        # Deep checks (expensive API/S3 calls)\n        if deep:\n            # Model Performance Metrics\n            needs_metrics = self.model_type in {\n                ModelType.REGRESSOR,\n                ModelType.UQ_REGRESSOR,\n                ModelType.ENSEMBLE_REGRESSOR,\n                ModelType.CLASSIFIER,\n            }\n            if needs_metrics and self.get_inference_metrics() is None:\n                health_issues.append(\"metrics_needed\")\n            else:\n                self.remove_health_tag(\"metrics_needed\")\n\n            # Endpoint\n            if not self.endpoints():\n                health_issues.append(\"no_endpoint\")\n            else:\n                self.remove_health_tag(\"no_endpoint\")\n\n        return health_issues\n\n    def sagemaker_model_object(self) -&gt; SagemakerModel:\n        \"\"\"Return the latest AWS Sagemaker Model object for this Workbench Model\n\n        Returns:\n           sagemaker.model.Model: AWS Sagemaker Model object\n        \"\"\"\n        return SagemakerModel(\n            model_data=self.model_data_url(),\n            sagemaker_session=self.sm_session,\n            image_uri=self.container_image(),\n        )\n\n    def list_inference_runs(self) -&gt; list[str]:\n        \"\"\"List the inference runs for this model\n\n        Returns:\n            list[str]: List of inference runs\n        \"\"\"\n\n        # Check if we have a model (if not return empty list)\n        if self.latest_model is None:\n            return []\n\n        # Check if we have model training metrics in our metadata\n        have_model_training = True if self.workbench_meta().get(\"workbench_training_metrics\") else False\n\n        # Now grab the list of directories from our inference path\n        inference_runs = []\n        if self.endpoint_inference_path:\n            directories = wr.s3.list_directories(path=self.endpoint_inference_path + \"/\")\n            inference_runs = [urlparse(directory).path.split(\"/\")[-2] for directory in directories]\n\n        # We're going to add the model training to the end of the list\n        if have_model_training:\n            inference_runs.append(\"model_training\")\n        return inference_runs\n\n    def delete_inference_run(self, inference_run_name: str):\n        \"\"\"Delete the inference run for this model\n\n        Args:\n            inference_run_name (str): Name of the inference run\n        \"\"\"\n        if inference_run_name == \"model_training\":\n            self.log.warning(\"Cannot delete model training data!\")\n            return\n\n        if self.endpoint_inference_path:\n            full_path = f\"{self.endpoint_inference_path}/{inference_run_name}\"\n            # Check if there are any objects at the path\n            if wr.s3.list_objects(full_path):\n                wr.s3.delete_objects(path=full_path)\n                self.log.important(f\"Deleted inference run {inference_run_name} for {self.model_name}\")\n            else:\n                self.log.warning(f\"Inference run {inference_run_name} not found for {self.model_name}!\")\n        else:\n            self.log.important(f\"No inference data found for {self.model_name}!\")\n\n    def get_inference_metrics(self, capture_name: str = \"auto\") -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the inference performance metrics for this model\n\n        Args:\n            capture_name (str, optional): Specific capture_name (default: \"auto\")\n        Returns:\n            pd.DataFrame: DataFrame of the Model Metrics\n\n        Note:\n            If a capture_name isn't specified this will try to the 'first' available metrics\n        \"\"\"\n        # Try to get the auto_capture 'training_holdout' or the training\n        if capture_name == \"auto\":\n            metric_list = self.list_inference_runs()\n            if metric_list:\n                return self.get_inference_metrics(metric_list[0])\n            else:\n                self.log.warning(f\"No performance metrics found for {self.model_name}!\")\n                return None\n\n        # Grab the metrics captured during model training (could return None)\n        if capture_name == \"model_training\":\n            # Sanity check the workbench metadata\n            if self.workbench_meta() is None:\n                error_msg = f\"Model {self.model_name} has no workbench_meta(). Either onboard() or delete this model!\"\n                self.log.critical(error_msg)\n                raise ValueError(error_msg)\n\n            metrics = self.workbench_meta().get(\"workbench_training_metrics\")\n            return pd.DataFrame.from_dict(metrics) if metrics else None\n\n        else:  # Specific capture_name (could return None)\n            s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_metrics.csv\"\n            metrics = pull_s3_data(s3_path, embedded_index=True)\n            if metrics is not None:\n                return metrics\n            else:\n                self.log.warning(f\"Performance metrics {capture_name} not found for {self.model_name}!\")\n                return None\n\n    def confusion_matrix(self, capture_name: str = \"auto\") -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the confusion_matrix for this model\n\n        Args:\n            capture_name (str, optional): Specific capture_name or \"training\" (default: \"auto\")\n        Returns:\n            pd.DataFrame: DataFrame of the Confusion Matrix (might be None)\n        \"\"\"\n\n        # Sanity check the workbench metadata\n        if self.workbench_meta() is None:\n            error_msg = f\"Model {self.model_name} has no workbench_meta(). Either onboard() or delete this model!\"\n            self.log.critical(error_msg)\n            raise ValueError(error_msg)\n\n        # Grab the metrics from the Workbench Metadata (try inference first, then training)\n        if capture_name == \"auto\":\n            cm = self.confusion_matrix(\"auto_inference\")\n            return cm if cm is not None else self.confusion_matrix(\"model_training\")\n\n        # Grab the confusion matrix captured during model training (could return None)\n        if capture_name == \"model_training\":\n            cm = self.workbench_meta().get(\"workbench_training_cm\")\n            return pd.DataFrame.from_dict(cm) if cm else None\n\n        else:  # Specific capture_name\n            s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_cm.csv\"\n            cm = pull_s3_data(s3_path, embedded_index=True)\n            if cm is not None:\n                return cm\n            else:\n                self.log.warning(f\"Confusion Matrix {capture_name} not found for {self.model_name}!\")\n                return None\n\n    def set_input(self, input: str, force: bool = False):\n        \"\"\"Override: Set the input data for this artifact\n\n        Args:\n            input (str): Name of input for this artifact\n            force (bool, optional): Force the input to be set (default: False)\n        Note:\n            We're going to not allow this to be used for Models\n        \"\"\"\n        if not force:\n            self.log.warning(f\"Model {self.name}: Does not allow manual override of the input!\")\n            return\n\n        # Okay we're going to allow this to be set\n        self.log.important(f\"{self.name}: Setting input to {input}...\")\n        self.log.important(\"Be careful with this! It breaks automatic provenance of the artifact!\")\n        self.upsert_workbench_meta({\"workbench_input\": input})\n\n    def size(self) -&gt; float:\n        \"\"\"Return the size of this data in MegaBytes\"\"\"\n        return 0.0\n\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n        return self.model_meta\n\n    def arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for the Model Package Group\"\"\"\n        return self.group_arn()\n\n    def group_arn(self) -&gt; Union[str, None]:\n        \"\"\"AWS ARN (Amazon Resource Name) for the Model Package Group\"\"\"\n        return self.model_meta[\"ModelPackageGroupArn\"] if self.model_meta else None\n\n    def model_package_arn(self) -&gt; Union[str, None]:\n        \"\"\"AWS ARN (Amazon Resource Name) for the Latest Model Package (within the Group)\"\"\"\n        if self.latest_model is None:\n            return None\n        return self.latest_model[\"ModelPackageArn\"]\n\n    def container_info(self) -&gt; Union[dict, None]:\n        \"\"\"Container Info for the Latest Model Package\"\"\"\n        return self.latest_model[\"InferenceSpecification\"][\"Containers\"][0] if self.latest_model else None\n\n    def container_image(self) -&gt; str:\n        \"\"\"Container Image for the Latest Model Package\"\"\"\n        return self.container_info()[\"Image\"]\n\n    def aws_url(self):\n        \"\"\"The AWS URL for looking at/querying this model\"\"\"\n        return f\"https://{self.aws_region}.console.aws.amazon.com/athena/home\"\n\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        if self.latest_model is None:\n            return \"-\"\n        return self.latest_model[\"CreationTime\"]\n\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        if self.latest_model is None:\n            return \"-\"\n        return self.latest_model[\"CreationTime\"]\n\n    def hash(self) -&gt; Optional[str]:\n        \"\"\"Return the hash for this artifact\n\n        Returns:\n            Optional[str]: The hash for this artifact\n        \"\"\"\n        model_url = self.model_data_url()\n        return compute_s3_object_hash(model_url, self.boto3_session)\n\n    def register_endpoint(self, endpoint_name: str):\n        \"\"\"Add this endpoint to the set of registered endpoints for the model\n\n        Args:\n            endpoint_name (str): Name of the endpoint\n        \"\"\"\n        self.log.important(f\"Registering Endpoint {endpoint_name} with Model {self.name}...\")\n        registered_endpoints = set(self.workbench_meta().get(\"workbench_registered_endpoints\", []))\n        registered_endpoints.add(endpoint_name)\n        self.upsert_workbench_meta({\"workbench_registered_endpoints\": list(registered_endpoints)})\n\n        # Remove any health tags\n        self.remove_health_tag(\"no_endpoint\")\n\n        # A new endpoint means we need to refresh our inference path\n        time.sleep(2)  # Give the AWS Metadata a chance to update\n        self.endpoint_inference_path = self.get_endpoint_inference_path()\n\n    def remove_endpoint(self, endpoint_name: str):\n        \"\"\"Remove this endpoint from the set of registered endpoints for the model\n\n        Args:\n            endpoint_name (str): Name of the endpoint\n        \"\"\"\n        self.log.important(f\"Removing Endpoint {endpoint_name} from Model {self.name}...\")\n        try:\n            registered_endpoints = set(self.workbench_meta().get(\"workbench_registered_endpoints\", []))\n            registered_endpoints.discard(endpoint_name)\n            self.upsert_workbench_meta({\"workbench_registered_endpoints\": list(registered_endpoints)})\n        except AttributeError:\n            self.log.warning(f\"Model {self.name} probably doesn't exist, skipping endpoint removal\")\n            return\n\n        # If we have NO endpionts, then set a health tags\n        if not registered_endpoints:\n            self.add_health_tag(\"no_endpoint\")\n            self.details()\n\n        # A new endpoint means we need to refresh our inference path\n        time.sleep(2)\n\n    def endpoints(self) -&gt; list[str]:\n        \"\"\"Get the list of registered endpoints for this Model\n\n        Returns:\n            list[str]: List of registered endpoints\n        \"\"\"\n        return self.workbench_meta().get(\"workbench_registered_endpoints\", [])\n\n    def get_endpoint_inference_path(self) -&gt; Union[str, None]:\n        \"\"\"Get the S3 Path for the Inference Data\n\n        Returns:\n            str: S3 Path for the Inference Data (or None if not found)\n        \"\"\"\n\n        # Look for any Registered Endpoints\n        registered_endpoints = self.workbench_meta().get(\"workbench_registered_endpoints\")\n\n        # Note: We may have 0 to N endpoints, so we find the one with the most recent artifacts\n        if registered_endpoints:\n            base = self.endpoints_s3_path\n            endpoint_inference_paths = [f\"{base}/{e}/inference\" for e in registered_endpoints]\n            inference_path = newest_path(endpoint_inference_paths, self.sm_session)\n\n            # If the ModelType is Regressor or Classifier we should log this\n            if inference_path is None and self.model_type in {ModelType.REGRESSOR, ModelType.CLASSIFIER}:\n                self.log.important(f\"No inference data found for {self.model_name}!\")\n                self.log.important(f\"Returning default inference path for {registered_endpoints[0]}...\")\n                self.log.important(f\"{endpoint_inference_paths[0]}\")\n                return endpoint_inference_paths[0]\n            else:\n                return inference_path\n        else:\n            self.log.warning(f\"No registered endpoints found for {self.model_name}!\")\n            return None\n\n    def set_target(self, target_column: str):\n        \"\"\"Set the target for this Model\n\n        Args:\n            target_column (str): Target column for this Model\n        \"\"\"\n        self.upsert_workbench_meta({\"workbench_model_target\": target_column})\n\n    def set_features(self, feature_columns: list[str]):\n        \"\"\"Set the features for this Model\n\n        Args:\n            feature_columns (list[str]): List of feature columns\n        \"\"\"\n        self.upsert_workbench_meta({\"workbench_model_features\": feature_columns})\n\n    def target(self) -&gt; Union[str, None]:\n        \"\"\"Return the target for this Model (if supervised, else None)\n\n        Returns:\n            str: Target column for this Model (if supervised, else None)\n        \"\"\"\n        return self.workbench_meta().get(\"workbench_model_target\")  # Returns None if not found\n\n    def features(self) -&gt; Union[list[str], None]:\n        \"\"\"Return a list of features used for this Model\n\n        Returns:\n            list[str]: List of features used for this Model\n        \"\"\"\n        return self.workbench_meta().get(\"workbench_model_features\")  # Returns None if not found\n\n    def class_labels(self) -&gt; Union[list[str], None]:\n        \"\"\"Return the class labels for this Model (if it's a classifier)\n\n        Returns:\n            list[str]: List of class labels\n        \"\"\"\n        if self.model_type == ModelType.CLASSIFIER:\n            return self.workbench_meta().get(\"class_labels\")  # Returns None if not found\n        else:\n            return None\n\n    def set_class_labels(self, labels: list[str]):\n        \"\"\"Return the class labels for this Model (if it's a classifier)\n\n        Args:\n            labels (list[str]): List of class labels\n        \"\"\"\n        if self.model_type == ModelType.CLASSIFIER:\n            self.upsert_workbench_meta({\"class_labels\": labels})\n        else:\n            self.log.error(f\"Model {self.model_name} is not a classifier!\")\n\n    def summary(self) -&gt; dict:\n        \"\"\"Summary information about this Model\n\n        Returns:\n            dict: Dictionary of summary information about this Model\n        \"\"\"\n        self.log.info(\"Computing Model Summary...\")\n        summary = super().summary()\n        summary[\"hyperparameters\"] = get_model_hyperparameters(self)\n        return summary\n\n    def details(self) -&gt; dict:\n        \"\"\"Additional Details about this Model\n\n        Returns:\n            dict: Dictionary of details about this Model\n        \"\"\"\n        self.log.info(\"Computing Model Details...\")\n        details = self.summary()\n        details[\"pipeline\"] = self.get_pipeline()\n        details[\"model_type\"] = self.model_type.value\n        details[\"model_package_group_arn\"] = self.group_arn()\n        details[\"model_package_arn\"] = self.model_package_arn()\n\n        # Sanity check is we have models in the group\n        if self.latest_model is None:\n            self.log.warning(f\"Model Package Group {self.model_name} has no models!\")\n            return details\n\n        # Grab the Model Details\n        details[\"description\"] = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n        details[\"version\"] = self.latest_model[\"ModelPackageVersion\"]\n        details[\"status\"] = self.latest_model[\"ModelPackageStatus\"]\n        details[\"approval_status\"] = self.latest_model.get(\"ModelApprovalStatus\", \"unknown\")\n        details[\"image\"] = self.container_image().split(\"/\")[-1]  # Shorten the image uri\n        details[\"hyperparameters\"] = get_model_hyperparameters(self)\n\n        # Grab the inference and container info\n        inference_spec = self.latest_model[\"InferenceSpecification\"]\n        container_info = self.container_info()\n        details[\"framework\"] = container_info.get(\"Framework\", \"unknown\")\n        details[\"framework_version\"] = container_info.get(\"FrameworkVersion\", \"unknown\")\n        details[\"inference_types\"] = inference_spec[\"SupportedRealtimeInferenceInstanceTypes\"]\n        details[\"transform_types\"] = inference_spec[\"SupportedTransformInstanceTypes\"]\n        details[\"content_types\"] = inference_spec[\"SupportedContentTypes\"]\n        details[\"response_types\"] = inference_spec[\"SupportedResponseMIMETypes\"]\n\n        # Grab the inference metadata\n        details[\"inference_meta\"] = self.get_inference_metadata()\n\n        # Return the details\n        return details\n\n    # Training View for this model\n    def training_view(self):\n        \"\"\"Get the training view for this model\"\"\"\n        from workbench.core.artifacts.feature_set_core import FeatureSetCore\n        from workbench.core.views import View\n\n        # Grab our FeatureSet\n        fs = FeatureSetCore(self.get_input())\n\n        # See if we have a training view for this model\n        my_model_training_view = f\"{self.name.replace('-', '_')}_training\".lower()\n        view = View(fs, my_model_training_view, auto_create_view=False)\n        if view.exists():\n            return view\n        else:\n            self.log.important(f\"No specific training view {my_model_training_view}, returning default training view\")\n            return fs.view(\"training\")\n\n    # Pipeline for this model\n    def get_pipeline(self) -&gt; str:\n        \"\"\"Get the pipeline for this model\"\"\"\n        return self.workbench_meta().get(\"workbench_pipeline\")\n\n    def set_pipeline(self, pipeline: str):\n        \"\"\"Set the pipeline for this model\n\n        Args:\n            pipeline (str): Pipeline that was used to create this model\n        \"\"\"\n        self.upsert_workbench_meta({\"workbench_pipeline\": pipeline})\n\n    def expected_meta(self) -&gt; list[str]:\n        \"\"\"Metadata we expect to see for this Model when it's ready\n        Returns:\n            list[str]: List of expected metadata keys\n        \"\"\"\n        # Our current list of expected metadata, we can add to this as needed\n        return [\"workbench_status\", \"workbench_training_metrics\", \"workbench_training_cm\"]\n\n    def is_model_unknown(self) -&gt; bool:\n        \"\"\"Is the Model Type unknown?\"\"\"\n        return self.model_type == ModelType.UNKNOWN\n\n    def _determine_model_type(self):\n        \"\"\"Internal: Determine the Model Type\"\"\"\n        model_type = input(\"Model Type? (classifier, regressor, uq_regressor, unsupervised, transformer): \")\n        if model_type == \"classifier\":\n            self._set_model_type(ModelType.CLASSIFIER)\n        elif model_type == \"regressor\":\n            self._set_model_type(ModelType.REGRESSOR)\n        elif model_type == \"uq_regressor\":\n            self._set_model_type(ModelType.UQ_REGRESSOR)\n        elif model_type == \"ensemble_regressor\":\n            self._set_model_type(ModelType.ENSEMBLE_REGRESSOR)\n        elif model_type == \"proxmity\":\n            self._set_model_type(ModelType.PROXIMITY)\n        elif model_type == \"transformer\":\n            self._set_model_type(ModelType.TRANSFORMER)\n        else:\n            self.log.warning(f\"Unknown Model Type {model_type}!\")\n            self._set_model_type(ModelType.UNKNOWN)\n\n    def onboard(self, ask_everything=False) -&gt; bool:\n        \"\"\"This is an interactive method that will onboard the Model (make it ready)\n\n        Args:\n            ask_everything (bool, optional): Ask for all the details. Defaults to False.\n\n        Returns:\n            bool: True if the Model is successfully onboarded, False otherwise\n        \"\"\"\n        # Set the status to onboarding\n        self.set_status(\"onboarding\")\n\n        # Determine the Model Type\n        while self.is_model_unknown():\n            self._determine_model_type()\n\n        # Is our input data set?\n        if self.get_input() in [\"\", \"unknown\"] or ask_everything:\n            input_data = input(\"Input Data?: \")\n            if input_data not in [\"None\", \"none\", \"\", \"unknown\"]:\n                self.set_input(input_data)\n\n        # Determine the Target Column (can be None)\n        target_column = self.target()\n        if target_column is None or ask_everything:\n            target_column = input(\"Target Column? (for unsupervised/transformer just type None): \")\n            if target_column in [\"None\", \"none\", \"\"]:\n                target_column = None\n\n        # Determine the Feature Columns\n        feature_columns = self.features()\n        if feature_columns is None or ask_everything:\n            feature_columns = input(\"Feature Columns? (use commas): \")\n            feature_columns = [e.strip() for e in feature_columns.split(\",\")]\n            if feature_columns in [[\"None\"], [\"none\"], [\"\"]]:\n                feature_columns = None\n\n        # Registered Endpoints?\n        endpoints = self.endpoints()\n        if not endpoints or ask_everything:\n            endpoints = input(\"Register Endpoints? (use commas for multiple): \")\n            endpoints = [e.strip() for e in endpoints.split(\",\")]\n            if endpoints in [[\"None\"], [\"none\"], [\"\"]]:\n                endpoints = None\n\n        # Model Owner?\n        owner = self.get_owner()\n        if owner in [None, \"unknown\"] or ask_everything:\n            owner = input(\"Model Owner: \")\n            if owner in [\"None\", \"none\", \"\"]:\n                owner = \"unknown\"\n\n        # Model Class Labels (if it's a classifier)\n        if self.model_type == ModelType.CLASSIFIER:\n            class_labels = self.class_labels()\n            if class_labels is None or ask_everything:\n                class_labels = input(\"Class Labels? (use commas): \")\n                class_labels = [e.strip() for e in class_labels.split(\",\")]\n                if class_labels in [[\"None\"], [\"none\"], [\"\"]]:\n                    class_labels = None\n            self.set_class_labels(class_labels)\n\n        # Now that we have all the details, let's onboard the Model with all the args\n        return self.onboard_with_args(self.model_type, target_column, feature_columns, endpoints, owner)\n\n    def onboard_with_args(\n        self,\n        model_type: ModelType,\n        target_column: str = None,\n        feature_list: list = None,\n        endpoints: list = None,\n        owner: str = None,\n    ) -&gt; bool:\n        \"\"\"Onboard the Model with the given arguments\n\n        Args:\n            model_type (ModelType): Model Type\n            target_column (str): Target Column\n            feature_list (list): List of Feature Columns\n            endpoints (list, optional): List of Endpoints. Defaults to None.\n            owner (str, optional): Model Owner. Defaults to None.\n        Returns:\n            bool: True if the Model is successfully onboarded, False otherwise\n        \"\"\"\n        # Set the status to onboarding\n        self.set_status(\"onboarding\")\n\n        # Set All the Details\n        self._set_model_type(model_type)\n        if target_column:\n            self.set_target(target_column)\n        if feature_list:\n            self.set_features(feature_list)\n        if endpoints:\n            for endpoint in endpoints:\n                self.register_endpoint(endpoint)\n        if owner:\n            self.set_owner(owner)\n\n        # Load the training metrics and inference metrics\n        self._load_training_metrics()\n        self._load_inference_metrics()\n\n        # Remove the needs_onboard tag\n        self.remove_health_tag(\"needs_onboard\")\n        self.set_status(\"ready\")\n\n        # Run a health check and refresh the meta\n        time.sleep(2)  # Give the AWS Metadata a chance to update\n        self.health_check(deep=True)\n        self.refresh_meta()\n        self.details()\n        return True\n\n    def model_data_url(self) -&gt; Optional[str]:\n        \"\"\"Retrieve the ModelDataUrl from the model's AWS metadata.\n\n        Returns:\n            Optional[str]: The ModelDataUrl if available, otherwise None.\n        \"\"\"\n        meta = self.aws_meta()\n        try:\n            return meta[\"ModelPackageList\"][0][\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]\n        except (KeyError, IndexError, TypeError):\n            return None\n\n    @deprecated(version=\"0.9\")\n    def source_dir_url(self) -&gt; Optional[str]:\n        \"\"\"Retrieve the model source directory from the model's AWS metadata.\n\n        Returns:\n            Optional[str]: The model source directory if available, otherwise None.\n        \"\"\"\n        meta = self.aws_meta()\n        try:\n            return meta[\"ModelPackageList\"][0][\"InferenceSpecification\"][\"Containers\"][0][\"Environment\"][\n                \"SAGEMAKER_SUBMIT_DIRECTORY\"\n            ]\n        except (KeyError, IndexError, TypeError):\n            return None\n\n    @deprecated(version=\"0.9\")\n    def entry_point(self) -&gt; Optional[str]:\n        \"\"\"Retrieve the entry point from the model's AWS metadata.\n\n        Returns:\n            Optional[str]: The entry point if available, otherwise None.\n        \"\"\"\n        meta = self.aws_meta()\n        try:\n            return meta[\"ModelPackageList\"][0][\"InferenceSpecification\"][\"Containers\"][0][\"Environment\"][\n                \"SAGEMAKER_PROGRAM\"\n            ]\n        except (KeyError, IndexError, TypeError):\n            return None\n\n    def shap_importance(self) -&gt; Optional[List[Tuple[str, float]]]:\n        \"\"\"Retrieve the SHAP Feature Importance for this model.\n\n        Returns:\n            Optional[List[Tuple[str, float]]]: List of tuples containing feature names and their importance scores\n        \"\"\"\n        return get_shap_importance(self.model_training_path)\n\n    def shap_values(self) -&gt; Optional[Union[pd.DataFrame, Dict[str, pd.DataFrame]]]:\n        \"\"\"Retrieve the SHAP values (contributions) for this model.\n\n        Returns:\n            Optional[Union[pd.DataFrame, dict]]: SHAP values (DataFrame or dict of DataFrames for multiclass)\n        \"\"\"\n        return get_shap_values(self.model_training_path, self.class_labels())\n\n    def shap_feature_values(self) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Retrieve the feature values for SHAP sample rows (used for plot coloring).\n\n        Returns:\n            Optional[pd.DataFrame]: Feature values for SHAP sample rows\n        \"\"\"\n        return get_shap_feature_values(self.model_training_path)\n\n    def supported_inference_instances(self) -&gt; Optional[list]:\n        \"\"\"Retrieve the supported endpoint inference instance types\n\n        Returns:\n            Optional[list]: List of supported inference instance types\n        \"\"\"\n        meta = self.aws_meta()\n        try:\n            return meta[\"ModelPackageList\"][0][\"InferenceSpecification\"][\"SupportedRealtimeInferenceInstanceTypes\"]\n        except (KeyError, IndexError, TypeError):\n            return None\n\n    def publish_prox_model(self, prox_model_name: str = None, include_all_columns: bool = False) -&gt; \"ModelCore\":\n        \"\"\"Create and publish a Proximity Model for this Model\n\n        Args:\n            prox_model_name (str, optional): Name of the Proximity Model (if not specified, a name will be generated)\n            include_all_columns (bool): Include all DataFrame columns in results (default: False)\n\n        Returns:\n            Model: The published Proximity Model\n        \"\"\"\n        if prox_model_name is None:\n            prox_model_name = self.model_name + \"-prox\"\n        return published_proximity_model(self, prox_model_name, include_all_columns=include_all_columns)\n\n    def delete(self):\n        \"\"\"Delete the Model Packages and the Model Group\"\"\"\n        if not self.exists():\n            self.log.warning(f\"Trying to delete a Model that doesn't exist: {self.name}\")\n\n        # Call the Class Method to delete the Model Group\n        ModelCore.managed_delete(model_group_name=self.name)\n\n    @classmethod\n    def managed_delete(cls, model_group_name: str):\n        \"\"\"Delete the Model Packages, Model Group, and S3 Storage Objects\n\n        Args:\n            model_group_name (str): The name of the Model Group to delete\n        \"\"\"\n        # Check if the model group exists in SageMaker\n        try:\n            cls.sm_client.describe_model_package_group(ModelPackageGroupName=model_group_name)\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] in [\"ValidationException\", \"ResourceNotFound\"]:\n                cls.log.info(f\"Model Group {model_group_name} not found!\")\n                return\n            else:\n                raise  # Re-raise unexpected errors\n\n        # Delete Model Packages within the Model Group\n        try:\n            paginator = cls.sm_client.get_paginator(\"list_model_packages\")\n            for page in paginator.paginate(ModelPackageGroupName=model_group_name):\n                for model_package in page[\"ModelPackageSummaryList\"]:\n                    package_arn = model_package[\"ModelPackageArn\"]\n                    cls.log.info(f\"Deleting Model Package {package_arn}...\")\n                    cls.sm_client.delete_model_package(ModelPackageName=package_arn)\n        except ClientError as e:\n            cls.log.error(f\"Error while deleting model packages: {e}\")\n            raise\n\n        # Delete the Model Package Group\n        cls.log.info(f\"Deleting Model Group {model_group_name}...\")\n        cls.sm_client.delete_model_package_group(ModelPackageGroupName=model_group_name)\n\n        # Delete S3 training artifacts\n        s3_delete_path = f\"{cls.models_s3_path}/training/{model_group_name}/\"\n        cls.log.info(f\"Deleting S3 Objects at {s3_delete_path}...\")\n        wr.s3.delete_objects(s3_delete_path, boto3_session=cls.boto3_session)\n\n        # Delete any dataframes that were stored in the Dataframe Cache\n        cls.log.info(\"Deleting Dataframe Cache Entries...\")\n        cls.df_cache.delete_recursive(model_group_name)\n\n        # Delete any dataframes that were stored in the Dataframe Store\n        cls.log.info(\"Deleting Dataframe Store Entries...\")\n        cls.df_store.delete_recursive(f\"workbench/models/{model_group_name}\")\n\n        # Delete anything we might have stored in the Parameter Store\n        cls.log.info(\"Deleting Parameter Store Entries...\")\n        cls.param_store.delete_recursive(f\"workbench/models/{model_group_name}\")\n\n    def _set_model_type(self, model_type: ModelType):\n        \"\"\"Internal: Set the Model Type for this Model\"\"\"\n        self.model_type = model_type\n        self.upsert_workbench_meta({\"workbench_model_type\": self.model_type.value})\n        self.remove_health_tag(\"model_type_unknown\")\n\n    def _get_model_type(self) -&gt; ModelType:\n        \"\"\"Internal: Query the Workbench Metadata to get the model type\n        Returns:\n            ModelType: The ModelType of this Model\n        Notes:\n            This is an internal method that should not be called directly\n            Use the model_type attribute instead\n        \"\"\"\n        model_type = self.workbench_meta().get(\"workbench_model_type\")\n        try:\n            return ModelType(model_type)\n        except ValueError:\n            self.log.warning(f\"Could not determine model type for {self.model_name}!\")\n            return ModelType.UNKNOWN\n\n    def _set_model_framework(self, model_framework: ModelFramework):\n        \"\"\"Internal: Set the Model Framework for this Model\"\"\"\n        self.model_framework = model_framework\n        self.upsert_workbench_meta({\"workbench_model_framework\": self.model_framework.value})\n        self.remove_health_tag(\"model_framework_unknown\")\n\n    def _get_model_framework(self) -&gt; ModelFramework:\n        \"\"\"Internal: Query the Workbench Metadata to get the model framework\n        Returns:\n            ModelFramework: The ModelFramework of this Model\n        Notes:\n            This is an internal method that should not be called directly\n            Use the model_framework attribute instead\n        \"\"\"\n        model_framework = self.workbench_meta().get(\"workbench_model_framework\")\n        try:\n            return ModelFramework(model_framework)\n        except ValueError:\n            self.log.warning(f\"Could not determine model framework for {self.model_name}!\")\n            return ModelFramework.UNKNOWN\n\n    def _load_training_metrics(self):\n        \"\"\"Internal: Retrieve the training metrics and Confusion Matrix for this model\n                     and load the data into the Workbench Metadata\n\n        Notes:\n            This may or may not exist based on whether we have access to TrainingJobAnalytics\n        \"\"\"\n        try:\n            df = TrainingJobAnalytics(training_job_name=self.training_job_name).dataframe()\n            if df.empty:\n                self.log.important(f\"No training job metrics found for {self.training_job_name}\")\n                self.upsert_workbench_meta({\"workbench_training_metrics\": None, \"workbench_training_cm\": None})\n                return\n            if self.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n                if \"timestamp\" in df.columns:\n                    df = df.drop(columns=[\"timestamp\"])\n\n                # We're going to pivot the DataFrame to get the desired structure\n                reg_metrics_df = df.set_index(\"metric_name\").T\n\n                # Store and return the metrics in the Workbench Metadata\n                self.upsert_workbench_meta(\n                    {\"workbench_training_metrics\": reg_metrics_df.to_dict(), \"workbench_training_cm\": None}\n                )\n                return\n\n        except (KeyError, botocore.exceptions.ClientError):\n            self.log.important(f\"No training job metrics found for {self.training_job_name}\")\n            # Store and return the metrics in the Workbench Metadata\n            self.upsert_workbench_meta({\"workbench_training_metrics\": None, \"workbench_training_cm\": None})\n            return\n\n        # We need additional processing for classification metrics\n        if self.model_type == ModelType.CLASSIFIER:\n            metrics_df, cm_df = self._process_classification_metrics(df)\n\n            # Store and return the metrics in the Workbench Metadata\n            self.upsert_workbench_meta(\n                {\"workbench_training_metrics\": metrics_df.to_dict(), \"workbench_training_cm\": cm_df.to_dict()}\n            )\n\n    def _load_inference_metrics(self, capture_name: str = \"auto_inference\"):\n        \"\"\"Internal: Retrieve the inference model metrics for this model\n                     and load the data into the Workbench Metadata\n\n        Args:\n            capture_name (str, optional): A specific capture_name (default: \"auto_inference\")\n        Notes:\n            This may or may not exist based on whether an Endpoint ran Inference\n        \"\"\"\n        s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_metrics.csv\"\n        inference_metrics = pull_s3_data(s3_path)\n\n        # Store data into the Workbench Metadata\n        metrics_storage = None if inference_metrics is None else inference_metrics.to_dict(\"records\")\n        self.upsert_workbench_meta({\"workbench_inference_metrics\": metrics_storage})\n\n    def get_inference_metadata(self, capture_name: str = \"auto_inference\") -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the inference metadata for this model\n\n        Args:\n            capture_name (str, optional): A specific capture_name (default: \"auto_inference\")\n\n        Returns:\n            dict: Dictionary of the inference metadata (might be None)\n        Notes:\n            Basically when Endpoint inference was run, name of the dataset, the MD5, etc\n        \"\"\"\n        # Sanity check the inference path (which may or may not exist)\n        if self.endpoint_inference_path is None:\n            return None\n\n        # Check for model_training capture_name\n        if capture_name == \"model_training\":\n            # Create a DataFrame with the training metadata\n            meta_df = pd.DataFrame(\n                [\n                    {\n                        \"name\": \"AWS Training Capture\",\n                        \"data_hash\": \"N/A\",\n                        \"num_rows\": \"-\",\n                        \"description\": \"-\",\n                    }\n                ]\n            )\n            return meta_df\n\n        # Pull the inference metadata\n        try:\n            s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_meta.json\"\n            return wr.s3.read_json(s3_path)\n        except NoFilesFound:\n            self.log.info(f\"Could not find model inference meta at {s3_path}...\")\n            return None\n\n    def get_inference_predictions(self, capture_name: str = \"full_cross_fold\") -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the captured prediction results for this model\n\n        Args:\n            capture_name (str, optional): Specific capture_name (default: full_cross_fold)\n\n        Returns:\n            pd.DataFrame: DataFrame of the Captured Predictions (might be None)\n        \"\"\"\n        self.log.important(f\"Grabbing {capture_name} predictions for {self.model_name}...\")\n\n        # Sanity check that the model should have predictions\n        has_predictions = self.model_type in [\n            ModelType.CLASSIFIER,\n            ModelType.REGRESSOR,\n            ModelType.UQ_REGRESSOR,\n            ModelType.ENSEMBLE_REGRESSOR,\n        ]\n        if not has_predictions:\n            self.log.warning(f\"No Predictions for {self.model_name}...\")\n            return None\n\n        # Special case for model_training\n        if capture_name == \"model_training\":\n            return self._get_validation_predictions()\n\n        # Construct the S3 path for the Inference Predictions\n        s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_predictions.csv\"\n        return pull_s3_data(s3_path)\n\n    def _get_validation_predictions(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Internal: Retrieve the captured prediction results for this model\n\n        Returns:\n            pd.DataFrame: DataFrame of the Captured Validation Predictions (might be None)\n        \"\"\"\n        # Sanity check the training path (which may or may not exist)\n        if self.model_training_path is None:\n            self.log.warning(f\"No Validation Predictions for {self.model_name}...\")\n            return None\n        self.log.important(f\"Grabbing Validation Predictions for {self.model_name}...\")\n        s3_path = f\"{self.model_training_path}/validation_predictions.csv\"\n        df = pull_s3_data(s3_path)\n        return df\n\n    def _extract_training_job_name(self) -&gt; Union[str, None]:\n        \"\"\"Internal: Extract the training job name from the ModelDataUrl\"\"\"\n        try:\n            model_data_url = self.container_info()[\"ModelDataUrl\"]\n            parsed_url = urllib.parse.urlparse(model_data_url)\n            training_job_name = parsed_url.path.lstrip(\"/\").split(\"/\")[0]\n            return training_job_name\n        except KeyError:\n            self.log.warning(f\"Could not extract training job name from {model_data_url}\")\n            return None\n\n    @staticmethod\n    def _process_classification_metrics(df: pd.DataFrame) -&gt; (pd.DataFrame, pd.DataFrame):\n        \"\"\"Internal: Process classification metrics into a more reasonable format\n        Args:\n            df (pd.DataFrame): DataFrame of training metrics\n        Returns:\n            (pd.DataFrame, pd.DataFrame): Tuple of DataFrames. Metrics and confusion matrix\n        \"\"\"\n        # Split into two DataFrames based on 'metric_name'\n        metrics_df = df[df[\"metric_name\"].str.startswith(\"Metrics:\")].copy()\n        cm_df = df[df[\"metric_name\"].str.startswith(\"ConfusionMatrix:\")].copy()\n\n        # Split the 'metric_name' into different parts\n        metrics_df[\"class\"] = metrics_df[\"metric_name\"].str.split(\":\").str[1]\n        metrics_df[\"metric_type\"] = metrics_df[\"metric_name\"].str.split(\":\").str[2]\n\n        # Pivot the DataFrame to get the desired structure\n        metrics_df = metrics_df.pivot(index=\"class\", columns=\"metric_type\", values=\"value\").reset_index()\n        metrics_df = metrics_df.rename_axis(None, axis=1)\n\n        # Now process the confusion matrix\n        cm_df[\"row_class\"] = cm_df[\"metric_name\"].str.split(\":\").str[1]\n        cm_df[\"col_class\"] = cm_df[\"metric_name\"].str.split(\":\").str[2]\n\n        # Pivot the DataFrame to create a form suitable for the heatmap\n        cm_df = cm_df.pivot(index=\"row_class\", columns=\"col_class\", values=\"value\")\n\n        # Convert the values in cm_df to integers\n        cm_df = cm_df.astype(int)\n\n        return metrics_df, cm_df\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.__init__","title":"<code>__init__(model_name, **kwargs)</code>","text":"<p>ModelCore Initialization Args:     model_name (str): Name of Model in Workbench.     **kwargs: Additional keyword arguments</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def __init__(self, model_name: str, **kwargs):\n    \"\"\"ModelCore Initialization\n    Args:\n        model_name (str): Name of Model in Workbench.\n        **kwargs: Additional keyword arguments\n    \"\"\"\n\n    # Make sure the model name is valid\n    self.is_name_valid(model_name, delimiter=\"-\", lower_case=False)\n\n    # Call SuperClass Initialization\n    super().__init__(model_name, **kwargs)\n\n    # Initialize our class attributes\n    self.latest_model = None\n    self.model_type = ModelType.UNKNOWN\n    self.model_training_path = None\n    self.endpoint_inference_path = None\n\n    # Grab an Cloud Platform Meta object and pull information for this Model\n    self.model_name = model_name\n    self.model_meta = self.meta.model(self.model_name)\n    if self.model_meta is None:\n        self.log.warning(f\"Could not find model {self.model_name} within current visibility scope\")\n        return\n    else:\n        # Is this a model package group without any models?\n        if len(self.model_meta[\"ModelPackageList\"]) == 0:\n            self.log.critical(f\"Model Group {self.model_name} has no Model Packages!\")\n            self.latest_model = None\n            self.training_job_name = None\n            self.add_health_tag(\"model_not_found\")\n            return\n        try:\n            self.latest_model = self.model_meta[\"ModelPackageList\"][0]\n            self.description = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n            self.training_job_name = self._extract_training_job_name()\n            self.model_type = self._get_model_type()\n            self.model_framework = self._get_model_framework()\n        except (IndexError, KeyError):\n            self.log.critical(f\"Model {self.model_name} appears to be malformed. Delete and recreate it!\")\n            return\n\n    # Set the Model Training S3 Path\n    self.model_training_path = f\"{self.models_s3_path}/{self.model_name}/training\"\n\n    # Get our Endpoint Inference Path (might be None)\n    self.endpoint_inference_path = self.get_endpoint_inference_path()\n\n    # Call SuperClass Post Initialization\n    super().__post_init__()\n\n    # All done\n    self.log.info(f\"Model Initialized: {self.model_name}\")\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.arn","title":"<code>arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for the Model Package Group</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for the Model Package Group\"\"\"\n    return self.group_arn()\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.aws_meta","title":"<code>aws_meta()</code>","text":"<p>Get ALL the AWS metadata for this artifact</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def aws_meta(self) -&gt; dict:\n    \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n    return self.model_meta\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.aws_url","title":"<code>aws_url()</code>","text":"<p>The AWS URL for looking at/querying this model</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def aws_url(self):\n    \"\"\"The AWS URL for looking at/querying this model\"\"\"\n    return f\"https://{self.aws_region}.console.aws.amazon.com/athena/home\"\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.class_labels","title":"<code>class_labels()</code>","text":"<p>Return the class labels for this Model (if it's a classifier)</p> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>list[str]: List of class labels</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def class_labels(self) -&gt; Union[list[str], None]:\n    \"\"\"Return the class labels for this Model (if it's a classifier)\n\n    Returns:\n        list[str]: List of class labels\n    \"\"\"\n    if self.model_type == ModelType.CLASSIFIER:\n        return self.workbench_meta().get(\"class_labels\")  # Returns None if not found\n    else:\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.confusion_matrix","title":"<code>confusion_matrix(capture_name='auto')</code>","text":"<p>Retrieve the confusion_matrix for this model</p> <p>Parameters:</p> Name Type Description Default <code>capture_name</code> <code>str</code> <p>Specific capture_name or \"training\" (default: \"auto\")</p> <code>'auto'</code> <p>Returns:     pd.DataFrame: DataFrame of the Confusion Matrix (might be None)</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def confusion_matrix(self, capture_name: str = \"auto\") -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the confusion_matrix for this model\n\n    Args:\n        capture_name (str, optional): Specific capture_name or \"training\" (default: \"auto\")\n    Returns:\n        pd.DataFrame: DataFrame of the Confusion Matrix (might be None)\n    \"\"\"\n\n    # Sanity check the workbench metadata\n    if self.workbench_meta() is None:\n        error_msg = f\"Model {self.model_name} has no workbench_meta(). Either onboard() or delete this model!\"\n        self.log.critical(error_msg)\n        raise ValueError(error_msg)\n\n    # Grab the metrics from the Workbench Metadata (try inference first, then training)\n    if capture_name == \"auto\":\n        cm = self.confusion_matrix(\"auto_inference\")\n        return cm if cm is not None else self.confusion_matrix(\"model_training\")\n\n    # Grab the confusion matrix captured during model training (could return None)\n    if capture_name == \"model_training\":\n        cm = self.workbench_meta().get(\"workbench_training_cm\")\n        return pd.DataFrame.from_dict(cm) if cm else None\n\n    else:  # Specific capture_name\n        s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_cm.csv\"\n        cm = pull_s3_data(s3_path, embedded_index=True)\n        if cm is not None:\n            return cm\n        else:\n            self.log.warning(f\"Confusion Matrix {capture_name} not found for {self.model_name}!\")\n            return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.container_image","title":"<code>container_image()</code>","text":"<p>Container Image for the Latest Model Package</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def container_image(self) -&gt; str:\n    \"\"\"Container Image for the Latest Model Package\"\"\"\n    return self.container_info()[\"Image\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.container_info","title":"<code>container_info()</code>","text":"<p>Container Info for the Latest Model Package</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def container_info(self) -&gt; Union[dict, None]:\n    \"\"\"Container Info for the Latest Model Package\"\"\"\n    return self.latest_model[\"InferenceSpecification\"][\"Containers\"][0] if self.latest_model else None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.created","title":"<code>created()</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    if self.latest_model is None:\n        return \"-\"\n    return self.latest_model[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.delete","title":"<code>delete()</code>","text":"<p>Delete the Model Packages and the Model Group</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the Model Packages and the Model Group\"\"\"\n    if not self.exists():\n        self.log.warning(f\"Trying to delete a Model that doesn't exist: {self.name}\")\n\n    # Call the Class Method to delete the Model Group\n    ModelCore.managed_delete(model_group_name=self.name)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.delete_inference_run","title":"<code>delete_inference_run(inference_run_name)</code>","text":"<p>Delete the inference run for this model</p> <p>Parameters:</p> Name Type Description Default <code>inference_run_name</code> <code>str</code> <p>Name of the inference run</p> required Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def delete_inference_run(self, inference_run_name: str):\n    \"\"\"Delete the inference run for this model\n\n    Args:\n        inference_run_name (str): Name of the inference run\n    \"\"\"\n    if inference_run_name == \"model_training\":\n        self.log.warning(\"Cannot delete model training data!\")\n        return\n\n    if self.endpoint_inference_path:\n        full_path = f\"{self.endpoint_inference_path}/{inference_run_name}\"\n        # Check if there are any objects at the path\n        if wr.s3.list_objects(full_path):\n            wr.s3.delete_objects(path=full_path)\n            self.log.important(f\"Deleted inference run {inference_run_name} for {self.model_name}\")\n        else:\n            self.log.warning(f\"Inference run {inference_run_name} not found for {self.model_name}!\")\n    else:\n        self.log.important(f\"No inference data found for {self.model_name}!\")\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.details","title":"<code>details()</code>","text":"<p>Additional Details about this Model</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of details about this Model</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def details(self) -&gt; dict:\n    \"\"\"Additional Details about this Model\n\n    Returns:\n        dict: Dictionary of details about this Model\n    \"\"\"\n    self.log.info(\"Computing Model Details...\")\n    details = self.summary()\n    details[\"pipeline\"] = self.get_pipeline()\n    details[\"model_type\"] = self.model_type.value\n    details[\"model_package_group_arn\"] = self.group_arn()\n    details[\"model_package_arn\"] = self.model_package_arn()\n\n    # Sanity check is we have models in the group\n    if self.latest_model is None:\n        self.log.warning(f\"Model Package Group {self.model_name} has no models!\")\n        return details\n\n    # Grab the Model Details\n    details[\"description\"] = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n    details[\"version\"] = self.latest_model[\"ModelPackageVersion\"]\n    details[\"status\"] = self.latest_model[\"ModelPackageStatus\"]\n    details[\"approval_status\"] = self.latest_model.get(\"ModelApprovalStatus\", \"unknown\")\n    details[\"image\"] = self.container_image().split(\"/\")[-1]  # Shorten the image uri\n    details[\"hyperparameters\"] = get_model_hyperparameters(self)\n\n    # Grab the inference and container info\n    inference_spec = self.latest_model[\"InferenceSpecification\"]\n    container_info = self.container_info()\n    details[\"framework\"] = container_info.get(\"Framework\", \"unknown\")\n    details[\"framework_version\"] = container_info.get(\"FrameworkVersion\", \"unknown\")\n    details[\"inference_types\"] = inference_spec[\"SupportedRealtimeInferenceInstanceTypes\"]\n    details[\"transform_types\"] = inference_spec[\"SupportedTransformInstanceTypes\"]\n    details[\"content_types\"] = inference_spec[\"SupportedContentTypes\"]\n    details[\"response_types\"] = inference_spec[\"SupportedResponseMIMETypes\"]\n\n    # Grab the inference metadata\n    details[\"inference_meta\"] = self.get_inference_metadata()\n\n    # Return the details\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.endpoints","title":"<code>endpoints()</code>","text":"<p>Get the list of registered endpoints for this Model</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of registered endpoints</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def endpoints(self) -&gt; list[str]:\n    \"\"\"Get the list of registered endpoints for this Model\n\n    Returns:\n        list[str]: List of registered endpoints\n    \"\"\"\n    return self.workbench_meta().get(\"workbench_registered_endpoints\", [])\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.entry_point","title":"<code>entry_point()</code>","text":"<p>Retrieve the entry point from the model's AWS metadata.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The entry point if available, otherwise None.</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>@deprecated(version=\"0.9\")\ndef entry_point(self) -&gt; Optional[str]:\n    \"\"\"Retrieve the entry point from the model's AWS metadata.\n\n    Returns:\n        Optional[str]: The entry point if available, otherwise None.\n    \"\"\"\n    meta = self.aws_meta()\n    try:\n        return meta[\"ModelPackageList\"][0][\"InferenceSpecification\"][\"Containers\"][0][\"Environment\"][\n            \"SAGEMAKER_PROGRAM\"\n        ]\n    except (KeyError, IndexError, TypeError):\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.exists","title":"<code>exists()</code>","text":"<p>Does the model metadata exist in the AWS Metadata?</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Does the model metadata exist in the AWS Metadata?\"\"\"\n    if self.model_meta is None:\n        self.log.info(f\"Model {self.model_name} not found in AWS Metadata!\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.expected_meta","title":"<code>expected_meta()</code>","text":"<p>Metadata we expect to see for this Model when it's ready Returns:     list[str]: List of expected metadata keys</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def expected_meta(self) -&gt; list[str]:\n    \"\"\"Metadata we expect to see for this Model when it's ready\n    Returns:\n        list[str]: List of expected metadata keys\n    \"\"\"\n    # Our current list of expected metadata, we can add to this as needed\n    return [\"workbench_status\", \"workbench_training_metrics\", \"workbench_training_cm\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.features","title":"<code>features()</code>","text":"<p>Return a list of features used for this Model</p> <p>Returns:</p> Type Description <code>Union[list[str], None]</code> <p>list[str]: List of features used for this Model</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def features(self) -&gt; Union[list[str], None]:\n    \"\"\"Return a list of features used for this Model\n\n    Returns:\n        list[str]: List of features used for this Model\n    \"\"\"\n    return self.workbench_meta().get(\"workbench_model_features\")  # Returns None if not found\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.get_endpoint_inference_path","title":"<code>get_endpoint_inference_path()</code>","text":"<p>Get the S3 Path for the Inference Data</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>S3 Path for the Inference Data (or None if not found)</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def get_endpoint_inference_path(self) -&gt; Union[str, None]:\n    \"\"\"Get the S3 Path for the Inference Data\n\n    Returns:\n        str: S3 Path for the Inference Data (or None if not found)\n    \"\"\"\n\n    # Look for any Registered Endpoints\n    registered_endpoints = self.workbench_meta().get(\"workbench_registered_endpoints\")\n\n    # Note: We may have 0 to N endpoints, so we find the one with the most recent artifacts\n    if registered_endpoints:\n        base = self.endpoints_s3_path\n        endpoint_inference_paths = [f\"{base}/{e}/inference\" for e in registered_endpoints]\n        inference_path = newest_path(endpoint_inference_paths, self.sm_session)\n\n        # If the ModelType is Regressor or Classifier we should log this\n        if inference_path is None and self.model_type in {ModelType.REGRESSOR, ModelType.CLASSIFIER}:\n            self.log.important(f\"No inference data found for {self.model_name}!\")\n            self.log.important(f\"Returning default inference path for {registered_endpoints[0]}...\")\n            self.log.important(f\"{endpoint_inference_paths[0]}\")\n            return endpoint_inference_paths[0]\n        else:\n            return inference_path\n    else:\n        self.log.warning(f\"No registered endpoints found for {self.model_name}!\")\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.get_inference_metadata","title":"<code>get_inference_metadata(capture_name='auto_inference')</code>","text":"<p>Retrieve the inference metadata for this model</p> <p>Parameters:</p> Name Type Description Default <code>capture_name</code> <code>str</code> <p>A specific capture_name (default: \"auto_inference\")</p> <code>'auto_inference'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[DataFrame, None]</code> <p>Dictionary of the inference metadata (might be None)</p> <p>Notes:     Basically when Endpoint inference was run, name of the dataset, the MD5, etc</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def get_inference_metadata(self, capture_name: str = \"auto_inference\") -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the inference metadata for this model\n\n    Args:\n        capture_name (str, optional): A specific capture_name (default: \"auto_inference\")\n\n    Returns:\n        dict: Dictionary of the inference metadata (might be None)\n    Notes:\n        Basically when Endpoint inference was run, name of the dataset, the MD5, etc\n    \"\"\"\n    # Sanity check the inference path (which may or may not exist)\n    if self.endpoint_inference_path is None:\n        return None\n\n    # Check for model_training capture_name\n    if capture_name == \"model_training\":\n        # Create a DataFrame with the training metadata\n        meta_df = pd.DataFrame(\n            [\n                {\n                    \"name\": \"AWS Training Capture\",\n                    \"data_hash\": \"N/A\",\n                    \"num_rows\": \"-\",\n                    \"description\": \"-\",\n                }\n            ]\n        )\n        return meta_df\n\n    # Pull the inference metadata\n    try:\n        s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_meta.json\"\n        return wr.s3.read_json(s3_path)\n    except NoFilesFound:\n        self.log.info(f\"Could not find model inference meta at {s3_path}...\")\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.get_inference_metrics","title":"<code>get_inference_metrics(capture_name='auto')</code>","text":"<p>Retrieve the inference performance metrics for this model</p> <p>Parameters:</p> Name Type Description Default <code>capture_name</code> <code>str</code> <p>Specific capture_name (default: \"auto\")</p> <code>'auto'</code> <p>Returns:     pd.DataFrame: DataFrame of the Model Metrics</p> Note <p>If a capture_name isn't specified this will try to the 'first' available metrics</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def get_inference_metrics(self, capture_name: str = \"auto\") -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the inference performance metrics for this model\n\n    Args:\n        capture_name (str, optional): Specific capture_name (default: \"auto\")\n    Returns:\n        pd.DataFrame: DataFrame of the Model Metrics\n\n    Note:\n        If a capture_name isn't specified this will try to the 'first' available metrics\n    \"\"\"\n    # Try to get the auto_capture 'training_holdout' or the training\n    if capture_name == \"auto\":\n        metric_list = self.list_inference_runs()\n        if metric_list:\n            return self.get_inference_metrics(metric_list[0])\n        else:\n            self.log.warning(f\"No performance metrics found for {self.model_name}!\")\n            return None\n\n    # Grab the metrics captured during model training (could return None)\n    if capture_name == \"model_training\":\n        # Sanity check the workbench metadata\n        if self.workbench_meta() is None:\n            error_msg = f\"Model {self.model_name} has no workbench_meta(). Either onboard() or delete this model!\"\n            self.log.critical(error_msg)\n            raise ValueError(error_msg)\n\n        metrics = self.workbench_meta().get(\"workbench_training_metrics\")\n        return pd.DataFrame.from_dict(metrics) if metrics else None\n\n    else:  # Specific capture_name (could return None)\n        s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_metrics.csv\"\n        metrics = pull_s3_data(s3_path, embedded_index=True)\n        if metrics is not None:\n            return metrics\n        else:\n            self.log.warning(f\"Performance metrics {capture_name} not found for {self.model_name}!\")\n            return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.get_inference_predictions","title":"<code>get_inference_predictions(capture_name='full_cross_fold')</code>","text":"<p>Retrieve the captured prediction results for this model</p> <p>Parameters:</p> Name Type Description Default <code>capture_name</code> <code>str</code> <p>Specific capture_name (default: full_cross_fold)</p> <code>'full_cross_fold'</code> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: DataFrame of the Captured Predictions (might be None)</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def get_inference_predictions(self, capture_name: str = \"full_cross_fold\") -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the captured prediction results for this model\n\n    Args:\n        capture_name (str, optional): Specific capture_name (default: full_cross_fold)\n\n    Returns:\n        pd.DataFrame: DataFrame of the Captured Predictions (might be None)\n    \"\"\"\n    self.log.important(f\"Grabbing {capture_name} predictions for {self.model_name}...\")\n\n    # Sanity check that the model should have predictions\n    has_predictions = self.model_type in [\n        ModelType.CLASSIFIER,\n        ModelType.REGRESSOR,\n        ModelType.UQ_REGRESSOR,\n        ModelType.ENSEMBLE_REGRESSOR,\n    ]\n    if not has_predictions:\n        self.log.warning(f\"No Predictions for {self.model_name}...\")\n        return None\n\n    # Special case for model_training\n    if capture_name == \"model_training\":\n        return self._get_validation_predictions()\n\n    # Construct the S3 path for the Inference Predictions\n    s3_path = f\"{self.endpoint_inference_path}/{capture_name}/inference_predictions.csv\"\n    return pull_s3_data(s3_path)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.get_pipeline","title":"<code>get_pipeline()</code>","text":"<p>Get the pipeline for this model</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def get_pipeline(self) -&gt; str:\n    \"\"\"Get the pipeline for this model\"\"\"\n    return self.workbench_meta().get(\"workbench_pipeline\")\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.group_arn","title":"<code>group_arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for the Model Package Group</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def group_arn(self) -&gt; Union[str, None]:\n    \"\"\"AWS ARN (Amazon Resource Name) for the Model Package Group\"\"\"\n    return self.model_meta[\"ModelPackageGroupArn\"] if self.model_meta else None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.hash","title":"<code>hash()</code>","text":"<p>Return the hash for this artifact</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The hash for this artifact</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def hash(self) -&gt; Optional[str]:\n    \"\"\"Return the hash for this artifact\n\n    Returns:\n        Optional[str]: The hash for this artifact\n    \"\"\"\n    model_url = self.model_data_url()\n    return compute_s3_object_hash(model_url, self.boto3_session)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.health_check","title":"<code>health_check(deep=False)</code>","text":"<p>Perform a health check on this model</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, perform more extensive (expensive) health checks (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of health issues</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def health_check(self, deep: bool = False) -&gt; list[str]:\n    \"\"\"Perform a health check on this model\n\n    Args:\n        deep (bool): If True, perform more extensive (expensive) health checks (default: False)\n\n    Returns:\n        list[str]: List of health issues\n    \"\"\"\n    # Call the base class health check\n    health_issues = super().health_check(deep=deep)\n\n    # Check if the model exists\n    if self.latest_model is None:\n        health_issues.append(\"model_not_found\")\n\n    # Model Type\n    if self._get_model_type() == ModelType.UNKNOWN:\n        health_issues.append(\"model_type_unknown\")\n    else:\n        self.remove_health_tag(\"model_type_unknown\")\n\n    # Deep checks (expensive API/S3 calls)\n    if deep:\n        # Model Performance Metrics\n        needs_metrics = self.model_type in {\n            ModelType.REGRESSOR,\n            ModelType.UQ_REGRESSOR,\n            ModelType.ENSEMBLE_REGRESSOR,\n            ModelType.CLASSIFIER,\n        }\n        if needs_metrics and self.get_inference_metrics() is None:\n            health_issues.append(\"metrics_needed\")\n        else:\n            self.remove_health_tag(\"metrics_needed\")\n\n        # Endpoint\n        if not self.endpoints():\n            health_issues.append(\"no_endpoint\")\n        else:\n            self.remove_health_tag(\"no_endpoint\")\n\n    return health_issues\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.is_model_unknown","title":"<code>is_model_unknown()</code>","text":"<p>Is the Model Type unknown?</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def is_model_unknown(self) -&gt; bool:\n    \"\"\"Is the Model Type unknown?\"\"\"\n    return self.model_type == ModelType.UNKNOWN\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.list_inference_runs","title":"<code>list_inference_runs()</code>","text":"<p>List the inference runs for this model</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of inference runs</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def list_inference_runs(self) -&gt; list[str]:\n    \"\"\"List the inference runs for this model\n\n    Returns:\n        list[str]: List of inference runs\n    \"\"\"\n\n    # Check if we have a model (if not return empty list)\n    if self.latest_model is None:\n        return []\n\n    # Check if we have model training metrics in our metadata\n    have_model_training = True if self.workbench_meta().get(\"workbench_training_metrics\") else False\n\n    # Now grab the list of directories from our inference path\n    inference_runs = []\n    if self.endpoint_inference_path:\n        directories = wr.s3.list_directories(path=self.endpoint_inference_path + \"/\")\n        inference_runs = [urlparse(directory).path.split(\"/\")[-2] for directory in directories]\n\n    # We're going to add the model training to the end of the list\n    if have_model_training:\n        inference_runs.append(\"model_training\")\n    return inference_runs\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.managed_delete","title":"<code>managed_delete(model_group_name)</code>  <code>classmethod</code>","text":"<p>Delete the Model Packages, Model Group, and S3 Storage Objects</p> <p>Parameters:</p> Name Type Description Default <code>model_group_name</code> <code>str</code> <p>The name of the Model Group to delete</p> required Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>@classmethod\ndef managed_delete(cls, model_group_name: str):\n    \"\"\"Delete the Model Packages, Model Group, and S3 Storage Objects\n\n    Args:\n        model_group_name (str): The name of the Model Group to delete\n    \"\"\"\n    # Check if the model group exists in SageMaker\n    try:\n        cls.sm_client.describe_model_package_group(ModelPackageGroupName=model_group_name)\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] in [\"ValidationException\", \"ResourceNotFound\"]:\n            cls.log.info(f\"Model Group {model_group_name} not found!\")\n            return\n        else:\n            raise  # Re-raise unexpected errors\n\n    # Delete Model Packages within the Model Group\n    try:\n        paginator = cls.sm_client.get_paginator(\"list_model_packages\")\n        for page in paginator.paginate(ModelPackageGroupName=model_group_name):\n            for model_package in page[\"ModelPackageSummaryList\"]:\n                package_arn = model_package[\"ModelPackageArn\"]\n                cls.log.info(f\"Deleting Model Package {package_arn}...\")\n                cls.sm_client.delete_model_package(ModelPackageName=package_arn)\n    except ClientError as e:\n        cls.log.error(f\"Error while deleting model packages: {e}\")\n        raise\n\n    # Delete the Model Package Group\n    cls.log.info(f\"Deleting Model Group {model_group_name}...\")\n    cls.sm_client.delete_model_package_group(ModelPackageGroupName=model_group_name)\n\n    # Delete S3 training artifacts\n    s3_delete_path = f\"{cls.models_s3_path}/training/{model_group_name}/\"\n    cls.log.info(f\"Deleting S3 Objects at {s3_delete_path}...\")\n    wr.s3.delete_objects(s3_delete_path, boto3_session=cls.boto3_session)\n\n    # Delete any dataframes that were stored in the Dataframe Cache\n    cls.log.info(\"Deleting Dataframe Cache Entries...\")\n    cls.df_cache.delete_recursive(model_group_name)\n\n    # Delete any dataframes that were stored in the Dataframe Store\n    cls.log.info(\"Deleting Dataframe Store Entries...\")\n    cls.df_store.delete_recursive(f\"workbench/models/{model_group_name}\")\n\n    # Delete anything we might have stored in the Parameter Store\n    cls.log.info(\"Deleting Parameter Store Entries...\")\n    cls.param_store.delete_recursive(f\"workbench/models/{model_group_name}\")\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.model_data_url","title":"<code>model_data_url()</code>","text":"<p>Retrieve the ModelDataUrl from the model's AWS metadata.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The ModelDataUrl if available, otherwise None.</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def model_data_url(self) -&gt; Optional[str]:\n    \"\"\"Retrieve the ModelDataUrl from the model's AWS metadata.\n\n    Returns:\n        Optional[str]: The ModelDataUrl if available, otherwise None.\n    \"\"\"\n    meta = self.aws_meta()\n    try:\n        return meta[\"ModelPackageList\"][0][\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]\n    except (KeyError, IndexError, TypeError):\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.model_package_arn","title":"<code>model_package_arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for the Latest Model Package (within the Group)</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def model_package_arn(self) -&gt; Union[str, None]:\n    \"\"\"AWS ARN (Amazon Resource Name) for the Latest Model Package (within the Group)\"\"\"\n    if self.latest_model is None:\n        return None\n    return self.latest_model[\"ModelPackageArn\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.modified","title":"<code>modified()</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    if self.latest_model is None:\n        return \"-\"\n    return self.latest_model[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.onboard","title":"<code>onboard(ask_everything=False)</code>","text":"<p>This is an interactive method that will onboard the Model (make it ready)</p> <p>Parameters:</p> Name Type Description Default <code>ask_everything</code> <code>bool</code> <p>Ask for all the details. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the Model is successfully onboarded, False otherwise</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def onboard(self, ask_everything=False) -&gt; bool:\n    \"\"\"This is an interactive method that will onboard the Model (make it ready)\n\n    Args:\n        ask_everything (bool, optional): Ask for all the details. Defaults to False.\n\n    Returns:\n        bool: True if the Model is successfully onboarded, False otherwise\n    \"\"\"\n    # Set the status to onboarding\n    self.set_status(\"onboarding\")\n\n    # Determine the Model Type\n    while self.is_model_unknown():\n        self._determine_model_type()\n\n    # Is our input data set?\n    if self.get_input() in [\"\", \"unknown\"] or ask_everything:\n        input_data = input(\"Input Data?: \")\n        if input_data not in [\"None\", \"none\", \"\", \"unknown\"]:\n            self.set_input(input_data)\n\n    # Determine the Target Column (can be None)\n    target_column = self.target()\n    if target_column is None or ask_everything:\n        target_column = input(\"Target Column? (for unsupervised/transformer just type None): \")\n        if target_column in [\"None\", \"none\", \"\"]:\n            target_column = None\n\n    # Determine the Feature Columns\n    feature_columns = self.features()\n    if feature_columns is None or ask_everything:\n        feature_columns = input(\"Feature Columns? (use commas): \")\n        feature_columns = [e.strip() for e in feature_columns.split(\",\")]\n        if feature_columns in [[\"None\"], [\"none\"], [\"\"]]:\n            feature_columns = None\n\n    # Registered Endpoints?\n    endpoints = self.endpoints()\n    if not endpoints or ask_everything:\n        endpoints = input(\"Register Endpoints? (use commas for multiple): \")\n        endpoints = [e.strip() for e in endpoints.split(\",\")]\n        if endpoints in [[\"None\"], [\"none\"], [\"\"]]:\n            endpoints = None\n\n    # Model Owner?\n    owner = self.get_owner()\n    if owner in [None, \"unknown\"] or ask_everything:\n        owner = input(\"Model Owner: \")\n        if owner in [\"None\", \"none\", \"\"]:\n            owner = \"unknown\"\n\n    # Model Class Labels (if it's a classifier)\n    if self.model_type == ModelType.CLASSIFIER:\n        class_labels = self.class_labels()\n        if class_labels is None or ask_everything:\n            class_labels = input(\"Class Labels? (use commas): \")\n            class_labels = [e.strip() for e in class_labels.split(\",\")]\n            if class_labels in [[\"None\"], [\"none\"], [\"\"]]:\n                class_labels = None\n        self.set_class_labels(class_labels)\n\n    # Now that we have all the details, let's onboard the Model with all the args\n    return self.onboard_with_args(self.model_type, target_column, feature_columns, endpoints, owner)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.onboard_with_args","title":"<code>onboard_with_args(model_type, target_column=None, feature_list=None, endpoints=None, owner=None)</code>","text":"<p>Onboard the Model with the given arguments</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>Model Type</p> required <code>target_column</code> <code>str</code> <p>Target Column</p> <code>None</code> <code>feature_list</code> <code>list</code> <p>List of Feature Columns</p> <code>None</code> <code>endpoints</code> <code>list</code> <p>List of Endpoints. Defaults to None.</p> <code>None</code> <code>owner</code> <code>str</code> <p>Model Owner. Defaults to None.</p> <code>None</code> <p>Returns:     bool: True if the Model is successfully onboarded, False otherwise</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def onboard_with_args(\n    self,\n    model_type: ModelType,\n    target_column: str = None,\n    feature_list: list = None,\n    endpoints: list = None,\n    owner: str = None,\n) -&gt; bool:\n    \"\"\"Onboard the Model with the given arguments\n\n    Args:\n        model_type (ModelType): Model Type\n        target_column (str): Target Column\n        feature_list (list): List of Feature Columns\n        endpoints (list, optional): List of Endpoints. Defaults to None.\n        owner (str, optional): Model Owner. Defaults to None.\n    Returns:\n        bool: True if the Model is successfully onboarded, False otherwise\n    \"\"\"\n    # Set the status to onboarding\n    self.set_status(\"onboarding\")\n\n    # Set All the Details\n    self._set_model_type(model_type)\n    if target_column:\n        self.set_target(target_column)\n    if feature_list:\n        self.set_features(feature_list)\n    if endpoints:\n        for endpoint in endpoints:\n            self.register_endpoint(endpoint)\n    if owner:\n        self.set_owner(owner)\n\n    # Load the training metrics and inference metrics\n    self._load_training_metrics()\n    self._load_inference_metrics()\n\n    # Remove the needs_onboard tag\n    self.remove_health_tag(\"needs_onboard\")\n    self.set_status(\"ready\")\n\n    # Run a health check and refresh the meta\n    time.sleep(2)  # Give the AWS Metadata a chance to update\n    self.health_check(deep=True)\n    self.refresh_meta()\n    self.details()\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.publish_prox_model","title":"<code>publish_prox_model(prox_model_name=None, include_all_columns=False)</code>","text":"<p>Create and publish a Proximity Model for this Model</p> <p>Parameters:</p> Name Type Description Default <code>prox_model_name</code> <code>str</code> <p>Name of the Proximity Model (if not specified, a name will be generated)</p> <code>None</code> <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in results (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>ModelCore</code> <p>The published Proximity Model</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def publish_prox_model(self, prox_model_name: str = None, include_all_columns: bool = False) -&gt; \"ModelCore\":\n    \"\"\"Create and publish a Proximity Model for this Model\n\n    Args:\n        prox_model_name (str, optional): Name of the Proximity Model (if not specified, a name will be generated)\n        include_all_columns (bool): Include all DataFrame columns in results (default: False)\n\n    Returns:\n        Model: The published Proximity Model\n    \"\"\"\n    if prox_model_name is None:\n        prox_model_name = self.model_name + \"-prox\"\n    return published_proximity_model(self, prox_model_name, include_all_columns=include_all_columns)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.refresh_meta","title":"<code>refresh_meta()</code>","text":"<p>Refresh the Artifact's metadata</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def refresh_meta(self):\n    \"\"\"Refresh the Artifact's metadata\"\"\"\n    self.model_meta = self.meta.model(self.model_name)\n    self.latest_model = self.model_meta[\"ModelPackageList\"][0]\n    self.description = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n    self.training_job_name = self._extract_training_job_name()\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.register_endpoint","title":"<code>register_endpoint(endpoint_name)</code>","text":"<p>Add this endpoint to the set of registered endpoints for the model</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>Name of the endpoint</p> required Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def register_endpoint(self, endpoint_name: str):\n    \"\"\"Add this endpoint to the set of registered endpoints for the model\n\n    Args:\n        endpoint_name (str): Name of the endpoint\n    \"\"\"\n    self.log.important(f\"Registering Endpoint {endpoint_name} with Model {self.name}...\")\n    registered_endpoints = set(self.workbench_meta().get(\"workbench_registered_endpoints\", []))\n    registered_endpoints.add(endpoint_name)\n    self.upsert_workbench_meta({\"workbench_registered_endpoints\": list(registered_endpoints)})\n\n    # Remove any health tags\n    self.remove_health_tag(\"no_endpoint\")\n\n    # A new endpoint means we need to refresh our inference path\n    time.sleep(2)  # Give the AWS Metadata a chance to update\n    self.endpoint_inference_path = self.get_endpoint_inference_path()\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.remove_endpoint","title":"<code>remove_endpoint(endpoint_name)</code>","text":"<p>Remove this endpoint from the set of registered endpoints for the model</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>Name of the endpoint</p> required Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def remove_endpoint(self, endpoint_name: str):\n    \"\"\"Remove this endpoint from the set of registered endpoints for the model\n\n    Args:\n        endpoint_name (str): Name of the endpoint\n    \"\"\"\n    self.log.important(f\"Removing Endpoint {endpoint_name} from Model {self.name}...\")\n    try:\n        registered_endpoints = set(self.workbench_meta().get(\"workbench_registered_endpoints\", []))\n        registered_endpoints.discard(endpoint_name)\n        self.upsert_workbench_meta({\"workbench_registered_endpoints\": list(registered_endpoints)})\n    except AttributeError:\n        self.log.warning(f\"Model {self.name} probably doesn't exist, skipping endpoint removal\")\n        return\n\n    # If we have NO endpionts, then set a health tags\n    if not registered_endpoints:\n        self.add_health_tag(\"no_endpoint\")\n        self.details()\n\n    # A new endpoint means we need to refresh our inference path\n    time.sleep(2)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.sagemaker_model_object","title":"<code>sagemaker_model_object()</code>","text":"<p>Return the latest AWS Sagemaker Model object for this Workbench Model</p> <p>Returns:</p> Type Description <code>Model</code> <p>sagemaker.model.Model: AWS Sagemaker Model object</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def sagemaker_model_object(self) -&gt; SagemakerModel:\n    \"\"\"Return the latest AWS Sagemaker Model object for this Workbench Model\n\n    Returns:\n       sagemaker.model.Model: AWS Sagemaker Model object\n    \"\"\"\n    return SagemakerModel(\n        model_data=self.model_data_url(),\n        sagemaker_session=self.sm_session,\n        image_uri=self.container_image(),\n    )\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.set_class_labels","title":"<code>set_class_labels(labels)</code>","text":"<p>Return the class labels for this Model (if it's a classifier)</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>list[str]</code> <p>List of class labels</p> required Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def set_class_labels(self, labels: list[str]):\n    \"\"\"Return the class labels for this Model (if it's a classifier)\n\n    Args:\n        labels (list[str]): List of class labels\n    \"\"\"\n    if self.model_type == ModelType.CLASSIFIER:\n        self.upsert_workbench_meta({\"class_labels\": labels})\n    else:\n        self.log.error(f\"Model {self.model_name} is not a classifier!\")\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.set_features","title":"<code>set_features(feature_columns)</code>","text":"<p>Set the features for this Model</p> <p>Parameters:</p> Name Type Description Default <code>feature_columns</code> <code>list[str]</code> <p>List of feature columns</p> required Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def set_features(self, feature_columns: list[str]):\n    \"\"\"Set the features for this Model\n\n    Args:\n        feature_columns (list[str]): List of feature columns\n    \"\"\"\n    self.upsert_workbench_meta({\"workbench_model_features\": feature_columns})\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.set_input","title":"<code>set_input(input, force=False)</code>","text":"<p>Override: Set the input data for this artifact</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>Name of input for this artifact</p> required <code>force</code> <code>bool</code> <p>Force the input to be set (default: False)</p> <code>False</code> <p>Note:     We're going to not allow this to be used for Models</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def set_input(self, input: str, force: bool = False):\n    \"\"\"Override: Set the input data for this artifact\n\n    Args:\n        input (str): Name of input for this artifact\n        force (bool, optional): Force the input to be set (default: False)\n    Note:\n        We're going to not allow this to be used for Models\n    \"\"\"\n    if not force:\n        self.log.warning(f\"Model {self.name}: Does not allow manual override of the input!\")\n        return\n\n    # Okay we're going to allow this to be set\n    self.log.important(f\"{self.name}: Setting input to {input}...\")\n    self.log.important(\"Be careful with this! It breaks automatic provenance of the artifact!\")\n    self.upsert_workbench_meta({\"workbench_input\": input})\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.set_pipeline","title":"<code>set_pipeline(pipeline)</code>","text":"<p>Set the pipeline for this model</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>str</code> <p>Pipeline that was used to create this model</p> required Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def set_pipeline(self, pipeline: str):\n    \"\"\"Set the pipeline for this model\n\n    Args:\n        pipeline (str): Pipeline that was used to create this model\n    \"\"\"\n    self.upsert_workbench_meta({\"workbench_pipeline\": pipeline})\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.set_target","title":"<code>set_target(target_column)</code>","text":"<p>Set the target for this Model</p> <p>Parameters:</p> Name Type Description Default <code>target_column</code> <code>str</code> <p>Target column for this Model</p> required Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def set_target(self, target_column: str):\n    \"\"\"Set the target for this Model\n\n    Args:\n        target_column (str): Target column for this Model\n    \"\"\"\n    self.upsert_workbench_meta({\"workbench_model_target\": target_column})\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.shap_feature_values","title":"<code>shap_feature_values()</code>","text":"<p>Retrieve the feature values for SHAP sample rows (used for plot coloring).</p> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>Optional[pd.DataFrame]: Feature values for SHAP sample rows</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def shap_feature_values(self) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Retrieve the feature values for SHAP sample rows (used for plot coloring).\n\n    Returns:\n        Optional[pd.DataFrame]: Feature values for SHAP sample rows\n    \"\"\"\n    return get_shap_feature_values(self.model_training_path)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.shap_importance","title":"<code>shap_importance()</code>","text":"<p>Retrieve the SHAP Feature Importance for this model.</p> <p>Returns:</p> Type Description <code>Optional[List[Tuple[str, float]]]</code> <p>Optional[List[Tuple[str, float]]]: List of tuples containing feature names and their importance scores</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def shap_importance(self) -&gt; Optional[List[Tuple[str, float]]]:\n    \"\"\"Retrieve the SHAP Feature Importance for this model.\n\n    Returns:\n        Optional[List[Tuple[str, float]]]: List of tuples containing feature names and their importance scores\n    \"\"\"\n    return get_shap_importance(self.model_training_path)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.shap_values","title":"<code>shap_values()</code>","text":"<p>Retrieve the SHAP values (contributions) for this model.</p> <p>Returns:</p> Type Description <code>Optional[Union[DataFrame, Dict[str, DataFrame]]]</code> <p>Optional[Union[pd.DataFrame, dict]]: SHAP values (DataFrame or dict of DataFrames for multiclass)</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def shap_values(self) -&gt; Optional[Union[pd.DataFrame, Dict[str, pd.DataFrame]]]:\n    \"\"\"Retrieve the SHAP values (contributions) for this model.\n\n    Returns:\n        Optional[Union[pd.DataFrame, dict]]: SHAP values (DataFrame or dict of DataFrames for multiclass)\n    \"\"\"\n    return get_shap_values(self.model_training_path, self.class_labels())\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.size","title":"<code>size()</code>","text":"<p>Return the size of this data in MegaBytes</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def size(self) -&gt; float:\n    \"\"\"Return the size of this data in MegaBytes\"\"\"\n    return 0.0\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.source_dir_url","title":"<code>source_dir_url()</code>","text":"<p>Retrieve the model source directory from the model's AWS metadata.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The model source directory if available, otherwise None.</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>@deprecated(version=\"0.9\")\ndef source_dir_url(self) -&gt; Optional[str]:\n    \"\"\"Retrieve the model source directory from the model's AWS metadata.\n\n    Returns:\n        Optional[str]: The model source directory if available, otherwise None.\n    \"\"\"\n    meta = self.aws_meta()\n    try:\n        return meta[\"ModelPackageList\"][0][\"InferenceSpecification\"][\"Containers\"][0][\"Environment\"][\n            \"SAGEMAKER_SUBMIT_DIRECTORY\"\n        ]\n    except (KeyError, IndexError, TypeError):\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.summary","title":"<code>summary()</code>","text":"<p>Summary information about this Model</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of summary information about this Model</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def summary(self) -&gt; dict:\n    \"\"\"Summary information about this Model\n\n    Returns:\n        dict: Dictionary of summary information about this Model\n    \"\"\"\n    self.log.info(\"Computing Model Summary...\")\n    summary = super().summary()\n    summary[\"hyperparameters\"] = get_model_hyperparameters(self)\n    return summary\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.supported_inference_instances","title":"<code>supported_inference_instances()</code>","text":"<p>Retrieve the supported endpoint inference instance types</p> <p>Returns:</p> Type Description <code>Optional[list]</code> <p>Optional[list]: List of supported inference instance types</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def supported_inference_instances(self) -&gt; Optional[list]:\n    \"\"\"Retrieve the supported endpoint inference instance types\n\n    Returns:\n        Optional[list]: List of supported inference instance types\n    \"\"\"\n    meta = self.aws_meta()\n    try:\n        return meta[\"ModelPackageList\"][0][\"InferenceSpecification\"][\"SupportedRealtimeInferenceInstanceTypes\"]\n    except (KeyError, IndexError, TypeError):\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.target","title":"<code>target()</code>","text":"<p>Return the target for this Model (if supervised, else None)</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>Target column for this Model (if supervised, else None)</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def target(self) -&gt; Union[str, None]:\n    \"\"\"Return the target for this Model (if supervised, else None)\n\n    Returns:\n        str: Target column for this Model (if supervised, else None)\n    \"\"\"\n    return self.workbench_meta().get(\"workbench_model_target\")  # Returns None if not found\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelCore.training_view","title":"<code>training_view()</code>","text":"<p>Get the training view for this model</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>def training_view(self):\n    \"\"\"Get the training view for this model\"\"\"\n    from workbench.core.artifacts.feature_set_core import FeatureSetCore\n    from workbench.core.views import View\n\n    # Grab our FeatureSet\n    fs = FeatureSetCore(self.get_input())\n\n    # See if we have a training view for this model\n    my_model_training_view = f\"{self.name.replace('-', '_')}_training\".lower()\n    view = View(fs, my_model_training_view, auto_create_view=False)\n    if view.exists():\n        return view\n    else:\n        self.log.important(f\"No specific training view {my_model_training_view}, returning default training view\")\n        return fs.view(\"training\")\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelFramework","title":"<code>ModelFramework</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerated Types for Workbench Model Frameworks</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>class ModelFramework(Enum):\n    \"\"\"Enumerated Types for Workbench Model Frameworks\"\"\"\n\n    SKLEARN = \"sklearn\"\n    XGBOOST = \"xgboost\"\n    LIGHTGBM = \"lightgbm\"\n    PYTORCH = \"pytorch\"\n    CHEMPROP = \"chemprop\"\n    TRANSFORMER = \"transformer\"\n    META = \"meta\"\n    UNKNOWN = \"unknown\"\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelImages","title":"<code>ModelImages</code>","text":"<p>Class for retrieving workbench inference images</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>class ModelImages:\n    \"\"\"Class for retrieving workbench inference images\"\"\"\n\n    # Account ID\n    ACCOUNT_ID = \"507740646243\"\n\n    # Image name mappings\n    IMAGE_NAMES = {\n        \"training\": \"py312-general-ml-training\",\n        \"inference\": \"py312-general-ml-inference\",\n        \"pytorch_training\": \"py312-pytorch-training\",\n        \"pytorch_inference\": \"py312-pytorch-inference\",\n        \"meta_training\": \"py312-meta-training\",\n        \"meta_inference\": \"py312-meta-inference\",\n    }\n\n    @classmethod\n    def get_image_uri(cls, region: str, image_type: str, version: str = \"latest\", architecture: str = \"x86_64\") -&gt; str:\n        \"\"\"\n        Dynamically construct ECR image URI.\n\n        Args:\n            region (str): AWS region (e.g., 'us-east-1', 'us-west-2')\n            image_type (str): Type of image (e.g., 'training', 'inference', 'pytorch_training')\n            version (str): Image version (e.g., '0.1', '0.2' defaults to 'latest')\n            architecture (str): CPU architecture (default: 'x86_64', currently unused but kept for compatibility)\n\n        Returns:\n            str: ECR image URI string\n        \"\"\"\n        if image_type not in cls.IMAGE_NAMES:\n            raise ValueError(f\"Unknown image_type: {image_type}. Valid types: {list(cls.IMAGE_NAMES.keys())}\")\n\n        image_name = cls.IMAGE_NAMES[image_type]\n        uri = f\"{cls.ACCOUNT_ID}.dkr.ecr.{region}.amazonaws.com/aws-ml-images/{image_name}:{version}\"\n\n        return uri\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelImages.get_image_uri","title":"<code>get_image_uri(region, image_type, version='latest', architecture='x86_64')</code>  <code>classmethod</code>","text":"<p>Dynamically construct ECR image URI.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>str</code> <p>AWS region (e.g., 'us-east-1', 'us-west-2')</p> required <code>image_type</code> <code>str</code> <p>Type of image (e.g., 'training', 'inference', 'pytorch_training')</p> required <code>version</code> <code>str</code> <p>Image version (e.g., '0.1', '0.2' defaults to 'latest')</p> <code>'latest'</code> <code>architecture</code> <code>str</code> <p>CPU architecture (default: 'x86_64', currently unused but kept for compatibility)</p> <code>'x86_64'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>ECR image URI string</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>@classmethod\ndef get_image_uri(cls, region: str, image_type: str, version: str = \"latest\", architecture: str = \"x86_64\") -&gt; str:\n    \"\"\"\n    Dynamically construct ECR image URI.\n\n    Args:\n        region (str): AWS region (e.g., 'us-east-1', 'us-west-2')\n        image_type (str): Type of image (e.g., 'training', 'inference', 'pytorch_training')\n        version (str): Image version (e.g., '0.1', '0.2' defaults to 'latest')\n        architecture (str): CPU architecture (default: 'x86_64', currently unused but kept for compatibility)\n\n    Returns:\n        str: ECR image URI string\n    \"\"\"\n    if image_type not in cls.IMAGE_NAMES:\n        raise ValueError(f\"Unknown image_type: {image_type}. Valid types: {list(cls.IMAGE_NAMES.keys())}\")\n\n    image_name = cls.IMAGE_NAMES[image_type]\n    uri = f\"{cls.ACCOUNT_ID}.dkr.ecr.{region}.amazonaws.com/aws-ml-images/{image_name}:{version}\"\n\n    return uri\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#workbench.core.artifacts.model_core.ModelType","title":"<code>ModelType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerated Types for Workbench Model Types</p> Source code in <code>src/workbench/core/artifacts/model_core.py</code> <pre><code>class ModelType(Enum):\n    \"\"\"Enumerated Types for Workbench Model Types\"\"\"\n\n    CLASSIFIER = \"classifier\"\n    REGRESSOR = \"regressor\"\n    CLUSTERER = \"clusterer\"\n    PROXIMITY = \"proximity\"\n    PROJECTION = \"projection\"\n    UQ_REGRESSOR = \"uq_regressor\"\n    ENSEMBLE_REGRESSOR = \"ensemble_regressor\"\n    TRANSFORMER = \"transformer\"\n    UNKNOWN = \"unknown\"\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/","title":"MonitorCore","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the Monitor API Class and voil\u00e0 it works the same.</p> <p>MonitorCore class for monitoring SageMaker endpoints</p>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore","title":"<code>MonitorCore</code>","text":"<p>Manages monitoring, baselines, and monitoring schedules for SageMaker endpoints</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>class MonitorCore:\n    \"\"\"Manages monitoring, baselines, and monitoring schedules for SageMaker endpoints\"\"\"\n\n    def __init__(self, endpoint_name, instance_type=\"ml.m5.large\"):\n        \"\"\"MonitorCore Class\n\n        Args:\n            endpoint_name (str): Name of the endpoint to set up monitoring for\n            instance_type (str): Instance type to use for monitoring. Defaults to \"ml.m5.large\".\n        \"\"\"\n        self.log = logging.getLogger(\"workbench\")\n        self.endpoint_name = endpoint_name\n        self.endpoint = EndpointCore(self.endpoint_name)\n\n        # Initialize Class Attributes\n        self.sagemaker_session = self.endpoint.sm_session\n        self.sagemaker_client = self.endpoint.sm_client\n        self.monitoring_path = self.endpoint.endpoint_monitoring_path\n        self.monitoring_schedule_name = f\"{self.endpoint_name}-monitoring-schedule\"\n        self.baseline_dir = f\"{self.monitoring_path}/baseline\"\n        self.baseline_csv_file = f\"{self.baseline_dir}/baseline.csv\"\n        self.constraints_json_file = f\"{self.baseline_dir}/constraints.json\"\n        self.statistics_json_file = f\"{self.baseline_dir}/statistics.json\"\n        self.preprocessing_script_file = f\"{self.monitoring_path}/preprocessor.py\"\n        self.workbench_role_arn = AWSAccountClamp().aws_session.get_workbench_execution_role_arn()\n        self.instance_type = instance_type\n\n        # Create DataCaptureCore instance for composition\n        self.data_capture = DataCaptureCore(endpoint_name)\n        self.data_capture_path = self.data_capture.data_capture_path\n\n        # Check if a monitoring schedule already exists for this endpoint\n        existing_schedule = self.monitoring_schedule_exists()\n\n        if existing_schedule:\n            # If a schedule exists, attach to it\n            self.model_monitor = DefaultModelMonitor.attach(\n                monitor_schedule_name=self.monitoring_schedule_name, sagemaker_session=self.sagemaker_session\n            )\n            self.log.info(f\"Attached to existing monitoring schedule for {self.endpoint_name}\")\n        else:\n            # Create a new model monitor\n            self.model_monitor = DefaultModelMonitor(\n                role=self.workbench_role_arn, instance_type=self.instance_type, sagemaker_session=self.sagemaker_session\n            )\n            self.log.info(f\"Initialized new model monitor for {self.endpoint_name}\")\n\n    def summary(self) -&gt; dict:\n        \"\"\"Return the summary of monitoring configuration\n\n        Returns:\n            dict: Summary of monitoring status\n        \"\"\"\n        if self.endpoint.is_serverless():\n            return {\n                \"endpoint_type\": \"serverless\",\n                \"baseline\": \"not supported\",\n                \"monitoring_schedule\": \"not supported\",\n            }\n        else:\n            summary = {\n                \"endpoint_type\": \"realtime\",\n                \"baseline\": self.baseline_exists(),\n                \"monitoring_schedule\": self.monitoring_schedule_exists(),\n                \"preprocessing\": self.preprocessing_exists(),\n            }\n            return summary\n\n    def details(self) -&gt; dict:\n        \"\"\"Return the details of the monitoring for the endpoint\n\n        Returns:\n            dict: The monitoring details for the endpoint\n        \"\"\"\n        result = self.summary()\n        info = {\n            \"monitoring_schedule_status\": \"Not Scheduled\",\n        }\n        result.update(info)\n\n        if self.preprocessing_exists():\n            result[\"preprocessing_script_file\"] = self.preprocessing_script_file\n\n        if self.baseline_exists():\n            result.update(\n                {\n                    \"baseline_csv_file\": self.baseline_csv_file,\n                    \"baseline_constraints_json_file\": self.constraints_json_file,\n                    \"baseline_statistics_json_file\": self.statistics_json_file,\n                }\n            )\n\n        if self.monitoring_schedule_exists():\n            schedule_details = self.sagemaker_client.describe_monitoring_schedule(\n                MonitoringScheduleName=self.monitoring_schedule_name\n            )\n\n            result.update(\n                {\n                    \"monitoring_schedule_name\": schedule_details.get(\"MonitoringScheduleName\"),\n                    \"monitoring_schedule_status\": schedule_details.get(\"MonitoringScheduleStatus\"),\n                    \"monitoring_path\": self.monitoring_path,\n                    \"creation_time\": datetime_string(schedule_details.get(\"CreationTime\")),\n                }\n            )\n\n            last_run = schedule_details.get(\"LastMonitoringExecutionSummary\", {})\n            if last_run:\n                # If no inference was run since the last monitoring schedule, the\n                # status will be \"Failed\" with reason \"Job inputs had no data\",\n                # so we check for that and set the status to \"No New Data\"\n                status = last_run.get(\"MonitoringExecutionStatus\")\n                reason = last_run.get(\"FailureReason\")\n                if status == \"Failed\" and reason == \"Job inputs had no data\":\n                    status = reason = \"No New Data\"\n                result.update(\n                    {\n                        \"last_run_status\": status,\n                        \"last_run_time\": datetime_string(last_run.get(\"ScheduledTime\")),\n                        \"failure_reason\": reason,\n                    }\n                )\n\n        return result\n\n    def enable_data_capture(self, capture_percentage=100):\n        \"\"\"Enable data capture for the endpoint\n\n        Args:\n            capture_percentage (int): Percentage of requests to capture (0-100, default 100)\n        \"\"\"\n        if self.endpoint.is_serverless():\n            self.log.warning(\"Data capture is not supported for serverless endpoints.\")\n            return\n\n        if self.data_capture.is_enabled():\n            self.log.info(f\"Data capture is already enabled for {self.endpoint_name}.\")\n            return\n\n        self.data_capture.enable(capture_percentage=capture_percentage)\n        self.log.important(f\"Enabled data capture for {self.endpoint_name} at {self.data_capture_path}\")\n\n    def baseline_exists(self) -&gt; bool:\n        \"\"\"\n        Check if baseline files exist in S3.\n\n        Returns:\n            bool: True if all files exist, False otherwise.\n        \"\"\"\n        files = [self.baseline_csv_file, self.constraints_json_file, self.statistics_json_file]\n        return all(wr.s3.does_object_exist(file) for file in files)\n\n    def preprocessing_exists(self) -&gt; bool:\n        \"\"\"\n        Check if preprocessing script exists in S3.\n\n        Returns:\n            bool: True if the preprocessing script exists, False otherwise.\n        \"\"\"\n        return wr.s3.does_object_exist(self.preprocessing_script_file)\n\n    def create_baseline(self, recreate: bool = False, baseline_csv: str = None):\n        \"\"\"Code to create a baseline for monitoring\n\n        Args:\n            recreate (bool): If True, recreate the baseline even if it already exists\n            baseline_csv (str): Optional path to a custom baseline CSV file. If provided,\n                                this will be used instead of pulling from the FeatureSet.\n        Notes:\n            This will create/write three files to the baseline_dir:\n            - baseline.csv\n            - constraints.json\n            - statistics.json\n        \"\"\"\n        # Check if this endpoint is a serverless endpoint\n        if self.endpoint.is_serverless():\n            self.log.warning(\"You can create a baseline but Model monitoring won't work for serverless endpoints\")\n\n        # Check if the baseline already exists\n        if self.baseline_exists() and not recreate:\n            self.log.info(f\"Baseline already exists for {self.endpoint_name}.\")\n            self.log.info(\"If you want to recreate it, set recreate=True.\")\n            return\n\n        # Get the features from the Model\n        model = Model(self.endpoint.get_input())\n        features = model.features()\n\n        # If a custom baseline CSV is provided, use it instead of pulling from the FeatureSet\n        if baseline_csv:\n            self.log.info(f\"Using custom baseline CSV: {baseline_csv}\")\n            # Ensure the file exists\n            if not wr.s3.does_object_exist(baseline_csv):\n                self.log.error(f\"Custom baseline CSV does not exist: {baseline_csv}\")\n                return\n            baseline_df = wr.s3.read_csv(baseline_csv)\n\n        # Create a baseline for monitoring (all rows from the FeatureSet)\n        else:\n            self.log.important(f\"Creating baseline for {self.endpoint_name} --&gt; {self.baseline_dir}\")\n            fs = FeatureSet(model.get_input())\n            baseline_df = fs.pull_dataframe()\n\n        # We only want the model features for our baseline\n        baseline_df = baseline_df[features]\n\n        # Sort the columns to ensure consistent ordering (AWS/Spark needs this)\n        baseline_df = baseline_df[sorted(baseline_df.columns)]\n\n        # Write the baseline to S3\n        wr.s3.to_csv(baseline_df, self.baseline_csv_file, index=False)\n\n        # Create the baseline files (constraints.json and statistics.json)\n        self.log.important(f\"Creating baseline files for {self.endpoint_name} --&gt; {self.baseline_dir}\")\n        self.model_monitor.suggest_baseline(\n            baseline_dataset=self.baseline_csv_file,\n            dataset_format=DatasetFormat.csv(header=True),\n            output_s3_uri=self.baseline_dir,\n        )\n\n        # List the S3 bucket baseline files\n        baseline_files = wr.s3.list_objects(self.baseline_dir)\n        self.log.important(\"Baseline files created:\")\n        for file in baseline_files:\n            self.log.important(f\" - {file}\")\n\n    def get_baseline(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Code to get the baseline CSV from the S3 baseline directory\n\n        Returns:\n            pd.DataFrame: The baseline CSV as a DataFrame (None if it doesn't exist)\n        \"\"\"\n        # Read the monitoring data from S3\n        if not wr.s3.does_object_exist(path=self.baseline_csv_file):\n            self.log.warning(\"baseline.csv data does not exist in S3.\")\n            return None\n        else:\n            return wr.s3.read_csv(self.baseline_csv_file)\n\n    def get_constraints(self) -&gt; dict:\n        \"\"\"Code to get the constraints from the baseline\n\n        Returns:\n            dict: The constraints associated with the monitor (constraints.json) (None if it doesn't exist)\n        \"\"\"\n        return get_monitor_json_data(self.constraints_json_file)\n\n    def get_statistics(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Code to get the statistics from the baseline\n\n        Returns:\n            pd.DataFrame: The statistics from the baseline (statistics.json) (None if it doesn't exist)\n        \"\"\"\n        # Use the utility function\n        return get_monitor_json_data(self.statistics_json_file)\n\n    def update_constraints(self, constraints_updates) -&gt; bool:\n        \"\"\"Update the constraints file with custom constraints or monitoring config\n\n        Args:\n            constraints_updates (dict): Dictionary of updates to apply to the constraints file.\n                - If key is \"monitoring_config\", updates the monitoring configuration\n                - Otherwise, treated as feature name with constraint updates\n\n        Returns:\n            bool: True if successful, False otherwise\n        \"\"\"\n        if not self.baseline_exists():\n            self.log.warning(\"Cannot update constraints without a baseline\")\n            return False\n\n        try:\n            # Read constraints file from S3\n            raw_json = read_content_from_s3(self.constraints_json_file)\n            constraints = json.loads(raw_json)\n\n            # Handle each update key\n            for key, updates in constraints_updates.items():\n                if key == \"monitoring_config\":\n                    # Update monitoring config\n                    if \"monitoring_config\" not in constraints:\n                        constraints[\"monitoring_config\"] = {}\n                    constraints[\"monitoring_config\"].update(updates)\n                else:\n                    # Update feature constraints\n                    feature_found = False\n                    for feature in constraints.get(\"features\", []):\n                        if feature[\"name\"] == key:\n                            feature.update(updates)\n                            feature_found = True\n                            break\n\n                    if not feature_found:\n                        self.log.warning(f\"Feature {key} not found in constraints\")\n\n            # Write updated constraints back to S3\n            upload_content_to_s3(json.dumps(constraints, indent=2), self.constraints_json_file)\n            self.log.important(f\"Updated constraints at {self.constraints_json_file}\")\n            return True\n        except Exception as e:\n            self.log.error(f\"Error updating constraints: {e}\")\n            return False\n\n    def create_monitoring_schedule(self, schedule: str = \"hourly\"):\n        \"\"\"Sets up the monitoring schedule for the model endpoint.\n\n        Args:\n            schedule (str): The schedule for the monitoring job (hourly or daily, defaults to hourly).\n        \"\"\"\n        # Check if this endpoint is a serverless endpoint\n        if self.endpoint.is_serverless():\n            self.log.warning(\"Monitoring Schedule is not currently supported for serverless endpoints.\")\n            return\n\n        # Set up the monitoring schedule, name, and output path\n        if schedule == \"daily\":\n            schedule = CronExpressionGenerator.daily()\n        else:\n            schedule = CronExpressionGenerator.hourly()\n\n        # Check if the baseline exists\n        if not self.baseline_exists():\n            self.log.warning(f\"Baseline does not exist for {self.endpoint_name}. Call create_baseline() first...\")\n            return\n\n        # If a monitoring schedule already exists, give an informative message\n        if self.monitoring_schedule_exists():\n            self.log.warning(f\"Monitoring schedule for {self.endpoint_name} already exists.\")\n            self.log.warning(\"If you want to create another one, delete existing schedule first.\")\n            return\n\n        # Check if data capture is enabled, if not enable it\n        if not self.data_capture.is_enabled():\n            self.log.warning(\"Data capture is not enabled for this endpoint. Enabling it now...\")\n            self.enable_data_capture(capture_percentage=100)\n\n        # Set up a NEW monitoring schedule\n        schedule_args = {\n            \"monitor_schedule_name\": self.monitoring_schedule_name,\n            \"endpoint_input\": self.endpoint_name,\n            \"output_s3_uri\": self.monitoring_path,\n            \"statistics\": self.statistics_json_file,\n            \"constraints\": self.constraints_json_file,\n            \"schedule_cron_expression\": schedule,\n        }\n\n        # Add preprocessing script to get rid of 'extra_column_check' violation (so stupid)\n        feature_list = self.get_baseline().columns.to_list()\n        script = preprocessing_script(feature_list)\n        upload_content_to_s3(script, self.preprocessing_script_file)\n        self.log.important(f\"Using preprocessing script: {self.preprocessing_script_file}\")\n        schedule_args[\"record_preprocessor_script\"] = self.preprocessing_script_file\n\n        # Create the monitoring schedule\n        self.model_monitor.create_monitoring_schedule(**schedule_args)\n        self.log.important(f\"New Monitoring schedule created for {self.endpoint_name}.\")\n\n    def monitoring_schedule_exists(self) -&gt; bool:\n        \"\"\"Check if a monitoring schedule already exists for this endpoint\n\n        Returns:\n            bool: True if a monitoring schedule exists, False otherwise\n        \"\"\"\n        try:\n            self.sagemaker_client.describe_monitoring_schedule(MonitoringScheduleName=self.monitoring_schedule_name)\n            return True\n        except self.sagemaker_client.exceptions.ResourceNotFound:\n            self.log.info(f\"No monitoring schedule exists for {self.endpoint_name}.\")\n            return False\n\n    def delete_monitoring_schedule(self):\n        \"\"\"Delete the monitoring schedule associated with this endpoint\"\"\"\n        if not self.monitoring_schedule_exists():\n            self.log.warning(f\"No monitoring schedule exists for {self.endpoint_name}.\")\n            return\n\n        # Use the model_monitor to delete the schedule\n        self.model_monitor.delete_monitoring_schedule()\n        self.log.important(f\"Deleted monitoring schedule for {self.endpoint_name}.\")\n\n    def get_monitoring_results(self, max_results=10) -&gt; pd.DataFrame:\n        \"\"\"Get the results of monitoring executions\n\n        Args:\n            max_results (int): Maximum number of results to return\n\n        Returns:\n            pd.DataFrame: DataFrame of monitoring results (empty if no schedule exists)\n        \"\"\"\n        if not self.monitoring_schedule_exists():\n            self.log.warning(f\"No monitoring schedule exists for {self.endpoint_name}.\")\n            return pd.DataFrame()\n\n        try:\n            # Get the monitoring executions\n            executions = self.sagemaker_client.list_monitoring_executions(\n                MonitoringScheduleName=self.monitoring_schedule_name,\n                MaxResults=max_results,\n                SortBy=\"ScheduledTime\",\n                SortOrder=\"Descending\",\n            )\n\n            # Extract the execution details\n            execution_details = []\n            for execution in executions.get(\"MonitoringExecutionSummaries\", []):\n                # Handle status - make \"no data\" failures more user-friendly\n                status = execution.get(\"MonitoringExecutionStatus\")\n                failure_reason = execution.get(\"FailureReason\")\n                if status == \"Failed\" and failure_reason == \"Job inputs had no data\":\n                    display_status = \"No Data\"\n                else:\n                    display_status = status\n\n                detail = {\n                    \"status\": display_status,\n                    \"scheduled_time\": (\n                        execution.get(\"ScheduledTime\").strftime(\"%m/%d %H:%M\")\n                        if execution.get(\"ScheduledTime\")\n                        else None\n                    ),\n                    \"job_start_time\": (\n                        execution.get(\"CreationTime\").strftime(\"%m/%d %H:%M\") if execution.get(\"CreationTime\") else None\n                    ),\n                    \"failure_reason\": failure_reason,\n                    \"monitoring_type\": execution.get(\"MonitoringType\"),\n                    \"processing_job_arn\": execution.get(\"ProcessingJobArn\"),\n                }\n\n                # For completed executions, get violation details\n                if status == \"Completed\" and detail[\"processing_job_arn\"]:\n                    try:\n                        result_path = f\"{self.monitoring_path}/{execution.get('CreationTime').strftime('%Y/%m/%d')}\"\n                        result_path += \"/constraint_violations.json\"\n                        if wr.s3.does_object_exist(result_path):\n                            violations_json = read_content_from_s3(result_path)\n                            violations = parse_monitoring_results(violations_json)\n                            detail[\"violations\"] = violations.get(\"constraint_violations\", [])\n                            detail[\"violation_count\"] = len(detail[\"violations\"])\n                        else:\n                            detail[\"violations\"] = []\n                            detail[\"violation_count\"] = 0\n                    except Exception as e:\n                        self.log.warning(f\"Error getting violations: {e}\")\n                        detail[\"violations\"] = []\n                        detail[\"violation_count\"] = -1\n                else:\n                    detail[\"violations\"] = []\n                    detail[\"violation_count\"] = 0 if display_status in [\"Failed\", \"No Data\"] else None\n\n                execution_details.append(detail)\n\n            return pd.DataFrame(execution_details)\n        except Exception as e:\n            self.log.error(f\"Error getting monitoring results: {e}\")\n            return pd.DataFrame()\n\n    def get_execution_details(self, processing_job_arn):\n        \"\"\"Get detailed information about a specific monitoring execution using processing job ARN\"\"\"\n        try:\n            # Extract just the job name from the ARN\n            job_name = processing_job_arn.split(\"/\")[-1]\n            details = self.sagemaker_client.describe_processing_job(ProcessingJobName=job_name)\n            return details\n        except Exception as e:\n            self.log.error(f\"Error getting execution details for {processing_job_arn}: {e}\")\n            return None\n\n    def setup_alerts(self, notification_email, threshold=1) -&gt; bool:\n        \"\"\"Set up CloudWatch alarms for monitoring violations with email notifications\n\n        Args:\n            notification_email (str): Email to send notifications\n            threshold (int): Number of violations to trigger an alarm\n\n        Returns:\n            bool: True if successful, False otherwise\n        \"\"\"\n        if not self.monitoring_schedule_exists():\n            self.log.warning(f\"No monitoring schedule exists for {self.endpoint_name}.\")\n            return False\n\n        try:\n            # Create CloudWatch client\n            boto3_session = self.sagemaker_session.boto_session\n            cloudwatch_client = boto3_session.client(\"cloudwatch\")\n            sns_client = boto3_session.client(\"sns\")\n\n            # Create a complete alarm configuration with required parameters\n            alarm_name = f\"{self.endpoint_name}-monitoring-violations\"\n\n            # Create CloudWatch alarm configuration\n            alarm_config = {\n                \"AlarmName\": alarm_name,\n                \"AlarmDescription\": f\"Monitoring violations for {self.endpoint_name}\",\n                \"MetricName\": \"ModelDataQualityMonitorViolations\",\n                \"Namespace\": \"AWS/SageMaker\",\n                \"Statistic\": \"Maximum\",\n                \"Dimensions\": [\n                    {\"Name\": \"MonitoringSchedule\", \"Value\": self.monitoring_schedule_name},\n                    {\"Name\": \"EndpointName\", \"Value\": self.endpoint_name},\n                ],\n                \"Period\": 300,  # 5 minutes\n                \"EvaluationPeriods\": 1,\n                \"Threshold\": threshold,\n                \"ComparisonOperator\": \"GreaterThanThreshold\",\n                \"TreatMissingData\": \"notBreaching\",\n            }\n\n            # Create SNS topic for notifications\n            topic_name = f\"{self.endpoint_name}-monitoring-alerts\"\n            topic_response = sns_client.create_topic(Name=topic_name)\n            topic_arn = topic_response[\"TopicArn\"]\n\n            # Subscribe email to topic\n            sns_client.subscribe(TopicArn=topic_arn, Protocol=\"email\", Endpoint=notification_email)\n\n            # Add SNS topic to alarm actions\n            alarm_config[\"AlarmActions\"] = [topic_arn]\n\n            # Create the alarm with the complete configuration\n            cloudwatch_client.put_metric_alarm(**alarm_config)\n\n            self.log.important(f\"Set up CloudWatch alarm with email notification to {notification_email}\")\n            return True\n        except Exception as e:\n            self.log.error(f\"Error setting up CloudWatch alarm: {e}\")\n            return False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of this MonitorCore object\n\n        Returns:\n            str: String representation of this MonitorCore object\n        \"\"\"\n        summary_dict = self.summary()\n        summary_items = [f\"  {repr(key)}: {repr(value)}\" for key, value in summary_dict.items()]\n        summary_str = f\"{self.__class__.__name__}: {self.endpoint_name}\\n\" + \",\\n\".join(summary_items)\n        return summary_str\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.__init__","title":"<code>__init__(endpoint_name, instance_type='ml.m5.large')</code>","text":"<p>MonitorCore Class</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>Name of the endpoint to set up monitoring for</p> required <code>instance_type</code> <code>str</code> <p>Instance type to use for monitoring. Defaults to \"ml.m5.large\".</p> <code>'ml.m5.large'</code> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def __init__(self, endpoint_name, instance_type=\"ml.m5.large\"):\n    \"\"\"MonitorCore Class\n\n    Args:\n        endpoint_name (str): Name of the endpoint to set up monitoring for\n        instance_type (str): Instance type to use for monitoring. Defaults to \"ml.m5.large\".\n    \"\"\"\n    self.log = logging.getLogger(\"workbench\")\n    self.endpoint_name = endpoint_name\n    self.endpoint = EndpointCore(self.endpoint_name)\n\n    # Initialize Class Attributes\n    self.sagemaker_session = self.endpoint.sm_session\n    self.sagemaker_client = self.endpoint.sm_client\n    self.monitoring_path = self.endpoint.endpoint_monitoring_path\n    self.monitoring_schedule_name = f\"{self.endpoint_name}-monitoring-schedule\"\n    self.baseline_dir = f\"{self.monitoring_path}/baseline\"\n    self.baseline_csv_file = f\"{self.baseline_dir}/baseline.csv\"\n    self.constraints_json_file = f\"{self.baseline_dir}/constraints.json\"\n    self.statistics_json_file = f\"{self.baseline_dir}/statistics.json\"\n    self.preprocessing_script_file = f\"{self.monitoring_path}/preprocessor.py\"\n    self.workbench_role_arn = AWSAccountClamp().aws_session.get_workbench_execution_role_arn()\n    self.instance_type = instance_type\n\n    # Create DataCaptureCore instance for composition\n    self.data_capture = DataCaptureCore(endpoint_name)\n    self.data_capture_path = self.data_capture.data_capture_path\n\n    # Check if a monitoring schedule already exists for this endpoint\n    existing_schedule = self.monitoring_schedule_exists()\n\n    if existing_schedule:\n        # If a schedule exists, attach to it\n        self.model_monitor = DefaultModelMonitor.attach(\n            monitor_schedule_name=self.monitoring_schedule_name, sagemaker_session=self.sagemaker_session\n        )\n        self.log.info(f\"Attached to existing monitoring schedule for {self.endpoint_name}\")\n    else:\n        # Create a new model monitor\n        self.model_monitor = DefaultModelMonitor(\n            role=self.workbench_role_arn, instance_type=self.instance_type, sagemaker_session=self.sagemaker_session\n        )\n        self.log.info(f\"Initialized new model monitor for {self.endpoint_name}\")\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of this MonitorCore object</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of this MonitorCore object</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of this MonitorCore object\n\n    Returns:\n        str: String representation of this MonitorCore object\n    \"\"\"\n    summary_dict = self.summary()\n    summary_items = [f\"  {repr(key)}: {repr(value)}\" for key, value in summary_dict.items()]\n    summary_str = f\"{self.__class__.__name__}: {self.endpoint_name}\\n\" + \",\\n\".join(summary_items)\n    return summary_str\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.baseline_exists","title":"<code>baseline_exists()</code>","text":"<p>Check if baseline files exist in S3.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all files exist, False otherwise.</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def baseline_exists(self) -&gt; bool:\n    \"\"\"\n    Check if baseline files exist in S3.\n\n    Returns:\n        bool: True if all files exist, False otherwise.\n    \"\"\"\n    files = [self.baseline_csv_file, self.constraints_json_file, self.statistics_json_file]\n    return all(wr.s3.does_object_exist(file) for file in files)\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.create_baseline","title":"<code>create_baseline(recreate=False, baseline_csv=None)</code>","text":"<p>Code to create a baseline for monitoring</p> <p>Parameters:</p> Name Type Description Default <code>recreate</code> <code>bool</code> <p>If True, recreate the baseline even if it already exists</p> <code>False</code> <code>baseline_csv</code> <code>str</code> <p>Optional path to a custom baseline CSV file. If provided,                 this will be used instead of pulling from the FeatureSet.</p> <code>None</code> <p>Notes:     This will create/write three files to the baseline_dir:     - baseline.csv     - constraints.json     - statistics.json</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def create_baseline(self, recreate: bool = False, baseline_csv: str = None):\n    \"\"\"Code to create a baseline for monitoring\n\n    Args:\n        recreate (bool): If True, recreate the baseline even if it already exists\n        baseline_csv (str): Optional path to a custom baseline CSV file. If provided,\n                            this will be used instead of pulling from the FeatureSet.\n    Notes:\n        This will create/write three files to the baseline_dir:\n        - baseline.csv\n        - constraints.json\n        - statistics.json\n    \"\"\"\n    # Check if this endpoint is a serverless endpoint\n    if self.endpoint.is_serverless():\n        self.log.warning(\"You can create a baseline but Model monitoring won't work for serverless endpoints\")\n\n    # Check if the baseline already exists\n    if self.baseline_exists() and not recreate:\n        self.log.info(f\"Baseline already exists for {self.endpoint_name}.\")\n        self.log.info(\"If you want to recreate it, set recreate=True.\")\n        return\n\n    # Get the features from the Model\n    model = Model(self.endpoint.get_input())\n    features = model.features()\n\n    # If a custom baseline CSV is provided, use it instead of pulling from the FeatureSet\n    if baseline_csv:\n        self.log.info(f\"Using custom baseline CSV: {baseline_csv}\")\n        # Ensure the file exists\n        if not wr.s3.does_object_exist(baseline_csv):\n            self.log.error(f\"Custom baseline CSV does not exist: {baseline_csv}\")\n            return\n        baseline_df = wr.s3.read_csv(baseline_csv)\n\n    # Create a baseline for monitoring (all rows from the FeatureSet)\n    else:\n        self.log.important(f\"Creating baseline for {self.endpoint_name} --&gt; {self.baseline_dir}\")\n        fs = FeatureSet(model.get_input())\n        baseline_df = fs.pull_dataframe()\n\n    # We only want the model features for our baseline\n    baseline_df = baseline_df[features]\n\n    # Sort the columns to ensure consistent ordering (AWS/Spark needs this)\n    baseline_df = baseline_df[sorted(baseline_df.columns)]\n\n    # Write the baseline to S3\n    wr.s3.to_csv(baseline_df, self.baseline_csv_file, index=False)\n\n    # Create the baseline files (constraints.json and statistics.json)\n    self.log.important(f\"Creating baseline files for {self.endpoint_name} --&gt; {self.baseline_dir}\")\n    self.model_monitor.suggest_baseline(\n        baseline_dataset=self.baseline_csv_file,\n        dataset_format=DatasetFormat.csv(header=True),\n        output_s3_uri=self.baseline_dir,\n    )\n\n    # List the S3 bucket baseline files\n    baseline_files = wr.s3.list_objects(self.baseline_dir)\n    self.log.important(\"Baseline files created:\")\n    for file in baseline_files:\n        self.log.important(f\" - {file}\")\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.create_monitoring_schedule","title":"<code>create_monitoring_schedule(schedule='hourly')</code>","text":"<p>Sets up the monitoring schedule for the model endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>str</code> <p>The schedule for the monitoring job (hourly or daily, defaults to hourly).</p> <code>'hourly'</code> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def create_monitoring_schedule(self, schedule: str = \"hourly\"):\n    \"\"\"Sets up the monitoring schedule for the model endpoint.\n\n    Args:\n        schedule (str): The schedule for the monitoring job (hourly or daily, defaults to hourly).\n    \"\"\"\n    # Check if this endpoint is a serverless endpoint\n    if self.endpoint.is_serverless():\n        self.log.warning(\"Monitoring Schedule is not currently supported for serverless endpoints.\")\n        return\n\n    # Set up the monitoring schedule, name, and output path\n    if schedule == \"daily\":\n        schedule = CronExpressionGenerator.daily()\n    else:\n        schedule = CronExpressionGenerator.hourly()\n\n    # Check if the baseline exists\n    if not self.baseline_exists():\n        self.log.warning(f\"Baseline does not exist for {self.endpoint_name}. Call create_baseline() first...\")\n        return\n\n    # If a monitoring schedule already exists, give an informative message\n    if self.monitoring_schedule_exists():\n        self.log.warning(f\"Monitoring schedule for {self.endpoint_name} already exists.\")\n        self.log.warning(\"If you want to create another one, delete existing schedule first.\")\n        return\n\n    # Check if data capture is enabled, if not enable it\n    if not self.data_capture.is_enabled():\n        self.log.warning(\"Data capture is not enabled for this endpoint. Enabling it now...\")\n        self.enable_data_capture(capture_percentage=100)\n\n    # Set up a NEW monitoring schedule\n    schedule_args = {\n        \"monitor_schedule_name\": self.monitoring_schedule_name,\n        \"endpoint_input\": self.endpoint_name,\n        \"output_s3_uri\": self.monitoring_path,\n        \"statistics\": self.statistics_json_file,\n        \"constraints\": self.constraints_json_file,\n        \"schedule_cron_expression\": schedule,\n    }\n\n    # Add preprocessing script to get rid of 'extra_column_check' violation (so stupid)\n    feature_list = self.get_baseline().columns.to_list()\n    script = preprocessing_script(feature_list)\n    upload_content_to_s3(script, self.preprocessing_script_file)\n    self.log.important(f\"Using preprocessing script: {self.preprocessing_script_file}\")\n    schedule_args[\"record_preprocessor_script\"] = self.preprocessing_script_file\n\n    # Create the monitoring schedule\n    self.model_monitor.create_monitoring_schedule(**schedule_args)\n    self.log.important(f\"New Monitoring schedule created for {self.endpoint_name}.\")\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.delete_monitoring_schedule","title":"<code>delete_monitoring_schedule()</code>","text":"<p>Delete the monitoring schedule associated with this endpoint</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def delete_monitoring_schedule(self):\n    \"\"\"Delete the monitoring schedule associated with this endpoint\"\"\"\n    if not self.monitoring_schedule_exists():\n        self.log.warning(f\"No monitoring schedule exists for {self.endpoint_name}.\")\n        return\n\n    # Use the model_monitor to delete the schedule\n    self.model_monitor.delete_monitoring_schedule()\n    self.log.important(f\"Deleted monitoring schedule for {self.endpoint_name}.\")\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.details","title":"<code>details()</code>","text":"<p>Return the details of the monitoring for the endpoint</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The monitoring details for the endpoint</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def details(self) -&gt; dict:\n    \"\"\"Return the details of the monitoring for the endpoint\n\n    Returns:\n        dict: The monitoring details for the endpoint\n    \"\"\"\n    result = self.summary()\n    info = {\n        \"monitoring_schedule_status\": \"Not Scheduled\",\n    }\n    result.update(info)\n\n    if self.preprocessing_exists():\n        result[\"preprocessing_script_file\"] = self.preprocessing_script_file\n\n    if self.baseline_exists():\n        result.update(\n            {\n                \"baseline_csv_file\": self.baseline_csv_file,\n                \"baseline_constraints_json_file\": self.constraints_json_file,\n                \"baseline_statistics_json_file\": self.statistics_json_file,\n            }\n        )\n\n    if self.monitoring_schedule_exists():\n        schedule_details = self.sagemaker_client.describe_monitoring_schedule(\n            MonitoringScheduleName=self.monitoring_schedule_name\n        )\n\n        result.update(\n            {\n                \"monitoring_schedule_name\": schedule_details.get(\"MonitoringScheduleName\"),\n                \"monitoring_schedule_status\": schedule_details.get(\"MonitoringScheduleStatus\"),\n                \"monitoring_path\": self.monitoring_path,\n                \"creation_time\": datetime_string(schedule_details.get(\"CreationTime\")),\n            }\n        )\n\n        last_run = schedule_details.get(\"LastMonitoringExecutionSummary\", {})\n        if last_run:\n            # If no inference was run since the last monitoring schedule, the\n            # status will be \"Failed\" with reason \"Job inputs had no data\",\n            # so we check for that and set the status to \"No New Data\"\n            status = last_run.get(\"MonitoringExecutionStatus\")\n            reason = last_run.get(\"FailureReason\")\n            if status == \"Failed\" and reason == \"Job inputs had no data\":\n                status = reason = \"No New Data\"\n            result.update(\n                {\n                    \"last_run_status\": status,\n                    \"last_run_time\": datetime_string(last_run.get(\"ScheduledTime\")),\n                    \"failure_reason\": reason,\n                }\n            )\n\n    return result\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.enable_data_capture","title":"<code>enable_data_capture(capture_percentage=100)</code>","text":"<p>Enable data capture for the endpoint</p> <p>Parameters:</p> Name Type Description Default <code>capture_percentage</code> <code>int</code> <p>Percentage of requests to capture (0-100, default 100)</p> <code>100</code> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def enable_data_capture(self, capture_percentage=100):\n    \"\"\"Enable data capture for the endpoint\n\n    Args:\n        capture_percentage (int): Percentage of requests to capture (0-100, default 100)\n    \"\"\"\n    if self.endpoint.is_serverless():\n        self.log.warning(\"Data capture is not supported for serverless endpoints.\")\n        return\n\n    if self.data_capture.is_enabled():\n        self.log.info(f\"Data capture is already enabled for {self.endpoint_name}.\")\n        return\n\n    self.data_capture.enable(capture_percentage=capture_percentage)\n    self.log.important(f\"Enabled data capture for {self.endpoint_name} at {self.data_capture_path}\")\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.get_baseline","title":"<code>get_baseline()</code>","text":"<p>Code to get the baseline CSV from the S3 baseline directory</p> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: The baseline CSV as a DataFrame (None if it doesn't exist)</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def get_baseline(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Code to get the baseline CSV from the S3 baseline directory\n\n    Returns:\n        pd.DataFrame: The baseline CSV as a DataFrame (None if it doesn't exist)\n    \"\"\"\n    # Read the monitoring data from S3\n    if not wr.s3.does_object_exist(path=self.baseline_csv_file):\n        self.log.warning(\"baseline.csv data does not exist in S3.\")\n        return None\n    else:\n        return wr.s3.read_csv(self.baseline_csv_file)\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.get_constraints","title":"<code>get_constraints()</code>","text":"<p>Code to get the constraints from the baseline</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The constraints associated with the monitor (constraints.json) (None if it doesn't exist)</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def get_constraints(self) -&gt; dict:\n    \"\"\"Code to get the constraints from the baseline\n\n    Returns:\n        dict: The constraints associated with the monitor (constraints.json) (None if it doesn't exist)\n    \"\"\"\n    return get_monitor_json_data(self.constraints_json_file)\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.get_execution_details","title":"<code>get_execution_details(processing_job_arn)</code>","text":"<p>Get detailed information about a specific monitoring execution using processing job ARN</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def get_execution_details(self, processing_job_arn):\n    \"\"\"Get detailed information about a specific monitoring execution using processing job ARN\"\"\"\n    try:\n        # Extract just the job name from the ARN\n        job_name = processing_job_arn.split(\"/\")[-1]\n        details = self.sagemaker_client.describe_processing_job(ProcessingJobName=job_name)\n        return details\n    except Exception as e:\n        self.log.error(f\"Error getting execution details for {processing_job_arn}: {e}\")\n        return None\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.get_monitoring_results","title":"<code>get_monitoring_results(max_results=10)</code>","text":"<p>Get the results of monitoring executions</p> <p>Parameters:</p> Name Type Description Default <code>max_results</code> <code>int</code> <p>Maximum number of results to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame of monitoring results (empty if no schedule exists)</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def get_monitoring_results(self, max_results=10) -&gt; pd.DataFrame:\n    \"\"\"Get the results of monitoring executions\n\n    Args:\n        max_results (int): Maximum number of results to return\n\n    Returns:\n        pd.DataFrame: DataFrame of monitoring results (empty if no schedule exists)\n    \"\"\"\n    if not self.monitoring_schedule_exists():\n        self.log.warning(f\"No monitoring schedule exists for {self.endpoint_name}.\")\n        return pd.DataFrame()\n\n    try:\n        # Get the monitoring executions\n        executions = self.sagemaker_client.list_monitoring_executions(\n            MonitoringScheduleName=self.monitoring_schedule_name,\n            MaxResults=max_results,\n            SortBy=\"ScheduledTime\",\n            SortOrder=\"Descending\",\n        )\n\n        # Extract the execution details\n        execution_details = []\n        for execution in executions.get(\"MonitoringExecutionSummaries\", []):\n            # Handle status - make \"no data\" failures more user-friendly\n            status = execution.get(\"MonitoringExecutionStatus\")\n            failure_reason = execution.get(\"FailureReason\")\n            if status == \"Failed\" and failure_reason == \"Job inputs had no data\":\n                display_status = \"No Data\"\n            else:\n                display_status = status\n\n            detail = {\n                \"status\": display_status,\n                \"scheduled_time\": (\n                    execution.get(\"ScheduledTime\").strftime(\"%m/%d %H:%M\")\n                    if execution.get(\"ScheduledTime\")\n                    else None\n                ),\n                \"job_start_time\": (\n                    execution.get(\"CreationTime\").strftime(\"%m/%d %H:%M\") if execution.get(\"CreationTime\") else None\n                ),\n                \"failure_reason\": failure_reason,\n                \"monitoring_type\": execution.get(\"MonitoringType\"),\n                \"processing_job_arn\": execution.get(\"ProcessingJobArn\"),\n            }\n\n            # For completed executions, get violation details\n            if status == \"Completed\" and detail[\"processing_job_arn\"]:\n                try:\n                    result_path = f\"{self.monitoring_path}/{execution.get('CreationTime').strftime('%Y/%m/%d')}\"\n                    result_path += \"/constraint_violations.json\"\n                    if wr.s3.does_object_exist(result_path):\n                        violations_json = read_content_from_s3(result_path)\n                        violations = parse_monitoring_results(violations_json)\n                        detail[\"violations\"] = violations.get(\"constraint_violations\", [])\n                        detail[\"violation_count\"] = len(detail[\"violations\"])\n                    else:\n                        detail[\"violations\"] = []\n                        detail[\"violation_count\"] = 0\n                except Exception as e:\n                    self.log.warning(f\"Error getting violations: {e}\")\n                    detail[\"violations\"] = []\n                    detail[\"violation_count\"] = -1\n            else:\n                detail[\"violations\"] = []\n                detail[\"violation_count\"] = 0 if display_status in [\"Failed\", \"No Data\"] else None\n\n            execution_details.append(detail)\n\n        return pd.DataFrame(execution_details)\n    except Exception as e:\n        self.log.error(f\"Error getting monitoring results: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Code to get the statistics from the baseline</p> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>pd.DataFrame: The statistics from the baseline (statistics.json) (None if it doesn't exist)</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def get_statistics(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Code to get the statistics from the baseline\n\n    Returns:\n        pd.DataFrame: The statistics from the baseline (statistics.json) (None if it doesn't exist)\n    \"\"\"\n    # Use the utility function\n    return get_monitor_json_data(self.statistics_json_file)\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.monitoring_schedule_exists","title":"<code>monitoring_schedule_exists()</code>","text":"<p>Check if a monitoring schedule already exists for this endpoint</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if a monitoring schedule exists, False otherwise</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def monitoring_schedule_exists(self) -&gt; bool:\n    \"\"\"Check if a monitoring schedule already exists for this endpoint\n\n    Returns:\n        bool: True if a monitoring schedule exists, False otherwise\n    \"\"\"\n    try:\n        self.sagemaker_client.describe_monitoring_schedule(MonitoringScheduleName=self.monitoring_schedule_name)\n        return True\n    except self.sagemaker_client.exceptions.ResourceNotFound:\n        self.log.info(f\"No monitoring schedule exists for {self.endpoint_name}.\")\n        return False\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.preprocessing_exists","title":"<code>preprocessing_exists()</code>","text":"<p>Check if preprocessing script exists in S3.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the preprocessing script exists, False otherwise.</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def preprocessing_exists(self) -&gt; bool:\n    \"\"\"\n    Check if preprocessing script exists in S3.\n\n    Returns:\n        bool: True if the preprocessing script exists, False otherwise.\n    \"\"\"\n    return wr.s3.does_object_exist(self.preprocessing_script_file)\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.setup_alerts","title":"<code>setup_alerts(notification_email, threshold=1)</code>","text":"<p>Set up CloudWatch alarms for monitoring violations with email notifications</p> <p>Parameters:</p> Name Type Description Default <code>notification_email</code> <code>str</code> <p>Email to send notifications</p> required <code>threshold</code> <code>int</code> <p>Number of violations to trigger an alarm</p> <code>1</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def setup_alerts(self, notification_email, threshold=1) -&gt; bool:\n    \"\"\"Set up CloudWatch alarms for monitoring violations with email notifications\n\n    Args:\n        notification_email (str): Email to send notifications\n        threshold (int): Number of violations to trigger an alarm\n\n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    if not self.monitoring_schedule_exists():\n        self.log.warning(f\"No monitoring schedule exists for {self.endpoint_name}.\")\n        return False\n\n    try:\n        # Create CloudWatch client\n        boto3_session = self.sagemaker_session.boto_session\n        cloudwatch_client = boto3_session.client(\"cloudwatch\")\n        sns_client = boto3_session.client(\"sns\")\n\n        # Create a complete alarm configuration with required parameters\n        alarm_name = f\"{self.endpoint_name}-monitoring-violations\"\n\n        # Create CloudWatch alarm configuration\n        alarm_config = {\n            \"AlarmName\": alarm_name,\n            \"AlarmDescription\": f\"Monitoring violations for {self.endpoint_name}\",\n            \"MetricName\": \"ModelDataQualityMonitorViolations\",\n            \"Namespace\": \"AWS/SageMaker\",\n            \"Statistic\": \"Maximum\",\n            \"Dimensions\": [\n                {\"Name\": \"MonitoringSchedule\", \"Value\": self.monitoring_schedule_name},\n                {\"Name\": \"EndpointName\", \"Value\": self.endpoint_name},\n            ],\n            \"Period\": 300,  # 5 minutes\n            \"EvaluationPeriods\": 1,\n            \"Threshold\": threshold,\n            \"ComparisonOperator\": \"GreaterThanThreshold\",\n            \"TreatMissingData\": \"notBreaching\",\n        }\n\n        # Create SNS topic for notifications\n        topic_name = f\"{self.endpoint_name}-monitoring-alerts\"\n        topic_response = sns_client.create_topic(Name=topic_name)\n        topic_arn = topic_response[\"TopicArn\"]\n\n        # Subscribe email to topic\n        sns_client.subscribe(TopicArn=topic_arn, Protocol=\"email\", Endpoint=notification_email)\n\n        # Add SNS topic to alarm actions\n        alarm_config[\"AlarmActions\"] = [topic_arn]\n\n        # Create the alarm with the complete configuration\n        cloudwatch_client.put_metric_alarm(**alarm_config)\n\n        self.log.important(f\"Set up CloudWatch alarm with email notification to {notification_email}\")\n        return True\n    except Exception as e:\n        self.log.error(f\"Error setting up CloudWatch alarm: {e}\")\n        return False\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.summary","title":"<code>summary()</code>","text":"<p>Return the summary of monitoring configuration</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Summary of monitoring status</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def summary(self) -&gt; dict:\n    \"\"\"Return the summary of monitoring configuration\n\n    Returns:\n        dict: Summary of monitoring status\n    \"\"\"\n    if self.endpoint.is_serverless():\n        return {\n            \"endpoint_type\": \"serverless\",\n            \"baseline\": \"not supported\",\n            \"monitoring_schedule\": \"not supported\",\n        }\n    else:\n        summary = {\n            \"endpoint_type\": \"realtime\",\n            \"baseline\": self.baseline_exists(),\n            \"monitoring_schedule\": self.monitoring_schedule_exists(),\n            \"preprocessing\": self.preprocessing_exists(),\n        }\n        return summary\n</code></pre>"},{"location":"core_classes/artifacts/monitor_core/#workbench.core.artifacts.monitor_core.MonitorCore.update_constraints","title":"<code>update_constraints(constraints_updates)</code>","text":"<p>Update the constraints file with custom constraints or monitoring config</p> <p>Parameters:</p> Name Type Description Default <code>constraints_updates</code> <code>dict</code> <p>Dictionary of updates to apply to the constraints file. - If key is \"monitoring_config\", updates the monitoring configuration - Otherwise, treated as feature name with constraint updates</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/workbench/core/artifacts/monitor_core.py</code> <pre><code>def update_constraints(self, constraints_updates) -&gt; bool:\n    \"\"\"Update the constraints file with custom constraints or monitoring config\n\n    Args:\n        constraints_updates (dict): Dictionary of updates to apply to the constraints file.\n            - If key is \"monitoring_config\", updates the monitoring configuration\n            - Otherwise, treated as feature name with constraint updates\n\n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    if not self.baseline_exists():\n        self.log.warning(\"Cannot update constraints without a baseline\")\n        return False\n\n    try:\n        # Read constraints file from S3\n        raw_json = read_content_from_s3(self.constraints_json_file)\n        constraints = json.loads(raw_json)\n\n        # Handle each update key\n        for key, updates in constraints_updates.items():\n            if key == \"monitoring_config\":\n                # Update monitoring config\n                if \"monitoring_config\" not in constraints:\n                    constraints[\"monitoring_config\"] = {}\n                constraints[\"monitoring_config\"].update(updates)\n            else:\n                # Update feature constraints\n                feature_found = False\n                for feature in constraints.get(\"features\", []):\n                    if feature[\"name\"] == key:\n                        feature.update(updates)\n                        feature_found = True\n                        break\n\n                if not feature_found:\n                    self.log.warning(f\"Feature {key} not found in constraints\")\n\n        # Write updated constraints back to S3\n        upload_content_to_s3(json.dumps(constraints, indent=2), self.constraints_json_file)\n        self.log.important(f\"Updated constraints at {self.constraints_json_file}\")\n        return True\n    except Exception as e:\n        self.log.error(f\"Error updating constraints: {e}\")\n        return False\n</code></pre>"},{"location":"core_classes/artifacts/overview/","title":"Workbench Artifacts","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p>"},{"location":"core_classes/artifacts/overview/#welcome-to-the-workbench-core-artifact-classes","title":"Welcome to the Workbench Core Artifact Classes","text":"<p>These classes provide low-level APIs for the Workbench package, they interact more directly with AWS Services and are therefore more complex with a fairly large number of methods. </p> <ul> <li>AthenaSource: Manages AWS Data Catalog and Athena</li> <li>FeatureSetCore: Manages AWS Feature Store and Feature Groups</li> <li>ModelCore: Manages the training and deployment of AWS Model Groups and Packages</li> <li>EndpointCore: Manages the deployment and invocations/inference on AWS Endpoints</li> </ul> <p></p>"},{"location":"core_classes/transforms/data_loaders_heavy/","title":"DataLoaders Heavy","text":"<p>These DataLoader Classes are intended to load larger dataset into AWS. For large data we need to use AWS Glue Jobs/Batch Jobs and in general the process is a bit more complicated and has less features.</p> <p>If you have smaller data please see DataLoaders Light</p> <p>Welcome to the Workbench DataLoaders Heavy Classes</p> <p>These classes provide low-level APIs for loading larger data into AWS services</p> <ul> <li>S3HeavyToDataSource: Loads large data from S3 into a DataSource</li> </ul>"},{"location":"core_classes/transforms/data_loaders_heavy/#workbench.core.transforms.data_loaders.heavy.S3HeavyToDataSource","title":"<code>S3HeavyToDataSource</code>","text":"Source code in <code>src/workbench/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>class S3HeavyToDataSource:\n    def __init__(self, glue_context: GlueContext, input_name: str, output_name: str):\n        \"\"\"S3HeavyToDataSource: Class to move HEAVY S3 Files into a Workbench DataSource\n\n        Args:\n            glue_context: GlueContext, AWS Glue Specific wrapper around SparkContext\n            input_name (str): The S3 Path to the files to be loaded\n            output_name (str): The Name of the Workbench DataSource to be created\n        \"\"\"\n        self.log = glue_context.get_logger()\n\n        # FIXME: Pull these from Parameter Store or Config\n        self.input_name = input_name\n        self.output_name = output_name\n        self.output_meta = {\"workbench_input\": self.input_name}\n        workbench_bucket = \"s3://sandbox-workbench-artifacts\"\n        self.data_sources_s3_path = workbench_bucket + \"/data-sources\"\n\n        # Our Spark Context\n        self.glue_context = glue_context\n\n    @staticmethod\n    def resolve_choice_fields(dyf):\n        # Get schema fields\n        schema_fields = dyf.schema().fields\n\n        # Collect choice fields\n        choice_fields = [(field.name, \"cast:long\") for field in schema_fields if field.dataType.typeName() == \"choice\"]\n        print(f\"Choice Fields: {choice_fields}\")\n\n        # If there are choice fields, resolve them\n        if choice_fields:\n            dyf = dyf.resolveChoice(specs=choice_fields)\n\n        return dyf\n\n    def timestamp_conversions(self, dyf: DynamicFrame, time_columns: list = []) -&gt; DynamicFrame:\n        \"\"\"Convert columns in the DynamicFrame to the correct data types\n        Args:\n            dyf (DynamicFrame): The DynamicFrame to convert\n            time_columns (list): A list of column names to convert to timestamp\n        Returns:\n            DynamicFrame: The converted DynamicFrame\n        \"\"\"\n\n        # Convert the timestamp columns to timestamp types\n        spark_df = dyf.toDF()\n        for column in time_columns:\n            spark_df = spark_df.withColumn(column, to_timestamp(col(column)))\n\n        # Convert the Spark DataFrame back to a Glue DynamicFrame and return\n        return DynamicFrame.fromDF(spark_df, self.glue_context, \"output_dyf\")\n\n    @staticmethod\n    def remove_periods_from_columns(dyf: DynamicFrame) -&gt; DynamicFrame:\n        \"\"\"Remove periods from column names in the DynamicFrame\n        Args:\n            dyf (DynamicFrame): The DynamicFrame to convert\n        Returns:\n            DynamicFrame: The converted DynamicFrame\n        \"\"\"\n        # Extract the column names from the schema\n        old_column_names = [field.name for field in dyf.schema().fields]\n\n        # Create a new list of renamed column names\n        new_column_names = [name.replace(\".\", \"_\") for name in old_column_names]\n        print(old_column_names)\n        print(new_column_names)\n\n        # Create a new DynamicFrame with renamed columns\n        for c_old, c_new in zip(old_column_names, new_column_names):\n            dyf = dyf.rename_field(f\"`{c_old}`\", c_new)\n        return dyf\n\n    def transform(\n        self,\n        input_type: str = \"json\",\n        timestamp_columns: list = None,\n        output_format: str = \"parquet\",\n    ):\n        \"\"\"Convert the CSV or JSON data into Parquet Format in the Workbench S3 Bucket, and\n        store the information about the data to the AWS Data Catalog workbench database\n        Args:\n            input_type (str): The type of input files, either 'csv' or 'json'\n            timestamp_columns (list): A list of column names to convert to timestamp\n            output_format (str): The format of the output files, either 'parquet' or 'orc'\n        \"\"\"\n\n        # Add some tags here\n        tags = [\"heavy\"]\n\n        # Create the Output Parquet file S3 Storage Path\n        s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_name}\"\n\n        # Read JSONL files from S3 and infer schema dynamically\n        self.log.info(f\"Reading JSONL files from {self.input_name}...\")\n        input_dyf = self.glue_context.create_dynamic_frame.from_options(\n            connection_type=\"s3\",\n            connection_options={\n                \"paths\": [self.input_name],\n                \"recurse\": True,\n                \"gzip\": True,\n            },\n            format=input_type,\n            # format_options={'jsonPath': 'auto'}, Look into this later\n        )\n        self.log.info(\"Incoming DataFrame...\")\n        input_dyf.show(5)\n        input_dyf.printSchema()\n\n        # Resolve Choice fields\n        resolved_dyf = self.resolve_choice_fields(input_dyf)\n\n        # The next couple of lines of code is for un-nesting any nested JSON\n        # Create a Dynamic Frame Collection (dfc)\n        dfc = Relationalize.apply(resolved_dyf, name=\"root\")\n\n        # Aggregate the collection into a single dynamic frame\n        output_dyf = dfc.select(\"root\")\n\n        print(\"Before TimeStamp Conversions\")\n        output_dyf.printSchema()\n\n        # Convert any timestamp columns\n        output_dyf = self.timestamp_conversions(output_dyf, timestamp_columns)\n\n        # Relationalize will put periods in the column names. This will cause\n        # problems later when we try to create a FeatureSet from this DataSource\n        output_dyf = self.remove_periods_from_columns(output_dyf)\n\n        print(\"After TimeStamp Conversions and Removing Periods from column names\")\n        output_dyf.printSchema()\n\n        # Write Parquet files to S3\n        self.log.info(f\"Writing Parquet files to {s3_storage_path}...\")\n        self.glue_context.purge_s3_path(s3_storage_path, {\"retentionPeriod\": 0})\n        self.glue_context.write_dynamic_frame.from_options(\n            frame=output_dyf,\n            connection_type=\"s3\",\n            connection_options={\n                \"path\": s3_storage_path\n                # \"partitionKeys\": [\"year\", \"month\", \"day\"],\n            },\n            format=output_format,\n        )\n\n        # Set up our Workbench metadata (description, tags, etc)\n        description = f\"Workbench data source: {self.output_name}\"\n        workbench_meta = {\"workbench_tags\": self.tag_delimiter.join(tags)}\n        for key, value in self.output_meta.items():\n            workbench_meta[key] = value\n\n        # Create a new table in the AWS Data Catalog\n        self.log.info(f\"Creating Data Catalog Table: {self.output_name}...\")\n\n        # Converting the Spark Types to Athena Types\n        def to_athena_type(col):\n            athena_type_map = {\"long\": \"bigint\"}\n            spark_type = col.dataType.typeName()\n            return athena_type_map.get(spark_type, spark_type)\n\n        column_name_types = [{\"Name\": col.name, \"Type\": to_athena_type(col)} for col in output_dyf.schema().fields]\n\n        # Our parameters for the Glue Data Catalog are different for Parquet and ORC\n        if output_format == \"parquet\":\n            glue_input_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"\n            glue_output_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"\n            serialization_library = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n        else:\n            glue_input_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n            glue_output_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n            serialization_library = \"org.apache.hadoop.hive.ql.io.orc.OrcSerde\"\n\n        table_input = {\n            \"Name\": self.output_name,\n            \"Description\": description,\n            \"Parameters\": workbench_meta,\n            \"TableType\": \"EXTERNAL_TABLE\",\n            \"StorageDescriptor\": {\n                \"Columns\": column_name_types,\n                \"Location\": s3_storage_path,\n                \"InputFormat\": glue_input_format,\n                \"OutputFormat\": glue_output_format,\n                \"Compressed\": True,\n                \"SerdeInfo\": {\n                    \"SerializationLibrary\": serialization_library,\n                },\n            },\n        }\n\n        # Delete the Data Catalog Table if it already exists\n        glue_client = boto3.client(\"glue\")\n        try:\n            glue_client.delete_table(DatabaseName=\"workbench\", Name=self.output_name)\n            self.log.info(f\"Deleting Data Catalog Table: {self.output_name}...\")\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] != \"EntityNotFoundException\":\n                raise e\n\n        self.log.info(f\"Creating Data Catalog Table: {self.output_name}...\")\n        glue_client.create_table(DatabaseName=\"workbench\", TableInput=table_input)\n\n        # All done!\n        self.log.info(f\"{self.input_name} --&gt; {self.output_name} complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_heavy/#workbench.core.transforms.data_loaders.heavy.S3HeavyToDataSource.__init__","title":"<code>__init__(glue_context, input_name, output_name)</code>","text":"<p>S3HeavyToDataSource: Class to move HEAVY S3 Files into a Workbench DataSource</p> <p>Parameters:</p> Name Type Description Default <code>glue_context</code> <code>GlueContext</code> <p>GlueContext, AWS Glue Specific wrapper around SparkContext</p> required <code>input_name</code> <code>str</code> <p>The S3 Path to the files to be loaded</p> required <code>output_name</code> <code>str</code> <p>The Name of the Workbench DataSource to be created</p> required Source code in <code>src/workbench/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def __init__(self, glue_context: GlueContext, input_name: str, output_name: str):\n    \"\"\"S3HeavyToDataSource: Class to move HEAVY S3 Files into a Workbench DataSource\n\n    Args:\n        glue_context: GlueContext, AWS Glue Specific wrapper around SparkContext\n        input_name (str): The S3 Path to the files to be loaded\n        output_name (str): The Name of the Workbench DataSource to be created\n    \"\"\"\n    self.log = glue_context.get_logger()\n\n    # FIXME: Pull these from Parameter Store or Config\n    self.input_name = input_name\n    self.output_name = output_name\n    self.output_meta = {\"workbench_input\": self.input_name}\n    workbench_bucket = \"s3://sandbox-workbench-artifacts\"\n    self.data_sources_s3_path = workbench_bucket + \"/data-sources\"\n\n    # Our Spark Context\n    self.glue_context = glue_context\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_heavy/#workbench.core.transforms.data_loaders.heavy.S3HeavyToDataSource.remove_periods_from_columns","title":"<code>remove_periods_from_columns(dyf)</code>  <code>staticmethod</code>","text":"<p>Remove periods from column names in the DynamicFrame Args:     dyf (DynamicFrame): The DynamicFrame to convert Returns:     DynamicFrame: The converted DynamicFrame</p> Source code in <code>src/workbench/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>@staticmethod\ndef remove_periods_from_columns(dyf: DynamicFrame) -&gt; DynamicFrame:\n    \"\"\"Remove periods from column names in the DynamicFrame\n    Args:\n        dyf (DynamicFrame): The DynamicFrame to convert\n    Returns:\n        DynamicFrame: The converted DynamicFrame\n    \"\"\"\n    # Extract the column names from the schema\n    old_column_names = [field.name for field in dyf.schema().fields]\n\n    # Create a new list of renamed column names\n    new_column_names = [name.replace(\".\", \"_\") for name in old_column_names]\n    print(old_column_names)\n    print(new_column_names)\n\n    # Create a new DynamicFrame with renamed columns\n    for c_old, c_new in zip(old_column_names, new_column_names):\n        dyf = dyf.rename_field(f\"`{c_old}`\", c_new)\n    return dyf\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_heavy/#workbench.core.transforms.data_loaders.heavy.S3HeavyToDataSource.timestamp_conversions","title":"<code>timestamp_conversions(dyf, time_columns=[])</code>","text":"<p>Convert columns in the DynamicFrame to the correct data types Args:     dyf (DynamicFrame): The DynamicFrame to convert     time_columns (list): A list of column names to convert to timestamp Returns:     DynamicFrame: The converted DynamicFrame</p> Source code in <code>src/workbench/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def timestamp_conversions(self, dyf: DynamicFrame, time_columns: list = []) -&gt; DynamicFrame:\n    \"\"\"Convert columns in the DynamicFrame to the correct data types\n    Args:\n        dyf (DynamicFrame): The DynamicFrame to convert\n        time_columns (list): A list of column names to convert to timestamp\n    Returns:\n        DynamicFrame: The converted DynamicFrame\n    \"\"\"\n\n    # Convert the timestamp columns to timestamp types\n    spark_df = dyf.toDF()\n    for column in time_columns:\n        spark_df = spark_df.withColumn(column, to_timestamp(col(column)))\n\n    # Convert the Spark DataFrame back to a Glue DynamicFrame and return\n    return DynamicFrame.fromDF(spark_df, self.glue_context, \"output_dyf\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_heavy/#workbench.core.transforms.data_loaders.heavy.S3HeavyToDataSource.transform","title":"<code>transform(input_type='json', timestamp_columns=None, output_format='parquet')</code>","text":"<p>Convert the CSV or JSON data into Parquet Format in the Workbench S3 Bucket, and store the information about the data to the AWS Data Catalog workbench database Args:     input_type (str): The type of input files, either 'csv' or 'json'     timestamp_columns (list): A list of column names to convert to timestamp     output_format (str): The format of the output files, either 'parquet' or 'orc'</p> Source code in <code>src/workbench/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def transform(\n    self,\n    input_type: str = \"json\",\n    timestamp_columns: list = None,\n    output_format: str = \"parquet\",\n):\n    \"\"\"Convert the CSV or JSON data into Parquet Format in the Workbench S3 Bucket, and\n    store the information about the data to the AWS Data Catalog workbench database\n    Args:\n        input_type (str): The type of input files, either 'csv' or 'json'\n        timestamp_columns (list): A list of column names to convert to timestamp\n        output_format (str): The format of the output files, either 'parquet' or 'orc'\n    \"\"\"\n\n    # Add some tags here\n    tags = [\"heavy\"]\n\n    # Create the Output Parquet file S3 Storage Path\n    s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_name}\"\n\n    # Read JSONL files from S3 and infer schema dynamically\n    self.log.info(f\"Reading JSONL files from {self.input_name}...\")\n    input_dyf = self.glue_context.create_dynamic_frame.from_options(\n        connection_type=\"s3\",\n        connection_options={\n            \"paths\": [self.input_name],\n            \"recurse\": True,\n            \"gzip\": True,\n        },\n        format=input_type,\n        # format_options={'jsonPath': 'auto'}, Look into this later\n    )\n    self.log.info(\"Incoming DataFrame...\")\n    input_dyf.show(5)\n    input_dyf.printSchema()\n\n    # Resolve Choice fields\n    resolved_dyf = self.resolve_choice_fields(input_dyf)\n\n    # The next couple of lines of code is for un-nesting any nested JSON\n    # Create a Dynamic Frame Collection (dfc)\n    dfc = Relationalize.apply(resolved_dyf, name=\"root\")\n\n    # Aggregate the collection into a single dynamic frame\n    output_dyf = dfc.select(\"root\")\n\n    print(\"Before TimeStamp Conversions\")\n    output_dyf.printSchema()\n\n    # Convert any timestamp columns\n    output_dyf = self.timestamp_conversions(output_dyf, timestamp_columns)\n\n    # Relationalize will put periods in the column names. This will cause\n    # problems later when we try to create a FeatureSet from this DataSource\n    output_dyf = self.remove_periods_from_columns(output_dyf)\n\n    print(\"After TimeStamp Conversions and Removing Periods from column names\")\n    output_dyf.printSchema()\n\n    # Write Parquet files to S3\n    self.log.info(f\"Writing Parquet files to {s3_storage_path}...\")\n    self.glue_context.purge_s3_path(s3_storage_path, {\"retentionPeriod\": 0})\n    self.glue_context.write_dynamic_frame.from_options(\n        frame=output_dyf,\n        connection_type=\"s3\",\n        connection_options={\n            \"path\": s3_storage_path\n            # \"partitionKeys\": [\"year\", \"month\", \"day\"],\n        },\n        format=output_format,\n    )\n\n    # Set up our Workbench metadata (description, tags, etc)\n    description = f\"Workbench data source: {self.output_name}\"\n    workbench_meta = {\"workbench_tags\": self.tag_delimiter.join(tags)}\n    for key, value in self.output_meta.items():\n        workbench_meta[key] = value\n\n    # Create a new table in the AWS Data Catalog\n    self.log.info(f\"Creating Data Catalog Table: {self.output_name}...\")\n\n    # Converting the Spark Types to Athena Types\n    def to_athena_type(col):\n        athena_type_map = {\"long\": \"bigint\"}\n        spark_type = col.dataType.typeName()\n        return athena_type_map.get(spark_type, spark_type)\n\n    column_name_types = [{\"Name\": col.name, \"Type\": to_athena_type(col)} for col in output_dyf.schema().fields]\n\n    # Our parameters for the Glue Data Catalog are different for Parquet and ORC\n    if output_format == \"parquet\":\n        glue_input_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"\n        glue_output_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"\n        serialization_library = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n    else:\n        glue_input_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n        glue_output_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n        serialization_library = \"org.apache.hadoop.hive.ql.io.orc.OrcSerde\"\n\n    table_input = {\n        \"Name\": self.output_name,\n        \"Description\": description,\n        \"Parameters\": workbench_meta,\n        \"TableType\": \"EXTERNAL_TABLE\",\n        \"StorageDescriptor\": {\n            \"Columns\": column_name_types,\n            \"Location\": s3_storage_path,\n            \"InputFormat\": glue_input_format,\n            \"OutputFormat\": glue_output_format,\n            \"Compressed\": True,\n            \"SerdeInfo\": {\n                \"SerializationLibrary\": serialization_library,\n            },\n        },\n    }\n\n    # Delete the Data Catalog Table if it already exists\n    glue_client = boto3.client(\"glue\")\n    try:\n        glue_client.delete_table(DatabaseName=\"workbench\", Name=self.output_name)\n        self.log.info(f\"Deleting Data Catalog Table: {self.output_name}...\")\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] != \"EntityNotFoundException\":\n            raise e\n\n    self.log.info(f\"Creating Data Catalog Table: {self.output_name}...\")\n    glue_client.create_table(DatabaseName=\"workbench\", TableInput=table_input)\n\n    # All done!\n    self.log.info(f\"{self.input_name} --&gt; {self.output_name} complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/","title":"DataLoaders Light","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>These DataLoader Classes are intended to load smaller dataset into AWS. If you have large data please see DataLoaders Heavy</p> <p>Welcome to the Workbench DataLoaders Light Classes</p> <p>These classes provide low-level APIs for loading smaller data into AWS services</p> <ul> <li>CSVToDataSource: Loads local CSV data into a DataSource</li> <li>JSONToDataSource: Loads local JSON data into a DataSource</li> <li>S3ToDataSourceLight: Loads S3 data into a DataSource</li> </ul>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.CSVToDataSource","title":"<code>CSVToDataSource</code>","text":"<p>               Bases: <code>Transform</code></p> <p>CSVToDataSource: Class to move local CSV Files into a Workbench DataSource</p> Common Usage <pre><code>csv_to_data = CSVToDataSource(csv_file_path, data_name)\ncsv_to_data.set_output_tags([\"abalone\", \"csv\", \"whatever\"])\ncsv_to_data.transform()\n</code></pre> Source code in <code>src/workbench/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>class CSVToDataSource(Transform):\n    \"\"\"CSVToDataSource: Class to move local CSV Files into a Workbench DataSource\n\n    Common Usage:\n        ```python\n        csv_to_data = CSVToDataSource(csv_file_path, data_name)\n        csv_to_data.set_output_tags([\"abalone\", \"csv\", \"whatever\"])\n        csv_to_data.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, csv_file_path: str, data_name: str):\n        \"\"\"CSVToDataSource: Class to move local CSV Files into a Workbench DataSource\n\n        Args:\n            csv_file_path (str): The path to the CSV file to be transformed\n            data_name (str): The Name of the Workbench DataSource to be created\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(csv_file_path, data_name)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.LOCAL_FILE\n        self.output_type = TransformOutput.DATA_SOURCE\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the local CSV file into Parquet Format in the Workbench Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog workbench database\n        \"\"\"\n\n        # Report the transformation initiation\n        csv_file = os.path.basename(self.input_name)\n        self.log.info(f\"Starting {csv_file} --&gt;  DataSource: {self.output_name}...\")\n\n        # Read in the Local CSV as a Pandas DataFrame\n        df = pd.read_csv(self.input_name, low_memory=False)\n        df = convert_object_columns(df)\n\n        # Use the Workbench Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_name)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{csv_file} --&gt;  DataSource: {self.output_name} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform\"\"\"\n        self.log.info(\"Post-Transform: S3 to DataSource...\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.CSVToDataSource.__init__","title":"<code>__init__(csv_file_path, data_name)</code>","text":"<p>CSVToDataSource: Class to move local CSV Files into a Workbench DataSource</p> <p>Parameters:</p> Name Type Description Default <code>csv_file_path</code> <code>str</code> <p>The path to the CSV file to be transformed</p> required <code>data_name</code> <code>str</code> <p>The Name of the Workbench DataSource to be created</p> required Source code in <code>src/workbench/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def __init__(self, csv_file_path: str, data_name: str):\n    \"\"\"CSVToDataSource: Class to move local CSV Files into a Workbench DataSource\n\n    Args:\n        csv_file_path (str): The path to the CSV file to be transformed\n        data_name (str): The Name of the Workbench DataSource to be created\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(csv_file_path, data_name)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.LOCAL_FILE\n    self.output_type = TransformOutput.DATA_SOURCE\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.CSVToDataSource.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform</p> Source code in <code>src/workbench/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform\"\"\"\n    self.log.info(\"Post-Transform: S3 to DataSource...\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.CSVToDataSource.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the local CSV file into Parquet Format in the Workbench Data Sources Bucket, and store the information about the data to the AWS Data Catalog workbench database</p> Source code in <code>src/workbench/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the local CSV file into Parquet Format in the Workbench Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog workbench database\n    \"\"\"\n\n    # Report the transformation initiation\n    csv_file = os.path.basename(self.input_name)\n    self.log.info(f\"Starting {csv_file} --&gt;  DataSource: {self.output_name}...\")\n\n    # Read in the Local CSV as a Pandas DataFrame\n    df = pd.read_csv(self.input_name, low_memory=False)\n    df = convert_object_columns(df)\n\n    # Use the Workbench Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_name)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{csv_file} --&gt;  DataSource: {self.output_name} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.JSONToDataSource","title":"<code>JSONToDataSource</code>","text":"<p>               Bases: <code>Transform</code></p> <p>JSONToDataSource: Class to move local JSON Files into a Workbench DataSource</p> Common Usage <pre><code>json_to_data = JSONToDataSource(json_file_path, data_name)\njson_to_data.set_output_tags([\"abalone\", \"json\", \"whatever\"])\njson_to_data.transform()\n</code></pre> Source code in <code>src/workbench/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>class JSONToDataSource(Transform):\n    \"\"\"JSONToDataSource: Class to move local JSON Files into a Workbench DataSource\n\n    Common Usage:\n        ```python\n        json_to_data = JSONToDataSource(json_file_path, data_name)\n        json_to_data.set_output_tags([\"abalone\", \"json\", \"whatever\"])\n        json_to_data.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, json_file_path: str, data_name: str):\n        \"\"\"JSONToDataSource: Class to move local JSON Files into a Workbench DataSource\n\n        Args:\n            json_file_path (str): The path to the JSON file to be transformed\n            data_name (str): The Name of the Workbench DataSource to be created\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(json_file_path, data_name)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.LOCAL_FILE\n        self.output_type = TransformOutput.DATA_SOURCE\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the local JSON file into Parquet Format in the Workbench Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog workbench database\n        \"\"\"\n\n        # Report the transformation initiation\n        json_file = os.path.basename(self.input_name)\n        self.log.info(f\"Starting {json_file} --&gt;  DataSource: {self.output_name}...\")\n\n        # Read in the Local JSON as a Pandas DataFrame\n        df = pd.read_json(self.input_name, lines=True)\n\n        # Use the Workbench Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_name)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{json_file} --&gt;  DataSource: {self.output_name} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform\"\"\"\n        self.log.info(\"Post-Transform: S3 to DataSource...\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.JSONToDataSource.__init__","title":"<code>__init__(json_file_path, data_name)</code>","text":"<p>JSONToDataSource: Class to move local JSON Files into a Workbench DataSource</p> <p>Parameters:</p> Name Type Description Default <code>json_file_path</code> <code>str</code> <p>The path to the JSON file to be transformed</p> required <code>data_name</code> <code>str</code> <p>The Name of the Workbench DataSource to be created</p> required Source code in <code>src/workbench/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def __init__(self, json_file_path: str, data_name: str):\n    \"\"\"JSONToDataSource: Class to move local JSON Files into a Workbench DataSource\n\n    Args:\n        json_file_path (str): The path to the JSON file to be transformed\n        data_name (str): The Name of the Workbench DataSource to be created\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(json_file_path, data_name)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.LOCAL_FILE\n    self.output_type = TransformOutput.DATA_SOURCE\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.JSONToDataSource.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform</p> Source code in <code>src/workbench/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform\"\"\"\n    self.log.info(\"Post-Transform: S3 to DataSource...\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.JSONToDataSource.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the local JSON file into Parquet Format in the Workbench Data Sources Bucket, and store the information about the data to the AWS Data Catalog workbench database</p> Source code in <code>src/workbench/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the local JSON file into Parquet Format in the Workbench Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog workbench database\n    \"\"\"\n\n    # Report the transformation initiation\n    json_file = os.path.basename(self.input_name)\n    self.log.info(f\"Starting {json_file} --&gt;  DataSource: {self.output_name}...\")\n\n    # Read in the Local JSON as a Pandas DataFrame\n    df = pd.read_json(self.input_name, lines=True)\n\n    # Use the Workbench Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_name)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{json_file} --&gt;  DataSource: {self.output_name} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.S3ToDataSourceLight","title":"<code>S3ToDataSourceLight</code>","text":"<p>               Bases: <code>Transform</code></p> <p>S3ToDataSourceLight: Class to move LIGHT S3 Files into a Workbench DataSource</p> Common Usage <pre><code>s3_to_data = S3ToDataSourceLight(s3_path, data_name, datatype=\"csv/json\")\ns3_to_data.set_output_tags([\"abalone\", \"whatever\"])\ns3_to_data.transform()\n</code></pre> Source code in <code>src/workbench/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>class S3ToDataSourceLight(Transform):\n    \"\"\"S3ToDataSourceLight: Class to move LIGHT S3 Files into a Workbench DataSource\n\n    Common Usage:\n        ```python\n        s3_to_data = S3ToDataSourceLight(s3_path, data_name, datatype=\"csv/json\")\n        s3_to_data.set_output_tags([\"abalone\", \"whatever\"])\n        s3_to_data.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, s3_path: str, data_name: str, datatype: str = \"csv\"):\n        \"\"\"S3ToDataSourceLight Initialization\n\n        Args:\n            s3_path (str): The S3 Path to the file to be transformed\n            data_name (str): The Name of the Workbench DataSource to be created\n            datatype (str): The datatype of the file to be transformed (defaults to \"csv\")\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(s3_path, data_name)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.S3_OBJECT\n        self.output_type = TransformOutput.DATA_SOURCE\n        self.datatype = datatype\n\n    def input_size_mb(self) -&gt; int:\n        \"\"\"Get the size of the input S3 object in MBytes\"\"\"\n        size_in_bytes = wr.s3.size_objects(self.input_name, boto3_session=self.boto3_session)[self.input_name]\n        size_in_mb = round(size_in_bytes / 1_000_000)\n        return size_in_mb\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the S3 CSV data into Parquet Format in the Workbench Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog workbench database\n        \"\"\"\n\n        # Sanity Check for S3 Object size\n        object_megabytes = self.input_size_mb()\n        if object_megabytes &gt; 100:\n            self.log.error(f\"S3 Object too big ({object_megabytes} MBytes): Use the S3ToDataSourceHeavy class!\")\n            return\n\n        # Read in the S3 CSV as a Pandas DataFrame\n        if self.datatype == \"csv\":\n            df = wr.s3.read_csv(self.input_name, low_memory=False, boto3_session=self.boto3_session)\n        else:\n            df = wr.s3.read_json(self.input_name, lines=True, boto3_session=self.boto3_session)\n\n        # Temporary hack to limit the number of columns in the dataframe\n        if len(df.columns) &gt; 40:\n            self.log.warning(f\"{self.input_name} Too Many Columns! Talk to Workbench Support...\")\n\n        # Convert object columns before sending to Workbench Data Source\n        df = convert_object_columns(df)\n\n        # Use the Workbench Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_name)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{self.input_name} --&gt;  DataSource: {self.output_name} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform\"\"\"\n        self.log.info(\"Post-Transform: S3 to DataSource...\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.S3ToDataSourceLight.__init__","title":"<code>__init__(s3_path, data_name, datatype='csv')</code>","text":"<p>S3ToDataSourceLight Initialization</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 Path to the file to be transformed</p> required <code>data_name</code> <code>str</code> <p>The Name of the Workbench DataSource to be created</p> required <code>datatype</code> <code>str</code> <p>The datatype of the file to be transformed (defaults to \"csv\")</p> <code>'csv'</code> Source code in <code>src/workbench/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def __init__(self, s3_path: str, data_name: str, datatype: str = \"csv\"):\n    \"\"\"S3ToDataSourceLight Initialization\n\n    Args:\n        s3_path (str): The S3 Path to the file to be transformed\n        data_name (str): The Name of the Workbench DataSource to be created\n        datatype (str): The datatype of the file to be transformed (defaults to \"csv\")\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(s3_path, data_name)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.S3_OBJECT\n    self.output_type = TransformOutput.DATA_SOURCE\n    self.datatype = datatype\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.S3ToDataSourceLight.input_size_mb","title":"<code>input_size_mb()</code>","text":"<p>Get the size of the input S3 object in MBytes</p> Source code in <code>src/workbench/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def input_size_mb(self) -&gt; int:\n    \"\"\"Get the size of the input S3 object in MBytes\"\"\"\n    size_in_bytes = wr.s3.size_objects(self.input_name, boto3_session=self.boto3_session)[self.input_name]\n    size_in_mb = round(size_in_bytes / 1_000_000)\n    return size_in_mb\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.S3ToDataSourceLight.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform</p> Source code in <code>src/workbench/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform\"\"\"\n    self.log.info(\"Post-Transform: S3 to DataSource...\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#workbench.core.transforms.data_loaders.light.S3ToDataSourceLight.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the S3 CSV data into Parquet Format in the Workbench Data Sources Bucket, and store the information about the data to the AWS Data Catalog workbench database</p> Source code in <code>src/workbench/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the S3 CSV data into Parquet Format in the Workbench Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog workbench database\n    \"\"\"\n\n    # Sanity Check for S3 Object size\n    object_megabytes = self.input_size_mb()\n    if object_megabytes &gt; 100:\n        self.log.error(f\"S3 Object too big ({object_megabytes} MBytes): Use the S3ToDataSourceHeavy class!\")\n        return\n\n    # Read in the S3 CSV as a Pandas DataFrame\n    if self.datatype == \"csv\":\n        df = wr.s3.read_csv(self.input_name, low_memory=False, boto3_session=self.boto3_session)\n    else:\n        df = wr.s3.read_json(self.input_name, lines=True, boto3_session=self.boto3_session)\n\n    # Temporary hack to limit the number of columns in the dataframe\n    if len(df.columns) &gt; 40:\n        self.log.warning(f\"{self.input_name} Too Many Columns! Talk to Workbench Support...\")\n\n    # Convert object columns before sending to Workbench Data Source\n    df = convert_object_columns(df)\n\n    # Use the Workbench Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_name)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{self.input_name} --&gt;  DataSource: {self.output_name} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/","title":"Data To Features","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas</p> <p>MolecularDescriptors: Compute a Feature Set based on RDKit Descriptors</p> An alternative to using this class is to use the <code>compute_descriptors</code> function directly. <p>df_features = compute_descriptors(df) to_features = PandasToFeatures(\"my_feature_set\")    to_features.set_input(df_features, id_column=\"id\")    to_features.set_output_tags([\"blah\", \"whatever\"])    to_features.transform()</p>"},{"location":"core_classes/transforms/data_to_features/#workbench.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight","title":"<code>DataToFeaturesLight</code>","text":"<p>               Bases: <code>Transform</code></p> <p>DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas</p> Common Usage <pre><code>to_features = DataToFeaturesLight(data_name, feature_name)\nto_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\nto_features.transform(id_column=\"id\"/None, event_time_column=\"date\"/None, query=str/None)\n</code></pre> Source code in <code>src/workbench/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>class DataToFeaturesLight(Transform):\n    \"\"\"DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas\n\n    Common Usage:\n        ```python\n        to_features = DataToFeaturesLight(data_name, feature_name)\n        to_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\n        to_features.transform(id_column=\"id\"/None, event_time_column=\"date\"/None, query=str/None)\n        ```\n    \"\"\"\n\n    def __init__(self, data_name: str, feature_name: str):\n        \"\"\"DataToFeaturesLight Initialization\n\n        Args:\n            data_name (str): The Name of the Workbench DataSource to be transformed\n            feature_name (str): The Name of the Workbench FeatureSet to be created\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(data_name, feature_name)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.DATA_SOURCE\n        self.output_type = TransformOutput.FEATURE_SET\n        self.input_df = None\n        self.output_df = None\n\n    def pre_transform(self, query: str = None, **kwargs):\n        \"\"\"Pull the input DataSource into our Input Pandas DataFrame\n        Args:\n            query(str): Optional query to filter the input DataFrame\n        \"\"\"\n\n        # Grab the Input (Data Source)\n        data_to_pandas = DataToPandas(self.input_name)\n        data_to_pandas.transform(query=query)\n        self.input_df = data_to_pandas.get_output()\n\n        # Check if there are any columns that are greater than 64 characters\n        for col in self.input_df.columns:\n            if len(col) &gt; 64:\n                raise ValueError(f\"Column name '{col}' &gt; 64 characters. AWS FeatureGroup limits to 64 characters.\")\n\n    def transform_impl(self, **kwargs):\n        \"\"\"Transform the input DataFrame into a Feature Set\"\"\"\n\n        # This is a reference implementation that should be overridden by the subclass\n        self.output_df = self.input_df\n\n    def post_transform(self, id_column, event_time_column=None, one_hot_columns=None, **kwargs):\n        \"\"\"At this point the output DataFrame should be populated, so publish it as a Feature Set\n\n        Args:\n            id_column (str): The ID column (must be specified, use \"auto\" for auto-generated IDs).\n            event_time_column (str, optional): The name of the event time column (default: None).\n            one_hot_columns (list, optional): The list of columns to one-hot encode (default: None).\n        \"\"\"\n        # Now publish to the output location\n        output_features = PandasToFeatures(self.output_name)\n        output_features.set_input(\n            self.output_df, id_column=id_column, event_time_column=event_time_column, one_hot_columns=one_hot_columns\n        )\n        output_features.set_output_tags(self.output_tags)\n        output_features.add_output_meta(self.output_meta)\n        output_features.transform()\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#workbench.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.__init__","title":"<code>__init__(data_name, feature_name)</code>","text":"<p>DataToFeaturesLight Initialization</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>The Name of the Workbench DataSource to be transformed</p> required <code>feature_name</code> <code>str</code> <p>The Name of the Workbench FeatureSet to be created</p> required Source code in <code>src/workbench/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def __init__(self, data_name: str, feature_name: str):\n    \"\"\"DataToFeaturesLight Initialization\n\n    Args:\n        data_name (str): The Name of the Workbench DataSource to be transformed\n        feature_name (str): The Name of the Workbench FeatureSet to be created\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(data_name, feature_name)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.DATA_SOURCE\n    self.output_type = TransformOutput.FEATURE_SET\n    self.input_df = None\n    self.output_df = None\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#workbench.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.post_transform","title":"<code>post_transform(id_column, event_time_column=None, one_hot_columns=None, **kwargs)</code>","text":"<p>At this point the output DataFrame should be populated, so publish it as a Feature Set</p> <p>Parameters:</p> Name Type Description Default <code>id_column</code> <code>str</code> <p>The ID column (must be specified, use \"auto\" for auto-generated IDs).</p> required <code>event_time_column</code> <code>str</code> <p>The name of the event time column (default: None).</p> <code>None</code> <code>one_hot_columns</code> <code>list</code> <p>The list of columns to one-hot encode (default: None).</p> <code>None</code> Source code in <code>src/workbench/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def post_transform(self, id_column, event_time_column=None, one_hot_columns=None, **kwargs):\n    \"\"\"At this point the output DataFrame should be populated, so publish it as a Feature Set\n\n    Args:\n        id_column (str): The ID column (must be specified, use \"auto\" for auto-generated IDs).\n        event_time_column (str, optional): The name of the event time column (default: None).\n        one_hot_columns (list, optional): The list of columns to one-hot encode (default: None).\n    \"\"\"\n    # Now publish to the output location\n    output_features = PandasToFeatures(self.output_name)\n    output_features.set_input(\n        self.output_df, id_column=id_column, event_time_column=event_time_column, one_hot_columns=one_hot_columns\n    )\n    output_features.set_output_tags(self.output_tags)\n    output_features.add_output_meta(self.output_meta)\n    output_features.transform()\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#workbench.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.pre_transform","title":"<code>pre_transform(query=None, **kwargs)</code>","text":"<p>Pull the input DataSource into our Input Pandas DataFrame Args:     query(str): Optional query to filter the input DataFrame</p> Source code in <code>src/workbench/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def pre_transform(self, query: str = None, **kwargs):\n    \"\"\"Pull the input DataSource into our Input Pandas DataFrame\n    Args:\n        query(str): Optional query to filter the input DataFrame\n    \"\"\"\n\n    # Grab the Input (Data Source)\n    data_to_pandas = DataToPandas(self.input_name)\n    data_to_pandas.transform(query=query)\n    self.input_df = data_to_pandas.get_output()\n\n    # Check if there are any columns that are greater than 64 characters\n    for col in self.input_df.columns:\n        if len(col) &gt; 64:\n            raise ValueError(f\"Column name '{col}' &gt; 64 characters. AWS FeatureGroup limits to 64 characters.\")\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#workbench.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.transform_impl","title":"<code>transform_impl(**kwargs)</code>","text":"<p>Transform the input DataFrame into a Feature Set</p> Source code in <code>src/workbench/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def transform_impl(self, **kwargs):\n    \"\"\"Transform the input DataFrame into a Feature Set\"\"\"\n\n    # This is a reference implementation that should be overridden by the subclass\n    self.output_df = self.input_df\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#workbench.core.transforms.data_to_features.light.molecular_descriptors.MolecularDescriptors","title":"<code>MolecularDescriptors</code>","text":"<p>               Bases: <code>DataToFeaturesLight</code></p> <p>MolecularDescriptors: Create a FeatureSet (RDKit Descriptors) from a DataSource</p> Common Usage <pre><code>to_features = MolecularDescriptors(data_name, feature_name)\nto_features.set_output_tags([\"aqsol\", \"whatever\"])\nto_features.transform()\n</code></pre> Source code in <code>src/workbench/core/transforms/data_to_features/light/molecular_descriptors.py</code> <pre><code>class MolecularDescriptors(DataToFeaturesLight):\n    \"\"\"MolecularDescriptors: Create a FeatureSet (RDKit Descriptors) from a DataSource\n\n    Common Usage:\n        ```python\n        to_features = MolecularDescriptors(data_name, feature_name)\n        to_features.set_output_tags([\"aqsol\", \"whatever\"])\n        to_features.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, data_name: str, feature_name: str):\n        \"\"\"MolecularDescriptors Initialization\n\n        Args:\n            data_name (str): The Name of the Workbench DataSource to be transformed\n            feature_name (str): The Name of the Workbench FeatureSet to be created\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(data_name, feature_name)\n\n    def transform_impl(self, **kwargs):\n        \"\"\"Compute a Feature Set based on RDKit Descriptors\"\"\"\n\n        # Compute/add all the Molecular Descriptors\n        self.output_df = compute_descriptors(self.input_df)\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#workbench.core.transforms.data_to_features.light.molecular_descriptors.MolecularDescriptors.__init__","title":"<code>__init__(data_name, feature_name)</code>","text":"<p>MolecularDescriptors Initialization</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>The Name of the Workbench DataSource to be transformed</p> required <code>feature_name</code> <code>str</code> <p>The Name of the Workbench FeatureSet to be created</p> required Source code in <code>src/workbench/core/transforms/data_to_features/light/molecular_descriptors.py</code> <pre><code>def __init__(self, data_name: str, feature_name: str):\n    \"\"\"MolecularDescriptors Initialization\n\n    Args:\n        data_name (str): The Name of the Workbench DataSource to be transformed\n        feature_name (str): The Name of the Workbench FeatureSet to be created\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(data_name, feature_name)\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#workbench.core.transforms.data_to_features.light.molecular_descriptors.MolecularDescriptors.transform_impl","title":"<code>transform_impl(**kwargs)</code>","text":"<p>Compute a Feature Set based on RDKit Descriptors</p> Source code in <code>src/workbench/core/transforms/data_to_features/light/molecular_descriptors.py</code> <pre><code>def transform_impl(self, **kwargs):\n    \"\"\"Compute a Feature Set based on RDKit Descriptors\"\"\"\n\n    # Compute/add all the Molecular Descriptors\n    self.output_df = compute_descriptors(self.input_df)\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/","title":"Features To Model","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>FeaturesToModel: Train/Create a Model from a Feature Set</p>"},{"location":"core_classes/transforms/features_to_model/#workbench.core.transforms.features_to_model.features_to_model.FeaturesToModel","title":"<code>FeaturesToModel</code>","text":"<p>               Bases: <code>Transform</code></p> <p>FeaturesToModel: Train/Create a Model from a FeatureSet</p> Common Usage <pre><code>from workbench.core.transforms.features_to_model.features_to_model import FeaturesToModel\nto_model = FeaturesToModel(feature_name, model_name, model_type=ModelType)\nto_model.set_output_tags([\"abalone\", \"public\", \"whatever\"])\nto_model.transform(target_column=\"class_number_of_rings\",\n                   feature_list=[\"my\", \"best\", \"features\"])\n</code></pre> Source code in <code>src/workbench/core/transforms/features_to_model/features_to_model.py</code> <pre><code>class FeaturesToModel(Transform):\n    \"\"\"FeaturesToModel: Train/Create a Model from a FeatureSet\n\n    Common Usage:\n        ```python\n        from workbench.core.transforms.features_to_model.features_to_model import FeaturesToModel\n        to_model = FeaturesToModel(feature_name, model_name, model_type=ModelType)\n        to_model.set_output_tags([\"abalone\", \"public\", \"whatever\"])\n        to_model.transform(target_column=\"class_number_of_rings\",\n                           feature_list=[\"my\", \"best\", \"features\"])\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_name: str,\n        model_name: str,\n        model_type: ModelType,\n        model_framework=ModelFramework.XGBOOST,\n        model_class=None,\n        model_import_str=None,\n        custom_script=None,\n        custom_args=None,\n        training_image=\"training\",\n        inference_image=\"inference\",\n        inference_arch=\"x86_64\",\n    ):\n        \"\"\"FeaturesToModel Initialization\n        Args:\n            feature_name (str): Name of the FeatureSet to use as input\n            model_name (str): Name of the Model to create as output\n            model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER, etc.\n            model_framework (ModelFramework, optional): The model framework (default ModelFramework.XGBOOST)\n            model_class (str, optional): The scikit model (e.g. KNeighborsRegressor) (default None)\n            model_import_str (str, optional): The import string for the model (default None)\n            custom_script (str, optional): Custom script to use for the model (default None)\n            custom_args (dict, optional): Custom arguments to pass to custom model scripts (default None)\n            training_image (str, optional): Training image (default \"training\")\n            inference_image (str, optional): Inference image (default \"inference\")\n            inference_arch (str, optional): Inference architecture (default \"x86_64\")\n        \"\"\"\n\n        # Make sure the model_name is a valid name\n        Artifact.is_name_valid(model_name, delimiter=\"-\", lower_case=False)\n\n        # Call superclass init\n        super().__init__(feature_name, model_name)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.FEATURE_SET\n        self.output_type = TransformOutput.MODEL\n        self.model_type = model_type\n        self.model_framework = model_framework\n        self.model_class = model_class\n        self.model_import_str = model_import_str\n        self.custom_script = str(custom_script) if custom_script else None\n        self.custom_args = custom_args if custom_args else {}\n        self.estimator = None\n        self.model_description = None\n        self.model_training_root = f\"{self.models_s3_path}/{self.output_name}/training\"\n        self.log.important(f\"FeaturesToModel.__init__: model_training_root = {self.model_training_root}\")\n        self.model_feature_list = None\n        self.target_column = None\n        self.class_labels = None\n        self.training_image = training_image\n        self.inference_image = inference_image\n        self.inference_arch = inference_arch\n\n    def transform_impl(\n        self,\n        target_column: Union[str, list[str]],\n        description: str = None,\n        feature_list: list = None,\n        train_all_data=False,\n        **kwargs,\n    ):\n        \"\"\"Generic Features to Model: Note you should create a new class and inherit from\n        this one to include specific logic for your Feature Set/Model\n        Args:\n            target_column (str or list[str]): Column name(s) of the target variable(s)\n            description (str): Description of the model (optional)\n            feature_list (list[str]): A list of columns for the features (default None, will try to guess)\n            train_all_data (bool): Train on ALL (100%) of the data (default False)\n        \"\"\"\n        # Delete the existing model (if it exists)\n        self.log.important(f\"Trying to delete existing model {self.output_name}...\")\n        ModelCore.managed_delete(self.output_name)\n\n        # Set our model description\n        self.model_description = description if description is not None else f\"Model created from {self.input_name}\"\n\n        # Get our Feature Set and create an S3 CSV Training dataset\n        feature_set = FeatureSetCore(self.input_name)\n        s3_training_path = feature_set.create_s3_training_data()\n        self.log.info(f\"Created new training data {s3_training_path}...\")\n\n        # Report the target column(s)\n        self.target_column = target_column\n        # Normalize target_column to a list for internal use\n        target_list = [target_column] if isinstance(target_column, str) else (target_column or [])\n        self.log.info(f\"Target column(s): {self.target_column}\")\n\n        # Did they specify a feature list?\n        if feature_list:\n            # AWS Feature Groups will also add these implicit columns, so remove them\n            aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\", \"training\"]\n            feature_list = [c for c in feature_list if c not in aws_cols]\n\n        # If they didn't specify a feature list, try to guess it\n        else:\n            # Try to figure out features with this logic\n            # - Don't include id, event_time, __index_level_0__, or training columns\n            # - Don't include AWS generated columns (e.g. write_time, api_invocation_time, is_deleted)\n            # - Don't include the target columns\n            # - Don't include any columns that are of type string or timestamp\n            # - The rest of the columns are assumed to be features\n            self.log.warning(\"Guessing at the feature list, HIGHLY RECOMMENDED to specify an explicit feature list!\")\n            all_columns = feature_set.columns\n            filter_list = [\n                \"id\",\n                \"auto_id\",\n                \"__index_level_0__\",\n                \"write_time\",\n                \"api_invocation_time\",\n                \"is_deleted\",\n                \"event_time\",\n                \"training\",\n            ] + target_list\n            feature_list = [c for c in all_columns if c not in filter_list]\n\n            # AWS Feature Store has 3 user column types (String, Integral, Fractional)\n            # and two internal types (Timestamp and Boolean). A Feature List for\n            # modeling can only contain Integral and Fractional types.\n            remove_columns = []\n            column_details = feature_set.column_details()\n            for column_name in feature_list:\n                if column_details[column_name] not in [\"Integral\", \"Fractional\"]:\n                    self.log.warning(\n                        f\"Removing {column_name} from feature list, improper type {column_details[column_name]}\"\n                    )\n                    remove_columns.append(column_name)\n\n            # Remove the columns that are not Integral or Fractional\n            feature_list = [c for c in feature_list if c not in remove_columns]\n\n        # Set the final feature list\n        self.model_feature_list = feature_list\n        self.log.important(f\"Feature List for Modeling: {self.model_feature_list}\")\n\n        # Set up our parameters for the model script\n        # ChemProp expects target_column as a list; other templates expect a string\n        target_for_template = target_list if self.model_framework == ModelFramework.CHEMPROP else self.target_column\n        template_params = {\n            \"model_imports\": self.model_import_str,\n            \"model_type\": self.model_type,\n            \"model_framework\": self.model_framework,\n            \"model_class\": self.model_class,\n            \"target_column\": target_for_template,\n            \"feature_list\": self.model_feature_list,\n            \"compressed_features\": feature_set.get_compressed_features(),\n            \"model_metrics_s3_path\": self.model_training_root,\n            \"train_all_data\": train_all_data,\n            \"id_column\": feature_set.id_column,\n            \"hyperparameters\": kwargs.get(\"hyperparameters\", {}),\n        }\n        self.log.important(f\"transform_impl: model_metrics_s3_path = {template_params['model_metrics_s3_path']}\")\n\n        # Custom Script\n        if self.custom_script:\n            script_path = self.custom_script\n            if self.custom_script.endswith(\".template\"):\n                # Model Type is an enumerated type, so we need to convert it to a string\n                template_params[\"model_type\"] = template_params[\"model_type\"].value\n\n                # Fill in the custom script template with specific parameters (include any custom args)\n                template_params.update(self.custom_args)\n                script_path = fill_template(self.custom_script, template_params, \"generated_model_script.py\")\n            self.log.info(f\"Custom script path: {script_path}\")\n\n        # We're using one of the built-in model script templates\n        else:\n            # Generate our model script\n            script_path = generate_model_script(template_params)\n\n        # Metric Definitions for Regression (matches model script output format)\n        if self.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n            metric_definitions = [\n                {\"Name\": \"rmse\", \"Regex\": r\"rmse: ([0-9.]+)\"},\n                {\"Name\": \"mae\", \"Regex\": r\"mae: ([0-9.]+)\"},\n                {\"Name\": \"medae\", \"Regex\": r\"medae: ([0-9.]+)\"},\n                {\"Name\": \"r2\", \"Regex\": r\"r2: ([0-9.-]+)\"},\n                {\"Name\": \"spearmanr\", \"Regex\": r\"spearmanr: ([0-9.-]+)\"},\n                {\"Name\": \"support\", \"Regex\": r\"support: ([0-9]+)\"},\n            ]\n\n        # Metric Definitions for Classification\n        elif self.model_type == ModelType.CLASSIFIER:\n            # We need to get creative with the Classification Metrics\n            # Note: Classification only supports single target\n            class_target = target_list[0] if target_list else self.target_column\n\n            # Grab all the target column class values (class labels)\n            table = feature_set.data_source.table\n            self.class_labels = feature_set.query(f'select DISTINCT {class_target} FROM \"{table}\"')[\n                class_target\n            ].to_list()\n\n            # Sanity check on the targets\n            if len(self.class_labels) &gt; 10:\n                msg = f\"Too many target classes ({len(self.class_labels)}) for classification, aborting!\"\n                self.log.critical(msg)\n                raise ValueError(msg)\n\n            # Dynamically create the metric definitions (per-class precision/recall/f1/support)\n            # Note: Confusion matrix metrics are skipped to stay under SageMaker's 40 metric limit\n            metrics = [\"precision\", \"recall\", \"f1\", \"support\"]\n            metric_definitions = []\n            for t in self.class_labels:\n                for m in metrics:\n                    metric_definitions.append({\"Name\": f\"Metrics:{t}:{m}\", \"Regex\": f\"Metrics:{t}:{m} ([0-9.]+)\"})\n\n        # If the model type is UNKNOWN, our metric_definitions will be empty\n        else:\n            self.log.important(f\"ModelType is {self.model_type}, skipping metric_definitions...\")\n            metric_definitions = []\n\n        # Take the full script path and extract the entry point and source directory\n        entry_point = str(Path(script_path).name)\n        source_dir = str(Path(script_path).parent)\n\n        # DIAG: S3 bucket investigation - log the source directory contents\n        self.log.important(f\"transform_impl: source_dir = {source_dir}\")\n        self.log.important(f\"transform_impl: entry_point = {entry_point}\")\n        source_files = os.listdir(source_dir)\n        self.log.important(f\"transform_impl: source_dir contents = {source_files}\")\n\n        # DIAG: S3 bucket investigation - verify the generated script has the correct S3 path\n        generated_script_path = os.path.join(source_dir, entry_point)\n        with open(generated_script_path, \"r\") as f:\n            script_content = f.read()\n        s3_match = re.search(r'\"model_metrics_s3_path\":\\s*\"([^\"]+)\"', script_content)\n        if s3_match:\n            baked_path = s3_match.group(1)\n            self.log.important(f\"transform_impl: VERIFY generated script model_metrics_s3_path = {baked_path}\")\n            if baked_path != self.model_training_root:\n                raise ValueError(\n                    f\"CRITICAL: Generated script has wrong S3 path!\\n\"\n                    f\"  Expected: {self.model_training_root}\\n\"\n                    f\"  Got:      {baked_path}\"\n                )\n\n        # Create a Sagemaker Model with our script\n        image = ModelImages.get_image_uri(self.sm_session.boto_region_name, self.training_image)\n\n        # Use user-specified instance or default based on framework\n        train_instance_type = kwargs.get(\"training_instance\")\n        if train_instance_type:\n            self.log.important(f\"Using user-specified instance {train_instance_type}\")\n        elif self.model_framework in [ModelFramework.CHEMPROP, ModelFramework.PYTORCH]:\n            train_instance_type = \"ml.g6.2xlarge\"  # NVIDIA L4 GPU + 8 vCPUs for data loading\n            self.log.important(f\"Using GPU instance {train_instance_type} for {self.model_framework.value}\")\n        else:\n            train_instance_type = \"ml.m5.xlarge\"\n\n        self.estimator = Estimator(\n            entry_point=entry_point,\n            source_dir=source_dir,\n            role=self.workbench_role_arn,\n            instance_count=1,\n            instance_type=train_instance_type,\n            sagemaker_session=self.sm_session,\n            image_uri=image,\n            metric_definitions=metric_definitions,\n        )\n\n        # Training Job Name based on the Model Name and today's date\n        training_date_time_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d-%H-%M\")\n        training_job_name = f\"{self.output_name}-{training_date_time_utc}\"\n\n        # DIAG: S3 bucket investigation - pre-fit integrity check to catch any corruption\n        # between the initial VERIFY and the SageMaker upload (investigating intermittent\n        # issue where wrong S3 bucket appears in the uploaded sourcedir.tar.gz)\n        with open(generated_script_path, \"rb\") as f:\n            pre_fit_content = f.read()\n        pre_fit_match = re.search(rb'\"model_metrics_s3_path\":\\s*\"([^\"]+)\"', pre_fit_content)\n        if pre_fit_match:\n            pre_fit_path = pre_fit_match.group(1).decode()\n            self.log.important(f\"transform_impl: PRE-FIT CHECK model_metrics_s3_path = {pre_fit_path}\")\n            if pre_fit_path != self.model_training_root:\n                raise ValueError(\n                    f\"CRITICAL: Generated script was modified between VERIFY and fit()!\\n\"\n                    f\"  Expected: {self.model_training_root}\\n\"\n                    f\"  Got:      {pre_fit_path}\"\n                )\n\n        # Train the estimator\n        self.log.important(f\"Training the Model {self.output_name} with Training Image {image}...\")\n        self.estimator.fit({\"train\": s3_training_path}, job_name=training_job_name)\n\n        # Now delete the training data\n        self.log.info(f\"Deleting training data {s3_training_path}...\")\n        wr.s3.delete_objects(\n            [s3_training_path, s3_training_path.replace(\".csv\", \".csv.metadata\")],\n            boto3_session=self.boto3_session,\n        )\n\n        # Create Model and officially Register\n        self.log.important(f\"Creating new model {self.output_name}...\")\n        self.create_and_register_model(**kwargs)\n\n        # Make a copy of the training view, to lock-in the training data used for this model\n        model_training_view_name = f\"{self.output_name.replace('-', '_')}_training\"\n        self.log.important(f\"Creating Model Training View: {model_training_view_name}...\")\n        feature_set.view(\"training\").copy(f\"{model_training_view_name}\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling onboard() on the Model\"\"\"\n        self.log.info(\"Post-Transform: Calling onboard() on the Model...\")\n        time.sleep(3)  # Give AWS time to complete Model register\n\n        # Store the model metadata information\n        output_model = ModelCore(self.output_name)\n        output_model._set_model_type(self.model_type)\n        output_model._set_model_framework(self.model_framework)\n        output_model.upsert_workbench_meta({\"workbench_model_features\": self.model_feature_list})\n        output_model.upsert_workbench_meta({\"workbench_model_target\": self.target_column})\n\n        # Store the class labels (if they exist)\n        if self.class_labels:\n            output_model.set_class_labels(self.class_labels)\n\n        # Call the Model onboard method\n        output_model.onboard_with_args(self.model_type, self.target_column, self.model_feature_list)\n\n    def create_and_register_model(self, aws_region=None, **kwargs):\n        \"\"\"Create and Register the Model\n\n        Args:\n            aws_region (str, optional): AWS Region to use (default None)\n            **kwargs (dict): Additional keyword arguments to pass to the model registration\n        \"\"\"\n\n        # Get the metadata/tags to push into AWS\n        aws_tags = self.get_aws_tags()\n\n        # Create model group (if it doesn't already exist)\n        self.sm_client.create_model_package_group(\n            ModelPackageGroupName=self.output_name,\n            ModelPackageGroupDescription=self.model_description,\n            Tags=aws_tags,\n        )\n\n        # Register our model\n        image = ModelImages.get_image_uri(\n            self.sm_session.boto_region_name, self.inference_image, architecture=self.inference_arch\n        )\n        self.log.important(f\"Registering model {self.output_name} with Inference Image {image}...\")\n        model = self.estimator.create_model(role=self.workbench_role_arn)\n        if aws_region:\n            self.log.important(f\"Setting AWS Region: {aws_region} for model {self.output_name}...\")\n            model.env = {\"AWS_REGION\": aws_region}\n        model.register(\n            model_package_group_name=self.output_name,\n            image_uri=image,\n            content_types=[\"text/csv\"],\n            response_types=[\"text/csv\"],\n            inference_instances=supported_instance_types(self.inference_arch),\n            transform_instances=[\"ml.m5.large\", \"ml.m5.xlarge\"],\n            approval_status=\"Approved\",\n            description=self.model_description,\n        )\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#workbench.core.transforms.features_to_model.features_to_model.FeaturesToModel.__init__","title":"<code>__init__(feature_name, model_name, model_type, model_framework=ModelFramework.XGBOOST, model_class=None, model_import_str=None, custom_script=None, custom_args=None, training_image='training', inference_image='inference', inference_arch='x86_64')</code>","text":"<p>FeaturesToModel Initialization Args:     feature_name (str): Name of the FeatureSet to use as input     model_name (str): Name of the Model to create as output     model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER, etc.     model_framework (ModelFramework, optional): The model framework (default ModelFramework.XGBOOST)     model_class (str, optional): The scikit model (e.g. KNeighborsRegressor) (default None)     model_import_str (str, optional): The import string for the model (default None)     custom_script (str, optional): Custom script to use for the model (default None)     custom_args (dict, optional): Custom arguments to pass to custom model scripts (default None)     training_image (str, optional): Training image (default \"training\")     inference_image (str, optional): Inference image (default \"inference\")     inference_arch (str, optional): Inference architecture (default \"x86_64\")</p> Source code in <code>src/workbench/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def __init__(\n    self,\n    feature_name: str,\n    model_name: str,\n    model_type: ModelType,\n    model_framework=ModelFramework.XGBOOST,\n    model_class=None,\n    model_import_str=None,\n    custom_script=None,\n    custom_args=None,\n    training_image=\"training\",\n    inference_image=\"inference\",\n    inference_arch=\"x86_64\",\n):\n    \"\"\"FeaturesToModel Initialization\n    Args:\n        feature_name (str): Name of the FeatureSet to use as input\n        model_name (str): Name of the Model to create as output\n        model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER, etc.\n        model_framework (ModelFramework, optional): The model framework (default ModelFramework.XGBOOST)\n        model_class (str, optional): The scikit model (e.g. KNeighborsRegressor) (default None)\n        model_import_str (str, optional): The import string for the model (default None)\n        custom_script (str, optional): Custom script to use for the model (default None)\n        custom_args (dict, optional): Custom arguments to pass to custom model scripts (default None)\n        training_image (str, optional): Training image (default \"training\")\n        inference_image (str, optional): Inference image (default \"inference\")\n        inference_arch (str, optional): Inference architecture (default \"x86_64\")\n    \"\"\"\n\n    # Make sure the model_name is a valid name\n    Artifact.is_name_valid(model_name, delimiter=\"-\", lower_case=False)\n\n    # Call superclass init\n    super().__init__(feature_name, model_name)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.FEATURE_SET\n    self.output_type = TransformOutput.MODEL\n    self.model_type = model_type\n    self.model_framework = model_framework\n    self.model_class = model_class\n    self.model_import_str = model_import_str\n    self.custom_script = str(custom_script) if custom_script else None\n    self.custom_args = custom_args if custom_args else {}\n    self.estimator = None\n    self.model_description = None\n    self.model_training_root = f\"{self.models_s3_path}/{self.output_name}/training\"\n    self.log.important(f\"FeaturesToModel.__init__: model_training_root = {self.model_training_root}\")\n    self.model_feature_list = None\n    self.target_column = None\n    self.class_labels = None\n    self.training_image = training_image\n    self.inference_image = inference_image\n    self.inference_arch = inference_arch\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#workbench.core.transforms.features_to_model.features_to_model.FeaturesToModel.create_and_register_model","title":"<code>create_and_register_model(aws_region=None, **kwargs)</code>","text":"<p>Create and Register the Model</p> <p>Parameters:</p> Name Type Description Default <code>aws_region</code> <code>str</code> <p>AWS Region to use (default None)</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the model registration</p> <code>{}</code> Source code in <code>src/workbench/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def create_and_register_model(self, aws_region=None, **kwargs):\n    \"\"\"Create and Register the Model\n\n    Args:\n        aws_region (str, optional): AWS Region to use (default None)\n        **kwargs (dict): Additional keyword arguments to pass to the model registration\n    \"\"\"\n\n    # Get the metadata/tags to push into AWS\n    aws_tags = self.get_aws_tags()\n\n    # Create model group (if it doesn't already exist)\n    self.sm_client.create_model_package_group(\n        ModelPackageGroupName=self.output_name,\n        ModelPackageGroupDescription=self.model_description,\n        Tags=aws_tags,\n    )\n\n    # Register our model\n    image = ModelImages.get_image_uri(\n        self.sm_session.boto_region_name, self.inference_image, architecture=self.inference_arch\n    )\n    self.log.important(f\"Registering model {self.output_name} with Inference Image {image}...\")\n    model = self.estimator.create_model(role=self.workbench_role_arn)\n    if aws_region:\n        self.log.important(f\"Setting AWS Region: {aws_region} for model {self.output_name}...\")\n        model.env = {\"AWS_REGION\": aws_region}\n    model.register(\n        model_package_group_name=self.output_name,\n        image_uri=image,\n        content_types=[\"text/csv\"],\n        response_types=[\"text/csv\"],\n        inference_instances=supported_instance_types(self.inference_arch),\n        transform_instances=[\"ml.m5.large\", \"ml.m5.xlarge\"],\n        approval_status=\"Approved\",\n        description=self.model_description,\n    )\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#workbench.core.transforms.features_to_model.features_to_model.FeaturesToModel.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling onboard() on the Model</p> Source code in <code>src/workbench/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling onboard() on the Model\"\"\"\n    self.log.info(\"Post-Transform: Calling onboard() on the Model...\")\n    time.sleep(3)  # Give AWS time to complete Model register\n\n    # Store the model metadata information\n    output_model = ModelCore(self.output_name)\n    output_model._set_model_type(self.model_type)\n    output_model._set_model_framework(self.model_framework)\n    output_model.upsert_workbench_meta({\"workbench_model_features\": self.model_feature_list})\n    output_model.upsert_workbench_meta({\"workbench_model_target\": self.target_column})\n\n    # Store the class labels (if they exist)\n    if self.class_labels:\n        output_model.set_class_labels(self.class_labels)\n\n    # Call the Model onboard method\n    output_model.onboard_with_args(self.model_type, self.target_column, self.model_feature_list)\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#workbench.core.transforms.features_to_model.features_to_model.FeaturesToModel.transform_impl","title":"<code>transform_impl(target_column, description=None, feature_list=None, train_all_data=False, **kwargs)</code>","text":"<p>Generic Features to Model: Note you should create a new class and inherit from this one to include specific logic for your Feature Set/Model Args:     target_column (str or list[str]): Column name(s) of the target variable(s)     description (str): Description of the model (optional)     feature_list (list[str]): A list of columns for the features (default None, will try to guess)     train_all_data (bool): Train on ALL (100%) of the data (default False)</p> Source code in <code>src/workbench/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def transform_impl(\n    self,\n    target_column: Union[str, list[str]],\n    description: str = None,\n    feature_list: list = None,\n    train_all_data=False,\n    **kwargs,\n):\n    \"\"\"Generic Features to Model: Note you should create a new class and inherit from\n    this one to include specific logic for your Feature Set/Model\n    Args:\n        target_column (str or list[str]): Column name(s) of the target variable(s)\n        description (str): Description of the model (optional)\n        feature_list (list[str]): A list of columns for the features (default None, will try to guess)\n        train_all_data (bool): Train on ALL (100%) of the data (default False)\n    \"\"\"\n    # Delete the existing model (if it exists)\n    self.log.important(f\"Trying to delete existing model {self.output_name}...\")\n    ModelCore.managed_delete(self.output_name)\n\n    # Set our model description\n    self.model_description = description if description is not None else f\"Model created from {self.input_name}\"\n\n    # Get our Feature Set and create an S3 CSV Training dataset\n    feature_set = FeatureSetCore(self.input_name)\n    s3_training_path = feature_set.create_s3_training_data()\n    self.log.info(f\"Created new training data {s3_training_path}...\")\n\n    # Report the target column(s)\n    self.target_column = target_column\n    # Normalize target_column to a list for internal use\n    target_list = [target_column] if isinstance(target_column, str) else (target_column or [])\n    self.log.info(f\"Target column(s): {self.target_column}\")\n\n    # Did they specify a feature list?\n    if feature_list:\n        # AWS Feature Groups will also add these implicit columns, so remove them\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\", \"training\"]\n        feature_list = [c for c in feature_list if c not in aws_cols]\n\n    # If they didn't specify a feature list, try to guess it\n    else:\n        # Try to figure out features with this logic\n        # - Don't include id, event_time, __index_level_0__, or training columns\n        # - Don't include AWS generated columns (e.g. write_time, api_invocation_time, is_deleted)\n        # - Don't include the target columns\n        # - Don't include any columns that are of type string or timestamp\n        # - The rest of the columns are assumed to be features\n        self.log.warning(\"Guessing at the feature list, HIGHLY RECOMMENDED to specify an explicit feature list!\")\n        all_columns = feature_set.columns\n        filter_list = [\n            \"id\",\n            \"auto_id\",\n            \"__index_level_0__\",\n            \"write_time\",\n            \"api_invocation_time\",\n            \"is_deleted\",\n            \"event_time\",\n            \"training\",\n        ] + target_list\n        feature_list = [c for c in all_columns if c not in filter_list]\n\n        # AWS Feature Store has 3 user column types (String, Integral, Fractional)\n        # and two internal types (Timestamp and Boolean). A Feature List for\n        # modeling can only contain Integral and Fractional types.\n        remove_columns = []\n        column_details = feature_set.column_details()\n        for column_name in feature_list:\n            if column_details[column_name] not in [\"Integral\", \"Fractional\"]:\n                self.log.warning(\n                    f\"Removing {column_name} from feature list, improper type {column_details[column_name]}\"\n                )\n                remove_columns.append(column_name)\n\n        # Remove the columns that are not Integral or Fractional\n        feature_list = [c for c in feature_list if c not in remove_columns]\n\n    # Set the final feature list\n    self.model_feature_list = feature_list\n    self.log.important(f\"Feature List for Modeling: {self.model_feature_list}\")\n\n    # Set up our parameters for the model script\n    # ChemProp expects target_column as a list; other templates expect a string\n    target_for_template = target_list if self.model_framework == ModelFramework.CHEMPROP else self.target_column\n    template_params = {\n        \"model_imports\": self.model_import_str,\n        \"model_type\": self.model_type,\n        \"model_framework\": self.model_framework,\n        \"model_class\": self.model_class,\n        \"target_column\": target_for_template,\n        \"feature_list\": self.model_feature_list,\n        \"compressed_features\": feature_set.get_compressed_features(),\n        \"model_metrics_s3_path\": self.model_training_root,\n        \"train_all_data\": train_all_data,\n        \"id_column\": feature_set.id_column,\n        \"hyperparameters\": kwargs.get(\"hyperparameters\", {}),\n    }\n    self.log.important(f\"transform_impl: model_metrics_s3_path = {template_params['model_metrics_s3_path']}\")\n\n    # Custom Script\n    if self.custom_script:\n        script_path = self.custom_script\n        if self.custom_script.endswith(\".template\"):\n            # Model Type is an enumerated type, so we need to convert it to a string\n            template_params[\"model_type\"] = template_params[\"model_type\"].value\n\n            # Fill in the custom script template with specific parameters (include any custom args)\n            template_params.update(self.custom_args)\n            script_path = fill_template(self.custom_script, template_params, \"generated_model_script.py\")\n        self.log.info(f\"Custom script path: {script_path}\")\n\n    # We're using one of the built-in model script templates\n    else:\n        # Generate our model script\n        script_path = generate_model_script(template_params)\n\n    # Metric Definitions for Regression (matches model script output format)\n    if self.model_type in [ModelType.REGRESSOR, ModelType.UQ_REGRESSOR, ModelType.ENSEMBLE_REGRESSOR]:\n        metric_definitions = [\n            {\"Name\": \"rmse\", \"Regex\": r\"rmse: ([0-9.]+)\"},\n            {\"Name\": \"mae\", \"Regex\": r\"mae: ([0-9.]+)\"},\n            {\"Name\": \"medae\", \"Regex\": r\"medae: ([0-9.]+)\"},\n            {\"Name\": \"r2\", \"Regex\": r\"r2: ([0-9.-]+)\"},\n            {\"Name\": \"spearmanr\", \"Regex\": r\"spearmanr: ([0-9.-]+)\"},\n            {\"Name\": \"support\", \"Regex\": r\"support: ([0-9]+)\"},\n        ]\n\n    # Metric Definitions for Classification\n    elif self.model_type == ModelType.CLASSIFIER:\n        # We need to get creative with the Classification Metrics\n        # Note: Classification only supports single target\n        class_target = target_list[0] if target_list else self.target_column\n\n        # Grab all the target column class values (class labels)\n        table = feature_set.data_source.table\n        self.class_labels = feature_set.query(f'select DISTINCT {class_target} FROM \"{table}\"')[\n            class_target\n        ].to_list()\n\n        # Sanity check on the targets\n        if len(self.class_labels) &gt; 10:\n            msg = f\"Too many target classes ({len(self.class_labels)}) for classification, aborting!\"\n            self.log.critical(msg)\n            raise ValueError(msg)\n\n        # Dynamically create the metric definitions (per-class precision/recall/f1/support)\n        # Note: Confusion matrix metrics are skipped to stay under SageMaker's 40 metric limit\n        metrics = [\"precision\", \"recall\", \"f1\", \"support\"]\n        metric_definitions = []\n        for t in self.class_labels:\n            for m in metrics:\n                metric_definitions.append({\"Name\": f\"Metrics:{t}:{m}\", \"Regex\": f\"Metrics:{t}:{m} ([0-9.]+)\"})\n\n    # If the model type is UNKNOWN, our metric_definitions will be empty\n    else:\n        self.log.important(f\"ModelType is {self.model_type}, skipping metric_definitions...\")\n        metric_definitions = []\n\n    # Take the full script path and extract the entry point and source directory\n    entry_point = str(Path(script_path).name)\n    source_dir = str(Path(script_path).parent)\n\n    # DIAG: S3 bucket investigation - log the source directory contents\n    self.log.important(f\"transform_impl: source_dir = {source_dir}\")\n    self.log.important(f\"transform_impl: entry_point = {entry_point}\")\n    source_files = os.listdir(source_dir)\n    self.log.important(f\"transform_impl: source_dir contents = {source_files}\")\n\n    # DIAG: S3 bucket investigation - verify the generated script has the correct S3 path\n    generated_script_path = os.path.join(source_dir, entry_point)\n    with open(generated_script_path, \"r\") as f:\n        script_content = f.read()\n    s3_match = re.search(r'\"model_metrics_s3_path\":\\s*\"([^\"]+)\"', script_content)\n    if s3_match:\n        baked_path = s3_match.group(1)\n        self.log.important(f\"transform_impl: VERIFY generated script model_metrics_s3_path = {baked_path}\")\n        if baked_path != self.model_training_root:\n            raise ValueError(\n                f\"CRITICAL: Generated script has wrong S3 path!\\n\"\n                f\"  Expected: {self.model_training_root}\\n\"\n                f\"  Got:      {baked_path}\"\n            )\n\n    # Create a Sagemaker Model with our script\n    image = ModelImages.get_image_uri(self.sm_session.boto_region_name, self.training_image)\n\n    # Use user-specified instance or default based on framework\n    train_instance_type = kwargs.get(\"training_instance\")\n    if train_instance_type:\n        self.log.important(f\"Using user-specified instance {train_instance_type}\")\n    elif self.model_framework in [ModelFramework.CHEMPROP, ModelFramework.PYTORCH]:\n        train_instance_type = \"ml.g6.2xlarge\"  # NVIDIA L4 GPU + 8 vCPUs for data loading\n        self.log.important(f\"Using GPU instance {train_instance_type} for {self.model_framework.value}\")\n    else:\n        train_instance_type = \"ml.m5.xlarge\"\n\n    self.estimator = Estimator(\n        entry_point=entry_point,\n        source_dir=source_dir,\n        role=self.workbench_role_arn,\n        instance_count=1,\n        instance_type=train_instance_type,\n        sagemaker_session=self.sm_session,\n        image_uri=image,\n        metric_definitions=metric_definitions,\n    )\n\n    # Training Job Name based on the Model Name and today's date\n    training_date_time_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d-%H-%M\")\n    training_job_name = f\"{self.output_name}-{training_date_time_utc}\"\n\n    # DIAG: S3 bucket investigation - pre-fit integrity check to catch any corruption\n    # between the initial VERIFY and the SageMaker upload (investigating intermittent\n    # issue where wrong S3 bucket appears in the uploaded sourcedir.tar.gz)\n    with open(generated_script_path, \"rb\") as f:\n        pre_fit_content = f.read()\n    pre_fit_match = re.search(rb'\"model_metrics_s3_path\":\\s*\"([^\"]+)\"', pre_fit_content)\n    if pre_fit_match:\n        pre_fit_path = pre_fit_match.group(1).decode()\n        self.log.important(f\"transform_impl: PRE-FIT CHECK model_metrics_s3_path = {pre_fit_path}\")\n        if pre_fit_path != self.model_training_root:\n            raise ValueError(\n                f\"CRITICAL: Generated script was modified between VERIFY and fit()!\\n\"\n                f\"  Expected: {self.model_training_root}\\n\"\n                f\"  Got:      {pre_fit_path}\"\n            )\n\n    # Train the estimator\n    self.log.important(f\"Training the Model {self.output_name} with Training Image {image}...\")\n    self.estimator.fit({\"train\": s3_training_path}, job_name=training_job_name)\n\n    # Now delete the training data\n    self.log.info(f\"Deleting training data {s3_training_path}...\")\n    wr.s3.delete_objects(\n        [s3_training_path, s3_training_path.replace(\".csv\", \".csv.metadata\")],\n        boto3_session=self.boto3_session,\n    )\n\n    # Create Model and officially Register\n    self.log.important(f\"Creating new model {self.output_name}...\")\n    self.create_and_register_model(**kwargs)\n\n    # Make a copy of the training view, to lock-in the training data used for this model\n    model_training_view_name = f\"{self.output_name.replace('-', '_')}_training\"\n    self.log.important(f\"Creating Model Training View: {model_training_view_name}...\")\n    feature_set.view(\"training\").copy(f\"{model_training_view_name}\")\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#supported-models","title":"Supported Models","text":"<p>Currently Workbench supports XGBoost (classifier/regressor), and Scikit Learn models. Those models can be created by just specifying different parameters to the <code>FeaturesToModel</code> class. The main issue with the supported models is they are vanilla versions with default parameters, any customization should be done with Custom Models</p>"},{"location":"core_classes/transforms/features_to_model/#xgboost","title":"XGBoost","text":"<pre><code>from workbench.core.transforms.features_to_model.features_to_model import FeaturesToModel\n\n# XGBoost Regression Model\ninput_name = \"abalone_features\"\noutput_name = \"abalone-regression\"\nto_model = FeaturesToModel(input_name, output_name, model_type=ModelType.REGRESSOR)\nto_model.set_output_tags([\"abalone\", \"public\"])\nto_model.transform(target_column=\"class_number_of_rings\", description=\"Abalone Regression\")\n\n# XGBoost Classification Model\ninput_name = \"wine_features\"\noutput_name = \"wine-classification\"\nto_model = FeaturesToModel(input_name, output_name, ModelType.CLASSIFIER)\nto_model.set_output_tags([\"wine\", \"public\"])\nto_model.transform(target_column=\"wine_class\", description=\"Wine Classification\")\n\n# Quantile Regression Model (Abalone)\ninput_name = \"abalone_features\"\noutput_name = \"abalone-quantile-reg\"\nto_model = FeaturesToModel(input_name, output_name, ModelType.UQ_REGRESSOR)\nto_model.set_output_tags([\"abalone\", \"quantiles\"])\nto_model.transform(target_column=\"class_number_of_rings\", description=\"Abalone Quantile Regression\")\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#scikit-learn","title":"Scikit-Learn","text":"<pre><code>from workbench.core.transforms.features_to_model.features_to_model import FeaturesToModel\n\n# Scikit-Learn Kmeans Clustering Model\ninput_name = \"wine_features\"\noutput_name = \"wine-clusters\"\nto_model = FeaturesToModel(\n    input_name,\n    output_name,\n    model_class=\"KMeans\",  # Clustering algorithm\n    model_import_str=\"from sklearn.cluster import KMeans\",  # Import statement for KMeans\n    model_type=ModelType.CLUSTERER,\n)\nto_model.set_output_tags([\"wine\", \"clustering\"])\nto_model.transform(target_column=None, description=\"Wine Clustering\", train_all_data=True)\n\n# Scikit-Learn HDBSCAN Clustering Model\ninput_name = \"wine_features\"\noutput_name = \"wine-clusters-hdbscan\"\nto_model = FeaturesToModel(\n    input_name,\n    output_name,\n    model_class=\"HDBSCAN\",  # Density-based clustering algorithm\n    model_import_str=\"from sklearn.cluster import HDBSCAN\",\n    model_type=ModelType.CLUSTERER,\n)\nto_model.set_output_tags([\"wine\", \"density-based clustering\"])\nto_model.transform(target_column=None, description=\"Wine Clustering with HDBSCAN\", train_all_data=True)\n\n# Scikit-Learn 2D Projection Model using UMAP\ninput_name = \"wine_features\"\noutput_name = \"wine-2d-projection\"\nto_model = FeaturesToModel(\n    input_name,\n    output_name,\n    model_class=\"UMAP\",\n    model_import_str=\"from umap import UMAP\",\n    model_type=ModelType.PROJECTION,\n)\nto_model.set_output_tags([\"wine\", \"2d-projection\"])\nto_model.transform(target_column=None, description=\"Wine 2D Projection\", train_all_data=True)\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#custom-models","title":"Custom Models","text":"<p>For custom models we recommend the following steps:</p> <p>Experimental</p> <p>The Workbench Custom Models are currently in experimental mode so have fun but expect issues. Requires <code>workbench &gt;= 0.8.60</code>. Feel free to submit issues to Workbench Github</p> <ul> <li>Copy the example custom model script into your own directory<ul> <li>See: Custom Model Script</li> </ul> </li> <li>Make a requirements.txt and put into the same directory</li> <li>Train/deploy the ^existing^ example<ul> <li>This is an important step, don't skip it</li> <li>If the existing model script trains/deploys your in great shape for the next step, if it doesn't then now is a good time to debug AWS account/permissions/etc.</li> </ul> </li> <li>Now customize the model script</li> <li>Train/deploy your custom script</li> </ul>"},{"location":"core_classes/transforms/features_to_model/#trainingdeploying-custom-models","title":"Training/Deploying Custom Models","text":"<pre><code>from workbench.api import ModelType\nfrom workbench.core.transforms.features_to_model.features_to_model import FeaturesToModel\n\n# Note this directory should also have a requirements.txt in it\nmy_custom_script = \"/full/path/to/my/directory/my_custom_script.py\"\ninput_name = \"wine_features\"    # FeatureSet you want to use\noutput_name = \"my-custom-model\" # change to whatever\ntarget_column = \"wine-class\"    # change to whatever\nto_model = FeaturesToModel(input_name, output_name,\n                           model_type=ModelType.CLASSIFIER, \n                           custom_script=my_custom_script)\nto_model.set_output_tags([\"your\", \"tags\"])\nto_model.transform(target_column=target_column, description=\"Custom Model\")\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#custom-models-create-an-endpointrun-inference","title":"Custom Models: Create an Endpoint/Run Inference","text":"<pre><code>from workbench.api import Model, Endpoint\n\nmodel = Model(\"my-custom-model\")\nend = model.to_endpoint()   # Note: This takes a while\n\n# Now run inference on my custom model :)\nend.auto_inference()\n\n# Run inference with my own dataframe\ndf = fs.pull_dataframe()  # Or whatever dataframe\nend.inference(df)\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"core_classes/transforms/model_to_endpoint/","title":"Model to Endpoint","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>ModelToEndpoint: Deploy an Endpoint for a Model</p>"},{"location":"core_classes/transforms/model_to_endpoint/#workbench.core.transforms.model_to_endpoint.model_to_endpoint.ModelToEndpoint","title":"<code>ModelToEndpoint</code>","text":"<p>               Bases: <code>Transform</code></p> <p>ModelToEndpoint: Deploy an Endpoint for a Model</p> Common Usage <pre><code>to_endpoint = ModelToEndpoint(model_name, endpoint_name)\nto_endpoint.set_output_tags([\"aqsol\", \"public\", \"whatever\"])\nto_endpoint.transform()\n</code></pre> Source code in <code>src/workbench/core/transforms/model_to_endpoint/model_to_endpoint.py</code> <pre><code>class ModelToEndpoint(Transform):\n    \"\"\"ModelToEndpoint: Deploy an Endpoint for a Model\n\n    Common Usage:\n        ```python\n        to_endpoint = ModelToEndpoint(model_name, endpoint_name)\n        to_endpoint.set_output_tags([\"aqsol\", \"public\", \"whatever\"])\n        to_endpoint.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, model_name: str, endpoint_name: str, serverless: bool = True, instance: str = None):\n        \"\"\"ModelToEndpoint Initialization\n        Args:\n            model_name(str): The Name of the input Model\n            endpoint_name(str): The Name of the output Endpoint\n            serverless(bool): Deploy the Endpoint in serverless mode (default: True)\n            instance(str): The instance type for Realtime Endpoints (default: None = auto-select)\n        \"\"\"\n        # Make sure the endpoint_name is a valid name\n        Artifact.is_name_valid(endpoint_name, delimiter=\"-\", lower_case=False)\n\n        # Call superclass init\n        super().__init__(model_name, endpoint_name)\n\n        # Set up all my instance attributes\n        self.serverless = serverless\n        self.instance = instance\n        self.input_type = TransformInput.MODEL\n        self.output_type = TransformOutput.ENDPOINT\n\n    def transform_impl(self, **kwargs):\n        \"\"\"Deploy an Endpoint for a Model\"\"\"\n\n        # Delete endpoint (if it already exists)\n        EndpointCore.managed_delete(self.output_name)\n\n        # Get the Model Package ARN for our input model\n        workbench_model = ModelCore(self.input_name)\n\n        # Deploy the model\n        self._deploy_model(workbench_model, **kwargs)\n\n        # Add this endpoint to the set of registered endpoints for the model\n        workbench_model.register_endpoint(self.output_name)\n\n        # This ensures that the endpoint is ready for use\n        time.sleep(5)  # We wait for AWS Lag\n        end = EndpointCore(self.output_name)\n        self.log.important(f\"Endpoint {end.name} is ready for use\")\n\n    def _deploy_model(\n        self,\n        workbench_model: ModelCore,\n        mem_size: int = 2048,\n        max_concurrency: int = 5,\n        data_capture: bool = False,\n        capture_percentage: int = 100,\n    ):\n        \"\"\"Internal Method: Deploy the Model\n\n        Args:\n            workbench_model(ModelCore): The Workbench ModelCore object to deploy\n            mem_size(int): Memory size for serverless deployment\n            max_concurrency(int): Max concurrency for serverless deployment\n            data_capture(bool): Enable data capture during deployment\n            capture_percentage(int): Percentage of data to capture. Defaults to 100.\n        \"\"\"\n        # Grab the specified Model Package\n        model_package_arn = workbench_model.model_package_arn()\n        model_package = ModelPackage(\n            role=self.workbench_role_arn,\n            model_package_arn=model_package_arn,\n            sagemaker_session=self.sm_session,\n        )\n\n        # Log the image that will be used for deployment\n        inference_image = self.sm_client.describe_model_package(ModelPackageName=model_package_arn)[\n            \"InferenceSpecification\"\n        ][\"Containers\"][0][\"Image\"]\n        self.log.important(f\"Deploying Model Package: {self.input_name} with Inference Image: {inference_image}\")\n\n        # Get the metadata/tags to push into AWS\n        aws_tags = self.get_aws_tags()\n\n        # Check the model framework for resource requirements\n        from workbench.api import ModelFramework\n\n        self.log.info(f\"Model Framework: {workbench_model.model_framework}\")\n        needs_more_resources = workbench_model.model_framework in [ModelFramework.PYTORCH, ModelFramework.CHEMPROP]\n\n        # Is this a serverless deployment?\n        serverless_config = None\n        if self.serverless:\n            # For PyTorch or ChemProp we need at least 4GB of memory\n            if needs_more_resources and mem_size &lt; 4096:\n                self.log.important(f\"{workbench_model.model_framework} needs at least 4GB of memory (setting to 4GB)\")\n                mem_size = 4096\n            serverless_config = ServerlessInferenceConfig(\n                memory_size_in_mb=mem_size,\n                max_concurrency=max_concurrency,\n            )\n            instance_type = \"serverless\"\n            self.log.important(f\"Serverless Config: Memory={mem_size}MB, MaxConcurrency={max_concurrency}\")\n        else:\n            # For realtime endpoints, use explicit instance if provided, otherwise auto-select\n            if self.instance:\n                instance_type = self.instance\n                self.log.important(f\"Realtime Endpoint: Using specified instance type: {instance_type}\")\n            elif needs_more_resources:\n                instance_type = \"ml.c7i.large\"\n                self.log.important(f\"{workbench_model.model_framework} needs more resources (using {instance_type})\")\n            else:\n                instance_type = \"ml.t2.medium\"\n                self.log.important(f\"Realtime Endpoint: Instance Type={instance_type}\")\n\n        # Configure data capture if requested (and not serverless)\n        data_capture_config = None\n        if data_capture and not self.serverless:\n            # Set up the S3 path for data capture\n            base_endpoint_path = f\"{workbench_model.endpoints_s3_path}/{self.output_name}\"\n            data_capture_path = f\"{base_endpoint_path}/data_capture\"\n            self.log.important(f\"Configuring Data Capture --&gt; {data_capture_path}\")\n            data_capture_config = DataCaptureConfig(\n                enable_capture=True,\n                sampling_percentage=capture_percentage,\n                destination_s3_uri=data_capture_path,\n            )\n        elif data_capture and self.serverless:\n            self.log.warning(\n                \"Data capture is not supported for serverless endpoints. Skipping data capture configuration.\"\n            )\n\n        # Deploy the Endpoint\n        self.log.important(f\"Deploying the Endpoint {self.output_name}...\")\n        try:\n            model_package.deploy(\n                initial_instance_count=1,\n                instance_type=instance_type,\n                serverless_inference_config=serverless_config,\n                endpoint_name=self.output_name,\n                serializer=CSVSerializer(),\n                deserializer=CSVDeserializer(),\n                data_capture_config=data_capture_config,\n                tags=aws_tags,\n                container_startup_health_check_timeout=300,\n            )\n        except ClientError as e:\n            # Check if this is the \"endpoint config already exists\" error\n            if \"Cannot create already existing endpoint configuration\" in str(e):\n                self.log.warning(\"Endpoint config already exists, deleting and retrying...\")\n                self.sm_client.delete_endpoint_config(EndpointConfigName=self.output_name)\n                # Retry the deploy\n                model_package.deploy(\n                    initial_instance_count=1,\n                    instance_type=instance_type,\n                    serverless_inference_config=serverless_config,\n                    endpoint_name=self.output_name,\n                    serializer=CSVSerializer(),\n                    deserializer=CSVDeserializer(),\n                    data_capture_config=data_capture_config,\n                    tags=aws_tags,\n                    container_startup_health_check_timeout=300,\n                )\n            else:\n                raise\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling onboard() for the Endpoint\"\"\"\n        self.log.info(\"Post-Transform: Calling onboard() for the Endpoint...\")\n\n        # Onboard the Endpoint\n        output_endpoint = EndpointCore(self.output_name)\n        output_endpoint.onboard_with_args(input_model=self.input_name)\n</code></pre>"},{"location":"core_classes/transforms/model_to_endpoint/#workbench.core.transforms.model_to_endpoint.model_to_endpoint.ModelToEndpoint.__init__","title":"<code>__init__(model_name, endpoint_name, serverless=True, instance=None)</code>","text":"<p>ModelToEndpoint Initialization Args:     model_name(str): The Name of the input Model     endpoint_name(str): The Name of the output Endpoint     serverless(bool): Deploy the Endpoint in serverless mode (default: True)     instance(str): The instance type for Realtime Endpoints (default: None = auto-select)</p> Source code in <code>src/workbench/core/transforms/model_to_endpoint/model_to_endpoint.py</code> <pre><code>def __init__(self, model_name: str, endpoint_name: str, serverless: bool = True, instance: str = None):\n    \"\"\"ModelToEndpoint Initialization\n    Args:\n        model_name(str): The Name of the input Model\n        endpoint_name(str): The Name of the output Endpoint\n        serverless(bool): Deploy the Endpoint in serverless mode (default: True)\n        instance(str): The instance type for Realtime Endpoints (default: None = auto-select)\n    \"\"\"\n    # Make sure the endpoint_name is a valid name\n    Artifact.is_name_valid(endpoint_name, delimiter=\"-\", lower_case=False)\n\n    # Call superclass init\n    super().__init__(model_name, endpoint_name)\n\n    # Set up all my instance attributes\n    self.serverless = serverless\n    self.instance = instance\n    self.input_type = TransformInput.MODEL\n    self.output_type = TransformOutput.ENDPOINT\n</code></pre>"},{"location":"core_classes/transforms/model_to_endpoint/#workbench.core.transforms.model_to_endpoint.model_to_endpoint.ModelToEndpoint.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling onboard() for the Endpoint</p> Source code in <code>src/workbench/core/transforms/model_to_endpoint/model_to_endpoint.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling onboard() for the Endpoint\"\"\"\n    self.log.info(\"Post-Transform: Calling onboard() for the Endpoint...\")\n\n    # Onboard the Endpoint\n    output_endpoint = EndpointCore(self.output_name)\n    output_endpoint.onboard_with_args(input_model=self.input_name)\n</code></pre>"},{"location":"core_classes/transforms/model_to_endpoint/#workbench.core.transforms.model_to_endpoint.model_to_endpoint.ModelToEndpoint.transform_impl","title":"<code>transform_impl(**kwargs)</code>","text":"<p>Deploy an Endpoint for a Model</p> Source code in <code>src/workbench/core/transforms/model_to_endpoint/model_to_endpoint.py</code> <pre><code>def transform_impl(self, **kwargs):\n    \"\"\"Deploy an Endpoint for a Model\"\"\"\n\n    # Delete endpoint (if it already exists)\n    EndpointCore.managed_delete(self.output_name)\n\n    # Get the Model Package ARN for our input model\n    workbench_model = ModelCore(self.input_name)\n\n    # Deploy the model\n    self._deploy_model(workbench_model, **kwargs)\n\n    # Add this endpoint to the set of registered endpoints for the model\n    workbench_model.register_endpoint(self.output_name)\n\n    # This ensures that the endpoint is ready for use\n    time.sleep(5)  # We wait for AWS Lag\n    end = EndpointCore(self.output_name)\n    self.log.important(f\"Endpoint {end.name} is ready for use\")\n</code></pre>"},{"location":"core_classes/transforms/overview/","title":"Transforms","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>Workbench currently has a large set of Transforms that go from one Artifact type to another (e.g. DataSource to FeatureSet). The Transforms will often have light and heavy versions depending on the scale of data that needs to be transformed.</p>"},{"location":"core_classes/transforms/overview/#transform-details","title":"Transform Details","text":"<ul> <li>DataLoaders Light: Loads various light/smaller data into AWS Data Catalog and Athena</li> <li>DataLoaders Heavy: Loads heavy/larger data (via Glue) into AWS Data Catalog and Athena</li> <li>DataToFeatures: Transforms a DataSource into a FeatureSet (AWS Feature Store/Group)</li> <li>FeaturesToModel: Trains and deploys an AWS Model Package/Group from a FeatureSet</li> <li>ModelToEndpoint: Manages the provisioning and deployment of a Model Endpoint</li> <li>PandasTransforms: Pandas DataFrame transforms and helper methods.</li> </ul>"},{"location":"core_classes/transforms/pandas_transforms/","title":"Pandas Transforms","text":"<p>API Classes</p> <p>The API Classes will often provide helpful methods that give you a DataFrame (data_source.query() for instance), so always check out the API Classes first.</p> <p>These Transforms will give you the ultimate in customization and flexibility when creating AWS Machine Learning Pipelines. Grab a Pandas DataFrame from a DataSource or FeatureSet process in whatever way for your use case and simply create another Workbench DataSource or FeatureSet from the resulting DataFrame.</p> <p>Lots of Options:</p> <p>Not for Large Data</p> <p>Pandas Transforms can't handle large datasets (&gt; 4 GigaBytes). For doing transforma on large data see our Heavy Transforms</p> <ul> <li>S3 --&gt; DF --&gt; DataSource</li> <li>DataSource --&gt; DF --&gt; DataSource</li> <li>DataSoruce --&gt; DF --&gt; FeatureSet</li> <li>Get Creative!</li> </ul> <p>Welcome to the Workbench Pandas Transform Classes</p> <p>These classes provide low-level APIs for using Pandas DataFrames</p> <ul> <li>DataToPandas: Pull a dataframe from a Workbench DataSource</li> <li>FeaturesToPandas: Pull a dataframe from a Workbench FeatureSet</li> <li>PandasToData: Create a Workbench DataSource using a Pandas DataFrame as the source</li> <li>PandasToFeatures: Create a Workbench FeatureSet using a Pandas DataFrame as the source</li> <li>PandasToFeaturesChunked: Create a Workbench FeatureSet using a Chunked/Streaming Pandas DataFrame as the source</li> </ul>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.DataToPandas","title":"<code>DataToPandas</code>","text":"<p>               Bases: <code>Transform</code></p> <p>DataToPandas: Class to transform a Data Source into a Pandas DataFrame</p> Common Usage <pre><code>data_to_df = DataToPandas(data_source_name)\ndata_to_df.transform(query=&lt;optional SQL query to filter/process data&gt;)\ndata_to_df.transform(max_rows=&lt;optional max rows to sample&gt;)\nmy_df = data_to_df.get_output()\n\nNote: query is the best way to use this class, so use it :)\n</code></pre> Source code in <code>src/workbench/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>class DataToPandas(Transform):\n    \"\"\"DataToPandas: Class to transform a Data Source into a Pandas DataFrame\n\n    Common Usage:\n        ```python\n        data_to_df = DataToPandas(data_source_name)\n        data_to_df.transform(query=&lt;optional SQL query to filter/process data&gt;)\n        data_to_df.transform(max_rows=&lt;optional max rows to sample&gt;)\n        my_df = data_to_df.get_output()\n\n        Note: query is the best way to use this class, so use it :)\n        ```\n    \"\"\"\n\n    def __init__(self, input_name: str):\n        \"\"\"DataToPandas Initialization\"\"\"\n\n        # Call superclass init\n        super().__init__(input_name, \"DataFrame\")\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.DATA_SOURCE\n        self.output_type = TransformOutput.PANDAS_DF\n        self.output_df = None\n\n    def transform_impl(self, query: str = None, max_rows=100000):\n        \"\"\"Convert the DataSource into a Pandas DataFrame\n        Args:\n            query(str): The query to run against the DataSource (default: None)\n            max_rows(int): The maximum number of rows to return (default: 100000)\n        \"\"\"\n\n        # Grab the Input (Data Source)\n        input_data = DataSourceFactory(self.input_name)\n        if not input_data.exists():\n            self.log.critical(f\"Data Check on {self.input_name} failed!\")\n            return\n\n        # If a query is provided, that overrides the queries below\n        if query:\n            self.log.info(f\"Querying {self.input_name} with {query}...\")\n            self.output_df = input_data.query(query)\n            return\n\n        # If the data source has more rows than max_rows, do a sample query\n        num_rows = input_data.num_rows()\n        if num_rows &gt; max_rows:\n            percentage = round(max_rows * 100.0 / num_rows)\n            self.log.important(f\"DataSource has {num_rows} rows.. sampling down to {max_rows}...\")\n            query = f\"SELECT * FROM {self.input_name} TABLESAMPLE BERNOULLI({percentage})\"\n        else:\n            query = f\"SELECT * FROM {self.input_name}\"\n\n        # Mark the transform as complete and set the output DataFrame\n        self.output_df = input_data.query(query)\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Any checks on the Pandas DataFrame that need to be done\"\"\"\n        self.log.info(\"Post-Transform: Checking Pandas DataFrame...\")\n        self.log.info(f\"DataFrame Shape: {self.output_df.shape}\")\n\n    def get_output(self) -&gt; pd.DataFrame:\n        \"\"\"Get the DataFrame Output from this Transform\"\"\"\n        return self.output_df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.DataToPandas.__init__","title":"<code>__init__(input_name)</code>","text":"<p>DataToPandas Initialization</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>def __init__(self, input_name: str):\n    \"\"\"DataToPandas Initialization\"\"\"\n\n    # Call superclass init\n    super().__init__(input_name, \"DataFrame\")\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.DATA_SOURCE\n    self.output_type = TransformOutput.PANDAS_DF\n    self.output_df = None\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.DataToPandas.get_output","title":"<code>get_output()</code>","text":"<p>Get the DataFrame Output from this Transform</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>def get_output(self) -&gt; pd.DataFrame:\n    \"\"\"Get the DataFrame Output from this Transform\"\"\"\n    return self.output_df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.DataToPandas.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Any checks on the Pandas DataFrame that need to be done</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Any checks on the Pandas DataFrame that need to be done\"\"\"\n    self.log.info(\"Post-Transform: Checking Pandas DataFrame...\")\n    self.log.info(f\"DataFrame Shape: {self.output_df.shape}\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.DataToPandas.transform_impl","title":"<code>transform_impl(query=None, max_rows=100000)</code>","text":"<p>Convert the DataSource into a Pandas DataFrame Args:     query(str): The query to run against the DataSource (default: None)     max_rows(int): The maximum number of rows to return (default: 100000)</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>def transform_impl(self, query: str = None, max_rows=100000):\n    \"\"\"Convert the DataSource into a Pandas DataFrame\n    Args:\n        query(str): The query to run against the DataSource (default: None)\n        max_rows(int): The maximum number of rows to return (default: 100000)\n    \"\"\"\n\n    # Grab the Input (Data Source)\n    input_data = DataSourceFactory(self.input_name)\n    if not input_data.exists():\n        self.log.critical(f\"Data Check on {self.input_name} failed!\")\n        return\n\n    # If a query is provided, that overrides the queries below\n    if query:\n        self.log.info(f\"Querying {self.input_name} with {query}...\")\n        self.output_df = input_data.query(query)\n        return\n\n    # If the data source has more rows than max_rows, do a sample query\n    num_rows = input_data.num_rows()\n    if num_rows &gt; max_rows:\n        percentage = round(max_rows * 100.0 / num_rows)\n        self.log.important(f\"DataSource has {num_rows} rows.. sampling down to {max_rows}...\")\n        query = f\"SELECT * FROM {self.input_name} TABLESAMPLE BERNOULLI({percentage})\"\n    else:\n        query = f\"SELECT * FROM {self.input_name}\"\n\n    # Mark the transform as complete and set the output DataFrame\n    self.output_df = input_data.query(query)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.FeaturesToPandas","title":"<code>FeaturesToPandas</code>","text":"<p>               Bases: <code>Transform</code></p> <p>FeaturesToPandas: Class to transform a FeatureSet into a Pandas DataFrame</p> Common Usage <pre><code>feature_to_df = FeaturesToPandas(feature_set_name)\nfeature_to_df.transform(max_rows=&lt;optional max rows to sample&gt;)\nmy_df = feature_to_df.get_output()\n</code></pre> Source code in <code>src/workbench/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>class FeaturesToPandas(Transform):\n    \"\"\"FeaturesToPandas: Class to transform a FeatureSet into a Pandas DataFrame\n\n    Common Usage:\n        ```python\n        feature_to_df = FeaturesToPandas(feature_set_name)\n        feature_to_df.transform(max_rows=&lt;optional max rows to sample&gt;)\n        my_df = feature_to_df.get_output()\n        ```\n    \"\"\"\n\n    def __init__(self, feature_set_name: str):\n        \"\"\"FeaturesToPandas Initialization\"\"\"\n\n        # Call superclass init\n        super().__init__(input_name=feature_set_name, output_name=\"DataFrame\")\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.FEATURE_SET\n        self.output_type = TransformOutput.PANDAS_DF\n        self.output_df = None\n        self.transform_run = False\n\n    def transform_impl(self, max_rows=100000):\n        \"\"\"Convert the FeatureSet into a Pandas DataFrame\"\"\"\n\n        # Grab the Input (Feature Set)\n        input_data = FeatureSetCore(self.input_name)\n        if not input_data.exists():\n            self.log.critical(f\"Feature Set Check on {self.input_name} failed!\")\n            return\n\n        # Grab the table for this Feature Set\n        table = input_data.athena_table\n\n        # Get the list of columns (and subtract metadata columns that might get added)\n        columns = input_data.columns\n        filter_columns = [\"write_time\", \"api_invocation_time\", \"is_deleted\"]\n        columns = \", \".join([x for x in columns if x not in filter_columns])\n\n        # Get the number of rows in the Feature Set\n        num_rows = input_data.num_rows()\n\n        # If the data source has more rows than max_rows, do a sample query\n        if num_rows &gt; max_rows:\n            percentage = round(max_rows * 100.0 / num_rows)\n            self.log.important(f\"DataSource has {num_rows} rows.. sampling down to {max_rows}...\")\n            query = f'SELECT {columns} FROM \"{table}\" TABLESAMPLE BERNOULLI({percentage})'\n        else:\n            query = f'SELECT {columns} FROM \"{table}\"'\n\n        # Mark the transform as complete and set the output DataFrame\n        self.transform_run = True\n        self.output_df = input_data.query(query)\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Any checks on the Pandas DataFrame that need to be done\"\"\"\n        self.log.info(\"Post-Transform: Checking Pandas DataFrame...\")\n        self.log.info(f\"DataFrame Shape: {self.output_df.shape}\")\n\n    def get_output(self) -&gt; pd.DataFrame:\n        \"\"\"Get the DataFrame Output from this Transform\"\"\"\n        if not self.transform_run:\n            self.transform()\n        return self.output_df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.FeaturesToPandas.__init__","title":"<code>__init__(feature_set_name)</code>","text":"<p>FeaturesToPandas Initialization</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>def __init__(self, feature_set_name: str):\n    \"\"\"FeaturesToPandas Initialization\"\"\"\n\n    # Call superclass init\n    super().__init__(input_name=feature_set_name, output_name=\"DataFrame\")\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.FEATURE_SET\n    self.output_type = TransformOutput.PANDAS_DF\n    self.output_df = None\n    self.transform_run = False\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.FeaturesToPandas.get_output","title":"<code>get_output()</code>","text":"<p>Get the DataFrame Output from this Transform</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>def get_output(self) -&gt; pd.DataFrame:\n    \"\"\"Get the DataFrame Output from this Transform\"\"\"\n    if not self.transform_run:\n        self.transform()\n    return self.output_df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.FeaturesToPandas.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Any checks on the Pandas DataFrame that need to be done</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Any checks on the Pandas DataFrame that need to be done\"\"\"\n    self.log.info(\"Post-Transform: Checking Pandas DataFrame...\")\n    self.log.info(f\"DataFrame Shape: {self.output_df.shape}\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.FeaturesToPandas.transform_impl","title":"<code>transform_impl(max_rows=100000)</code>","text":"<p>Convert the FeatureSet into a Pandas DataFrame</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>def transform_impl(self, max_rows=100000):\n    \"\"\"Convert the FeatureSet into a Pandas DataFrame\"\"\"\n\n    # Grab the Input (Feature Set)\n    input_data = FeatureSetCore(self.input_name)\n    if not input_data.exists():\n        self.log.critical(f\"Feature Set Check on {self.input_name} failed!\")\n        return\n\n    # Grab the table for this Feature Set\n    table = input_data.athena_table\n\n    # Get the list of columns (and subtract metadata columns that might get added)\n    columns = input_data.columns\n    filter_columns = [\"write_time\", \"api_invocation_time\", \"is_deleted\"]\n    columns = \", \".join([x for x in columns if x not in filter_columns])\n\n    # Get the number of rows in the Feature Set\n    num_rows = input_data.num_rows()\n\n    # If the data source has more rows than max_rows, do a sample query\n    if num_rows &gt; max_rows:\n        percentage = round(max_rows * 100.0 / num_rows)\n        self.log.important(f\"DataSource has {num_rows} rows.. sampling down to {max_rows}...\")\n        query = f'SELECT {columns} FROM \"{table}\" TABLESAMPLE BERNOULLI({percentage})'\n    else:\n        query = f'SELECT {columns} FROM \"{table}\"'\n\n    # Mark the transform as complete and set the output DataFrame\n    self.transform_run = True\n    self.output_df = input_data.query(query)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData","title":"<code>PandasToData</code>","text":"<p>               Bases: <code>Transform</code></p> <p>PandasToData: Class to publish a Pandas DataFrame as a DataSource</p> Common Usage <pre><code>df_to_data = PandasToData(output_name)\ndf_to_data.set_output_tags([\"test\", \"small\"])\ndf_to_data.set_input(test_df)\ndf_to_data.transform()\n</code></pre> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>class PandasToData(Transform):\n    \"\"\"PandasToData: Class to publish a Pandas DataFrame as a DataSource\n\n    Common Usage:\n        ```python\n        df_to_data = PandasToData(output_name)\n        df_to_data.set_output_tags([\"test\", \"small\"])\n        df_to_data.set_input(test_df)\n        df_to_data.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, output_name: str, output_format: str = \"parquet\", catalog_db: str = \"workbench\"):\n        \"\"\"PandasToData Initialization\n        Args:\n            output_name (str): The Name of the DataSource to create\n            output_format (str): The file format to store the S3 object data in (default: \"parquet\")\n            catalog_db (str): The AWS Data Catalog Database to use (default: \"workbench\")\n        \"\"\"\n\n        # Make sure the output_name is a valid name/id\n        Artifact.is_name_valid(output_name)\n\n        # Call superclass init\n        super().__init__(\"DataFrame\", output_name, catalog_db)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.PANDAS_DF\n        self.output_type = TransformOutput.DATA_SOURCE\n        self.output_df = None\n\n        # Give a message that Parquet is best in most cases\n        if output_format != \"parquet\":\n            self.log.warning(\"Parquet format works the best in most cases please consider using it\")\n        self.output_format = output_format\n\n    def set_input(self, input_df: pd.DataFrame):\n        \"\"\"Set the DataFrame Input for this Transform\"\"\"\n        self.output_df = input_df.copy()\n\n    def delete_existing(self):\n        # Delete the existing FeatureSet if it exists\n        self.log.info(f\"Deleting the {self.output_name} DataSource...\")\n        AthenaSource.managed_delete(self.output_name)\n        time.sleep(1)\n\n    def convert_object_to_string(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Try to automatically convert object columns to string columns\"\"\"\n        for c in df.columns[df.dtypes == \"object\"]:  # Look at the object columns\n            try:\n                df[c] = df[c].astype(\"string\")\n                df[c] = df[c].str.replace(\"'\", '\"')  # This is for nested JSON\n            except (ParserError, ValueError, TypeError):\n                self.log.info(f\"Column {c} could not be converted to string...\")\n        return df\n\n    def convert_object_to_datetime(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Try to automatically convert object columns to datetime or string columns\"\"\"\n        for c in df.columns[df.dtypes == \"object\"]:  # Look at the object columns\n            try:\n                df[c] = pd.to_datetime(df[c])\n            except (ParserError, ValueError, TypeError):\n                self.log.debug(f\"Column {c} could not be converted to datetime...\")\n        return df\n\n    @staticmethod\n    def convert_datetime_columns(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Convert datetime columns to ISO-8601 string\"\"\"\n        datetime_type = [\"datetime\", \"datetime64\", \"datetime64[ns]\", \"datetimetz\"]\n        for c in df.select_dtypes(include=datetime_type).columns:\n            df[c] = df[c].map(datetime_to_iso8601)\n            df[c] = df[c].astype(pd.StringDtype())\n        return df\n\n    def pre_transform(self, **kwargs):\n        \"\"\"Pre-Transform: Delete the existing DataSource if it exists\"\"\"\n        self.delete_existing()\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the Pandas DataFrame into Parquet Format in the Workbench S3 Bucket, and\n        store the information about the data to the AWS Data Catalog workbench database\n\n        Args:\n            overwrite (bool): Overwrite the existing data in the Workbench S3 Bucket (default: True)\n        \"\"\"\n        self.log.info(f\"DataFrame to Workbench DataSource: {self.output_name}...\")\n\n        # Set up our metadata storage\n        workbench_meta = {\"workbench_tags\": self.output_tags}\n        workbench_meta.update(self.output_meta)\n\n        # Create the Output Parquet file S3 Storage Path\n        s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_name}\"\n\n        # Convert columns names to lowercase, Athena will not work with uppercase column names\n        if str(self.output_df.columns) != str(self.output_df.columns.str.lower()):\n            for c in self.output_df.columns:\n                if c != c.lower():\n                    self.log.important(f\"Column name {c} converted to lowercase: {c.lower()}\")\n            self.output_df.columns = self.output_df.columns.str.lower()\n\n        # Convert Object Columns to String\n        self.output_df = self.convert_object_to_string(self.output_df)\n\n        # Note: Both of these conversions may not be necessary, so we're leaving them commented out\n        \"\"\"\n        # Convert Object Columns to Datetime\n        self.output_df = self.convert_object_to_datetime(self.output_df)\n\n        # Now convert datetime columns to ISO-8601 string\n        # self.output_df = self.convert_datetime_columns(self.output_df)\n        \"\"\"\n\n        # Write out the DataFrame to AWS Data Catalog in either Parquet or JSONL format\n        description = f\"Workbench data source: {self.output_name}\"\n        glue_table_settings = {\"description\": description, \"parameters\": workbench_meta}\n        if self.output_format == \"parquet\":\n            wr.s3.to_parquet(\n                self.output_df,\n                path=s3_storage_path,\n                dataset=True,\n                mode=\"overwrite\",\n                database=self.data_catalog_db,\n                table=self.output_name,\n                filename_prefix=f\"{self.output_name}_\",\n                boto3_session=self.boto3_session,\n                partition_cols=None,\n                glue_table_settings=glue_table_settings,\n                sanitize_columns=False,\n            )  # FIXME: Have some logic around partition columns\n\n        # Note: In general Parquet works will for most uses cases. We recommend using Parquet\n        #       You can use JSON_EXTRACT on Parquet string field, and it works great.\n        elif self.output_format == \"jsonl\":\n            self.log.warning(\"We recommend using Parquet format for most use cases\")\n            self.log.warning(\"If you have a use case that requires JSONL please contact Workbench support\")\n            self.log.warning(\"We'd like to understand what functionality JSONL is providing that isn't already\")\n            self.log.warning(\"provided with Parquet and JSON_EXTRACT() for your Athena Queries\")\n            wr.s3.to_json(\n                self.output_df,\n                path=s3_storage_path,\n                orient=\"records\",\n                lines=True,\n                date_format=\"iso\",\n                dataset=True,\n                mode=\"overwrite\",\n                database=self.data_catalog_db,\n                table=self.output_name,\n                filename_prefix=f\"{self.output_name}_\",\n                boto3_session=self.boto3_session,\n                partition_cols=None,\n                glue_table_settings=glue_table_settings,\n            )\n        else:\n            raise ValueError(f\"Unsupported file format: {self.output_format}\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling onboard() fnr the DataSource\"\"\"\n        self.log.info(\"Post-Transform: Calling onboard() for the DataSource...\")\n\n        # Onboard the DataSource\n        output_data_source = DataSourceFactory(self.output_name)\n        output_data_source.onboard()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData.__init__","title":"<code>__init__(output_name, output_format='parquet', catalog_db='workbench')</code>","text":"<p>PandasToData Initialization Args:     output_name (str): The Name of the DataSource to create     output_format (str): The file format to store the S3 object data in (default: \"parquet\")     catalog_db (str): The AWS Data Catalog Database to use (default: \"workbench\")</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def __init__(self, output_name: str, output_format: str = \"parquet\", catalog_db: str = \"workbench\"):\n    \"\"\"PandasToData Initialization\n    Args:\n        output_name (str): The Name of the DataSource to create\n        output_format (str): The file format to store the S3 object data in (default: \"parquet\")\n        catalog_db (str): The AWS Data Catalog Database to use (default: \"workbench\")\n    \"\"\"\n\n    # Make sure the output_name is a valid name/id\n    Artifact.is_name_valid(output_name)\n\n    # Call superclass init\n    super().__init__(\"DataFrame\", output_name, catalog_db)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.PANDAS_DF\n    self.output_type = TransformOutput.DATA_SOURCE\n    self.output_df = None\n\n    # Give a message that Parquet is best in most cases\n    if output_format != \"parquet\":\n        self.log.warning(\"Parquet format works the best in most cases please consider using it\")\n    self.output_format = output_format\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData.convert_datetime_columns","title":"<code>convert_datetime_columns(df)</code>  <code>staticmethod</code>","text":"<p>Convert datetime columns to ISO-8601 string</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>@staticmethod\ndef convert_datetime_columns(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Convert datetime columns to ISO-8601 string\"\"\"\n    datetime_type = [\"datetime\", \"datetime64\", \"datetime64[ns]\", \"datetimetz\"]\n    for c in df.select_dtypes(include=datetime_type).columns:\n        df[c] = df[c].map(datetime_to_iso8601)\n        df[c] = df[c].astype(pd.StringDtype())\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData.convert_object_to_datetime","title":"<code>convert_object_to_datetime(df)</code>","text":"<p>Try to automatically convert object columns to datetime or string columns</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def convert_object_to_datetime(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Try to automatically convert object columns to datetime or string columns\"\"\"\n    for c in df.columns[df.dtypes == \"object\"]:  # Look at the object columns\n        try:\n            df[c] = pd.to_datetime(df[c])\n        except (ParserError, ValueError, TypeError):\n            self.log.debug(f\"Column {c} could not be converted to datetime...\")\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData.convert_object_to_string","title":"<code>convert_object_to_string(df)</code>","text":"<p>Try to automatically convert object columns to string columns</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def convert_object_to_string(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Try to automatically convert object columns to string columns\"\"\"\n    for c in df.columns[df.dtypes == \"object\"]:  # Look at the object columns\n        try:\n            df[c] = df[c].astype(\"string\")\n            df[c] = df[c].str.replace(\"'\", '\"')  # This is for nested JSON\n        except (ParserError, ValueError, TypeError):\n            self.log.info(f\"Column {c} could not be converted to string...\")\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling onboard() fnr the DataSource</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling onboard() fnr the DataSource\"\"\"\n    self.log.info(\"Post-Transform: Calling onboard() for the DataSource...\")\n\n    # Onboard the DataSource\n    output_data_source = DataSourceFactory(self.output_name)\n    output_data_source.onboard()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData.pre_transform","title":"<code>pre_transform(**kwargs)</code>","text":"<p>Pre-Transform: Delete the existing DataSource if it exists</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def pre_transform(self, **kwargs):\n    \"\"\"Pre-Transform: Delete the existing DataSource if it exists\"\"\"\n    self.delete_existing()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData.set_input","title":"<code>set_input(input_df)</code>","text":"<p>Set the DataFrame Input for this Transform</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def set_input(self, input_df: pd.DataFrame):\n    \"\"\"Set the DataFrame Input for this Transform\"\"\"\n    self.output_df = input_df.copy()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToData.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the Pandas DataFrame into Parquet Format in the Workbench S3 Bucket, and store the information about the data to the AWS Data Catalog workbench database</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Overwrite the existing data in the Workbench S3 Bucket (default: True)</p> <code>True</code> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the Pandas DataFrame into Parquet Format in the Workbench S3 Bucket, and\n    store the information about the data to the AWS Data Catalog workbench database\n\n    Args:\n        overwrite (bool): Overwrite the existing data in the Workbench S3 Bucket (default: True)\n    \"\"\"\n    self.log.info(f\"DataFrame to Workbench DataSource: {self.output_name}...\")\n\n    # Set up our metadata storage\n    workbench_meta = {\"workbench_tags\": self.output_tags}\n    workbench_meta.update(self.output_meta)\n\n    # Create the Output Parquet file S3 Storage Path\n    s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_name}\"\n\n    # Convert columns names to lowercase, Athena will not work with uppercase column names\n    if str(self.output_df.columns) != str(self.output_df.columns.str.lower()):\n        for c in self.output_df.columns:\n            if c != c.lower():\n                self.log.important(f\"Column name {c} converted to lowercase: {c.lower()}\")\n        self.output_df.columns = self.output_df.columns.str.lower()\n\n    # Convert Object Columns to String\n    self.output_df = self.convert_object_to_string(self.output_df)\n\n    # Note: Both of these conversions may not be necessary, so we're leaving them commented out\n    \"\"\"\n    # Convert Object Columns to Datetime\n    self.output_df = self.convert_object_to_datetime(self.output_df)\n\n    # Now convert datetime columns to ISO-8601 string\n    # self.output_df = self.convert_datetime_columns(self.output_df)\n    \"\"\"\n\n    # Write out the DataFrame to AWS Data Catalog in either Parquet or JSONL format\n    description = f\"Workbench data source: {self.output_name}\"\n    glue_table_settings = {\"description\": description, \"parameters\": workbench_meta}\n    if self.output_format == \"parquet\":\n        wr.s3.to_parquet(\n            self.output_df,\n            path=s3_storage_path,\n            dataset=True,\n            mode=\"overwrite\",\n            database=self.data_catalog_db,\n            table=self.output_name,\n            filename_prefix=f\"{self.output_name}_\",\n            boto3_session=self.boto3_session,\n            partition_cols=None,\n            glue_table_settings=glue_table_settings,\n            sanitize_columns=False,\n        )  # FIXME: Have some logic around partition columns\n\n    # Note: In general Parquet works will for most uses cases. We recommend using Parquet\n    #       You can use JSON_EXTRACT on Parquet string field, and it works great.\n    elif self.output_format == \"jsonl\":\n        self.log.warning(\"We recommend using Parquet format for most use cases\")\n        self.log.warning(\"If you have a use case that requires JSONL please contact Workbench support\")\n        self.log.warning(\"We'd like to understand what functionality JSONL is providing that isn't already\")\n        self.log.warning(\"provided with Parquet and JSON_EXTRACT() for your Athena Queries\")\n        wr.s3.to_json(\n            self.output_df,\n            path=s3_storage_path,\n            orient=\"records\",\n            lines=True,\n            date_format=\"iso\",\n            dataset=True,\n            mode=\"overwrite\",\n            database=self.data_catalog_db,\n            table=self.output_name,\n            filename_prefix=f\"{self.output_name}_\",\n            boto3_session=self.boto3_session,\n            partition_cols=None,\n            glue_table_settings=glue_table_settings,\n        )\n    else:\n        raise ValueError(f\"Unsupported file format: {self.output_format}\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures","title":"<code>PandasToFeatures</code>","text":"<p>               Bases: <code>Transform</code></p> <p>PandasToFeatures: Class to publish a Pandas DataFrame into a FeatureSet</p> Common Usage <pre><code>to_features = PandasToFeatures(output_name)\nto_features.set_output_tags([\"my\", \"awesome\", \"data\"])\nto_features.set_input(df, id_column=\"my_id\")\nto_features.transform()\n</code></pre> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>class PandasToFeatures(Transform):\n    \"\"\"PandasToFeatures: Class to publish a Pandas DataFrame into a FeatureSet\n\n    Common Usage:\n        ```python\n        to_features = PandasToFeatures(output_name)\n        to_features.set_output_tags([\"my\", \"awesome\", \"data\"])\n        to_features.set_input(df, id_column=\"my_id\")\n        to_features.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, output_name: str):\n        \"\"\"PandasToFeatures Initialization\n\n        Args:\n            output_name (str): The Name of the FeatureSet to create\n        \"\"\"\n\n        # Make sure the output_name is a valid name\n        Artifact.is_name_valid(output_name)\n\n        # Call superclass init\n        super().__init__(\"DataFrame\", output_name)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.PANDAS_DF\n        self.output_type = TransformOutput.FEATURE_SET\n        self.id_column = None\n        self.event_time_column = None\n        self.one_hot_columns = []\n        self.categorical_dtypes = {}  # Used for streaming/chunking\n        self.output_df = None\n        self.table_format = TableFormatEnum.ICEBERG\n        self.incoming_hold_out_ids = None\n\n        # These will be set in the transform method\n        self.output_feature_group = None\n        self.output_feature_set = None\n        self.expected_rows = 0\n\n    def set_input(self, input_df: pd.DataFrame, id_column=None, event_time_column=None, one_hot_columns=None):\n        \"\"\"Set the Input DataFrame for this Transform\n\n        Args:\n            input_df (pd.DataFrame): The input DataFrame.\n            id_column (str, optional): The ID column (if not specified, an 'auto_id' will be generated).\n            event_time_column (str, optional): The name of the event time column (default: None).\n            one_hot_columns (list, optional): The list of columns to one-hot encode (default: None).\n        \"\"\"\n        self.id_column = id_column\n        self.event_time_column = event_time_column\n        self.output_df = input_df.copy()\n        self.one_hot_columns = one_hot_columns or []\n\n        # Warn about known AWS Iceberg bug with event_time_column\n        if event_time_column is not None:\n            self.log.warning(\n                f\"event_time_column='{event_time_column}' specified. Note: AWS has a known bug with \"\n                \"Iceberg FeatureGroups where varying event times across multiple days can cause \"\n                \"duplicate rows in the offline store. Setting event_time_column=None.\"\n            )\n            self.event_time_column = None\n\n        # Now Prepare the DataFrame for its journey into an AWS FeatureGroup\n        self.prep_dataframe()\n\n    def delete_existing(self):\n        # Delete the existing FeatureSet if it exists\n        self.log.info(f\"Deleting the {self.output_name} FeatureSet...\")\n        FeatureSetCore.managed_delete(self.output_name)\n        time.sleep(1)\n\n    def _ensure_id_column(self):\n        \"\"\"Internal: AWS Feature Store requires an Id field\"\"\"\n        if self.id_column is None:\n            self.log.warning(\"Generating an 'auto_id' column, we highly recommended setting the 'id_column'\")\n            self.output_df[\"auto_id\"] = self.output_df.index\n            self.id_column = \"auto_id\"\n            return\n        if self.id_column not in self.output_df.columns:\n            error_msg = f\"Id column {self.id_column} not found in the DataFrame\"\n            self.log.critical(error_msg)\n            raise ValueError(error_msg)\n\n    def _ensure_event_time(self):\n        \"\"\"Internal: AWS Feature Store requires an event_time field for all data stored\"\"\"\n        if self.event_time_column is None or self.event_time_column not in self.output_df.columns:\n            self.log.info(\"Generating an event_time column before FeatureSet Creation...\")\n            self.event_time_column = \"event_time\"\n            self.output_df[self.event_time_column] = pd.Timestamp(\"now\", tz=\"UTC\")\n\n        # The event_time_column is defined, so we need to make sure it's in ISO-8601 string format\n        # Note: AWS Feature Store only a particular ISO-8601 format not ALL ISO-8601 formats\n        time_column = self.output_df[self.event_time_column]\n\n        # Check if the event_time_column is of type object or string convert it to DateTime\n        if time_column.dtypes == \"object\" or time_column.dtypes.name == \"string\":\n            self.log.info(f\"Converting {self.event_time_column} to DateTime...\")\n            time_column = pd.to_datetime(time_column)\n\n        # Let's make sure it the right type for Feature Store\n        if pd.api.types.is_datetime64_any_dtype(time_column):\n            self.log.info(f\"Converting {self.event_time_column} to ISOFormat Date String before FeatureSet Creation...\")\n\n            # Convert the datetime DType to ISO-8601 string\n            # TableFormat=ICEBERG does not support alternate formats for event_time field, it only supports String type.\n            time_column = time_column.map(datetime_to_iso8601)\n            self.output_df[self.event_time_column] = time_column.astype(\"string\")\n\n    def _convert_objs_to_string(self):\n        \"\"\"Internal: AWS Feature Store doesn't know how to store object dtypes, so convert to String\"\"\"\n        for col in self.output_df:\n            if pd.api.types.is_object_dtype(self.output_df[col].dtype):\n                self.output_df[col] = self.output_df[col].astype(pd.StringDtype())\n\n    def process_column_name(self, column: str, shorten: bool = False) -&gt; str:\n        \"\"\"Call various methods to make sure the column is ready for Feature Store\n        Args:\n            column (str): The column name to process\n            shorten (bool): Should we shorten the column name? (default: False)\n        \"\"\"\n        self.log.debug(f\"Processing column {column}...\")\n\n        # Make sure the column name is valid\n        column = self.sanitize_column_name(column)\n\n        # Make sure the column name isn't too long\n        if shorten:\n            column = self.shorten_column_name(column)\n\n        return column\n\n    def shorten_column_name(self, name, max_length=20):\n        if len(name) &lt;= max_length:\n            return name\n\n        # Start building the new name from the end\n        parts = name.split(\"_\")[::-1]\n        new_name = \"\"\n        for part in parts:\n            if len(new_name) + len(part) + 1 &lt;= max_length:  # +1 for the underscore\n                new_name = f\"{part}_{new_name}\" if new_name else part\n            else:\n                break\n\n        # If new_name is empty, just use the last part of the original name\n        if not new_name:\n            new_name = parts[0]\n\n        self.log.info(f\"Shortening {name} to {new_name}\")\n        return new_name\n\n    def sanitize_column_name(self, name):\n        # Remove all invalid characters\n        sanitized = re.sub(\"[^a-zA-Z0-9-_]\", \"_\", name)\n        sanitized = re.sub(\"_+\", \"_\", sanitized)\n        sanitized = sanitized.strip(\"_\")\n\n        # Log the change if the name was altered\n        if sanitized != name:\n            self.log.info(f\"Sanitizing {name} to {sanitized}\")\n\n        return sanitized\n\n    def one_hot_encode(self, df, one_hot_columns: list) -&gt; pd.DataFrame:\n        \"\"\"One Hot Encoding for Categorical Columns with additional column name management\n\n        Args:\n            df (pd.DataFrame): The DataFrame to process\n            one_hot_columns (list): The list of columns to one-hot encode\n\n        Returns:\n            pd.DataFrame: The DataFrame with one-hot encoded columns\n        \"\"\"\n\n        # Grab the current list of columns\n        current_columns = list(df.columns)\n\n        # Now convert the list of columns into Categorical and then One-Hot Encode\n        self.convert_columns_to_categorical(one_hot_columns)\n        self.log.important(f\"One-Hot encoding columns: {one_hot_columns}\")\n        df = pd.get_dummies(df, columns=one_hot_columns)\n\n        # Compute the new columns generated by get_dummies\n        new_columns = list(set(df.columns) - set(current_columns))\n        self.log.important(f\"New columns generated: {new_columns}\")\n\n        # Convert new columns to int32\n        df[new_columns] = df[new_columns].astype(\"int32\")\n\n        # For the new columns we're going to shorten the names\n        renamed_columns = {col: self.process_column_name(col) for col in new_columns}\n\n        # Rename the columns in the DataFrame\n        df.rename(columns=renamed_columns, inplace=True)\n\n        return df\n\n    # Helper Methods\n    def convert_columns_to_categorical(self, columns: list):\n        \"\"\"Convert column to Categorical type\"\"\"\n        for feature in columns:\n            if feature not in [self.event_time_column, self.id_column]:\n                unique_values = self.output_df[feature].nunique()\n                if 1 &lt; unique_values &lt; 10:\n                    self.log.important(f\"Converting column {feature} to categorical (unique {unique_values})\")\n                    self.output_df[feature] = self.output_df[feature].astype(\"category\")\n                else:\n                    self.log.warning(f\"Column {feature} too many unique values {unique_values} skipping...\")\n\n    def manual_categorical_converter(self):\n        \"\"\"Used for Streaming: Convert object and string types to Categorical\n\n        Note:\n            This method is used for streaming/chunking. You can set the\n            categorical_dtypes attribute to a dictionary of column names and\n            their respective categorical types.\n        \"\"\"\n        for column, cat_d_type in self.categorical_dtypes.items():\n            self.output_df[column] = self.output_df[column].astype(cat_d_type)\n\n    @staticmethod\n    def convert_column_types(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Convert the types of the DataFrame to the correct types for the Feature Store\"\"\"\n        for column in list(df.select_dtypes(include=\"bool\").columns):\n            df[column] = df[column].astype(\"int32\")\n        for column in list(df.select_dtypes(include=\"category\").columns):\n            df[column] = df[column].astype(\"str\")\n\n        # Select all columns that are of datetime dtype and convert them to ISO-8601 strings\n        for column in [col for col in df.columns if pd.api.types.is_datetime64_any_dtype(df[col])]:\n            df[column] = df[column].map(datetime_to_iso8601).astype(\"string\")\n\n        \"\"\"FIXME Not sure we need these conversions\n        for column in list(df.select_dtypes(include=\"object\").columns):\n            df[column] = df[column].astype(\"string\")\n        for column in list(df.select_dtypes(include=[pd.Int64Dtype]).columns):\n            df[column] = df[column].astype(\"int64\")\n        for column in list(df.select_dtypes(include=[pd.Float64Dtype]).columns):\n            df[column] = df[column].astype(\"float64\")\n        \"\"\"\n        return df\n\n    def prep_dataframe(self):\n        \"\"\"Prep the DataFrame for Feature Store Creation\"\"\"\n        self.log.info(\"Prep the output_df (cat_convert, convert types, and lowercase columns)...\")\n\n        # Remove any columns generated from AWS\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n        self.output_df = self.output_df.drop(columns=aws_cols, errors=\"ignore\")\n\n        # If one-hot columns are provided then one-hot encode them\n        if self.one_hot_columns:\n            self.output_df = self.one_hot_encode(self.output_df, self.one_hot_columns)\n\n        # Convert columns names to lowercase, Athena will not work with uppercase column names\n        if str(self.output_df.columns) != str(self.output_df.columns.str.lower()):\n            for c in self.output_df.columns:\n                if c != c.lower():\n                    self.log.important(f\"Column name {c} converted to lowercase: {c.lower()}\")\n            self.output_df.columns = self.output_df.columns.str.lower()\n\n        # Check for duplicate column names in the dataframe\n        if len(self.output_df.columns) != len(set(self.output_df.columns)):\n            self.log.critical(\"Duplicate column names detected in the DataFrame\")\n            duplicates = self.output_df.columns[self.output_df.columns.duplicated()].tolist()\n            self.log.critical(f\"Duplicated columns: {duplicates}\")\n            raise ValueError(\"Duplicate column names detected in the DataFrame\")\n\n        # Make sure we have the required id and event_time columns\n        self._ensure_id_column()\n        self._ensure_event_time()\n\n        # Check for a training column (Workbench uses dynamic training columns)\n        if \"training\" in self.output_df.columns:\n            self.log.important(\n                \"\"\"Training column detected: Since FeatureSets are read-only, Workbench creates a training view\n                that can be dynamically changed. We'll use this training column to create a training view.\"\"\"\n            )\n            self.incoming_hold_out_ids = self.output_df[~self.output_df[\"training\"]][self.id_column].tolist()\n            self.output_df = self.output_df.drop(columns=[\"training\"])\n\n        # We need to convert some of our column types to the correct types\n        # Feature Store only supports these data types:\n        # - Integral\n        # - Fractional\n        # - String (timestamp/datetime types need to be converted to string)\n        self.output_df = self.convert_column_types(self.output_df)\n\n    def create_feature_group(self):\n        \"\"\"Create a Feature Group, load our Feature Definitions, and wait for it to be ready\"\"\"\n\n        # Create a Feature Group and load our Feature Definitions\n        my_feature_group = FeatureGroup(name=self.output_name, sagemaker_session=self.sm_session)\n        my_feature_group.load_feature_definitions(data_frame=self.output_df)\n\n        # Create the Output S3 Storage Path for this Feature Set\n        s3_storage_path = f\"{self.feature_sets_s3_path}/{self.output_name}\"\n\n        # Get the metadata/tags to push into AWS\n        aws_tags = self.get_aws_tags()\n\n        # Create the Feature Group\n        my_feature_group.create(\n            s3_uri=s3_storage_path,\n            record_identifier_name=self.id_column,\n            event_time_feature_name=self.event_time_column,\n            role_arn=self.workbench_role_arn,\n            enable_online_store=True,\n            table_format=self.table_format,\n            tags=aws_tags,\n        )\n\n        # Ensure/wait for the feature group to be created\n        self.ensure_feature_group_created(my_feature_group)\n        return my_feature_group\n\n    def pre_transform(self, **kwargs):\n        \"\"\"Pre-Transform: Delete any existing FeatureSet and Create the Feature Group\"\"\"\n        self.delete_existing()\n        self.output_feature_group = self.create_feature_group()\n\n    def mac_spawn_hack(self):\n        \"\"\"Workaround for macOS Tahoe fork/spawn issue with SageMaker FeatureStore ingest.\n\n        See: https://github.com/aws/sagemaker-python-sdk/issues/5312\n        macOS Tahoe 26+ has issues with forked processes creating boto3 sessions.\n        This forces spawn mode on macOS to avoid the hang.\n        \"\"\"\n        import platform\n\n        if platform.system() == \"Darwin\":  # macOS\n            self.log.warning(\"macOS detected, forcing 'spawn' mode for multiprocessing (Tahoe hang workaround)\")\n            import multiprocessing\n\n            try:\n                import multiprocess\n\n                multiprocess.set_start_method(\"spawn\", force=True)\n            except (RuntimeError, ImportError):\n                pass  # Already set or multiprocess not available\n            try:\n                multiprocessing.set_start_method(\"spawn\", force=True)\n            except RuntimeError:\n                pass  # Already set\n\n    def transform_impl(self):\n        \"\"\"Transform Implementation: Ingest the data into the Feature Group\"\"\"\n\n        # Workaround for macOS Tahoe hang issue\n        self.mac_spawn_hack()\n\n        # Now we actually push the data into the Feature Group (called ingestion)\n        self.log.important(f\"Ingesting rows into Feature Group {self.output_name}...\")\n        ingest_manager = self.output_feature_group.ingest(self.output_df, max_workers=8, max_processes=4, wait=False)\n        try:\n            ingest_manager.wait()\n        except IngestionError as exc:\n            self.log.warning(f\"Some rows had an ingesting error: {exc}\")\n\n        # Report on any rows that failed to ingest\n        if ingest_manager.failed_rows:\n            self.log.warning(f\"Number of Failed Rows: {len(ingest_manager.failed_rows)}\")\n\n            # Log failed row details\n            failed_data = self.output_df.iloc[ingest_manager.failed_rows]\n            for idx, row in failed_data.iterrows():\n                self.log.warning(f\"Failed Row {idx}: {row.to_dict()}\")\n\n        # Keep track of the number of rows we expect to be ingested\n        self.expected_rows += len(self.output_df) - len(ingest_manager.failed_rows)\n        self.log.info(f\"Added rows: {len(self.output_df)}\")\n        self.log.info(f\"Failed rows: {len(ingest_manager.failed_rows)}\")\n        self.log.info(f\"Total rows ingested: {self.expected_rows}\")\n\n        # We often need to wait a bit for AWS to fully register the new Feature Group\n        self.log.important(f\"Waiting for AWS to register the new Feature Group {self.output_name}...\")\n        time.sleep(30)\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Populating Offline Storage and onboard()\"\"\"\n        self.log.info(\"Post-Transform: Populating Offline Storage and onboard()...\")\n\n        # Feature Group Ingestion takes a while, so we need to wait for it to finish\n        self.output_feature_set = FeatureSetCore(self.output_name)\n        self.log.important(\"Waiting for AWS Feature Group Offline storage to be ready...\")\n        self.log.important(\"This will often take 10-20 minutes...go have coffee or lunch :)\")\n        self.output_feature_set.set_status(\"initializing\")\n        self.wait_for_rows(self.expected_rows)\n\n        # Call the FeatureSet onboard method to compute a bunch of EDA stuff\n        self.output_feature_set.onboard()\n\n        # Set Hold Out Ids (if we got them during creation)\n        if self.incoming_hold_out_ids:\n            self.output_feature_set.set_training_holdouts(self.incoming_hold_out_ids)\n\n    def ensure_feature_group_created(self, feature_group):\n        status = feature_group.describe().get(\"FeatureGroupStatus\")\n        while status == \"Creating\":\n            self.log.debug(\"FeatureSet being Created\u2026\")\n            time.sleep(5)\n            status = feature_group.describe().get(\"FeatureGroupStatus\")\n\n        if status == \"Created\":\n            self.log.info(f\"FeatureSet {feature_group.name} successfully created\")\n        else:\n            # Get the detailed failure reason\n            description = feature_group.describe()\n            failure_reason = description.get(\"FailureReason\", \"No failure reason provided\")\n            self.log.critical(f\"FeatureSet {feature_group.name} creation failed with status: {status}\")\n            self.log.critical(f\"Failure reason: {failure_reason}\")\n\n    def wait_for_rows(self, expected_rows: int):\n        \"\"\"Wait for AWS Feature Group to fully populate the Offline Storage\"\"\"\n        rows = self.output_feature_set.num_rows()\n\n        # Wait for the rows to be populated\n        self.log.info(f\"Waiting for AWS Feature Group {self.output_name} Offline Storage...\")\n        max_retry = 20\n        num_retry = 0\n        sleep_time = 30\n        while rows &lt; expected_rows and num_retry &lt; max_retry:\n            num_retry += 1\n            time.sleep(sleep_time)\n            rows = self.output_feature_set.num_rows()\n            self.log.info(f\"Checking Offline Storage {self.output_name}: {rows}/{expected_rows} rows\")\n        if rows == expected_rows:\n            self.log.important(f\"Success: Reached Expected Rows ({rows} rows)...\")\n        else:\n            msg = f\"Did not reach expected rows ({rows}/{expected_rows})...(probably AWS lag)\"\n            self.log.warning(msg)\n            self.log.monitor(msg)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.__init__","title":"<code>__init__(output_name)</code>","text":"<p>PandasToFeatures Initialization</p> <p>Parameters:</p> Name Type Description Default <code>output_name</code> <code>str</code> <p>The Name of the FeatureSet to create</p> required Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def __init__(self, output_name: str):\n    \"\"\"PandasToFeatures Initialization\n\n    Args:\n        output_name (str): The Name of the FeatureSet to create\n    \"\"\"\n\n    # Make sure the output_name is a valid name\n    Artifact.is_name_valid(output_name)\n\n    # Call superclass init\n    super().__init__(\"DataFrame\", output_name)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.PANDAS_DF\n    self.output_type = TransformOutput.FEATURE_SET\n    self.id_column = None\n    self.event_time_column = None\n    self.one_hot_columns = []\n    self.categorical_dtypes = {}  # Used for streaming/chunking\n    self.output_df = None\n    self.table_format = TableFormatEnum.ICEBERG\n    self.incoming_hold_out_ids = None\n\n    # These will be set in the transform method\n    self.output_feature_group = None\n    self.output_feature_set = None\n    self.expected_rows = 0\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.convert_column_types","title":"<code>convert_column_types(df)</code>  <code>staticmethod</code>","text":"<p>Convert the types of the DataFrame to the correct types for the Feature Store</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>@staticmethod\ndef convert_column_types(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Convert the types of the DataFrame to the correct types for the Feature Store\"\"\"\n    for column in list(df.select_dtypes(include=\"bool\").columns):\n        df[column] = df[column].astype(\"int32\")\n    for column in list(df.select_dtypes(include=\"category\").columns):\n        df[column] = df[column].astype(\"str\")\n\n    # Select all columns that are of datetime dtype and convert them to ISO-8601 strings\n    for column in [col for col in df.columns if pd.api.types.is_datetime64_any_dtype(df[col])]:\n        df[column] = df[column].map(datetime_to_iso8601).astype(\"string\")\n\n    \"\"\"FIXME Not sure we need these conversions\n    for column in list(df.select_dtypes(include=\"object\").columns):\n        df[column] = df[column].astype(\"string\")\n    for column in list(df.select_dtypes(include=[pd.Int64Dtype]).columns):\n        df[column] = df[column].astype(\"int64\")\n    for column in list(df.select_dtypes(include=[pd.Float64Dtype]).columns):\n        df[column] = df[column].astype(\"float64\")\n    \"\"\"\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.convert_columns_to_categorical","title":"<code>convert_columns_to_categorical(columns)</code>","text":"<p>Convert column to Categorical type</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def convert_columns_to_categorical(self, columns: list):\n    \"\"\"Convert column to Categorical type\"\"\"\n    for feature in columns:\n        if feature not in [self.event_time_column, self.id_column]:\n            unique_values = self.output_df[feature].nunique()\n            if 1 &lt; unique_values &lt; 10:\n                self.log.important(f\"Converting column {feature} to categorical (unique {unique_values})\")\n                self.output_df[feature] = self.output_df[feature].astype(\"category\")\n            else:\n                self.log.warning(f\"Column {feature} too many unique values {unique_values} skipping...\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.create_feature_group","title":"<code>create_feature_group()</code>","text":"<p>Create a Feature Group, load our Feature Definitions, and wait for it to be ready</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def create_feature_group(self):\n    \"\"\"Create a Feature Group, load our Feature Definitions, and wait for it to be ready\"\"\"\n\n    # Create a Feature Group and load our Feature Definitions\n    my_feature_group = FeatureGroup(name=self.output_name, sagemaker_session=self.sm_session)\n    my_feature_group.load_feature_definitions(data_frame=self.output_df)\n\n    # Create the Output S3 Storage Path for this Feature Set\n    s3_storage_path = f\"{self.feature_sets_s3_path}/{self.output_name}\"\n\n    # Get the metadata/tags to push into AWS\n    aws_tags = self.get_aws_tags()\n\n    # Create the Feature Group\n    my_feature_group.create(\n        s3_uri=s3_storage_path,\n        record_identifier_name=self.id_column,\n        event_time_feature_name=self.event_time_column,\n        role_arn=self.workbench_role_arn,\n        enable_online_store=True,\n        table_format=self.table_format,\n        tags=aws_tags,\n    )\n\n    # Ensure/wait for the feature group to be created\n    self.ensure_feature_group_created(my_feature_group)\n    return my_feature_group\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.mac_spawn_hack","title":"<code>mac_spawn_hack()</code>","text":"<p>Workaround for macOS Tahoe fork/spawn issue with SageMaker FeatureStore ingest.</p> <p>See: https://github.com/aws/sagemaker-python-sdk/issues/5312 macOS Tahoe 26+ has issues with forked processes creating boto3 sessions. This forces spawn mode on macOS to avoid the hang.</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def mac_spawn_hack(self):\n    \"\"\"Workaround for macOS Tahoe fork/spawn issue with SageMaker FeatureStore ingest.\n\n    See: https://github.com/aws/sagemaker-python-sdk/issues/5312\n    macOS Tahoe 26+ has issues with forked processes creating boto3 sessions.\n    This forces spawn mode on macOS to avoid the hang.\n    \"\"\"\n    import platform\n\n    if platform.system() == \"Darwin\":  # macOS\n        self.log.warning(\"macOS detected, forcing 'spawn' mode for multiprocessing (Tahoe hang workaround)\")\n        import multiprocessing\n\n        try:\n            import multiprocess\n\n            multiprocess.set_start_method(\"spawn\", force=True)\n        except (RuntimeError, ImportError):\n            pass  # Already set or multiprocess not available\n        try:\n            multiprocessing.set_start_method(\"spawn\", force=True)\n        except RuntimeError:\n            pass  # Already set\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.manual_categorical_converter","title":"<code>manual_categorical_converter()</code>","text":"<p>Used for Streaming: Convert object and string types to Categorical</p> Note <p>This method is used for streaming/chunking. You can set the categorical_dtypes attribute to a dictionary of column names and their respective categorical types.</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def manual_categorical_converter(self):\n    \"\"\"Used for Streaming: Convert object and string types to Categorical\n\n    Note:\n        This method is used for streaming/chunking. You can set the\n        categorical_dtypes attribute to a dictionary of column names and\n        their respective categorical types.\n    \"\"\"\n    for column, cat_d_type in self.categorical_dtypes.items():\n        self.output_df[column] = self.output_df[column].astype(cat_d_type)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.one_hot_encode","title":"<code>one_hot_encode(df, one_hot_columns)</code>","text":"<p>One Hot Encoding for Categorical Columns with additional column name management</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to process</p> required <code>one_hot_columns</code> <code>list</code> <p>The list of columns to one-hot encode</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with one-hot encoded columns</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def one_hot_encode(self, df, one_hot_columns: list) -&gt; pd.DataFrame:\n    \"\"\"One Hot Encoding for Categorical Columns with additional column name management\n\n    Args:\n        df (pd.DataFrame): The DataFrame to process\n        one_hot_columns (list): The list of columns to one-hot encode\n\n    Returns:\n        pd.DataFrame: The DataFrame with one-hot encoded columns\n    \"\"\"\n\n    # Grab the current list of columns\n    current_columns = list(df.columns)\n\n    # Now convert the list of columns into Categorical and then One-Hot Encode\n    self.convert_columns_to_categorical(one_hot_columns)\n    self.log.important(f\"One-Hot encoding columns: {one_hot_columns}\")\n    df = pd.get_dummies(df, columns=one_hot_columns)\n\n    # Compute the new columns generated by get_dummies\n    new_columns = list(set(df.columns) - set(current_columns))\n    self.log.important(f\"New columns generated: {new_columns}\")\n\n    # Convert new columns to int32\n    df[new_columns] = df[new_columns].astype(\"int32\")\n\n    # For the new columns we're going to shorten the names\n    renamed_columns = {col: self.process_column_name(col) for col in new_columns}\n\n    # Rename the columns in the DataFrame\n    df.rename(columns=renamed_columns, inplace=True)\n\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Populating Offline Storage and onboard()</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Populating Offline Storage and onboard()\"\"\"\n    self.log.info(\"Post-Transform: Populating Offline Storage and onboard()...\")\n\n    # Feature Group Ingestion takes a while, so we need to wait for it to finish\n    self.output_feature_set = FeatureSetCore(self.output_name)\n    self.log.important(\"Waiting for AWS Feature Group Offline storage to be ready...\")\n    self.log.important(\"This will often take 10-20 minutes...go have coffee or lunch :)\")\n    self.output_feature_set.set_status(\"initializing\")\n    self.wait_for_rows(self.expected_rows)\n\n    # Call the FeatureSet onboard method to compute a bunch of EDA stuff\n    self.output_feature_set.onboard()\n\n    # Set Hold Out Ids (if we got them during creation)\n    if self.incoming_hold_out_ids:\n        self.output_feature_set.set_training_holdouts(self.incoming_hold_out_ids)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.pre_transform","title":"<code>pre_transform(**kwargs)</code>","text":"<p>Pre-Transform: Delete any existing FeatureSet and Create the Feature Group</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def pre_transform(self, **kwargs):\n    \"\"\"Pre-Transform: Delete any existing FeatureSet and Create the Feature Group\"\"\"\n    self.delete_existing()\n    self.output_feature_group = self.create_feature_group()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.prep_dataframe","title":"<code>prep_dataframe()</code>","text":"<p>Prep the DataFrame for Feature Store Creation</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def prep_dataframe(self):\n    \"\"\"Prep the DataFrame for Feature Store Creation\"\"\"\n    self.log.info(\"Prep the output_df (cat_convert, convert types, and lowercase columns)...\")\n\n    # Remove any columns generated from AWS\n    aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n    self.output_df = self.output_df.drop(columns=aws_cols, errors=\"ignore\")\n\n    # If one-hot columns are provided then one-hot encode them\n    if self.one_hot_columns:\n        self.output_df = self.one_hot_encode(self.output_df, self.one_hot_columns)\n\n    # Convert columns names to lowercase, Athena will not work with uppercase column names\n    if str(self.output_df.columns) != str(self.output_df.columns.str.lower()):\n        for c in self.output_df.columns:\n            if c != c.lower():\n                self.log.important(f\"Column name {c} converted to lowercase: {c.lower()}\")\n        self.output_df.columns = self.output_df.columns.str.lower()\n\n    # Check for duplicate column names in the dataframe\n    if len(self.output_df.columns) != len(set(self.output_df.columns)):\n        self.log.critical(\"Duplicate column names detected in the DataFrame\")\n        duplicates = self.output_df.columns[self.output_df.columns.duplicated()].tolist()\n        self.log.critical(f\"Duplicated columns: {duplicates}\")\n        raise ValueError(\"Duplicate column names detected in the DataFrame\")\n\n    # Make sure we have the required id and event_time columns\n    self._ensure_id_column()\n    self._ensure_event_time()\n\n    # Check for a training column (Workbench uses dynamic training columns)\n    if \"training\" in self.output_df.columns:\n        self.log.important(\n            \"\"\"Training column detected: Since FeatureSets are read-only, Workbench creates a training view\n            that can be dynamically changed. We'll use this training column to create a training view.\"\"\"\n        )\n        self.incoming_hold_out_ids = self.output_df[~self.output_df[\"training\"]][self.id_column].tolist()\n        self.output_df = self.output_df.drop(columns=[\"training\"])\n\n    # We need to convert some of our column types to the correct types\n    # Feature Store only supports these data types:\n    # - Integral\n    # - Fractional\n    # - String (timestamp/datetime types need to be converted to string)\n    self.output_df = self.convert_column_types(self.output_df)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.process_column_name","title":"<code>process_column_name(column, shorten=False)</code>","text":"<p>Call various methods to make sure the column is ready for Feature Store Args:     column (str): The column name to process     shorten (bool): Should we shorten the column name? (default: False)</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def process_column_name(self, column: str, shorten: bool = False) -&gt; str:\n    \"\"\"Call various methods to make sure the column is ready for Feature Store\n    Args:\n        column (str): The column name to process\n        shorten (bool): Should we shorten the column name? (default: False)\n    \"\"\"\n    self.log.debug(f\"Processing column {column}...\")\n\n    # Make sure the column name is valid\n    column = self.sanitize_column_name(column)\n\n    # Make sure the column name isn't too long\n    if shorten:\n        column = self.shorten_column_name(column)\n\n    return column\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.set_input","title":"<code>set_input(input_df, id_column=None, event_time_column=None, one_hot_columns=None)</code>","text":"<p>Set the Input DataFrame for this Transform</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>id_column</code> <code>str</code> <p>The ID column (if not specified, an 'auto_id' will be generated).</p> <code>None</code> <code>event_time_column</code> <code>str</code> <p>The name of the event time column (default: None).</p> <code>None</code> <code>one_hot_columns</code> <code>list</code> <p>The list of columns to one-hot encode (default: None).</p> <code>None</code> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def set_input(self, input_df: pd.DataFrame, id_column=None, event_time_column=None, one_hot_columns=None):\n    \"\"\"Set the Input DataFrame for this Transform\n\n    Args:\n        input_df (pd.DataFrame): The input DataFrame.\n        id_column (str, optional): The ID column (if not specified, an 'auto_id' will be generated).\n        event_time_column (str, optional): The name of the event time column (default: None).\n        one_hot_columns (list, optional): The list of columns to one-hot encode (default: None).\n    \"\"\"\n    self.id_column = id_column\n    self.event_time_column = event_time_column\n    self.output_df = input_df.copy()\n    self.one_hot_columns = one_hot_columns or []\n\n    # Warn about known AWS Iceberg bug with event_time_column\n    if event_time_column is not None:\n        self.log.warning(\n            f\"event_time_column='{event_time_column}' specified. Note: AWS has a known bug with \"\n            \"Iceberg FeatureGroups where varying event times across multiple days can cause \"\n            \"duplicate rows in the offline store. Setting event_time_column=None.\"\n        )\n        self.event_time_column = None\n\n    # Now Prepare the DataFrame for its journey into an AWS FeatureGroup\n    self.prep_dataframe()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.transform_impl","title":"<code>transform_impl()</code>","text":"<p>Transform Implementation: Ingest the data into the Feature Group</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def transform_impl(self):\n    \"\"\"Transform Implementation: Ingest the data into the Feature Group\"\"\"\n\n    # Workaround for macOS Tahoe hang issue\n    self.mac_spawn_hack()\n\n    # Now we actually push the data into the Feature Group (called ingestion)\n    self.log.important(f\"Ingesting rows into Feature Group {self.output_name}...\")\n    ingest_manager = self.output_feature_group.ingest(self.output_df, max_workers=8, max_processes=4, wait=False)\n    try:\n        ingest_manager.wait()\n    except IngestionError as exc:\n        self.log.warning(f\"Some rows had an ingesting error: {exc}\")\n\n    # Report on any rows that failed to ingest\n    if ingest_manager.failed_rows:\n        self.log.warning(f\"Number of Failed Rows: {len(ingest_manager.failed_rows)}\")\n\n        # Log failed row details\n        failed_data = self.output_df.iloc[ingest_manager.failed_rows]\n        for idx, row in failed_data.iterrows():\n            self.log.warning(f\"Failed Row {idx}: {row.to_dict()}\")\n\n    # Keep track of the number of rows we expect to be ingested\n    self.expected_rows += len(self.output_df) - len(ingest_manager.failed_rows)\n    self.log.info(f\"Added rows: {len(self.output_df)}\")\n    self.log.info(f\"Failed rows: {len(ingest_manager.failed_rows)}\")\n    self.log.info(f\"Total rows ingested: {self.expected_rows}\")\n\n    # We often need to wait a bit for AWS to fully register the new Feature Group\n    self.log.important(f\"Waiting for AWS to register the new Feature Group {self.output_name}...\")\n    time.sleep(30)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeatures.wait_for_rows","title":"<code>wait_for_rows(expected_rows)</code>","text":"<p>Wait for AWS Feature Group to fully populate the Offline Storage</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def wait_for_rows(self, expected_rows: int):\n    \"\"\"Wait for AWS Feature Group to fully populate the Offline Storage\"\"\"\n    rows = self.output_feature_set.num_rows()\n\n    # Wait for the rows to be populated\n    self.log.info(f\"Waiting for AWS Feature Group {self.output_name} Offline Storage...\")\n    max_retry = 20\n    num_retry = 0\n    sleep_time = 30\n    while rows &lt; expected_rows and num_retry &lt; max_retry:\n        num_retry += 1\n        time.sleep(sleep_time)\n        rows = self.output_feature_set.num_rows()\n        self.log.info(f\"Checking Offline Storage {self.output_name}: {rows}/{expected_rows} rows\")\n    if rows == expected_rows:\n        self.log.important(f\"Success: Reached Expected Rows ({rows} rows)...\")\n    else:\n        msg = f\"Did not reach expected rows ({rows}/{expected_rows})...(probably AWS lag)\"\n        self.log.warning(msg)\n        self.log.monitor(msg)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeaturesChunked","title":"<code>PandasToFeaturesChunked</code>","text":"<p>               Bases: <code>Transform</code></p> <p>PandasToFeaturesChunked:  Class to manage a bunch of chunked Pandas DataFrames into a FeatureSet</p> Common Usage <pre><code>to_features = PandasToFeaturesChunked(output_name, id_column=\"id\"/None, event_time_column=\"date\"/None)\nto_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\ncat_column_info = {\"sex\": [\"M\", \"F\", \"I\"]}\nto_features.set_categorical_info(cat_column_info)\nto_features.add_chunk(df)\nto_features.add_chunk(df)\n...\nto_features.finalize()\n</code></pre> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>class PandasToFeaturesChunked(Transform):\n    \"\"\"PandasToFeaturesChunked:  Class to manage a bunch of chunked Pandas DataFrames into a FeatureSet\n\n    Common Usage:\n        ```python\n        to_features = PandasToFeaturesChunked(output_name, id_column=\"id\"/None, event_time_column=\"date\"/None)\n        to_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\n        cat_column_info = {\"sex\": [\"M\", \"F\", \"I\"]}\n        to_features.set_categorical_info(cat_column_info)\n        to_features.add_chunk(df)\n        to_features.add_chunk(df)\n        ...\n        to_features.finalize()\n        ```\n    \"\"\"\n\n    def __init__(self, output_name: str, id_column=None, event_time_column=None):\n        \"\"\"PandasToFeaturesChunked Initialization\"\"\"\n\n        # Make sure the output_name is a valid name\n        Artifact.is_name_valid(output_name)\n\n        # Call superclass init\n        super().__init__(\"DataFrame\", output_name)\n\n        # Set up all my instance attributes\n        self.id_column = id_column\n        self.event_time_column = event_time_column\n        self.first_chunk = None\n        self.pandas_to_features = PandasToFeatures(output_name)\n\n    def set_categorical_info(self, cat_column_info: dict[list[str]]):\n        \"\"\"Set the Categorical Columns\n        Args:\n            cat_column_info (dict[list[str]]): Dictionary of categorical columns and their possible values\n        \"\"\"\n\n        # Create the CategoricalDtypes\n        cat_d_types = {}\n        for col, vals in cat_column_info.items():\n            cat_d_types[col] = CategoricalDtype(categories=vals)\n\n        # Now set the CategoricalDtypes on our underlying PandasToFeatures\n        self.pandas_to_features.categorical_dtypes = cat_d_types\n\n    def add_chunk(self, chunk_df: pd.DataFrame):\n        \"\"\"Add a Chunk of Data to the FeatureSet\"\"\"\n\n        # Is this the first chunk? If so we need to run the pre_transform\n        if self.first_chunk is None:\n            self.log.info(f\"Adding first chunk {chunk_df.shape}...\")\n            self.first_chunk = chunk_df\n            self.pandas_to_features.set_input(chunk_df, self.id_column, self.event_time_column)\n            self.pandas_to_features.pre_transform()\n            self.pandas_to_features.transform_impl()\n        else:\n            self.log.info(f\"Adding chunk {chunk_df.shape}...\")\n            self.pandas_to_features.set_input(chunk_df, self.id_column, self.event_time_column)\n            self.pandas_to_features.transform_impl()\n\n    def pre_transform(self, **kwargs):\n        \"\"\"Pre-Transform: Create the Feature Group with Chunked Data\"\"\"\n\n        # Loading data into a Feature Group takes a while, so set status to loading\n        FeatureSetCore(self.output_name).set_status(\"loading\")\n\n    def transform_impl(self):\n        \"\"\"Required implementation of the Transform interface\"\"\"\n        self.log.warning(\"PandasToFeaturesChunked.transform_impl() called.  This is a no-op.\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Any Post Transform Steps\"\"\"\n        self.pandas_to_features.post_transform()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeaturesChunked.__init__","title":"<code>__init__(output_name, id_column=None, event_time_column=None)</code>","text":"<p>PandasToFeaturesChunked Initialization</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def __init__(self, output_name: str, id_column=None, event_time_column=None):\n    \"\"\"PandasToFeaturesChunked Initialization\"\"\"\n\n    # Make sure the output_name is a valid name\n    Artifact.is_name_valid(output_name)\n\n    # Call superclass init\n    super().__init__(\"DataFrame\", output_name)\n\n    # Set up all my instance attributes\n    self.id_column = id_column\n    self.event_time_column = event_time_column\n    self.first_chunk = None\n    self.pandas_to_features = PandasToFeatures(output_name)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeaturesChunked.add_chunk","title":"<code>add_chunk(chunk_df)</code>","text":"<p>Add a Chunk of Data to the FeatureSet</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def add_chunk(self, chunk_df: pd.DataFrame):\n    \"\"\"Add a Chunk of Data to the FeatureSet\"\"\"\n\n    # Is this the first chunk? If so we need to run the pre_transform\n    if self.first_chunk is None:\n        self.log.info(f\"Adding first chunk {chunk_df.shape}...\")\n        self.first_chunk = chunk_df\n        self.pandas_to_features.set_input(chunk_df, self.id_column, self.event_time_column)\n        self.pandas_to_features.pre_transform()\n        self.pandas_to_features.transform_impl()\n    else:\n        self.log.info(f\"Adding chunk {chunk_df.shape}...\")\n        self.pandas_to_features.set_input(chunk_df, self.id_column, self.event_time_column)\n        self.pandas_to_features.transform_impl()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeaturesChunked.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Any Post Transform Steps</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Any Post Transform Steps\"\"\"\n    self.pandas_to_features.post_transform()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeaturesChunked.pre_transform","title":"<code>pre_transform(**kwargs)</code>","text":"<p>Pre-Transform: Create the Feature Group with Chunked Data</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def pre_transform(self, **kwargs):\n    \"\"\"Pre-Transform: Create the Feature Group with Chunked Data\"\"\"\n\n    # Loading data into a Feature Group takes a while, so set status to loading\n    FeatureSetCore(self.output_name).set_status(\"loading\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeaturesChunked.set_categorical_info","title":"<code>set_categorical_info(cat_column_info)</code>","text":"<p>Set the Categorical Columns Args:     cat_column_info (dict[list[str]]): Dictionary of categorical columns and their possible values</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def set_categorical_info(self, cat_column_info: dict[list[str]]):\n    \"\"\"Set the Categorical Columns\n    Args:\n        cat_column_info (dict[list[str]]): Dictionary of categorical columns and their possible values\n    \"\"\"\n\n    # Create the CategoricalDtypes\n    cat_d_types = {}\n    for col, vals in cat_column_info.items():\n        cat_d_types[col] = CategoricalDtype(categories=vals)\n\n    # Now set the CategoricalDtypes on our underlying PandasToFeatures\n    self.pandas_to_features.categorical_dtypes = cat_d_types\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#workbench.core.transforms.pandas_transforms.PandasToFeaturesChunked.transform_impl","title":"<code>transform_impl()</code>","text":"<p>Required implementation of the Transform interface</p> Source code in <code>src/workbench/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def transform_impl(self):\n    \"\"\"Required implementation of the Transform interface\"\"\"\n    self.log.warning(\"PandasToFeaturesChunked.transform_impl() called.  This is a no-op.\")\n</code></pre>"},{"location":"core_classes/transforms/transform/","title":"Transform","text":"<p>API Classes</p> <p>The API Classes will use Transforms internally. So model.to_endpoint() uses the ModelToEndpoint() transform. If you need more control over the Transform you can use the Core Classes directly.</p> <p>The Workbench Transform class is a base/abstract class that defines API implemented by all the child classes (DataLoaders, DataSourceToFeatureSet, ModelToEndpoint, etc).</p> <p>Transform: Base Class for all transforms within Workbench Inherited Classes must implement the abstract transform_impl() method</p>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform","title":"<code>Transform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Transform: Abstract Base Class for all transforms within Workbench. Inherited Classes must implement the abstract transform_impl() method</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>class Transform(ABC):\n    \"\"\"Transform: Abstract Base Class for all transforms within Workbench. Inherited Classes\n    must implement the abstract transform_impl() method\"\"\"\n\n    def __init__(self, input_name: str, output_name: str, catalog_db: str = \"workbench\"):\n        \"\"\"Transform Initialization\n\n        Args:\n            input_name (str): The Name of the Input Artifact\n            output_name (str): The Name of the Output Artifact\n            catalog_db (str): The AWS Data Catalog Database to use (default: \"workbench\")\n        \"\"\"\n\n        self.log = logging.getLogger(\"workbench\")\n        self.input_type = None\n        self.output_type = None\n        self.output_tags = \"\"\n        self.input_name = str(input_name)  # Occasionally we get a pathlib.Path object\n        self.output_name = str(output_name)  # Occasionally we get a pathlib.Path object\n        self.output_meta = {\"workbench_input\": self.input_name}\n        self.data_catalog_db = catalog_db\n\n        # Grab our Workbench Bucket\n        cm = ConfigManager()\n        if not cm.config_okay():\n            self.log.error(\"Workbench Configuration Incomplete...\")\n            self.log.error(\"Run the 'workbench' command and follow the prompts...\")\n            raise FatalConfigError()\n        self.workbench_bucket = cm.get_config(\"WORKBENCH_BUCKET\")\n        self.log.important(f\"Transform.__init__: WORKBENCH_BUCKET = {self.workbench_bucket}\")\n        self.data_sources_s3_path = \"s3://\" + self.workbench_bucket + \"/data-sources\"\n        self.feature_sets_s3_path = \"s3://\" + self.workbench_bucket + \"/feature-sets\"\n        self.models_s3_path = \"s3://\" + self.workbench_bucket + \"/models\"\n        self.endpoints_sets_s3_path = \"s3://\" + self.workbench_bucket + \"/endpoints\"\n\n        # Grab a Workbench Role ARN, Boto3, SageMaker Session, and SageMaker Client\n        self.aws_account_clamp = AWSAccountClamp()\n        self.workbench_role_arn = self.aws_account_clamp.aws_session.get_workbench_execution_role_arn()\n        self.boto3_session = self.aws_account_clamp.boto3_session\n        self.sm_session = self.aws_account_clamp.sagemaker_session()\n        self.sm_client = self.aws_account_clamp.sagemaker_client()\n\n        # Delimiter for storing lists in AWS Tags\n        self.tag_delimiter = \"::\"\n\n    @abstractmethod\n    def transform_impl(self, **kwargs):\n        \"\"\"Abstract Method: Implement the Transformation from Input to Output\"\"\"\n        pass\n\n    def pre_transform(self, **kwargs):\n        \"\"\"Perform any Pre-Transform operations\"\"\"\n        self.log.debug(\"Pre-Transform...\")\n\n    @abstractmethod\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform ensures that the output Artifact is ready for use\"\"\"\n        pass\n\n    def set_output_tags(self, tags: Union[list, str]):\n        \"\"\"Set the tags that will be associated with the output object\n        Args:\n            tags (Union[list, str]): The list of tags or a '::' separated string of tags\"\"\"\n        if isinstance(tags, list):\n            self.output_tags = self.tag_delimiter.join(tags)\n        else:\n            self.output_tags = tags\n\n    def add_output_meta(self, meta: dict):\n        \"\"\"Add additional metadata that will be associated with the output artifact\n        Args:\n            meta (dict): A dictionary of metadata\"\"\"\n        self.output_meta = self.output_meta | meta\n\n    @staticmethod\n    def convert_to_aws_tags(metadata: dict):\n        \"\"\"Convert a dictionary to the AWS tag format (list of dicts)\n        [ {Key: key_name, Value: value}, {..}, ...]\"\"\"\n        return [{\"Key\": key, \"Value\": value} for key, value in metadata.items()]\n\n    def get_aws_tags(self):\n        \"\"\"Get the metadata/tags and convert them into AWS Tag Format\"\"\"\n        # Set up our metadata storage\n        workbench_meta = {\"workbench_tags\": self.output_tags}\n        for key, value in self.output_meta.items():\n            workbench_meta[key] = value\n        aws_tags = self.convert_to_aws_tags(workbench_meta)\n        return aws_tags\n\n    @final\n    def transform(self, **kwargs):\n        \"\"\"Perform the Transformation from Input to Output with pre_transform() and post_transform() invocations\"\"\"\n        self.pre_transform(**kwargs)\n        self.transform_impl(**kwargs)\n        self.post_transform(**kwargs)\n\n    def input_type(self) -&gt; TransformInput:\n        \"\"\"What Input Type does this Transform Consume\"\"\"\n        return self.input_type\n\n    def output_type(self) -&gt; TransformOutput:\n        \"\"\"What Output Type does this Transform Produce\"\"\"\n        return self.output_type\n\n    def set_input_name(self, input_name: str):\n        \"\"\"Set the Input Name (Name) for this Transform\"\"\"\n        self.input_name = input_name\n\n    def set_output_name(self, output_name: str):\n        \"\"\"Set the Output Name (Name) for this Transform\"\"\"\n        self.output_name = output_name\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.__init__","title":"<code>__init__(input_name, output_name, catalog_db='workbench')</code>","text":"<p>Transform Initialization</p> <p>Parameters:</p> Name Type Description Default <code>input_name</code> <code>str</code> <p>The Name of the Input Artifact</p> required <code>output_name</code> <code>str</code> <p>The Name of the Output Artifact</p> required <code>catalog_db</code> <code>str</code> <p>The AWS Data Catalog Database to use (default: \"workbench\")</p> <code>'workbench'</code> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def __init__(self, input_name: str, output_name: str, catalog_db: str = \"workbench\"):\n    \"\"\"Transform Initialization\n\n    Args:\n        input_name (str): The Name of the Input Artifact\n        output_name (str): The Name of the Output Artifact\n        catalog_db (str): The AWS Data Catalog Database to use (default: \"workbench\")\n    \"\"\"\n\n    self.log = logging.getLogger(\"workbench\")\n    self.input_type = None\n    self.output_type = None\n    self.output_tags = \"\"\n    self.input_name = str(input_name)  # Occasionally we get a pathlib.Path object\n    self.output_name = str(output_name)  # Occasionally we get a pathlib.Path object\n    self.output_meta = {\"workbench_input\": self.input_name}\n    self.data_catalog_db = catalog_db\n\n    # Grab our Workbench Bucket\n    cm = ConfigManager()\n    if not cm.config_okay():\n        self.log.error(\"Workbench Configuration Incomplete...\")\n        self.log.error(\"Run the 'workbench' command and follow the prompts...\")\n        raise FatalConfigError()\n    self.workbench_bucket = cm.get_config(\"WORKBENCH_BUCKET\")\n    self.log.important(f\"Transform.__init__: WORKBENCH_BUCKET = {self.workbench_bucket}\")\n    self.data_sources_s3_path = \"s3://\" + self.workbench_bucket + \"/data-sources\"\n    self.feature_sets_s3_path = \"s3://\" + self.workbench_bucket + \"/feature-sets\"\n    self.models_s3_path = \"s3://\" + self.workbench_bucket + \"/models\"\n    self.endpoints_sets_s3_path = \"s3://\" + self.workbench_bucket + \"/endpoints\"\n\n    # Grab a Workbench Role ARN, Boto3, SageMaker Session, and SageMaker Client\n    self.aws_account_clamp = AWSAccountClamp()\n    self.workbench_role_arn = self.aws_account_clamp.aws_session.get_workbench_execution_role_arn()\n    self.boto3_session = self.aws_account_clamp.boto3_session\n    self.sm_session = self.aws_account_clamp.sagemaker_session()\n    self.sm_client = self.aws_account_clamp.sagemaker_client()\n\n    # Delimiter for storing lists in AWS Tags\n    self.tag_delimiter = \"::\"\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.add_output_meta","title":"<code>add_output_meta(meta)</code>","text":"<p>Add additional metadata that will be associated with the output artifact Args:     meta (dict): A dictionary of metadata</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def add_output_meta(self, meta: dict):\n    \"\"\"Add additional metadata that will be associated with the output artifact\n    Args:\n        meta (dict): A dictionary of metadata\"\"\"\n    self.output_meta = self.output_meta | meta\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.convert_to_aws_tags","title":"<code>convert_to_aws_tags(metadata)</code>  <code>staticmethod</code>","text":"<p>Convert a dictionary to the AWS tag format (list of dicts) [ {Key: key_name, Value: value}, {..}, ...]</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>@staticmethod\ndef convert_to_aws_tags(metadata: dict):\n    \"\"\"Convert a dictionary to the AWS tag format (list of dicts)\n    [ {Key: key_name, Value: value}, {..}, ...]\"\"\"\n    return [{\"Key\": key, \"Value\": value} for key, value in metadata.items()]\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.get_aws_tags","title":"<code>get_aws_tags()</code>","text":"<p>Get the metadata/tags and convert them into AWS Tag Format</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def get_aws_tags(self):\n    \"\"\"Get the metadata/tags and convert them into AWS Tag Format\"\"\"\n    # Set up our metadata storage\n    workbench_meta = {\"workbench_tags\": self.output_tags}\n    for key, value in self.output_meta.items():\n        workbench_meta[key] = value\n    aws_tags = self.convert_to_aws_tags(workbench_meta)\n    return aws_tags\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.input_type","title":"<code>input_type()</code>","text":"<p>What Input Type does this Transform Consume</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def input_type(self) -&gt; TransformInput:\n    \"\"\"What Input Type does this Transform Consume\"\"\"\n    return self.input_type\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.output_type","title":"<code>output_type()</code>","text":"<p>What Output Type does this Transform Produce</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def output_type(self) -&gt; TransformOutput:\n    \"\"\"What Output Type does this Transform Produce\"\"\"\n    return self.output_type\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.post_transform","title":"<code>post_transform(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Post-Transform ensures that the output Artifact is ready for use</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>@abstractmethod\ndef post_transform(self, **kwargs):\n    \"\"\"Post-Transform ensures that the output Artifact is ready for use\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.pre_transform","title":"<code>pre_transform(**kwargs)</code>","text":"<p>Perform any Pre-Transform operations</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def pre_transform(self, **kwargs):\n    \"\"\"Perform any Pre-Transform operations\"\"\"\n    self.log.debug(\"Pre-Transform...\")\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.set_input_name","title":"<code>set_input_name(input_name)</code>","text":"<p>Set the Input Name (Name) for this Transform</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def set_input_name(self, input_name: str):\n    \"\"\"Set the Input Name (Name) for this Transform\"\"\"\n    self.input_name = input_name\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.set_output_name","title":"<code>set_output_name(output_name)</code>","text":"<p>Set the Output Name (Name) for this Transform</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def set_output_name(self, output_name: str):\n    \"\"\"Set the Output Name (Name) for this Transform\"\"\"\n    self.output_name = output_name\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.set_output_tags","title":"<code>set_output_tags(tags)</code>","text":"<p>Set the tags that will be associated with the output object Args:     tags (Union[list, str]): The list of tags or a '::' separated string of tags</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>def set_output_tags(self, tags: Union[list, str]):\n    \"\"\"Set the tags that will be associated with the output object\n    Args:\n        tags (Union[list, str]): The list of tags or a '::' separated string of tags\"\"\"\n    if isinstance(tags, list):\n        self.output_tags = self.tag_delimiter.join(tags)\n    else:\n        self.output_tags = tags\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.transform","title":"<code>transform(**kwargs)</code>","text":"<p>Perform the Transformation from Input to Output with pre_transform() and post_transform() invocations</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>@final\ndef transform(self, **kwargs):\n    \"\"\"Perform the Transformation from Input to Output with pre_transform() and post_transform() invocations\"\"\"\n    self.pre_transform(**kwargs)\n    self.transform_impl(**kwargs)\n    self.post_transform(**kwargs)\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.Transform.transform_impl","title":"<code>transform_impl(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract Method: Implement the Transformation from Input to Output</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>@abstractmethod\ndef transform_impl(self, **kwargs):\n    \"\"\"Abstract Method: Implement the Transformation from Input to Output\"\"\"\n    pass\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.TransformInput","title":"<code>TransformInput</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerated Types for Workbench Transform Inputs</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>class TransformInput(Enum):\n    \"\"\"Enumerated Types for Workbench Transform Inputs\"\"\"\n\n    LOCAL_FILE = auto()\n    PANDAS_DF = auto()\n    SPARK_DF = auto()\n    S3_OBJECT = auto()\n    DATA_SOURCE = auto()\n    FEATURE_SET = auto()\n    MODEL = auto()\n</code></pre>"},{"location":"core_classes/transforms/transform/#workbench.core.transforms.transform.TransformOutput","title":"<code>TransformOutput</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerated Types for Workbench Transform Outputs</p> Source code in <code>src/workbench/core/transforms/transform.py</code> <pre><code>class TransformOutput(Enum):\n    \"\"\"Enumerated Types for Workbench Transform Outputs\"\"\"\n\n    PANDAS_DF = auto()\n    SPARK_DF = auto()\n    S3_OBJECT = auto()\n    DATA_SOURCE = auto()\n    FEATURE_SET = auto()\n    MODEL = auto()\n    ENDPOINT = auto()\n</code></pre>"},{"location":"core_classes/views/computation_view/","title":"Computation View","text":"<p>Experimental</p> <p>The Workbench View classes are currently in experimental mode so have fun but expect issues and API changes going forward.</p> <p>Note: This class can be automatically invoked from DataSource/FeatureSet <code>set_computation_columns()</code> DataSource or FeatureSet. If you need more control then you can use this class directly.</p> <p>ComputationView Class: Create a View with a subset of columns for display purposes</p>"},{"location":"core_classes/views/computation_view/#workbench.core.views.computation_view.ComputationView","title":"<code>ComputationView</code>","text":"<p>               Bases: <code>ColumnSubsetView</code></p> <p>ComputationView Class: Create a View with a subset of columns for computation purposes</p> Common Usage <pre><code># Create a default ComputationView\nfs = FeatureSet(\"test_features\")\ncomp_view = ComputationView.create(fs)\ndf = comp_view.pull_dataframe()\n\n# Create a ComputationView with a specific set of columns\ncomp_view = ComputationView.create(fs, column_list=[\"my_col1\", \"my_col2\"])\n\n# Query the view\ndf = comp_view.query(f\"SELECT * FROM {comp_view.table} where prediction &gt; 0.5\")\n</code></pre> Source code in <code>src/workbench/core/views/computation_view.py</code> <pre><code>class ComputationView(ColumnSubsetView):\n    \"\"\"ComputationView Class: Create a View with a subset of columns for computation purposes\n\n    Common Usage:\n        ```python\n        # Create a default ComputationView\n        fs = FeatureSet(\"test_features\")\n        comp_view = ComputationView.create(fs)\n        df = comp_view.pull_dataframe()\n\n        # Create a ComputationView with a specific set of columns\n        comp_view = ComputationView.create(fs, column_list=[\"my_col1\", \"my_col2\"])\n\n        # Query the view\n        df = comp_view.query(f\"SELECT * FROM {comp_view.table} where prediction &gt; 0.5\")\n        ```\n    \"\"\"\n\n    @classmethod\n    def create(\n        cls,\n        artifact: Union[DataSource, FeatureSet],\n        source_table: str = None,\n        column_list: Union[list[str], None] = None,\n        column_limit: int = 30,\n    ) -&gt; Union[View, None]:\n        \"\"\"Factory method to create and return a ComputationView instance.\n\n        Args:\n            artifact (Union[DataSource, FeatureSet]): The DataSource or FeatureSet object\n            source_table (str, optional): The table/view to create the view from. Defaults to None\n            column_list (Union[list[str], None], optional): A list of columns to include. Defaults to None.\n            column_limit (int, optional): The max number of columns to include. Defaults to 30.\n\n        Returns:\n            Union[View, None]: The created View object (or None if failed to create the view)\n        \"\"\"\n        # Use the create logic directly from ColumnSubsetView with the \"computation\" view name\n        return ColumnSubsetView.create(\"computation\", artifact, source_table, column_list, column_limit)\n</code></pre>"},{"location":"core_classes/views/computation_view/#workbench.core.views.computation_view.ComputationView.create","title":"<code>create(artifact, source_table=None, column_list=None, column_limit=30)</code>  <code>classmethod</code>","text":"<p>Factory method to create and return a ComputationView instance.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Union[DataSource, FeatureSet]</code> <p>The DataSource or FeatureSet object</p> required <code>source_table</code> <code>str</code> <p>The table/view to create the view from. Defaults to None</p> <code>None</code> <code>column_list</code> <code>Union[list[str], None]</code> <p>A list of columns to include. Defaults to None.</p> <code>None</code> <code>column_limit</code> <code>int</code> <p>The max number of columns to include. Defaults to 30.</p> <code>30</code> <p>Returns:</p> Type Description <code>Union[View, None]</code> <p>Union[View, None]: The created View object (or None if failed to create the view)</p> Source code in <code>src/workbench/core/views/computation_view.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    artifact: Union[DataSource, FeatureSet],\n    source_table: str = None,\n    column_list: Union[list[str], None] = None,\n    column_limit: int = 30,\n) -&gt; Union[View, None]:\n    \"\"\"Factory method to create and return a ComputationView instance.\n\n    Args:\n        artifact (Union[DataSource, FeatureSet]): The DataSource or FeatureSet object\n        source_table (str, optional): The table/view to create the view from. Defaults to None\n        column_list (Union[list[str], None], optional): A list of columns to include. Defaults to None.\n        column_limit (int, optional): The max number of columns to include. Defaults to 30.\n\n    Returns:\n        Union[View, None]: The created View object (or None if failed to create the view)\n    \"\"\"\n    # Use the create logic directly from ColumnSubsetView with the \"computation\" view name\n    return ColumnSubsetView.create(\"computation\", artifact, source_table, column_list, column_limit)\n</code></pre>"},{"location":"core_classes/views/computation_view/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"core_classes/views/display_view/","title":"Display View","text":"<p>Experimental</p> <p>The Workbench View classes are currently in experimental mode so have fun but expect issues and API changes going forward.</p> <p>Note: This class will be used in the future to fine tune what columns get displayed. For now just use the DataSource/FeatureSet <code>set_computation_columns()</code> DataSource or FeatureSet</p> <p>DisplayView Class: Create a View with a subset of columns for display purposes</p>"},{"location":"core_classes/views/display_view/#workbench.core.views.display_view.DisplayView","title":"<code>DisplayView</code>","text":"<p>               Bases: <code>ColumnSubsetView</code></p> <p>DisplayView Class: Create a View with a subset of columns for display purposes</p> Common Usage <pre><code># Create a default DisplayView\nfs = FeatureSet(\"test_features\")\ndisplay_view = DisplayView.create(fs)\ndf = display_view.pull_dataframe()\n\n# Create a DisplayView with a specific set of columns\ndisplay_view = DisplayView.create(fs, column_list=[\"my_col1\", \"my_col2\"])\n\n# Query the view\ndf = display_view.query(f\"SELECT * FROM {display_view.table} where awesome = 'yes'\")\n</code></pre> Source code in <code>src/workbench/core/views/display_view.py</code> <pre><code>class DisplayView(ColumnSubsetView):\n    \"\"\"DisplayView Class: Create a View with a subset of columns for display purposes\n\n    Common Usage:\n        ```python\n        # Create a default DisplayView\n        fs = FeatureSet(\"test_features\")\n        display_view = DisplayView.create(fs)\n        df = display_view.pull_dataframe()\n\n        # Create a DisplayView with a specific set of columns\n        display_view = DisplayView.create(fs, column_list=[\"my_col1\", \"my_col2\"])\n\n        # Query the view\n        df = display_view.query(f\"SELECT * FROM {display_view.table} where awesome = 'yes'\")\n        ```\n    \"\"\"\n\n    @classmethod\n    def create(\n        cls,\n        artifact: Union[DataSource, FeatureSet],\n        source_table: str = None,\n        column_list: Union[list[str], None] = None,\n        column_limit: int = 30,\n    ) -&gt; Union[View, None]:\n        \"\"\"Factory method to create and return a DisplayView instance.\n\n        Args:\n            artifact (Union[DataSource, FeatureSet]): The DataSource or FeatureSet object\n            source_table (str, optional): The table/view to create the view from. Defaults to None\n            column_list (Union[list[str], None], optional): A list of columns to include. Defaults to None.\n            column_limit (int, optional): The max number of columns to include. Defaults to 30.\n\n        Returns:\n            Union[View, None]: The created View object (or None if failed to create the view)\n        \"\"\"\n        # Use the create logic directly from ColumnSubsetView with the \"display\" view name\n        return ColumnSubsetView.create(\"display\", artifact, source_table, column_list, column_limit)\n</code></pre>"},{"location":"core_classes/views/display_view/#workbench.core.views.display_view.DisplayView.create","title":"<code>create(artifact, source_table=None, column_list=None, column_limit=30)</code>  <code>classmethod</code>","text":"<p>Factory method to create and return a DisplayView instance.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Union[DataSource, FeatureSet]</code> <p>The DataSource or FeatureSet object</p> required <code>source_table</code> <code>str</code> <p>The table/view to create the view from. Defaults to None</p> <code>None</code> <code>column_list</code> <code>Union[list[str], None]</code> <p>A list of columns to include. Defaults to None.</p> <code>None</code> <code>column_limit</code> <code>int</code> <p>The max number of columns to include. Defaults to 30.</p> <code>30</code> <p>Returns:</p> Type Description <code>Union[View, None]</code> <p>Union[View, None]: The created View object (or None if failed to create the view)</p> Source code in <code>src/workbench/core/views/display_view.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    artifact: Union[DataSource, FeatureSet],\n    source_table: str = None,\n    column_list: Union[list[str], None] = None,\n    column_limit: int = 30,\n) -&gt; Union[View, None]:\n    \"\"\"Factory method to create and return a DisplayView instance.\n\n    Args:\n        artifact (Union[DataSource, FeatureSet]): The DataSource or FeatureSet object\n        source_table (str, optional): The table/view to create the view from. Defaults to None\n        column_list (Union[list[str], None], optional): A list of columns to include. Defaults to None.\n        column_limit (int, optional): The max number of columns to include. Defaults to 30.\n\n    Returns:\n        Union[View, None]: The created View object (or None if failed to create the view)\n    \"\"\"\n    # Use the create logic directly from ColumnSubsetView with the \"display\" view name\n    return ColumnSubsetView.create(\"display\", artifact, source_table, column_list, column_limit)\n</code></pre>"},{"location":"core_classes/views/display_view/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"core_classes/views/inference_view/","title":"Inference View","text":"<p>Experimental</p> <p>The Workbench View classes are currently in experimental mode so have fun but expect issues and API changes going forward.</p> <p>InferenceView Class: A View that does endpoint inference and computes residuals</p>"},{"location":"core_classes/views/inference_view/#workbench.core.views.inference_view.InferenceView","title":"<code>InferenceView</code>","text":"<p>InferenceView Class: A View that does endpoint inference and computes residuals</p> Common Usage <pre><code># Grab a Model\nmodel = Model(\"abalone-regression\")\n\n# Create an InferenceView\ninf_view = InferenceView.create(model)\nmy_df = inf_view.pull_dataframe(limit=5)\n\n# Query the view\ndf = inf_view.query(f\"SELECT * FROM {inf_view.table} where residuals &gt; 0.5\")\n</code></pre> Source code in <code>src/workbench/core/views/inference_view.py</code> <pre><code>class InferenceView:\n    \"\"\"InferenceView Class: A View that does endpoint inference and computes residuals\n\n    Common Usage:\n        ```python\n        # Grab a Model\n        model = Model(\"abalone-regression\")\n\n        # Create an InferenceView\n        inf_view = InferenceView.create(model)\n        my_df = inf_view.pull_dataframe(limit=5)\n\n        # Query the view\n        df = inf_view.query(f\"SELECT * FROM {inf_view.table} where residuals &gt; 0.5\")\n        ```\n    \"\"\"\n\n    @classmethod\n    def create(\n        cls,\n        model: Model,\n    ) -&gt; Union[View, None]:\n        \"\"\"Create a View that does endpoint inference and computes residuals\n\n        Args:\n            model (Model): The Model object to use for the target and features\n\n        Returns:\n            Union[View, None]: The created View object (or None if failed)\n        \"\"\"\n        # Log view creation\n        log.important(\"Creating Inference View...\")\n\n        # Pull in data from the FeatureSet\n        fs = FeatureSet(model.get_input())\n        df = fs.pull_dataframe()\n\n        # Grab the target from the model\n        target = model.target()\n\n        # Run inference on the data\n        end = Endpoint(model.endpoints()[0])\n        df = end.inference(df)\n\n        # Determine if the target is a classification or regression target\n        if model.model_type == ModelType.REGRESSOR:\n            df[\"residuals\"] = df[target] - df[\"prediction\"]\n            df[\"residuals_abs\"] = df[\"residuals\"].abs()\n        elif model.model_type == ModelType.CLASSIFIER:\n            class_labels = model.class_labels()\n            class_index = {label: i for i, label in enumerate(class_labels)}\n            df[\"residuals\"] = df[\"prediction\"].map(class_index) - df[target].map(class_index)\n            df[\"residuals_abs\"] = df[\"residuals\"].abs()\n        else:\n            log.warning(f\"Model type {model.model_type} has undefined residuals computation\")\n            df[\"residuals\"] = 0\n            df[\"residuals_abs\"] = 0\n\n        # Save the inference results to an inference view\n        view_name = f\"inf_{model.name.replace('-', '_')}\"\n        return PandasToView.create(view_name, fs, df=df, id_column=fs.id_column)\n</code></pre>"},{"location":"core_classes/views/inference_view/#workbench.core.views.inference_view.InferenceView.create","title":"<code>create(model)</code>  <code>classmethod</code>","text":"<p>Create a View that does endpoint inference and computes residuals</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Model object to use for the target and features</p> required <p>Returns:</p> Type Description <code>Union[View, None]</code> <p>Union[View, None]: The created View object (or None if failed)</p> Source code in <code>src/workbench/core/views/inference_view.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    model: Model,\n) -&gt; Union[View, None]:\n    \"\"\"Create a View that does endpoint inference and computes residuals\n\n    Args:\n        model (Model): The Model object to use for the target and features\n\n    Returns:\n        Union[View, None]: The created View object (or None if failed)\n    \"\"\"\n    # Log view creation\n    log.important(\"Creating Inference View...\")\n\n    # Pull in data from the FeatureSet\n    fs = FeatureSet(model.get_input())\n    df = fs.pull_dataframe()\n\n    # Grab the target from the model\n    target = model.target()\n\n    # Run inference on the data\n    end = Endpoint(model.endpoints()[0])\n    df = end.inference(df)\n\n    # Determine if the target is a classification or regression target\n    if model.model_type == ModelType.REGRESSOR:\n        df[\"residuals\"] = df[target] - df[\"prediction\"]\n        df[\"residuals_abs\"] = df[\"residuals\"].abs()\n    elif model.model_type == ModelType.CLASSIFIER:\n        class_labels = model.class_labels()\n        class_index = {label: i for i, label in enumerate(class_labels)}\n        df[\"residuals\"] = df[\"prediction\"].map(class_index) - df[target].map(class_index)\n        df[\"residuals_abs\"] = df[\"residuals\"].abs()\n    else:\n        log.warning(f\"Model type {model.model_type} has undefined residuals computation\")\n        df[\"residuals\"] = 0\n        df[\"residuals_abs\"] = 0\n\n    # Save the inference results to an inference view\n    view_name = f\"inf_{model.name.replace('-', '_')}\"\n    return PandasToView.create(view_name, fs, df=df, id_column=fs.id_column)\n</code></pre>"},{"location":"core_classes/views/inference_view/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"core_classes/views/overview/","title":"Views","text":"<p>View Examples</p> <p>Examples of using the Views classes to extend the functionality of Workbench Artifacts are in the Examples section at the bottom of this page. </p> <p>Views are a powerful way to filter and agument your DataSources and FeatureSets. With Views you can subset columns, rows, and even add data to existing Workbench Artifacts. If you want to compute outliers, runs some statistics or engineer some new features, Views are an easy way to change, modify, and add to DataSources and FeatureSets.</p> <p>If you're looking to read and pull data from a view please see the Views documentation.</p>"},{"location":"core_classes/views/overview/#view-constructor-classes","title":"View Constructor Classes","text":"<p>These classes provide APIs for creating Views for DataSources and FeatureSets.</p> <ul> <li>DisplayView: The Display View is leveraged by the web views/components and allows fine tuning of the UI for the Workbench Dashboard.</li> <li>ComputationView: The Computation View controls which columns have descriptive stats, outliers, and correlation calculations. Typically the computation view is a superset of the display view.</li> <li>TrainingView: The Training View will add a 'training' column to the data for model training, validation, and testing. Each row will have a 1 or 0 indicated whether is was used in the model training.</li> <li>InferenceView: The Inference View runs endpoint inference and computes residuals\"\"\"</li> </ul>"},{"location":"core_classes/views/overview/#examples","title":"Examples","text":"<p>All of the Workbench Examples are in the Workbench Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p> <p>Listing Views</p> views.py<pre><code>from workbench.api.data_source import DataSource\n\n# Convert the Data Source to a Feature Set\ntest_data = DataSource('test_data')\ntest_data.views()\n[\"display\", \"training\", \"computation\"]\n</code></pre> <p>Getting a Particular View</p> views.py<pre><code>from workbench.api.feature_set import FeatureSet\n\nfs = FeatureSet('test_features')\n\n# Grab the columns for the display view\ndisplay_view = fs.view(\"display\")\ndisplay_view.columns\n['id', 'name', 'height', 'weight', 'salary', ...]\n\n# Pull the dataframe for this view\ndf = display_view.pull_dataframe()\n    id       name     height      weight         salary ...\n0   58  Person 58  71.781227  275.088196  162053.140625  \n</code></pre> <p>View Queries</p> <p>All Workbench Views are stored in AWS Athena, so any query that you can make with Athena is accessible through the View Query API.</p> view_query.py<pre><code>from workbench.api.feature_set import FeatureSet\n\n# Grab a FeatureSet View\nfs = FeatureSet(\"abalone_features\")\nd_view = fs.view(\"display\")\n\n# Make some queries using the Athena backend\ndf = t_view(f\"select * from {d_view.table} where height &gt; .3\")\nprint(df.head())\n\ndf = t_view.query(\"select * from abalone_features where class_number_of_rings &lt; 3\")\nprint(df.head())\n</code></pre> <p>Output</p> <pre><code>  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  shell_weight  class_number_of_rings\n0   M   0.705     0.565   0.515         2.210          1.1075          0.4865        0.5120                     10\n1   F   0.455     0.355   1.130         0.594          0.3320          0.1160        0.1335                      8\n\n  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  shell_weight  class_number_of_rings\n0   I   0.075     0.055   0.010         0.002          0.0010          0.0005        0.0015                      1\n1   I   0.150     0.100   0.025         0.015          0.0045          0.0040         0.0050                      2\n</code></pre> <p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"core_classes/views/training_view/","title":"Training View","text":"<p>Experimental</p> <p>The Workbench View classes are currently in experimental mode so have fun but expect issues and API changes going forward.</p> <p>TrainingView Class: A View with an additional training column that marks holdout ids</p>"},{"location":"core_classes/views/training_view/#workbench.core.views.training_view.TrainingView","title":"<code>TrainingView</code>","text":"<p>               Bases: <code>CreateView</code></p> <p>TrainingView Class: A View with an additional training column (80/20 or holdout ids). The TrainingView class creates a SQL view that includes all columns from the source table along with an additional boolean column named \"training\". This view can also include a SQL filter expression to filter the rows included in the view.</p> Common Usage <pre><code># Create a default TrainingView\nfs = FeatureSet(\"test_features\")\ntraining_view = TrainingView.create(fs)\ndf = training_view.pull_dataframe()\n\n# Create a TrainingView with a specific filter expression\ntraining_view = TrainingView.create(fs, id_column=\"auto_id\", filter_expression=\"age &gt; 30\")\ndf = training_view.pull_dataframe()\n\n# Query the view\ndf = training_view.query(f\"SELECT * FROM {training_view.table} where training = TRUE\")\n</code></pre> Source code in <code>src/workbench/core/views/training_view.py</code> <pre><code>class TrainingView(CreateView):\n    \"\"\"TrainingView Class: A View with an additional training column (80/20 or holdout ids).\n    The TrainingView class creates a SQL view that includes all columns from the source table\n    along with an additional boolean column named \"training\". This view can also include\n    a SQL filter expression to filter the rows included in the view.\n\n\n    Common Usage:\n        ```python\n        # Create a default TrainingView\n        fs = FeatureSet(\"test_features\")\n        training_view = TrainingView.create(fs)\n        df = training_view.pull_dataframe()\n\n        # Create a TrainingView with a specific filter expression\n        training_view = TrainingView.create(fs, id_column=\"auto_id\", filter_expression=\"age &gt; 30\")\n        df = training_view.pull_dataframe()\n\n        # Query the view\n        df = training_view.query(f\"SELECT * FROM {training_view.table} where training = TRUE\")\n        ```\n    \"\"\"\n\n    @classmethod\n    def create(\n        cls,\n        feature_set: FeatureSet,\n        *,  # Enforce keyword arguments after feature_set\n        id_column: str = None,\n        holdout_ids: Union[list[str], list[int], None] = None,\n        filter_expression: str = None,\n        source_table: str = None,\n    ) -&gt; Union[View, None]:\n        \"\"\"Factory method to create and return a TrainingView instance.\n\n        Args:\n            feature_set (FeatureSet): A FeatureSet object\n            id_column (str, optional): The name of the id column. Defaults to None.\n            holdout_ids (Union[list[str], list[int], None], optional): A list of holdout ids. Defaults to None.\n            filter_expression (str, optional): SQL filter expression (e.g., \"age &gt; 25 AND status = 'active'\").\n                                               Defaults to None.\n            source_table (str, optional): The table/view to create the view from. Defaults to None.\n\n        Returns:\n            Union[View, None]: The created View object (or None if failed to create the view)\n        \"\"\"\n        # Instantiate the TrainingView with \"training\" as the view name\n        instance = cls(\"training\", feature_set, source_table)\n\n        # Drop any columns generated from AWS\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n        source_table_columns = get_column_list(instance.data_source, instance.source_table)\n        column_list = [col for col in source_table_columns if col not in aws_cols]\n\n        # Sanity check on the id column\n        if not id_column:\n            instance.log.important(\"No id column specified, we'll try the auto_id_column ..\")\n            if not instance.auto_id_column:\n                instance.log.error(\"No id column specified and no auto_id_column found, aborting ..\")\n                return None\n            else:\n                if instance.auto_id_column not in column_list:\n                    instance.log.error(\n                        f\"Auto id column {instance.auto_id_column} not found in column list, aborting ..\"\n                    )\n                    return None\n                else:\n                    id_column = instance.auto_id_column\n\n        # Enclose each column name in double quotes\n        sql_columns = \", \".join([f'\"{column}\"' for column in column_list])\n\n        # Build the training assignment logic\n        if holdout_ids:\n            # Format the list of holdout ids for SQL IN clause\n            if all(isinstance(id, str) for id in holdout_ids):\n                formatted_holdout_ids = \", \".join(f\"'{id}'\" for id in holdout_ids)\n            else:\n                formatted_holdout_ids = \", \".join(map(str, holdout_ids))\n\n            training_logic = f\"\"\"CASE\n                WHEN {id_column} IN ({formatted_holdout_ids}) THEN False\n                ELSE True\n            END AS training\"\"\"\n        else:\n            # Default 80/20 split using modulo\n            training_logic = f\"\"\"CASE\n                WHEN MOD(ROW_NUMBER() OVER (ORDER BY {id_column}), 10) &lt; 8 THEN True\n                ELSE False\n            END AS training\"\"\"\n\n        # Build WHERE clause if filter_expression is provided\n        where_clause = f\"\\nWHERE {filter_expression}\" if filter_expression else \"\"\n\n        # Construct the CREATE VIEW query\n        create_view_query = f\"\"\"\n        CREATE OR REPLACE VIEW {instance.table} AS\n        SELECT {sql_columns}, {training_logic}\n        FROM {instance.source_table}{where_clause}\n        \"\"\"\n\n        # Execute the CREATE VIEW query\n        instance.data_source.execute_statement(create_view_query)\n\n        # Return the View\n        return View(instance.data_source, instance.view_name, auto_create_view=False)\n\n    @classmethod\n    def create_with_sql(\n        cls,\n        feature_set: FeatureSet,\n        *,\n        sql_query: str,\n        id_column: str = None,\n    ) -&gt; Union[View, None]:\n        \"\"\"Factory method to create a TrainingView from a custom SQL query.\n\n        This method takes a complete SQL query and adds the default 80/20 training split.\n        Use this when you need complex queries like UNION ALL for oversampling.\n\n        Args:\n            feature_set (FeatureSet): A FeatureSet object\n            sql_query (str): Complete SELECT query (without the final semicolon)\n            id_column (str, optional): The name of the id column for training split. Defaults to None.\n\n        Returns:\n            Union[View, None]: The created View object (or None if failed)\n        \"\"\"\n        # Instantiate the TrainingView\n        instance = cls(\"training\", feature_set)\n\n        # Sanity check on the id column\n        if not id_column:\n            instance.log.important(\"No id column specified, using auto_id_column\")\n            if not instance.auto_id_column:\n                instance.log.error(\"No id column specified and no auto_id_column found, aborting\")\n                return None\n            id_column = instance.auto_id_column\n\n        # Default 80/20 split using modulo\n        training_logic = f\"\"\"CASE\n            WHEN MOD(ROW_NUMBER() OVER (ORDER BY {id_column}), 10) &lt; 8 THEN True\n            ELSE False\n        END AS training\"\"\"\n\n        # Wrap the custom query and add training column\n        create_view_query = f\"\"\"\n        CREATE OR REPLACE VIEW {instance.table} AS\n        SELECT *, {training_logic}\n        FROM ({sql_query}) AS custom_source\n        \"\"\"\n\n        # Execute the CREATE VIEW query\n        instance.data_source.execute_statement(create_view_query)\n\n        # Return the View\n        return View(instance.data_source, instance.view_name, auto_create_view=False)\n</code></pre>"},{"location":"core_classes/views/training_view/#workbench.core.views.training_view.TrainingView.create","title":"<code>create(feature_set, *, id_column=None, holdout_ids=None, filter_expression=None, source_table=None)</code>  <code>classmethod</code>","text":"<p>Factory method to create and return a TrainingView instance.</p> <p>Parameters:</p> Name Type Description Default <code>feature_set</code> <code>FeatureSet</code> <p>A FeatureSet object</p> required <code>id_column</code> <code>str</code> <p>The name of the id column. Defaults to None.</p> <code>None</code> <code>holdout_ids</code> <code>Union[list[str], list[int], None]</code> <p>A list of holdout ids. Defaults to None.</p> <code>None</code> <code>filter_expression</code> <code>str</code> <p>SQL filter expression (e.g., \"age &gt; 25 AND status = 'active'\").                                Defaults to None.</p> <code>None</code> <code>source_table</code> <code>str</code> <p>The table/view to create the view from. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[View, None]</code> <p>Union[View, None]: The created View object (or None if failed to create the view)</p> Source code in <code>src/workbench/core/views/training_view.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    feature_set: FeatureSet,\n    *,  # Enforce keyword arguments after feature_set\n    id_column: str = None,\n    holdout_ids: Union[list[str], list[int], None] = None,\n    filter_expression: str = None,\n    source_table: str = None,\n) -&gt; Union[View, None]:\n    \"\"\"Factory method to create and return a TrainingView instance.\n\n    Args:\n        feature_set (FeatureSet): A FeatureSet object\n        id_column (str, optional): The name of the id column. Defaults to None.\n        holdout_ids (Union[list[str], list[int], None], optional): A list of holdout ids. Defaults to None.\n        filter_expression (str, optional): SQL filter expression (e.g., \"age &gt; 25 AND status = 'active'\").\n                                           Defaults to None.\n        source_table (str, optional): The table/view to create the view from. Defaults to None.\n\n    Returns:\n        Union[View, None]: The created View object (or None if failed to create the view)\n    \"\"\"\n    # Instantiate the TrainingView with \"training\" as the view name\n    instance = cls(\"training\", feature_set, source_table)\n\n    # Drop any columns generated from AWS\n    aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\"]\n    source_table_columns = get_column_list(instance.data_source, instance.source_table)\n    column_list = [col for col in source_table_columns if col not in aws_cols]\n\n    # Sanity check on the id column\n    if not id_column:\n        instance.log.important(\"No id column specified, we'll try the auto_id_column ..\")\n        if not instance.auto_id_column:\n            instance.log.error(\"No id column specified and no auto_id_column found, aborting ..\")\n            return None\n        else:\n            if instance.auto_id_column not in column_list:\n                instance.log.error(\n                    f\"Auto id column {instance.auto_id_column} not found in column list, aborting ..\"\n                )\n                return None\n            else:\n                id_column = instance.auto_id_column\n\n    # Enclose each column name in double quotes\n    sql_columns = \", \".join([f'\"{column}\"' for column in column_list])\n\n    # Build the training assignment logic\n    if holdout_ids:\n        # Format the list of holdout ids for SQL IN clause\n        if all(isinstance(id, str) for id in holdout_ids):\n            formatted_holdout_ids = \", \".join(f\"'{id}'\" for id in holdout_ids)\n        else:\n            formatted_holdout_ids = \", \".join(map(str, holdout_ids))\n\n        training_logic = f\"\"\"CASE\n            WHEN {id_column} IN ({formatted_holdout_ids}) THEN False\n            ELSE True\n        END AS training\"\"\"\n    else:\n        # Default 80/20 split using modulo\n        training_logic = f\"\"\"CASE\n            WHEN MOD(ROW_NUMBER() OVER (ORDER BY {id_column}), 10) &lt; 8 THEN True\n            ELSE False\n        END AS training\"\"\"\n\n    # Build WHERE clause if filter_expression is provided\n    where_clause = f\"\\nWHERE {filter_expression}\" if filter_expression else \"\"\n\n    # Construct the CREATE VIEW query\n    create_view_query = f\"\"\"\n    CREATE OR REPLACE VIEW {instance.table} AS\n    SELECT {sql_columns}, {training_logic}\n    FROM {instance.source_table}{where_clause}\n    \"\"\"\n\n    # Execute the CREATE VIEW query\n    instance.data_source.execute_statement(create_view_query)\n\n    # Return the View\n    return View(instance.data_source, instance.view_name, auto_create_view=False)\n</code></pre>"},{"location":"core_classes/views/training_view/#workbench.core.views.training_view.TrainingView.create_with_sql","title":"<code>create_with_sql(feature_set, *, sql_query, id_column=None)</code>  <code>classmethod</code>","text":"<p>Factory method to create a TrainingView from a custom SQL query.</p> <p>This method takes a complete SQL query and adds the default 80/20 training split. Use this when you need complex queries like UNION ALL for oversampling.</p> <p>Parameters:</p> Name Type Description Default <code>feature_set</code> <code>FeatureSet</code> <p>A FeatureSet object</p> required <code>sql_query</code> <code>str</code> <p>Complete SELECT query (without the final semicolon)</p> required <code>id_column</code> <code>str</code> <p>The name of the id column for training split. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[View, None]</code> <p>Union[View, None]: The created View object (or None if failed)</p> Source code in <code>src/workbench/core/views/training_view.py</code> <pre><code>@classmethod\ndef create_with_sql(\n    cls,\n    feature_set: FeatureSet,\n    *,\n    sql_query: str,\n    id_column: str = None,\n) -&gt; Union[View, None]:\n    \"\"\"Factory method to create a TrainingView from a custom SQL query.\n\n    This method takes a complete SQL query and adds the default 80/20 training split.\n    Use this when you need complex queries like UNION ALL for oversampling.\n\n    Args:\n        feature_set (FeatureSet): A FeatureSet object\n        sql_query (str): Complete SELECT query (without the final semicolon)\n        id_column (str, optional): The name of the id column for training split. Defaults to None.\n\n    Returns:\n        Union[View, None]: The created View object (or None if failed)\n    \"\"\"\n    # Instantiate the TrainingView\n    instance = cls(\"training\", feature_set)\n\n    # Sanity check on the id column\n    if not id_column:\n        instance.log.important(\"No id column specified, using auto_id_column\")\n        if not instance.auto_id_column:\n            instance.log.error(\"No id column specified and no auto_id_column found, aborting\")\n            return None\n        id_column = instance.auto_id_column\n\n    # Default 80/20 split using modulo\n    training_logic = f\"\"\"CASE\n        WHEN MOD(ROW_NUMBER() OVER (ORDER BY {id_column}), 10) &lt; 8 THEN True\n        ELSE False\n    END AS training\"\"\"\n\n    # Wrap the custom query and add training column\n    create_view_query = f\"\"\"\n    CREATE OR REPLACE VIEW {instance.table} AS\n    SELECT *, {training_logic}\n    FROM ({sql_query}) AS custom_source\n    \"\"\"\n\n    # Execute the CREATE VIEW query\n    instance.data_source.execute_statement(create_view_query)\n\n    # Return the View\n    return View(instance.data_source, instance.view_name, auto_create_view=False)\n</code></pre>"},{"location":"core_classes/views/training_view/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord</p>"},{"location":"data_algorithms/overview/","title":"Data Algorithms","text":"<p>Data Algorithms</p> <p>WIP: These classes are currently actively being developed and are subject to change in both API and functionality over time. They provide a set of data algorithms for various types of data storage. We currently have subdirectorys for:</p> <ul> <li>DataFrames: A set of algorithms that consume/return a Panda dataFrame.</li> <li>Graphs: Algorithms for node/edge graphs, specifically focused on NetworkX graphs.</li> <li>Spark: A set of algorithms that consume/return a Spark dataFrame.</li> <li> <p>SQL: SQL queries that provide a wide range of functionality:</p> <ul> <li>Outliers</li> <li>Descriptive Stats</li> <li>Correlations</li> <li>and More</li> </ul> </li> </ul> <p>Welcome to the Workbench Data Algorithms</p> <p>Docs TBD</p>"},{"location":"data_algorithms/overview/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"data_algorithms/dataframes/overview/","title":"Pandas Dataframe Algorithms","text":"<p>Pandas Dataframes</p> <p>Pandas dataframes are obviously not going to scale as well as our Spark and SQL Algorithms, but for 'moderate' sized data these algorithms provide some nice functionality.</p> <p>Pandas Dataframe Algorithms</p> <p>Workbench has a growing set of algorithms and data processing tools for Pandas Dataframes. In general these algorithm will take a dataframe as input and give you back a dataframe with additional columns.</p>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.feature_space_proximity.FeatureSpaceProximity","title":"<code>FeatureSpaceProximity</code>","text":"<p>               Bases: <code>Proximity</code></p> <p>Proximity computations for numeric feature spaces using Euclidean distance.</p> Source code in <code>src/workbench/algorithms/dataframe/feature_space_proximity.py</code> <pre><code>class FeatureSpaceProximity(Proximity):\n    \"\"\"Proximity computations for numeric feature spaces using Euclidean distance.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        id_column: str,\n        features: List[str],\n        target: Optional[str] = None,\n        include_all_columns: bool = False,\n    ):\n        \"\"\"\n        Initialize the FeatureSpaceProximity class.\n\n        Args:\n            df: DataFrame containing data for neighbor computations.\n            id_column: Name of the column used as the identifier.\n            features: List of feature column names to be used for neighbor computations.\n            target: Name of the target column. Defaults to None.\n            include_all_columns: Include all DataFrame columns in neighbor results. Defaults to False.\n        \"\"\"\n        # Validate and filter features before calling parent init\n        self._raw_features = features\n        super().__init__(\n            df, id_column=id_column, features=features, target=target, include_all_columns=include_all_columns\n        )\n\n    def _prepare_data(self) -&gt; None:\n        \"\"\"Filter out non-numeric features and drop NaN rows.\"\"\"\n        # Validate features\n        self.features = self._validate_features(self.df, self._raw_features)\n\n        # Drop NaN rows for the features we're using\n        self.df = self.df.dropna(subset=self.features).copy()\n\n    def _validate_features(self, df: pd.DataFrame, features: List[str]) -&gt; List[str]:\n        \"\"\"Remove non-numeric features and log warnings.\"\"\"\n        non_numeric = [f for f in features if f not in df.select_dtypes(include=[\"number\"]).columns]\n        if non_numeric:\n            log.warning(f\"Non-numeric features {non_numeric} aren't currently supported, excluding them\")\n        return [f for f in features if f not in non_numeric]\n\n    def _build_model(self) -&gt; None:\n        \"\"\"Standardize features and fit Nearest Neighbors model.\"\"\"\n        self.scaler = StandardScaler()\n        X = self.scaler.fit_transform(self.df[self.features])\n        self.nn = NearestNeighbors().fit(X)\n\n    def _transform_features(self, df: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\"Transform features using the fitted scaler.\"\"\"\n        return self.scaler.transform(df[self.features])\n\n    def _project_2d(self) -&gt; None:\n        \"\"\"Project the numeric features to 2D for visualization.\"\"\"\n        if len(self.features) &gt;= 2:\n            self.df = Projection2D().fit_transform(self.df, features=self.features)\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.feature_space_proximity.FeatureSpaceProximity.__init__","title":"<code>__init__(df, id_column, features, target=None, include_all_columns=False)</code>","text":"<p>Initialize the FeatureSpaceProximity class.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing data for neighbor computations.</p> required <code>id_column</code> <code>str</code> <p>Name of the column used as the identifier.</p> required <code>features</code> <code>List[str]</code> <p>List of feature column names to be used for neighbor computations.</p> required <code>target</code> <code>Optional[str]</code> <p>Name of the target column. Defaults to None.</p> <code>None</code> <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in neighbor results. Defaults to False.</p> <code>False</code> Source code in <code>src/workbench/algorithms/dataframe/feature_space_proximity.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    id_column: str,\n    features: List[str],\n    target: Optional[str] = None,\n    include_all_columns: bool = False,\n):\n    \"\"\"\n    Initialize the FeatureSpaceProximity class.\n\n    Args:\n        df: DataFrame containing data for neighbor computations.\n        id_column: Name of the column used as the identifier.\n        features: List of feature column names to be used for neighbor computations.\n        target: Name of the target column. Defaults to None.\n        include_all_columns: Include all DataFrame columns in neighbor results. Defaults to False.\n    \"\"\"\n    # Validate and filter features before calling parent init\n    self._raw_features = features\n    super().__init__(\n        df, id_column=id_column, features=features, target=target, include_all_columns=include_all_columns\n    )\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.fingerprint_proximity.FingerprintProximity","title":"<code>FingerprintProximity</code>","text":"<p>               Bases: <code>Proximity</code></p> <p>Proximity computations for binary fingerprints using Tanimoto similarity.</p> <p>Note: Tanimoto similarity is equivalent to Jaccard similarity for binary vectors. Tanimoto(A, B) = |A \u2229 B| / |A \u222a B|</p> Source code in <code>src/workbench/algorithms/dataframe/fingerprint_proximity.py</code> <pre><code>class FingerprintProximity(Proximity):\n    \"\"\"Proximity computations for binary fingerprints using Tanimoto similarity.\n\n    Note: Tanimoto similarity is equivalent to Jaccard similarity for binary vectors.\n    Tanimoto(A, B) = |A \u2229 B| / |A \u222a B|\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        id_column: str,\n        fingerprint_column: Optional[str] = None,\n        target: Optional[str] = None,\n        include_all_columns: bool = False,\n        radius: int = 2,\n        n_bits: int = 1024,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the FingerprintProximity class for binary fingerprint similarity.\n\n        Args:\n            df: DataFrame containing fingerprints or SMILES.\n            id_column: Name of the column used as an identifier.\n            fingerprint_column: Name of the column containing fingerprints (bit strings).\n                If None, looks for existing \"fingerprint\" column or computes from SMILES.\n            target: Name of the target column. Defaults to None.\n            include_all_columns: Include all DataFrame columns in neighbor results. Defaults to False.\n            radius: Radius for Morgan fingerprint computation (default: 2).\n            n_bits: Number of bits for fingerprint (default: 1024).\n        \"\"\"\n        # Store fingerprint computation parameters\n        self._fp_radius = radius\n        self._fp_n_bits = n_bits\n\n        # Store the requested fingerprint column (may be None)\n        self._fingerprint_column_arg = fingerprint_column\n\n        # Determine fingerprint column name (but don't compute yet - that happens in _prepare_data)\n        self.fingerprint_column = self._resolve_fingerprint_column_name(df, fingerprint_column)\n\n        # Call parent constructor with fingerprint_column as the only \"feature\"\n        super().__init__(\n            df,\n            id_column=id_column,\n            features=[self.fingerprint_column],\n            target=target,\n            include_all_columns=include_all_columns,\n        )\n\n    @staticmethod\n    def _resolve_fingerprint_column_name(df: pd.DataFrame, fingerprint_column: Optional[str]) -&gt; str:\n        \"\"\"\n        Determine the fingerprint column name, validating it exists or can be computed.\n\n        Args:\n            df: Input DataFrame.\n            fingerprint_column: Explicitly specified fingerprint column, or None.\n\n        Returns:\n            Name of the fingerprint column to use.\n\n        Raises:\n            ValueError: If no fingerprint column exists and no SMILES column found.\n        \"\"\"\n        # If explicitly provided, validate it exists\n        if fingerprint_column is not None:\n            if fingerprint_column not in df.columns:\n                raise ValueError(f\"Fingerprint column '{fingerprint_column}' not found in DataFrame\")\n            return fingerprint_column\n\n        # Check for existing \"fingerprint\" column\n        if \"fingerprint\" in df.columns:\n            log.info(\"Using existing 'fingerprint' column\")\n            return \"fingerprint\"\n\n        # Will need to compute from SMILES - validate SMILES column exists\n        smiles_column = next((col for col in df.columns if col.lower() == \"smiles\"), None)\n        if smiles_column is None:\n            raise ValueError(\n                \"No fingerprint column provided and no SMILES column found. \"\n                \"Either provide a fingerprint_column or include a 'smiles' column in the DataFrame.\"\n            )\n\n        # Fingerprints will be computed in _prepare_data\n        return \"fingerprint\"\n\n    def _prepare_data(self) -&gt; None:\n        \"\"\"Compute fingerprints from SMILES if needed.\"\"\"\n        # If fingerprint column doesn't exist yet, compute it\n        if self.fingerprint_column not in self.df.columns:\n            log.info(f\"Computing Morgan fingerprints (radius={self._fp_radius}, n_bits={self._fp_n_bits})...\")\n            self.df = compute_morgan_fingerprints(self.df, radius=self._fp_radius, n_bits=self._fp_n_bits)\n\n    def _build_model(self) -&gt; None:\n        \"\"\"\n        Build the fingerprint proximity model for Tanimoto similarity.\n\n        For binary fingerprints: uses Jaccard distance (1 - Tanimoto)\n        For count fingerprints: uses weighted Tanimoto (Ruzicka) distance\n        \"\"\"\n        # Convert fingerprint strings to matrix and detect format\n        self.X, self._is_count_fp = self._fingerprints_to_matrix(self.df)\n\n        if self._is_count_fp:\n            # Weighted Tanimoto (Ruzicka) for count vectors: 1 - \u03a3min(A,B)/\u03a3max(A,B)\n            log.info(\"Building NearestNeighbors model (weighted Tanimoto for count fingerprints)...\")\n\n            def ruzicka_distance(a, b):\n                \"\"\"Ruzicka distance = 1 - weighted Tanimoto similarity.\"\"\"\n                min_sum = np.minimum(a, b).sum()\n                max_sum = np.maximum(a, b).sum()\n                if max_sum == 0:\n                    return 0.0\n                return 1.0 - (min_sum / max_sum)\n\n            self.nn = NearestNeighbors(metric=ruzicka_distance, algorithm=\"ball_tree\").fit(self.X)\n        else:\n            # Standard Jaccard for binary fingerprints\n            log.info(\"Building NearestNeighbors model (Jaccard/Tanimoto for binary fingerprints)...\")\n            self.nn = NearestNeighbors(metric=\"jaccard\", algorithm=\"ball_tree\").fit(self.X)\n\n    def _transform_features(self, df: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\"\n        Transform fingerprints to matrix for querying.\n\n        Args:\n            df: DataFrame containing fingerprints to transform.\n\n        Returns:\n            Feature matrix for the fingerprints (binary or count based on self._is_count_fp).\n        \"\"\"\n        matrix, _ = self._fingerprints_to_matrix(df)\n        return matrix\n\n    def _fingerprints_to_matrix(self, df: pd.DataFrame) -&gt; tuple[np.ndarray, bool]:\n        \"\"\"\n        Convert fingerprint strings to a numpy matrix.\n\n        Supports two formats (auto-detected):\n            - Bitstrings: \"10110010...\" \u2192 binary matrix (bool), is_count=False\n            - Count vectors: \"0,3,0,1,5,...\" \u2192 count matrix (uint8), is_count=True\n\n        Args:\n            df: DataFrame containing fingerprint column.\n\n        Returns:\n            Tuple of (2D numpy array, is_count_fingerprint boolean)\n        \"\"\"\n        # Auto-detect format based on first fingerprint\n        sample = str(df[self.fingerprint_column].iloc[0])\n        if \",\" in sample:\n            # Count vector format: preserve counts for weighted Tanimoto\n            fingerprint_values = df[self.fingerprint_column].apply(\n                lambda fp: np.array([int(x) for x in fp.split(\",\")], dtype=np.uint8)\n            )\n            return np.vstack(fingerprint_values), True\n        else:\n            # Bitstring format: binary values\n            fingerprint_bits = df[self.fingerprint_column].apply(\n                lambda fp: np.array([int(bit) for bit in fp], dtype=np.bool_)\n            )\n            return np.vstack(fingerprint_bits), False\n\n    def _precompute_metrics(self) -&gt; None:\n        \"\"\"Precompute metrics, adding Tanimoto similarity alongside distance.\"\"\"\n        # Call parent to compute nn_distance (Jaccard), nn_id, nn_target, nn_target_diff\n        super()._precompute_metrics()\n\n        # Add Tanimoto similarity (keep nn_distance for internal use by target_gradients)\n        self.df[\"nn_similarity\"] = 1 - self.df[\"nn_distance\"]\n\n    def _set_core_columns(self) -&gt; None:\n        \"\"\"Set core columns using nn_similarity instead of nn_distance.\"\"\"\n        self.core_columns = [self.id_column, \"nn_similarity\", \"nn_id\"]\n        if self.target:\n            self.core_columns.extend([self.target, \"nn_target\", \"nn_target_diff\"])\n\n    def _project_2d(self) -&gt; None:\n        \"\"\"Project the fingerprint matrix to 2D for visualization using UMAP.\"\"\"\n        if self._is_count_fp:\n            # For count fingerprints, convert to binary for UMAP projection (Jaccard needs binary)\n            X_binary = (self.X &gt; 0).astype(np.bool_)\n            self.df = Projection2D().fit_transform(self.df, feature_matrix=X_binary, metric=\"jaccard\")\n        else:\n            self.df = Projection2D().fit_transform(self.df, feature_matrix=self.X, metric=\"jaccard\")\n\n    def isolated(self, top_percent: float = 1.0) -&gt; pd.DataFrame:\n        \"\"\"\n        Find isolated data points based on Tanimoto similarity to nearest neighbor.\n\n        Args:\n            top_percent: Percentage of most isolated data points to return (e.g., 1.0 returns top 1%)\n\n        Returns:\n            DataFrame of observations with lowest Tanimoto similarity, sorted ascending\n        \"\"\"\n        # For Tanimoto similarity, isolated means LOW similarity to nearest neighbor\n        percentile = top_percent\n        threshold = np.percentile(self.df[\"nn_similarity\"], percentile)\n        isolated = self.df[self.df[\"nn_similarity\"] &lt;= threshold].copy()\n        isolated = isolated.sort_values(\"nn_similarity\", ascending=True).reset_index(drop=True)\n        return isolated if self.include_all_columns else isolated[self.core_columns]\n\n    def proximity_stats(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Return distribution statistics for nearest neighbor Tanimoto similarity.\n\n        Returns:\n            DataFrame with similarity distribution statistics (count, mean, std, percentiles)\n        \"\"\"\n        return (\n            self.df[\"nn_similarity\"]\n            .describe(percentiles=[0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n            .to_frame()\n        )\n\n    def neighbors(\n        self,\n        id_or_ids: Union[str, int, List[Union[str, int]]],\n        n_neighbors: Optional[int] = 5,\n        min_similarity: Optional[float] = None,\n        include_self: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Return neighbors for ID(s) from the existing dataset.\n\n        Args:\n            id_or_ids: Single ID or list of IDs to look up\n            n_neighbors: Number of neighbors to return (default: 5, ignored if min_similarity is set)\n            min_similarity: If provided, find all neighbors with Tanimoto similarity &gt;= this value (0-1)\n            include_self: Whether to include self in results (default: True)\n\n        Returns:\n            DataFrame containing neighbors with Tanimoto similarity scores\n        \"\"\"\n        # Convert min_similarity to radius (Jaccard distance = 1 - Tanimoto similarity)\n        radius = 1 - min_similarity if min_similarity is not None else None\n\n        # Call parent method (returns Jaccard distance)\n        neighbors_df = super().neighbors(\n            id_or_ids=id_or_ids,\n            n_neighbors=n_neighbors,\n            radius=radius,\n            include_self=include_self,\n        )\n\n        # Convert Jaccard distance to Tanimoto similarity\n        neighbors_df[\"similarity\"] = 1 - neighbors_df[\"distance\"]\n        neighbors_df.drop(columns=[\"distance\"], inplace=True)\n\n        return neighbors_df\n\n    def neighbors_from_smiles(\n        self,\n        smiles: Union[str, List[str]],\n        n_neighbors: int = 5,\n        min_similarity: Optional[float] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Find neighbors for SMILES strings not in the reference dataset.\n\n        Args:\n            smiles: Single SMILES string or list of SMILES to query\n            n_neighbors: Number of neighbors to return (default: 5, ignored if min_similarity is set)\n            min_similarity: If provided, find all neighbors with Tanimoto similarity &gt;= this value (0-1)\n\n        Returns:\n            DataFrame containing neighbors with Tanimoto similarity scores.\n            The 'query_id' column contains the SMILES string (or index if list).\n        \"\"\"\n        # Normalize to list\n        smiles_list = [smiles] if isinstance(smiles, str) else smiles\n\n        # Build a temporary DataFrame with the query SMILES\n        query_df = pd.DataFrame({\"smiles\": smiles_list})\n\n        # Compute fingerprints using same parameters as the reference dataset\n        query_df = compute_morgan_fingerprints(query_df, radius=self._fp_radius, n_bits=self._fp_n_bits)\n\n        # Transform to matrix (use same format detection as reference)\n        X_query, _ = self._fingerprints_to_matrix(query_df)\n\n        # Query the model\n        if min_similarity is not None:\n            radius = 1 - min_similarity\n            distances, indices = self.nn.radius_neighbors(X_query, radius=radius)\n        else:\n            distances, indices = self.nn.kneighbors(X_query, n_neighbors=n_neighbors)\n\n        # Build results\n        results = []\n        for i, (dists, nbrs) in enumerate(zip(distances, indices)):\n            query_id = smiles_list[i]\n\n            for neighbor_idx, dist in zip(nbrs, dists):\n                neighbor_row = self.df.iloc[neighbor_idx]\n                neighbor_id = neighbor_row[self.id_column]\n                similarity = 1.0 - dist if dist &gt; 1e-6 else 1.0\n\n                result = {\n                    \"query_id\": query_id,\n                    \"neighbor_id\": neighbor_id,\n                    \"similarity\": similarity,\n                }\n\n                # Add target if present\n                if self.target and self.target in self.df.columns:\n                    result[self.target] = neighbor_row[self.target]\n\n                # Include all columns if requested\n                if self.include_all_columns:\n                    for col in self.df.columns:\n                        if col not in [self.id_column, \"query_id\", \"neighbor_id\", \"similarity\"]:\n                            result[f\"neighbor_{col}\"] = neighbor_row[col]\n\n                results.append(result)\n\n        df_results = pd.DataFrame(results)\n\n        # Sort by query_id then similarity descending\n        if len(df_results) &gt; 0:\n            df_results = df_results.sort_values([\"query_id\", \"similarity\"], ascending=[True, False]).reset_index(\n                drop=True\n            )\n\n        return df_results\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.fingerprint_proximity.FingerprintProximity.__init__","title":"<code>__init__(df, id_column, fingerprint_column=None, target=None, include_all_columns=False, radius=2, n_bits=1024)</code>","text":"<p>Initialize the FingerprintProximity class for binary fingerprint similarity.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing fingerprints or SMILES.</p> required <code>id_column</code> <code>str</code> <p>Name of the column used as an identifier.</p> required <code>fingerprint_column</code> <code>Optional[str]</code> <p>Name of the column containing fingerprints (bit strings). If None, looks for existing \"fingerprint\" column or computes from SMILES.</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>Name of the target column. Defaults to None.</p> <code>None</code> <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in neighbor results. Defaults to False.</p> <code>False</code> <code>radius</code> <code>int</code> <p>Radius for Morgan fingerprint computation (default: 2).</p> <code>2</code> <code>n_bits</code> <code>int</code> <p>Number of bits for fingerprint (default: 1024).</p> <code>1024</code> Source code in <code>src/workbench/algorithms/dataframe/fingerprint_proximity.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    id_column: str,\n    fingerprint_column: Optional[str] = None,\n    target: Optional[str] = None,\n    include_all_columns: bool = False,\n    radius: int = 2,\n    n_bits: int = 1024,\n) -&gt; None:\n    \"\"\"\n    Initialize the FingerprintProximity class for binary fingerprint similarity.\n\n    Args:\n        df: DataFrame containing fingerprints or SMILES.\n        id_column: Name of the column used as an identifier.\n        fingerprint_column: Name of the column containing fingerprints (bit strings).\n            If None, looks for existing \"fingerprint\" column or computes from SMILES.\n        target: Name of the target column. Defaults to None.\n        include_all_columns: Include all DataFrame columns in neighbor results. Defaults to False.\n        radius: Radius for Morgan fingerprint computation (default: 2).\n        n_bits: Number of bits for fingerprint (default: 1024).\n    \"\"\"\n    # Store fingerprint computation parameters\n    self._fp_radius = radius\n    self._fp_n_bits = n_bits\n\n    # Store the requested fingerprint column (may be None)\n    self._fingerprint_column_arg = fingerprint_column\n\n    # Determine fingerprint column name (but don't compute yet - that happens in _prepare_data)\n    self.fingerprint_column = self._resolve_fingerprint_column_name(df, fingerprint_column)\n\n    # Call parent constructor with fingerprint_column as the only \"feature\"\n    super().__init__(\n        df,\n        id_column=id_column,\n        features=[self.fingerprint_column],\n        target=target,\n        include_all_columns=include_all_columns,\n    )\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.fingerprint_proximity.FingerprintProximity.isolated","title":"<code>isolated(top_percent=1.0)</code>","text":"<p>Find isolated data points based on Tanimoto similarity to nearest neighbor.</p> <p>Parameters:</p> Name Type Description Default <code>top_percent</code> <code>float</code> <p>Percentage of most isolated data points to return (e.g., 1.0 returns top 1%)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame of observations with lowest Tanimoto similarity, sorted ascending</p> Source code in <code>src/workbench/algorithms/dataframe/fingerprint_proximity.py</code> <pre><code>def isolated(self, top_percent: float = 1.0) -&gt; pd.DataFrame:\n    \"\"\"\n    Find isolated data points based on Tanimoto similarity to nearest neighbor.\n\n    Args:\n        top_percent: Percentage of most isolated data points to return (e.g., 1.0 returns top 1%)\n\n    Returns:\n        DataFrame of observations with lowest Tanimoto similarity, sorted ascending\n    \"\"\"\n    # For Tanimoto similarity, isolated means LOW similarity to nearest neighbor\n    percentile = top_percent\n    threshold = np.percentile(self.df[\"nn_similarity\"], percentile)\n    isolated = self.df[self.df[\"nn_similarity\"] &lt;= threshold].copy()\n    isolated = isolated.sort_values(\"nn_similarity\", ascending=True).reset_index(drop=True)\n    return isolated if self.include_all_columns else isolated[self.core_columns]\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.fingerprint_proximity.FingerprintProximity.neighbors","title":"<code>neighbors(id_or_ids, n_neighbors=5, min_similarity=None, include_self=True)</code>","text":"<p>Return neighbors for ID(s) from the existing dataset.</p> <p>Parameters:</p> Name Type Description Default <code>id_or_ids</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Single ID or list of IDs to look up</p> required <code>n_neighbors</code> <code>Optional[int]</code> <p>Number of neighbors to return (default: 5, ignored if min_similarity is set)</p> <code>5</code> <code>min_similarity</code> <code>Optional[float]</code> <p>If provided, find all neighbors with Tanimoto similarity &gt;= this value (0-1)</p> <code>None</code> <code>include_self</code> <code>bool</code> <p>Whether to include self in results (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing neighbors with Tanimoto similarity scores</p> Source code in <code>src/workbench/algorithms/dataframe/fingerprint_proximity.py</code> <pre><code>def neighbors(\n    self,\n    id_or_ids: Union[str, int, List[Union[str, int]]],\n    n_neighbors: Optional[int] = 5,\n    min_similarity: Optional[float] = None,\n    include_self: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return neighbors for ID(s) from the existing dataset.\n\n    Args:\n        id_or_ids: Single ID or list of IDs to look up\n        n_neighbors: Number of neighbors to return (default: 5, ignored if min_similarity is set)\n        min_similarity: If provided, find all neighbors with Tanimoto similarity &gt;= this value (0-1)\n        include_self: Whether to include self in results (default: True)\n\n    Returns:\n        DataFrame containing neighbors with Tanimoto similarity scores\n    \"\"\"\n    # Convert min_similarity to radius (Jaccard distance = 1 - Tanimoto similarity)\n    radius = 1 - min_similarity if min_similarity is not None else None\n\n    # Call parent method (returns Jaccard distance)\n    neighbors_df = super().neighbors(\n        id_or_ids=id_or_ids,\n        n_neighbors=n_neighbors,\n        radius=radius,\n        include_self=include_self,\n    )\n\n    # Convert Jaccard distance to Tanimoto similarity\n    neighbors_df[\"similarity\"] = 1 - neighbors_df[\"distance\"]\n    neighbors_df.drop(columns=[\"distance\"], inplace=True)\n\n    return neighbors_df\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.fingerprint_proximity.FingerprintProximity.neighbors_from_smiles","title":"<code>neighbors_from_smiles(smiles, n_neighbors=5, min_similarity=None)</code>","text":"<p>Find neighbors for SMILES strings not in the reference dataset.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>Union[str, List[str]]</code> <p>Single SMILES string or list of SMILES to query</p> required <code>n_neighbors</code> <code>int</code> <p>Number of neighbors to return (default: 5, ignored if min_similarity is set)</p> <code>5</code> <code>min_similarity</code> <code>Optional[float]</code> <p>If provided, find all neighbors with Tanimoto similarity &gt;= this value (0-1)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing neighbors with Tanimoto similarity scores.</p> <code>DataFrame</code> <p>The 'query_id' column contains the SMILES string (or index if list).</p> Source code in <code>src/workbench/algorithms/dataframe/fingerprint_proximity.py</code> <pre><code>def neighbors_from_smiles(\n    self,\n    smiles: Union[str, List[str]],\n    n_neighbors: int = 5,\n    min_similarity: Optional[float] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Find neighbors for SMILES strings not in the reference dataset.\n\n    Args:\n        smiles: Single SMILES string or list of SMILES to query\n        n_neighbors: Number of neighbors to return (default: 5, ignored if min_similarity is set)\n        min_similarity: If provided, find all neighbors with Tanimoto similarity &gt;= this value (0-1)\n\n    Returns:\n        DataFrame containing neighbors with Tanimoto similarity scores.\n        The 'query_id' column contains the SMILES string (or index if list).\n    \"\"\"\n    # Normalize to list\n    smiles_list = [smiles] if isinstance(smiles, str) else smiles\n\n    # Build a temporary DataFrame with the query SMILES\n    query_df = pd.DataFrame({\"smiles\": smiles_list})\n\n    # Compute fingerprints using same parameters as the reference dataset\n    query_df = compute_morgan_fingerprints(query_df, radius=self._fp_radius, n_bits=self._fp_n_bits)\n\n    # Transform to matrix (use same format detection as reference)\n    X_query, _ = self._fingerprints_to_matrix(query_df)\n\n    # Query the model\n    if min_similarity is not None:\n        radius = 1 - min_similarity\n        distances, indices = self.nn.radius_neighbors(X_query, radius=radius)\n    else:\n        distances, indices = self.nn.kneighbors(X_query, n_neighbors=n_neighbors)\n\n    # Build results\n    results = []\n    for i, (dists, nbrs) in enumerate(zip(distances, indices)):\n        query_id = smiles_list[i]\n\n        for neighbor_idx, dist in zip(nbrs, dists):\n            neighbor_row = self.df.iloc[neighbor_idx]\n            neighbor_id = neighbor_row[self.id_column]\n            similarity = 1.0 - dist if dist &gt; 1e-6 else 1.0\n\n            result = {\n                \"query_id\": query_id,\n                \"neighbor_id\": neighbor_id,\n                \"similarity\": similarity,\n            }\n\n            # Add target if present\n            if self.target and self.target in self.df.columns:\n                result[self.target] = neighbor_row[self.target]\n\n            # Include all columns if requested\n            if self.include_all_columns:\n                for col in self.df.columns:\n                    if col not in [self.id_column, \"query_id\", \"neighbor_id\", \"similarity\"]:\n                        result[f\"neighbor_{col}\"] = neighbor_row[col]\n\n            results.append(result)\n\n    df_results = pd.DataFrame(results)\n\n    # Sort by query_id then similarity descending\n    if len(df_results) &gt; 0:\n        df_results = df_results.sort_values([\"query_id\", \"similarity\"], ascending=[True, False]).reset_index(\n            drop=True\n        )\n\n    return df_results\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.fingerprint_proximity.FingerprintProximity.proximity_stats","title":"<code>proximity_stats()</code>","text":"<p>Return distribution statistics for nearest neighbor Tanimoto similarity.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with similarity distribution statistics (count, mean, std, percentiles)</p> Source code in <code>src/workbench/algorithms/dataframe/fingerprint_proximity.py</code> <pre><code>def proximity_stats(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Return distribution statistics for nearest neighbor Tanimoto similarity.\n\n    Returns:\n        DataFrame with similarity distribution statistics (count, mean, std, percentiles)\n    \"\"\"\n    return (\n        self.df[\"nn_similarity\"]\n        .describe(percentiles=[0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n        .to_frame()\n    )\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.projection_2d.Projection2D","title":"<code>Projection2D</code>","text":"<p>Perform Dimensionality Reduction on a DataFrame using TSNE, MDS, PCA, or UMAP.</p> Source code in <code>src/workbench/algorithms/dataframe/projection_2d.py</code> <pre><code>class Projection2D:\n    \"\"\"Perform Dimensionality Reduction on a DataFrame using TSNE, MDS, PCA, or UMAP.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the Projection2D class.\"\"\"\n        self.log = logging.getLogger(\"workbench\")\n        self.projection_model = None\n\n    def fit_transform(\n        self,\n        input_df: pd.DataFrame,\n        features: list = None,\n        feature_matrix: np.ndarray = None,\n        metric: str = \"euclidean\",\n        projection: str = \"UMAP\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fit and transform a DataFrame using the selected dimensionality reduction method.\n\n        This method creates a copy of the input DataFrame, processes the specified features\n        for normalization and projection, and returns a new DataFrame with added 'x' and 'y' columns\n        containing the projected 2D coordinates.\n\n        Args:\n            input_df (pd.DataFrame): The DataFrame containing features to project.\n            features (list, optional): List of feature column names. If None, numeric columns are auto-selected.\n            feature_matrix (np.ndarray, optional): Pre-computed feature matrix. If provided, features is ignored\n                and no scaling is applied (caller is responsible for appropriate preprocessing).\n            metric (str, optional): Distance metric for UMAP (e.g., 'euclidean', 'jaccard'). Default 'euclidean'.\n            projection (str, optional): The projection to use ('UMAP', 'TSNE', 'MDS' or 'PCA'). Default 'UMAP'.\n\n        Returns:\n            pd.DataFrame: A new DataFrame (a copy of input_df) with added 'x' and 'y' columns.\n        \"\"\"\n        # Create a copy of the input DataFrame\n        df = input_df.copy()\n\n        # If a feature matrix is provided, use it directly (no scaling)\n        if feature_matrix is not None:\n            if len(feature_matrix) != len(df):\n                self.log.critical(\"feature_matrix length must match DataFrame length.\")\n                return df\n            X_processed = feature_matrix\n        else:\n            # Auto-identify numeric features if none are provided\n            if features is None:\n                features = [col for col in df.select_dtypes(include=\"number\").columns if not col.endswith(\"id\")]\n                self.log.info(f\"Auto-identified numeric features: {features}\")\n\n            if len(features) &lt; 2 or df.empty:\n                self.log.critical(\"At least two numeric features are required, and DataFrame must not be empty.\")\n                return df\n\n            # Process a copy of the feature data for projection\n            X = df[features]\n            X = X.apply(lambda col: col.fillna(col.mean()))\n            X_processed = StandardScaler().fit_transform(X)\n\n        # Select the projection method (using df for perplexity calculation)\n        self.projection_model = self._get_projection_model(projection, df, metric=metric)\n\n        # Apply the projection on the processed data\n        projection_result = self.projection_model.fit_transform(X_processed)\n        df[[\"x\", \"y\"]] = projection_result\n\n        # Resolve coincident points and return the new DataFrame\n        return self.resolve_coincident_points(df)\n\n    def _get_projection_model(self, projection: str, df: pd.DataFrame, metric: str = \"euclidean\"):\n        \"\"\"Select and return the appropriate projection model.\n\n        Args:\n            projection (str): The projection method ('TSNE', 'MDS', 'PCA', or 'UMAP').\n            df (pd.DataFrame): The DataFrame being transformed (used for computing perplexity).\n            metric (str): Distance metric for UMAP (default 'euclidean').\n\n        Returns:\n            A dimensionality reduction model instance.\n        \"\"\"\n        if projection == \"TSNE\":\n            perplexity = min(40, len(df) - 1)\n            self.log.info(f\"Projection: TSNE with perplexity {perplexity}\")\n            return TSNE(perplexity=perplexity)\n\n        if projection == \"MDS\":\n            self.log.info(\"Projection: MDS\")\n            return MDS(n_components=2, random_state=0)\n\n        if projection == \"PCA\":\n            self.log.info(\"Projection: PCA\")\n            return PCA(n_components=2)\n\n        if projection == \"UMAP\" and UMAP_AVAILABLE:\n            # UMAP default n_neighbors=15, adjust if dataset is smaller\n            n_neighbors = min(15, len(df) - 1)\n            if n_neighbors &lt; 15:\n                self.log.warning(\n                    f\"Dataset size ({len(df)}) smaller than default n_neighbors, using n_neighbors={n_neighbors}\"\n                )\n            self.log.info(f\"Projection: UMAP with metric={metric}, n_neighbors={n_neighbors}\")\n            return umap.UMAP(n_components=2, metric=metric, n_neighbors=n_neighbors)\n\n        self.log.warning(\n            f\"Projection method '{projection}' not recognized or UMAP not available. Falling back to TSNE.\"\n        )\n        return TSNE(perplexity=min(40, len(df) - 1))\n\n    @staticmethod\n    def resolve_coincident_points(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Resolve coincident points using random jitter\n\n        Args:\n            df (pd.DataFrame): DataFrame with x and y coordinates.\n\n        Returns:\n            pd.DataFrame: DataFrame with resolved coincident points\n        \"\"\"\n\n        # Set jitter size based on rounding precision\n        precision = 3\n        jitter_amount = 10 ** (-precision) * 2  # 2x the rounding precision\n\n        # Create rounded values for grouping\n        rounded = pd.DataFrame(\n            {\"x_round\": df[\"x\"].round(precision), \"y_round\": df[\"y\"].round(precision), \"idx\": df.index}\n        )\n\n        # Find duplicates\n        duplicated = rounded.duplicated(subset=[\"x_round\", \"y_round\"], keep=False)\n        if not duplicated.any():\n            return df\n\n        # Get the dtypes of the columns\n        x_dtype = df[\"x\"].dtype\n        y_dtype = df[\"y\"].dtype\n\n        # Process each group\n        for (x_round, y_round), group in rounded[duplicated].groupby([\"x_round\", \"y_round\"]):\n            indices = group[\"idx\"].values\n            if len(indices) &lt;= 1:\n                continue\n\n            # Apply random jitter to all points\n            for i, idx in enumerate(indices):\n                # Generate and apply properly typed offsets\n                dx = np.array(jitter_amount * (np.random.random() * 2 - 1), dtype=x_dtype)\n                dy = np.array(jitter_amount * (np.random.random() * 2 - 1), dtype=y_dtype)\n                df.loc[idx, \"x\"] += dx\n                df.loc[idx, \"y\"] += dy\n\n        return df\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.projection_2d.Projection2D.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Projection2D class.</p> Source code in <code>src/workbench/algorithms/dataframe/projection_2d.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the Projection2D class.\"\"\"\n    self.log = logging.getLogger(\"workbench\")\n    self.projection_model = None\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.projection_2d.Projection2D.fit_transform","title":"<code>fit_transform(input_df, features=None, feature_matrix=None, metric='euclidean', projection='UMAP')</code>","text":"<p>Fit and transform a DataFrame using the selected dimensionality reduction method.</p> <p>This method creates a copy of the input DataFrame, processes the specified features for normalization and projection, and returns a new DataFrame with added 'x' and 'y' columns containing the projected 2D coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>The DataFrame containing features to project.</p> required <code>features</code> <code>list</code> <p>List of feature column names. If None, numeric columns are auto-selected.</p> <code>None</code> <code>feature_matrix</code> <code>ndarray</code> <p>Pre-computed feature matrix. If provided, features is ignored and no scaling is applied (caller is responsible for appropriate preprocessing).</p> <code>None</code> <code>metric</code> <code>str</code> <p>Distance metric for UMAP (e.g., 'euclidean', 'jaccard'). Default 'euclidean'.</p> <code>'euclidean'</code> <code>projection</code> <code>str</code> <p>The projection to use ('UMAP', 'TSNE', 'MDS' or 'PCA'). Default 'UMAP'.</p> <code>'UMAP'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame (a copy of input_df) with added 'x' and 'y' columns.</p> Source code in <code>src/workbench/algorithms/dataframe/projection_2d.py</code> <pre><code>def fit_transform(\n    self,\n    input_df: pd.DataFrame,\n    features: list = None,\n    feature_matrix: np.ndarray = None,\n    metric: str = \"euclidean\",\n    projection: str = \"UMAP\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fit and transform a DataFrame using the selected dimensionality reduction method.\n\n    This method creates a copy of the input DataFrame, processes the specified features\n    for normalization and projection, and returns a new DataFrame with added 'x' and 'y' columns\n    containing the projected 2D coordinates.\n\n    Args:\n        input_df (pd.DataFrame): The DataFrame containing features to project.\n        features (list, optional): List of feature column names. If None, numeric columns are auto-selected.\n        feature_matrix (np.ndarray, optional): Pre-computed feature matrix. If provided, features is ignored\n            and no scaling is applied (caller is responsible for appropriate preprocessing).\n        metric (str, optional): Distance metric for UMAP (e.g., 'euclidean', 'jaccard'). Default 'euclidean'.\n        projection (str, optional): The projection to use ('UMAP', 'TSNE', 'MDS' or 'PCA'). Default 'UMAP'.\n\n    Returns:\n        pd.DataFrame: A new DataFrame (a copy of input_df) with added 'x' and 'y' columns.\n    \"\"\"\n    # Create a copy of the input DataFrame\n    df = input_df.copy()\n\n    # If a feature matrix is provided, use it directly (no scaling)\n    if feature_matrix is not None:\n        if len(feature_matrix) != len(df):\n            self.log.critical(\"feature_matrix length must match DataFrame length.\")\n            return df\n        X_processed = feature_matrix\n    else:\n        # Auto-identify numeric features if none are provided\n        if features is None:\n            features = [col for col in df.select_dtypes(include=\"number\").columns if not col.endswith(\"id\")]\n            self.log.info(f\"Auto-identified numeric features: {features}\")\n\n        if len(features) &lt; 2 or df.empty:\n            self.log.critical(\"At least two numeric features are required, and DataFrame must not be empty.\")\n            return df\n\n        # Process a copy of the feature data for projection\n        X = df[features]\n        X = X.apply(lambda col: col.fillna(col.mean()))\n        X_processed = StandardScaler().fit_transform(X)\n\n    # Select the projection method (using df for perplexity calculation)\n    self.projection_model = self._get_projection_model(projection, df, metric=metric)\n\n    # Apply the projection on the processed data\n    projection_result = self.projection_model.fit_transform(X_processed)\n    df[[\"x\", \"y\"]] = projection_result\n\n    # Resolve coincident points and return the new DataFrame\n    return self.resolve_coincident_points(df)\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#workbench.algorithms.dataframe.projection_2d.Projection2D.resolve_coincident_points","title":"<code>resolve_coincident_points(df)</code>  <code>staticmethod</code>","text":"<p>Resolve coincident points using random jitter</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with x and y coordinates.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with resolved coincident points</p> Source code in <code>src/workbench/algorithms/dataframe/projection_2d.py</code> <pre><code>@staticmethod\ndef resolve_coincident_points(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Resolve coincident points using random jitter\n\n    Args:\n        df (pd.DataFrame): DataFrame with x and y coordinates.\n\n    Returns:\n        pd.DataFrame: DataFrame with resolved coincident points\n    \"\"\"\n\n    # Set jitter size based on rounding precision\n    precision = 3\n    jitter_amount = 10 ** (-precision) * 2  # 2x the rounding precision\n\n    # Create rounded values for grouping\n    rounded = pd.DataFrame(\n        {\"x_round\": df[\"x\"].round(precision), \"y_round\": df[\"y\"].round(precision), \"idx\": df.index}\n    )\n\n    # Find duplicates\n    duplicated = rounded.duplicated(subset=[\"x_round\", \"y_round\"], keep=False)\n    if not duplicated.any():\n        return df\n\n    # Get the dtypes of the columns\n    x_dtype = df[\"x\"].dtype\n    y_dtype = df[\"y\"].dtype\n\n    # Process each group\n    for (x_round, y_round), group in rounded[duplicated].groupby([\"x_round\", \"y_round\"]):\n        indices = group[\"idx\"].values\n        if len(indices) &lt;= 1:\n            continue\n\n        # Apply random jitter to all points\n        for i, idx in enumerate(indices):\n            # Generate and apply properly typed offsets\n            dx = np.array(jitter_amount * (np.random.random() * 2 - 1), dtype=x_dtype)\n            dy = np.array(jitter_amount * (np.random.random() * 2 - 1), dtype=y_dtype)\n            df.loc[idx, \"x\"] += dx\n            df.loc[idx, \"y\"] += dy\n\n    return df\n</code></pre>"},{"location":"data_algorithms/dataframes/overview/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"data_algorithms/graphs/overview/","title":"Graph Algorithms","text":"<p>Graph Algorithms</p> <p>WIP: These classes are currently actively being developed and are subject to change in both API and functionality over time.</p> <ul> <li>TBD: TBD</li> </ul> <p>Graph Algorithms</p> <p>Docs TBD</p>"},{"location":"data_algorithms/graphs/overview/#workbench.algorithms.graph.light.proximity_graph.ProximityGraph","title":"<code>ProximityGraph</code>","text":"<p>Build a NetworkX graph using the Proximity class.</p> Source code in <code>src/workbench/algorithms/graph/light/proximity_graph.py</code> <pre><code>class ProximityGraph:\n    \"\"\"Build a NetworkX graph using the Proximity class.\"\"\"\n\n    def __init__(self):\n        \"\"\"Build a NetworkX graph using the Proximity class.\"\"\"\n\n        # The graph is stored as NetworkX graph\n        self._nx_graph = None\n\n        # GraphStore\n        self.graph_store = GraphStore()\n\n    def build_graph(self, proximity_instance: Proximity) -&gt; None:\n        \"\"\"\n        Build a NetworkX graph using a Proximity class.\n\n        Args:\n            proximity_instance (Proximity): An instance of a Proximity class to compute neighbors.\n        \"\"\"\n        # Retrieve all neighbors and their distances\n        prox = proximity_instance\n        node_df = prox.df\n        id_column = prox.id_column\n        log.info(\"Retrieving all neighbors...\")\n        all_neighbors_df = prox.all_neighbors()\n\n        # Add nodes with attributes (features)\n        log.info(\"Adding nodes to the proximity graph...\")\n        self._nx_graph = nx.Graph()\n\n        # Check for duplicate IDs in the node DataFrame\n        if not node_df[id_column].is_unique:\n            log.error(f\"Column '{id_column}' contains duplicate values. Using first occurrence for each ID...\")\n            node_df = node_df.drop_duplicates(subset=[id_column], keep=\"first\")\n\n        # Set the id_column as index and add nodes\n        self._nx_graph.add_nodes_from(node_df.set_index(id_column, drop=False).to_dict(\"index\").items())\n\n        # Determine edge weights based on proximity type\n        # if prox.proximity_type == ProximityType.SIMILARITY:\n        #     all_neighbors_df[\"weight\"] = all_neighbors_df[\"similarity\"]\n        # elif prox.proximity_type == ProximityType.DISTANCE:\n\n        # Normalize and invert distance\n        max_distance = all_neighbors_df[\"distance\"].max()\n        all_neighbors_df[\"weight\"] = 1.0 - all_neighbors_df[\"distance\"] / max_distance\n\n        # Add edges to the graph\n        log.info(\"Adding edges to the graph...\")\n        min_edges = 2\n        min_weight = 0.8\n\n        # Group by source ID and process each group\n        for source_id, group in all_neighbors_df.groupby(id_column):\n            # Sort by weight (assuming higher is better)\n            sorted_group = group.sort_values(\"weight\", ascending=False)\n\n            # Take all edges up to min_edges (or all if fewer)\n            actual_min_edges = min(len(sorted_group), min_edges)\n            top_edges = sorted_group.iloc[:actual_min_edges]\n\n            # Also take any additional neighbors above min_weight (beyond the top edges)\n            high_weight_edges = sorted_group.iloc[actual_min_edges:][\n                sorted_group.iloc[actual_min_edges:][\"weight\"] &gt; min_weight\n            ]\n\n            # Combine both sets\n            edges_to_add = pd.concat([top_edges, high_weight_edges])\n\n            # Add all edges at once\n            self._nx_graph.add_edges_from(\n                [(source_id, row[\"neighbor_id\"], {\"weight\": row[\"weight\"]}) for _, row in edges_to_add.iterrows()]\n            )\n\n    @property\n    def nx_graph(self) -&gt; nx.Graph:\n        \"\"\"\n        Get the NetworkX graph object.\n\n        Returns:\n            nx.Graph: The NetworkX graph object.\n        \"\"\"\n        return self._nx_graph\n\n    def load_graph(self, graph_path: str):\n        \"\"\"\n        Load a graph from the GraphStore.\n\n        Args:\n            graph_path (str): The path to the graph in GraphStore.\n        \"\"\"\n        self._nx_graph = self.graph_store.get(graph_path)\n\n    def store_graph(self, graph_path: str):\n        \"\"\"\n        Store the graph in the GraphStore.\n\n        Args:\n            graph_path (str): The path to store the graph in GraphStore.\n        \"\"\"\n        self.graph_store.upsert(graph_path, self._nx_graph)\n\n    def get_neighborhood(self, node_id: Union[str, int], radius: int = 1) -&gt; nx.Graph:\n        \"\"\"\n        Get a subgraph containing nodes within a given number of hops around a specific node.\n\n        Args:\n            node_id: The ID of the node to center the neighborhood around.\n            radius: The number of hops to consider around the node (default: 1).\n\n        Returns:\n            nx.Graph: A subgraph containing the specified neighborhood.\n        \"\"\"\n        # Use NetworkX's ego_graph to extract the neighborhood within the given radius\n        if node_id in self._nx_graph:\n            return nx.ego_graph(self._nx_graph, node_id, radius=radius)\n        else:\n            raise ValueError(f\"Node ID '{node_id}' not found in the graph.\")\n</code></pre>"},{"location":"data_algorithms/graphs/overview/#workbench.algorithms.graph.light.proximity_graph.ProximityGraph.nx_graph","title":"<code>nx_graph</code>  <code>property</code>","text":"<p>Get the NetworkX graph object.</p> <p>Returns:</p> Type Description <code>Graph</code> <p>nx.Graph: The NetworkX graph object.</p>"},{"location":"data_algorithms/graphs/overview/#workbench.algorithms.graph.light.proximity_graph.ProximityGraph.__init__","title":"<code>__init__()</code>","text":"<p>Build a NetworkX graph using the Proximity class.</p> Source code in <code>src/workbench/algorithms/graph/light/proximity_graph.py</code> <pre><code>def __init__(self):\n    \"\"\"Build a NetworkX graph using the Proximity class.\"\"\"\n\n    # The graph is stored as NetworkX graph\n    self._nx_graph = None\n\n    # GraphStore\n    self.graph_store = GraphStore()\n</code></pre>"},{"location":"data_algorithms/graphs/overview/#workbench.algorithms.graph.light.proximity_graph.ProximityGraph.build_graph","title":"<code>build_graph(proximity_instance)</code>","text":"<p>Build a NetworkX graph using a Proximity class.</p> <p>Parameters:</p> Name Type Description Default <code>proximity_instance</code> <code>Proximity</code> <p>An instance of a Proximity class to compute neighbors.</p> required Source code in <code>src/workbench/algorithms/graph/light/proximity_graph.py</code> <pre><code>def build_graph(self, proximity_instance: Proximity) -&gt; None:\n    \"\"\"\n    Build a NetworkX graph using a Proximity class.\n\n    Args:\n        proximity_instance (Proximity): An instance of a Proximity class to compute neighbors.\n    \"\"\"\n    # Retrieve all neighbors and their distances\n    prox = proximity_instance\n    node_df = prox.df\n    id_column = prox.id_column\n    log.info(\"Retrieving all neighbors...\")\n    all_neighbors_df = prox.all_neighbors()\n\n    # Add nodes with attributes (features)\n    log.info(\"Adding nodes to the proximity graph...\")\n    self._nx_graph = nx.Graph()\n\n    # Check for duplicate IDs in the node DataFrame\n    if not node_df[id_column].is_unique:\n        log.error(f\"Column '{id_column}' contains duplicate values. Using first occurrence for each ID...\")\n        node_df = node_df.drop_duplicates(subset=[id_column], keep=\"first\")\n\n    # Set the id_column as index and add nodes\n    self._nx_graph.add_nodes_from(node_df.set_index(id_column, drop=False).to_dict(\"index\").items())\n\n    # Determine edge weights based on proximity type\n    # if prox.proximity_type == ProximityType.SIMILARITY:\n    #     all_neighbors_df[\"weight\"] = all_neighbors_df[\"similarity\"]\n    # elif prox.proximity_type == ProximityType.DISTANCE:\n\n    # Normalize and invert distance\n    max_distance = all_neighbors_df[\"distance\"].max()\n    all_neighbors_df[\"weight\"] = 1.0 - all_neighbors_df[\"distance\"] / max_distance\n\n    # Add edges to the graph\n    log.info(\"Adding edges to the graph...\")\n    min_edges = 2\n    min_weight = 0.8\n\n    # Group by source ID and process each group\n    for source_id, group in all_neighbors_df.groupby(id_column):\n        # Sort by weight (assuming higher is better)\n        sorted_group = group.sort_values(\"weight\", ascending=False)\n\n        # Take all edges up to min_edges (or all if fewer)\n        actual_min_edges = min(len(sorted_group), min_edges)\n        top_edges = sorted_group.iloc[:actual_min_edges]\n\n        # Also take any additional neighbors above min_weight (beyond the top edges)\n        high_weight_edges = sorted_group.iloc[actual_min_edges:][\n            sorted_group.iloc[actual_min_edges:][\"weight\"] &gt; min_weight\n        ]\n\n        # Combine both sets\n        edges_to_add = pd.concat([top_edges, high_weight_edges])\n\n        # Add all edges at once\n        self._nx_graph.add_edges_from(\n            [(source_id, row[\"neighbor_id\"], {\"weight\": row[\"weight\"]}) for _, row in edges_to_add.iterrows()]\n        )\n</code></pre>"},{"location":"data_algorithms/graphs/overview/#workbench.algorithms.graph.light.proximity_graph.ProximityGraph.get_neighborhood","title":"<code>get_neighborhood(node_id, radius=1)</code>","text":"<p>Get a subgraph containing nodes within a given number of hops around a specific node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>Union[str, int]</code> <p>The ID of the node to center the neighborhood around.</p> required <code>radius</code> <code>int</code> <p>The number of hops to consider around the node (default: 1).</p> <code>1</code> <p>Returns:</p> Type Description <code>Graph</code> <p>nx.Graph: A subgraph containing the specified neighborhood.</p> Source code in <code>src/workbench/algorithms/graph/light/proximity_graph.py</code> <pre><code>def get_neighborhood(self, node_id: Union[str, int], radius: int = 1) -&gt; nx.Graph:\n    \"\"\"\n    Get a subgraph containing nodes within a given number of hops around a specific node.\n\n    Args:\n        node_id: The ID of the node to center the neighborhood around.\n        radius: The number of hops to consider around the node (default: 1).\n\n    Returns:\n        nx.Graph: A subgraph containing the specified neighborhood.\n    \"\"\"\n    # Use NetworkX's ego_graph to extract the neighborhood within the given radius\n    if node_id in self._nx_graph:\n        return nx.ego_graph(self._nx_graph, node_id, radius=radius)\n    else:\n        raise ValueError(f\"Node ID '{node_id}' not found in the graph.\")\n</code></pre>"},{"location":"data_algorithms/graphs/overview/#workbench.algorithms.graph.light.proximity_graph.ProximityGraph.load_graph","title":"<code>load_graph(graph_path)</code>","text":"<p>Load a graph from the GraphStore.</p> <p>Parameters:</p> Name Type Description Default <code>graph_path</code> <code>str</code> <p>The path to the graph in GraphStore.</p> required Source code in <code>src/workbench/algorithms/graph/light/proximity_graph.py</code> <pre><code>def load_graph(self, graph_path: str):\n    \"\"\"\n    Load a graph from the GraphStore.\n\n    Args:\n        graph_path (str): The path to the graph in GraphStore.\n    \"\"\"\n    self._nx_graph = self.graph_store.get(graph_path)\n</code></pre>"},{"location":"data_algorithms/graphs/overview/#workbench.algorithms.graph.light.proximity_graph.ProximityGraph.store_graph","title":"<code>store_graph(graph_path)</code>","text":"<p>Store the graph in the GraphStore.</p> <p>Parameters:</p> Name Type Description Default <code>graph_path</code> <code>str</code> <p>The path to store the graph in GraphStore.</p> required Source code in <code>src/workbench/algorithms/graph/light/proximity_graph.py</code> <pre><code>def store_graph(self, graph_path: str):\n    \"\"\"\n    Store the graph in the GraphStore.\n\n    Args:\n        graph_path (str): The path to store the graph in GraphStore.\n    \"\"\"\n    self.graph_store.upsert(graph_path, self._nx_graph)\n</code></pre>"},{"location":"data_algorithms/graphs/overview/#workbench.algorithms.graph.light.proximity_graph.show_graph","title":"<code>show_graph(graph, id_column)</code>","text":"<p>Display the graph using Plotly.</p> Source code in <code>src/workbench/algorithms/graph/light/proximity_graph.py</code> <pre><code>def show_graph(graph, id_column):\n    \"\"\"Display the graph using Plotly.\"\"\"\n    graph_plot = GraphPlot()\n    properties = graph_plot.update_properties(graph, labels=id_column, hover_text=\"all\")\n    fig = properties[0]\n    fig.update_layout(paper_bgcolor=\"rgb(30,30,30)\", plot_bgcolor=\"rgb(30,30,30)\")\n    fig.show()\n</code></pre>"},{"location":"data_algorithms/graphs/overview/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"data_algorithms/spark/overview/","title":"Graph Algorithms","text":"<p>Graph Algorithms</p> <p>WIP: These classes are currently actively being developed and are subject to change in both API and functionality over time.</p> <ul> <li>TBD: TBD</li> </ul> <p>Graph Algorithms</p> <p>Docs TBD</p> <p>ComputationView Class: Create a View with a subset of columns for display purposes</p>"},{"location":"data_algorithms/spark/overview/#workbench.core.views.computation_view.ComputationView","title":"<code>ComputationView</code>","text":"<p>               Bases: <code>ColumnSubsetView</code></p> <p>ComputationView Class: Create a View with a subset of columns for computation purposes</p> Common Usage <pre><code># Create a default ComputationView\nfs = FeatureSet(\"test_features\")\ncomp_view = ComputationView.create(fs)\ndf = comp_view.pull_dataframe()\n\n# Create a ComputationView with a specific set of columns\ncomp_view = ComputationView.create(fs, column_list=[\"my_col1\", \"my_col2\"])\n\n# Query the view\ndf = comp_view.query(f\"SELECT * FROM {comp_view.table} where prediction &gt; 0.5\")\n</code></pre> Source code in <code>src/workbench/core/views/computation_view.py</code> <pre><code>class ComputationView(ColumnSubsetView):\n    \"\"\"ComputationView Class: Create a View with a subset of columns for computation purposes\n\n    Common Usage:\n        ```python\n        # Create a default ComputationView\n        fs = FeatureSet(\"test_features\")\n        comp_view = ComputationView.create(fs)\n        df = comp_view.pull_dataframe()\n\n        # Create a ComputationView with a specific set of columns\n        comp_view = ComputationView.create(fs, column_list=[\"my_col1\", \"my_col2\"])\n\n        # Query the view\n        df = comp_view.query(f\"SELECT * FROM {comp_view.table} where prediction &gt; 0.5\")\n        ```\n    \"\"\"\n\n    @classmethod\n    def create(\n        cls,\n        artifact: Union[DataSource, FeatureSet],\n        source_table: str = None,\n        column_list: Union[list[str], None] = None,\n        column_limit: int = 30,\n    ) -&gt; Union[View, None]:\n        \"\"\"Factory method to create and return a ComputationView instance.\n\n        Args:\n            artifact (Union[DataSource, FeatureSet]): The DataSource or FeatureSet object\n            source_table (str, optional): The table/view to create the view from. Defaults to None\n            column_list (Union[list[str], None], optional): A list of columns to include. Defaults to None.\n            column_limit (int, optional): The max number of columns to include. Defaults to 30.\n\n        Returns:\n            Union[View, None]: The created View object (or None if failed to create the view)\n        \"\"\"\n        # Use the create logic directly from ColumnSubsetView with the \"computation\" view name\n        return ColumnSubsetView.create(\"computation\", artifact, source_table, column_list, column_limit)\n</code></pre>"},{"location":"data_algorithms/spark/overview/#workbench.core.views.computation_view.ComputationView.create","title":"<code>create(artifact, source_table=None, column_list=None, column_limit=30)</code>  <code>classmethod</code>","text":"<p>Factory method to create and return a ComputationView instance.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Union[DataSource, FeatureSet]</code> <p>The DataSource or FeatureSet object</p> required <code>source_table</code> <code>str</code> <p>The table/view to create the view from. Defaults to None</p> <code>None</code> <code>column_list</code> <code>Union[list[str], None]</code> <p>A list of columns to include. Defaults to None.</p> <code>None</code> <code>column_limit</code> <code>int</code> <p>The max number of columns to include. Defaults to 30.</p> <code>30</code> <p>Returns:</p> Type Description <code>Union[View, None]</code> <p>Union[View, None]: The created View object (or None if failed to create the view)</p> Source code in <code>src/workbench/core/views/computation_view.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    artifact: Union[DataSource, FeatureSet],\n    source_table: str = None,\n    column_list: Union[list[str], None] = None,\n    column_limit: int = 30,\n) -&gt; Union[View, None]:\n    \"\"\"Factory method to create and return a ComputationView instance.\n\n    Args:\n        artifact (Union[DataSource, FeatureSet]): The DataSource or FeatureSet object\n        source_table (str, optional): The table/view to create the view from. Defaults to None\n        column_list (Union[list[str], None], optional): A list of columns to include. Defaults to None.\n        column_limit (int, optional): The max number of columns to include. Defaults to 30.\n\n    Returns:\n        Union[View, None]: The created View object (or None if failed to create the view)\n    \"\"\"\n    # Use the create logic directly from ColumnSubsetView with the \"computation\" view name\n    return ColumnSubsetView.create(\"computation\", artifact, source_table, column_list, column_limit)\n</code></pre>"},{"location":"data_algorithms/spark/overview/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"data_algorithms/sql/overview/","title":"SQL Algorithms","text":"<p>SQL Algorithms</p> <p>One of the main benefit of SQL Algorithms is that the 'heavy lifting' is all done on the SQL Database, so if you have large datassets this is the place for you.</p> <ul> <li> <p>SQL: SQL queries that provide a wide range of functionality:</p> <ul> <li>Outliers</li> <li>Descriptive Stats</li> <li>Correlations</li> <li>and More Oulliers</li> </ul> </li> </ul> <p>SQL based Outliers: Compute outliers for all the columns in a DataSource using SQL</p> <p>SQL based Descriptive Stats: Compute Descriptive Stats for all the numeric columns in a DataSource using SQL</p> <p>SQL based Correlations: Compute Correlations for all the numeric columns in a DataSource using SQL</p>"},{"location":"data_algorithms/sql/overview/#workbench.algorithms.sql.outliers.Outliers","title":"<code>Outliers</code>","text":"<p>Outliers: Class to compute outliers for all the columns in a DataSource using SQL</p> Source code in <code>src/workbench/algorithms/sql/outliers.py</code> <pre><code>class Outliers:\n    \"\"\"Outliers: Class to compute outliers for all the columns in a DataSource using SQL\"\"\"\n\n    def __init__(self):\n        \"\"\"SQLOutliers Initialization\"\"\"\n        self.outlier_group = \"unknown\"\n\n    def compute_outliers(\n        self, data_source: DataSourceAbstract, scale: float = 1.5, use_stddev: bool = False\n    ) -&gt; pd.DataFrame:\n        \"\"\"Compute outliers for all the numeric columns in a DataSource\n        Args:\n            data_source(DataSource): The DataSource that we're computing outliers on\n            scale (float): The scale to use for either the IQR or stddev outlier calculation (default: 1.5)\n            use_stddev (bool): Option to use the standard deviation for the outlier calculation (default: False)\n        Returns:\n            pd.DataFrame: A DataFrame of outliers for this DataSource\n        Notes:\n            Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma)\n            The scale parameter can be adjusted to change the IQR multiplier\n        \"\"\"\n\n        # Note: If use_stddev is True, then the scale parameter needs to be adjusted\n        if use_stddev and scale == 1.5:  # If the default scale is used, adjust it\n            scale = 2.5\n\n        # Compute the numeric outliers\n        outlier_df = self._numeric_outliers(data_source, scale, use_stddev)\n\n        # If there are no outliers, return a DataFrame with the computation columns but no rows\n        if outlier_df is None:\n            columns = data_source.view(\"computation\").columns\n            return pd.DataFrame(columns=columns + [\"outlier_group\"])\n\n        # Get the top N outliers for each outlier group\n        outlier_df = self.get_top_n_outliers(outlier_df)\n\n        # Make sure the dataframe isn't too big, if it's too big sample it down\n        if len(outlier_df) &gt; 500:\n            log.important(f\"Outliers DataFrame is too large {len(outlier_df)}, sampling down to 500 rows\")\n            outlier_df = outlier_df.sample(300)\n\n        # Sort by outlier_group and reset the index\n        outlier_df = outlier_df.sort_values(\"outlier_group\").reset_index(drop=True)\n        return outlier_df\n\n    def _numeric_outliers(self, data_source: DataSourceAbstract, scale: float, use_stddev=False) -&gt; pd.DataFrame:\n        \"\"\"Internal method to compute outliers for all numeric columns\n        Args:\n            data_source(DataSource): The DataSource that we're computing outliers on\n            scale (float): The scale to use for the IQR outlier calculation\n            use_stddev (bool): Option to use the standard deviation for the outlier calculation (default: False)\n        Returns:\n            pd.DataFrame: A DataFrame of all the outliers combined\n        \"\"\"\n\n        # Grab the column stats and descriptive stats for this DataSource\n        column_stats = data_source.column_stats()\n        descriptive_stats = data_source.descriptive_stats()\n\n        # If there are no numeric columns, return None\n        if not descriptive_stats:\n            log.warning(\"No numeric columns found in the current computation view of the DataSource\")\n            log.warning(\"If the data source was created from a DataFrame, ensure that the DataFrame was properly typed\")\n            log.warning(\"Recommendation: Properly type the DataFrame and recreate the Workbench artifact\")\n            return None\n\n        # Get the column names and types from the DataSource\n        column_details = data_source.view(\"computation\").column_details()\n\n        # For every column in the data_source that is numeric get the outliers\n        # This loop computes the columns, lower bounds, and upper bounds for the SQL query\n        log.info(\"Computing Outliers for numeric columns...\")\n        numeric = [\"tinyint\", \"smallint\", \"int\", \"bigint\", \"float\", \"double\", \"decimal\"]\n        columns = []\n        lower_bounds = []\n        upper_bounds = []\n        for column, data_type in column_details.items():\n            if data_type in numeric:\n                # Skip columns that just have one value (or are all nans)\n                if column_stats[column][\"unique\"] &lt;= 1:\n                    log.info(f\"Skipping unary column {column} with value {descriptive_stats[column]['min']}\")\n                    continue\n\n                # Skip columns that are 'binary' columns\n                if column_stats[column][\"unique\"] == 2:\n                    log.info(f\"Skipping binary column {column}\")\n                    continue\n\n                # Do they want to use the stddev instead of IQR?\n                if use_stddev:\n                    mean = descriptive_stats[column][\"mean\"]\n                    stddev = descriptive_stats[column][\"stddev\"]\n                    lower_bound = mean - (stddev * scale)\n                    upper_bound = mean + (stddev * scale)\n\n                # Compute the IQR for this column\n                else:\n                    iqr = descriptive_stats[column][\"q3\"] - descriptive_stats[column][\"q1\"]\n                    lower_bound = descriptive_stats[column][\"q1\"] - (iqr * scale)\n                    upper_bound = descriptive_stats[column][\"q3\"] + (iqr * scale)\n\n                # Add the column, lower bound, and upper bound to the lists\n                columns.append(column)\n                lower_bounds.append(lower_bound)\n                upper_bounds.append(upper_bound)\n\n        # Compute the SQL query\n        query = self._multi_column_outlier_query(data_source, columns, lower_bounds, upper_bounds)\n        outlier_df = data_source.query(query)\n\n        # Label the outlier groups\n        outlier_df = self._label_outlier_groups(outlier_df, columns, lower_bounds, upper_bounds)\n        return outlier_df\n\n    @staticmethod\n    def _multi_column_outlier_query(\n        data_source: DataSourceAbstract, columns: list, lower_bounds: list, upper_bounds: list\n    ) -&gt; str:\n        \"\"\"Internal method to compute outliers for multiple columns\n        Args:\n            data_source(DataSource): The DataSource that we're computing outliers on\n            columns(list): The columns to compute outliers on\n            lower_bounds(list): The lower bounds for outliers\n            upper_bounds(list): The upper bounds for outliers\n        Returns:\n            str: A SQL query to compute outliers for multiple columns\n        \"\"\"\n        # Grab the DataSource computation table name\n        table = data_source.view(\"computation\").table\n\n        # Get the column names and types from the DataSource\n        column_details = data_source.view(\"computation\").column_details()\n        sql_columns = \", \".join([f'\"{col}\"' for col in column_details.keys()])\n\n        query = f'SELECT {sql_columns} FROM \"{table}\" WHERE '\n        for col, lb, ub in zip(columns, lower_bounds, upper_bounds):\n            query += f\"({col} &lt; {lb} OR {col} &gt; {ub}) OR \"\n        query = query[:-4]\n\n        # Add a limit just in case\n        query += \" LIMIT 5000\"\n        return query\n\n    @staticmethod\n    def _label_outlier_groups(\n        outlier_df: pd.DataFrame, columns: list, lower_bounds: list, upper_bounds: list\n    ) -&gt; pd.DataFrame:\n        \"\"\"Internal method to label outliers by group.\n        Args:\n            outlier_df(pd.DataFrame): The DataFrame of outliers\n            columns(list): The columns for which to compute outliers\n            lower_bounds(list): The lower bounds for each column\n            upper_bounds(list): The upper bounds for each column\n        Returns:\n            pd.DataFrame: A DataFrame with an added 'outlier_group' column, indicating the type of outlier.\n        \"\"\"\n\n        column_outlier_dfs = []\n        for col, lb, ub in zip(columns, lower_bounds, upper_bounds):\n            mask_low = outlier_df[col] &lt; lb\n            mask_high = outlier_df[col] &gt; ub\n\n            low_df = outlier_df[mask_low].copy()\n            low_df[\"outlier_group\"] = f\"{col}_low\"\n\n            high_df = outlier_df[mask_high].copy()\n            high_df[\"outlier_group\"] = f\"{col}_high\"\n\n            column_outlier_dfs.extend([low_df, high_df])\n\n        # If there are no outliers, return the original DataFrame with an empty 'outlier_group' column\n        if not column_outlier_dfs:\n            log.critical(\"No outliers found in the data source.. probably something is wrong\")\n            return outlier_df.assign(outlier_group=\"\")\n\n        # Concatenate the DataFrames and return\n        return pd.concat(column_outlier_dfs, ignore_index=True)\n\n    @staticmethod\n    def get_top_n_outliers(outlier_df: pd.DataFrame, n: int = 10) -&gt; pd.DataFrame:\n        \"\"\"Function to retrieve the top N highest and lowest outliers for each outlier group.\n\n        Args:\n            outlier_df (pd.DataFrame): The DataFrame of outliers with 'outlier_group' column\n            n (int): Number of top outliers to retrieve for each group, defaults to 10\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the top N outliers for each outlier group\n        \"\"\"\n\n        def get_extreme_values(group: pd.DataFrame) -&gt; pd.DataFrame:\n            \"\"\"Helper function to get the top N extreme values from a group.\"\"\"\n            col, extreme_type = group.name.rsplit(\"_\", 1)\n            if extreme_type == \"low\":\n                return group.nsmallest(n, col)\n            else:\n                return group.nlargest(n, col)\n\n        # Group by 'outlier_group' and apply the helper function, explicitly selecting columns to silence warning\n        top_outliers = outlier_df.groupby(\"outlier_group\", group_keys=False)[outlier_df.columns].apply(\n            get_extreme_values\n        )\n        return top_outliers.reset_index(drop=True)\n</code></pre>"},{"location":"data_algorithms/sql/overview/#workbench.algorithms.sql.outliers.Outliers.__init__","title":"<code>__init__()</code>","text":"<p>SQLOutliers Initialization</p> Source code in <code>src/workbench/algorithms/sql/outliers.py</code> <pre><code>def __init__(self):\n    \"\"\"SQLOutliers Initialization\"\"\"\n    self.outlier_group = \"unknown\"\n</code></pre>"},{"location":"data_algorithms/sql/overview/#workbench.algorithms.sql.outliers.Outliers.compute_outliers","title":"<code>compute_outliers(data_source, scale=1.5, use_stddev=False)</code>","text":"<p>Compute outliers for all the numeric columns in a DataSource Args:     data_source(DataSource): The DataSource that we're computing outliers on     scale (float): The scale to use for either the IQR or stddev outlier calculation (default: 1.5)     use_stddev (bool): Option to use the standard deviation for the outlier calculation (default: False) Returns:     pd.DataFrame: A DataFrame of outliers for this DataSource Notes:     Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma)     The scale parameter can be adjusted to change the IQR multiplier</p> Source code in <code>src/workbench/algorithms/sql/outliers.py</code> <pre><code>def compute_outliers(\n    self, data_source: DataSourceAbstract, scale: float = 1.5, use_stddev: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Compute outliers for all the numeric columns in a DataSource\n    Args:\n        data_source(DataSource): The DataSource that we're computing outliers on\n        scale (float): The scale to use for either the IQR or stddev outlier calculation (default: 1.5)\n        use_stddev (bool): Option to use the standard deviation for the outlier calculation (default: False)\n    Returns:\n        pd.DataFrame: A DataFrame of outliers for this DataSource\n    Notes:\n        Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma)\n        The scale parameter can be adjusted to change the IQR multiplier\n    \"\"\"\n\n    # Note: If use_stddev is True, then the scale parameter needs to be adjusted\n    if use_stddev and scale == 1.5:  # If the default scale is used, adjust it\n        scale = 2.5\n\n    # Compute the numeric outliers\n    outlier_df = self._numeric_outliers(data_source, scale, use_stddev)\n\n    # If there are no outliers, return a DataFrame with the computation columns but no rows\n    if outlier_df is None:\n        columns = data_source.view(\"computation\").columns\n        return pd.DataFrame(columns=columns + [\"outlier_group\"])\n\n    # Get the top N outliers for each outlier group\n    outlier_df = self.get_top_n_outliers(outlier_df)\n\n    # Make sure the dataframe isn't too big, if it's too big sample it down\n    if len(outlier_df) &gt; 500:\n        log.important(f\"Outliers DataFrame is too large {len(outlier_df)}, sampling down to 500 rows\")\n        outlier_df = outlier_df.sample(300)\n\n    # Sort by outlier_group and reset the index\n    outlier_df = outlier_df.sort_values(\"outlier_group\").reset_index(drop=True)\n    return outlier_df\n</code></pre>"},{"location":"data_algorithms/sql/overview/#workbench.algorithms.sql.outliers.Outliers.get_top_n_outliers","title":"<code>get_top_n_outliers(outlier_df, n=10)</code>  <code>staticmethod</code>","text":"<p>Function to retrieve the top N highest and lowest outliers for each outlier group.</p> <p>Parameters:</p> Name Type Description Default <code>outlier_df</code> <code>DataFrame</code> <p>The DataFrame of outliers with 'outlier_group' column</p> required <code>n</code> <code>int</code> <p>Number of top outliers to retrieve for each group, defaults to 10</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the top N outliers for each outlier group</p> Source code in <code>src/workbench/algorithms/sql/outliers.py</code> <pre><code>@staticmethod\ndef get_top_n_outliers(outlier_df: pd.DataFrame, n: int = 10) -&gt; pd.DataFrame:\n    \"\"\"Function to retrieve the top N highest and lowest outliers for each outlier group.\n\n    Args:\n        outlier_df (pd.DataFrame): The DataFrame of outliers with 'outlier_group' column\n        n (int): Number of top outliers to retrieve for each group, defaults to 10\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the top N outliers for each outlier group\n    \"\"\"\n\n    def get_extreme_values(group: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Helper function to get the top N extreme values from a group.\"\"\"\n        col, extreme_type = group.name.rsplit(\"_\", 1)\n        if extreme_type == \"low\":\n            return group.nsmallest(n, col)\n        else:\n            return group.nlargest(n, col)\n\n    # Group by 'outlier_group' and apply the helper function, explicitly selecting columns to silence warning\n    top_outliers = outlier_df.groupby(\"outlier_group\", group_keys=False)[outlier_df.columns].apply(\n        get_extreme_values\n    )\n    return top_outliers.reset_index(drop=True)\n</code></pre>"},{"location":"data_algorithms/sql/overview/#workbench.algorithms.sql.descriptive_stats.descriptive_stats","title":"<code>descriptive_stats(data_source)</code>","text":"<p>Compute Descriptive Stats for all the numeric columns in a DataSource Args:     data_source(DataSource): The DataSource that we're computing descriptive stats on Returns:     dict(dict): A dictionary of descriptive stats for each column in this format          {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4, 'mean': 2.5, 'stddev': 1.5},           'col2': ...}</p> Source code in <code>src/workbench/algorithms/sql/descriptive_stats.py</code> <pre><code>def descriptive_stats(data_source: DataSourceAbstract) -&gt; dict[dict]:\n    \"\"\"Compute Descriptive Stats for all the numeric columns in a DataSource\n    Args:\n        data_source(DataSource): The DataSource that we're computing descriptive stats on\n    Returns:\n        dict(dict): A dictionary of descriptive stats for each column in this format\n             {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4, 'mean': 2.5, 'stddev': 1.5},\n              'col2': ...}\n    \"\"\"\n    # Grab the DataSource computation view table name\n    table = data_source.view(\"computation\").table\n\n    # Figure out which columns are numeric\n    num_type = [\"double\", \"float\", \"int\", \"bigint\", \"smallint\", \"tinyint\"]\n    details = data_source.view(\"computation\").column_details()\n    numeric = [column for column, data_type in details.items() if data_type in num_type]\n\n    # Sanity Check for numeric columns\n    if len(numeric) == 0:\n        log.warning(\"No numeric columns found in the current computation view of the DataSource\")\n        log.warning(\"If the data source was created from a DataFrame, ensure that the DataFrame was properly typed\")\n        log.warning(\"Recommendation: Properly type the DataFrame and recreate the Workbench artifact\")\n        return {}\n\n    # Build the query\n    query = descriptive_stats_query(numeric, table)\n\n    # Run the query\n    log.debug(query)\n    result_df = data_source.query(query)\n\n    # Process the results\n    # Note: The result_df is a DataFrame with a single row and a column for each stat metric\n    stats_dict = result_df.to_dict(orient=\"index\")[0]\n\n    # Convert the dictionary to a nested dictionary\n    # Note: The keys are in the format col1__col2\n    nested_descriptive_stats = defaultdict(dict)\n    for key, value in stats_dict.items():\n        col1, col2 = key.split(\"___\")\n        nested_descriptive_stats[col1][col2] = value\n\n    # Return the nested dictionary\n    return dict(nested_descriptive_stats)\n</code></pre>"},{"location":"data_algorithms/sql/overview/#workbench.algorithms.sql.descriptive_stats.descriptive_stats_query","title":"<code>descriptive_stats_query(columns, table_name)</code>","text":"<p>Build a query to compute the descriptive stats for all columns in a table Args:     columns(list(str)): The columns to compute descriptive stats on     table_name(str): The table to compute descriptive stats on Returns:     str: The SQL query to compute descriptive stats</p> Source code in <code>src/workbench/algorithms/sql/descriptive_stats.py</code> <pre><code>def descriptive_stats_query(columns: list[str], table_name: str) -&gt; str:\n    \"\"\"Build a query to compute the descriptive stats for all columns in a table\n    Args:\n        columns(list(str)): The columns to compute descriptive stats on\n        table_name(str): The table to compute descriptive stats on\n    Returns:\n        str: The SQL query to compute descriptive stats\n    \"\"\"\n    query = f'SELECT &lt;&lt;column_descriptive_stats&gt;&gt; FROM \"{table_name}\"'\n    column_descriptive_stats = \"\"\n    for c in columns:\n        column_descriptive_stats += (\n            f'min(\"{c}\") AS \"{c}___min\", '\n            f'approx_percentile(\"{c}\", 0.25) AS \"{c}___q1\", '\n            f'approx_percentile(\"{c}\", 0.5) AS \"{c}___median\", '\n            f'approx_percentile(\"{c}\", 0.75) AS \"{c}___q3\", '\n            f'max(\"{c}\") AS \"{c}___max\", '\n            f'avg(\"{c}\") AS \"{c}___mean\", '\n            f'stddev(\"{c}\") AS \"{c}___stddev\", '\n        )\n    query = query.replace(\"&lt;&lt;column_descriptive_stats&gt;&gt;\", column_descriptive_stats[:-2])\n\n    # Return the query\n    return query\n</code></pre>"},{"location":"data_algorithms/sql/overview/#workbench.algorithms.sql.correlations.correlation_query","title":"<code>correlation_query(columns, table_name)</code>","text":"<p>Build a query to compute the correlations between columns in a table</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list(str)</code> <p>The columns to compute correlations on</p> required <code>table_name</code> <code>str</code> <p>The table to compute correlations on</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The SQL query to compute correlations</p> Pearson correlation coefficient ranges from -1 to 1: <p>+1 indicates a perfect positive linear relationship. -1 indicates a perfect negative linear relationship.  0 indicates no linear relationship.</p> Source code in <code>src/workbench/algorithms/sql/correlations.py</code> <pre><code>def correlation_query(columns: list[str], table_name: str) -&gt; str:\n    \"\"\"Build a query to compute the correlations between columns in a table\n\n    Args:\n        columns (list(str)): The columns to compute correlations on\n        table_name (str): The table to compute correlations on\n\n    Returns:\n        str: The SQL query to compute correlations\n\n    Notes: Pearson correlation coefficient ranges from -1 to 1:\n           +1 indicates a perfect positive linear relationship.\n           -1 indicates a perfect negative linear relationship.\n            0 indicates no linear relationship.\n    \"\"\"\n    query = f'SELECT &lt;&lt;cross_correlations&gt;&gt; FROM \"{table_name}\"'\n    cross_correlations = \"\"\n    for c in columns:\n        for d in columns:\n            if c != d:\n                cross_correlations += f'corr(\"{c}\", \"{d}\") AS \"{c}__{d}\", '\n    query = query.replace(\"&lt;&lt;cross_correlations&gt;&gt;\", cross_correlations[:-2])\n\n    # Return the query\n    return query\n</code></pre>"},{"location":"data_algorithms/sql/overview/#workbench.algorithms.sql.correlations.correlations","title":"<code>correlations(data_source)</code>","text":"<p>Compute Correlations for all the numeric columns in a DataSource Args:     data_source(DataSource): The DataSource that we're computing correlations on Returns:     dict(dict): A dictionary of correlations for each column in this format          {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},           'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}</p> Source code in <code>src/workbench/algorithms/sql/correlations.py</code> <pre><code>def correlations(data_source: DataSourceAbstract) -&gt; dict[dict]:\n    \"\"\"Compute Correlations for all the numeric columns in a DataSource\n    Args:\n        data_source(DataSource): The DataSource that we're computing correlations on\n    Returns:\n        dict(dict): A dictionary of correlations for each column in this format\n             {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},\n              'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}\n    \"\"\"\n    data_source.log.info(\"Computing Correlations for numeric columns...\")\n\n    # Figure out which columns are numeric\n    num_type = [\"double\", \"float\", \"int\", \"bigint\", \"smallint\", \"tinyint\"]\n    details = data_source.view(\"computation\").column_details()\n\n    # Get the numeric columns\n    numeric = [column for column, data_type in details.items() if data_type in num_type]\n\n    # If we have at least two numeric columns, compute the correlations\n    if len(numeric) &lt; 2:\n        return {}\n\n    # Grab the DataSource computation table name\n    table = data_source.view(\"computation\").table\n\n    # Build the query\n    query = correlation_query(numeric, table)\n\n    # Run the query\n    log.debug(query)\n    result_df = data_source.query(query)\n\n    # Drop any columns that have NaNs\n    result_df = result_df.dropna(axis=1)\n\n    # Process the results\n    # Note: The result_df is a DataFrame with a single row and a column for each pairwise correlation\n    correlation_dict = result_df.to_dict(orient=\"index\")[0]\n\n    # Convert the dictionary to a nested dictionary\n    # Note: The keys are in the format col1__col2\n    nested_corr = defaultdict(dict)\n    for key, value in correlation_dict.items():\n        col1, col2 = key.split(\"__\")\n        nested_corr[col1][col2] = value\n\n    # Sort the nested dictionaries\n    sorted_dict = {}\n    for key, sub_dict in nested_corr.items():\n        sorted_dict[key] = {k: v for k, v in sorted(sub_dict.items(), key=lambda item: item[1], reverse=True)}\n    return sorted_dict\n</code></pre>"},{"location":"data_algorithms/sql/overview/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"enterprise/","title":"Workbench Enterprise","text":"<p>The Workbench API and User Interfaces cover a broad set of AWS Machine Learning services and provide easy to use abstractions and visualizations of your AWS ML data. We offer a wide range of options to best fit your companies needs.</p> Accelerate ML Pipeline development with an Enterprise License! Free Enterprise: Lite Enterprise: Standard Enterprise: Pro Python API \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 Workbench REPL \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 Dashboard \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 AWS Onboarding \u2796 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 Dashboard Plugins \u2796 \ud83d\udfe2 \ud83d\udfe2 \ud83d\udfe2 Custom Pages \u2796 \u2796 \ud83d\udfe2 \ud83d\udfe2 Themes \u2796 \u2796 \ud83d\udfe2 \ud83d\udfe2 ML Pipelines \u2796 \u2796 \u2796 \ud83d\udfe2 Project Branding \u2796 \u2796 \u2796 \ud83d\udfe2 Prioritized Feature Requests \u2796 \u2796 \u2796 \ud83d\udfe2 Pricing \u2796 $1500* $3000* $4000* <p>*USD per month, includes AWS setup, support, and training: Everything needed to accelerate your AWS ML Development team. Interested in Data Science/Engineering consulting? We have top notch Consultants with a depth and breadth of AWS ML/DS/Engineering expertise.</p>"},{"location":"enterprise/#try-workbench","title":"Try Workbench","text":"<p>We encourage new users to try out the free version, first. We offer support in our Discord channel and our Documentation has instructions for how to get started with Workbench. So try it out and when you're ready to accelerate your AWS ML Adventure with an Enterprise licence contact us at Workbench Sales</p>"},{"location":"enterprise/#data-engineeringscience-consulting","title":"Data Engineering/Science Consulting","text":"<p>Alongside our Workbench Enterprise offerings, we provide comprehensive consulting services and domain expertise through our Partnerships. We specialize in AWS Machine Learning Systems and our extended team of Data Scientists and Engineers, have Masters and Ph.D. degrees in Computer Science, Chemistry, and Pharmacology. We also have a parntership with Nomic Networks to support our Network Security Clients.</p> <p>Using AWS and Workbench, our experts are equipped to deliver tailored solutions that are focused on your project needs and deliverables. For more information please touch base and we'll set up a free initial consultation Workbench Consulting</p>"},{"location":"enterprise/#contact-us","title":"Contact Us","text":"<p>Contact us on our Discord channel, we're happy to answer any questions that you might have about Workbench and accelerating your AWS ML Pipelines. You can also send us email at Workbench Info or  Workbench Sales</p>"},{"location":"enterprise/private_saas/","title":"Benefits of a Private SaaS Architecture","text":""},{"location":"enterprise/private_saas/#self-hosted-vs-private-saas-vs-public-saas","title":"Self Hosted vs Private SaaS vs Public SaaS?","text":"<p>At the top level your team/project is making a decision about how they are going to build, expand, support, and maintain a machine learning pipeline.</p> <p>Conceptual ML Pipeline</p> <pre><code>Data \u2b95 Features \u2b95 Models \u2b95 Deployment (end-user application)\n</code></pre> <p>Concrete/Real World Example</p> <pre><code>S3 \u2b95 Glue Job \u2b95 Data Catalog \u2b95 FeatureGroups \u2b95 Models \u2b95 Endpoints \u2b95 App\n</code></pre> <p>When building out a framework to support ML Pipelines there are three main options:</p> <ul> <li>Self Hosted</li> <li>Private SaaS</li> <li>Public SaaS</li> </ul> <p>The other choice, that we're not going to cover here, is whether you use AWS, Azure, GCP, or something else. Workbench is architected and powered by a broad and rich set of AWS ML Pipeline services. We believe that AWS provides the best set of functionality and APIs for flexible, real world ML architectures.</p> <p></p>"},{"location":"enterprise/private_saas/#resources","title":"Resources","text":"<p>See our full presentation on the Workbench Private SaaS Architecture</p>"},{"location":"enterprise/project_branding/","title":"Project Branding","text":"<p>The Workbench Dashboard can be customized extensively. Using Workbench Project Branding allows you to change page headers, titles, and logos to match your project. All user interfaces will reflect your project name and company logos. </p>"},{"location":"enterprise/project_branding/#contact-us","title":"Contact Us","text":"<p>Contact us on our Discord channel, we're happy to answer any questions that you might have about Workbench and accelerating your AWS ML Pipelines. You can also send us email at Workbench Info or  Workbench Sales.</p>"},{"location":"enterprise/themes/","title":"Workbench Themes","text":"<p>The Workbench Dashboard can be customized extensively. Using Workbench Themes allows you to customize the User Interfaces to suit your preferences, including completely customized color palettes and fonts. We offer a set of default 'dark' and 'light' themes, but we'll also customize the theme to match your company's color palette and logos.</p>"},{"location":"enterprise/themes/#contact-us","title":"Contact Us","text":"<p>Contact us on our Discord channel, we're happy to answer any questions that you might have about Workbench and accelerating your AWS ML Pipelines. You can also send us email at Workbench Info or  Workbench Sales.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>For the initial setup of ADMET Workbench we'll be using the Workbench REPL. When you start <code>workbench</code> it will recognize that it needs to complete the initial configuration and will guide you through that process.</p> <p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and ADMET Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"getting_started/#initial-setupconfig","title":"Initial Setup/Config","text":"<p>Notes: ADMET Workbench uses your existing AWS account/profile/SSO. So if you don't already have an AWS Profile or SSO Setup you'll need to do that first AWS Setup</p> <p>Okay so after you've completed your AWS Setup you can now install ADMET Workbench.</p> <p><pre><code>&gt; pip install workbench\n&gt; workbench &lt;-- This starts the REPL\n\nWelcome to Workbench!\nLooks like this is your first time using Workbench...\nLet's get you set up...\nAWS_PROFILE: my_aws_profile\nWORKBENCH_BUCKET: my-company-workbench\n[optional] REDIS_HOST(localhost): my-redis.cache.amazon (or leave blank)\n[optional] REDIS_PORT(6379):\n[optional] REDIS_PASSWORD():\n[optional] WORKBENCH_API_KEY(open_source): my_api_key (or leave blank)\n</code></pre> That's It: You're now all set. This configuration only needs to be ONCE :)</p>"},{"location":"getting_started/#data-scientistsengineers","title":"Data Scientists/Engineers","text":"<ul> <li>Workbench REPL: Workbench REPL</li> <li>Using Workbench for ML Pipelines: Workbench API Classes</li> <li>SCP Workbench Github: Github Repo</li> </ul>"},{"location":"getting_started/#aws-administrators","title":"AWS Administrators","text":"<p>For companies that are setting up ADMET Workbench on an internal AWS Account: Company AWS Setup</p>"},{"location":"getting_started/#additional-resources","title":"Additional Resources","text":"<ul> <li>Workbench Core Classes: Core Classes</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"glue/","title":"AWS Glue Jobs","text":"<p>AWS Glue Simplified</p> <p>AWS Glue Jobs are a great way to automate ETL and data processing. Workbench takes all the hassle out of creating and debugging Glue Jobs. Follow this guide and empower your Glue Jobs with Workbench!</p> <p>Workbench make creating, testing, and debugging of AWS Glue Jobs easy. The exact same Workbench API Classes are used in your Glue Jobs. Also since Workbench manages the roles for both API and Glue Jobs you'll be able to test new Glue Jobs locally and minimizes surprises when deploying your Glue Job.</p>"},{"location":"glue/#glue-job-setup","title":"Glue Job Setup","text":"<p>Setting up a AWS Glue Job that uses Workbench is straight forward. Workbench can be 'installed' on AWS Glue via the <code>--additional-python-modules</code> parameter and then you can use the Workbench API just like normal. </p> <p></p> <p>Here are the settings and a screen shot to guide you. There are several ways to set up and run Glue Jobs, with either the Workbench-ExecutionRole or using the WorkbenchAPIPolicy. Please feel free to contact Workbench support if you need any help with setting up Glue Jobs.</p> <ul> <li>IAM Role: Workbench-ExecutionRole</li> <li>Type: Spark</li> <li>Glue Version: Glue 4.0</li> <li>Worker Type: G.1X</li> <li>Number of Workers: 2</li> <li>Job Parameters</li> <li>--additional-python-modules: workbench&gt;=0.4.6</li> <li>--workbench-bucket: &lt;your workbench bucket&gt;</li> </ul> <p>Glue IAM Role Details</p> <p>If your Glue Jobs already use an existing IAM Role then you can add the <code>WorkbenchAPIPolicy</code> to that Role to enable the Glue Job to perform Workbench API Tasks.</p>"},{"location":"glue/#workbench-glue-example","title":"Workbench Glue Example","text":"<p>Anyone familiar with a typical Glue Job should be pleasantly surpised by how simple the example below is. Also Workbench allows you to test Glue Jobs locally using the same code that you use for script and Notebooks (see Glue Testing)</p> <p>Glue Job Arguments</p> <p>AWS Glue Jobs take arguments in the form of Job Parameters (see screenshot above). There's a Workbench utility function <code>get_resolved_options</code> that turns these Job Parameters into a nice dictionary for ease of use.</p> examples/glue_hello_world.py<pre><code>import sys\n\n# Workbench Imports\nfrom workbench.api.data_source import DataSource\nfrom workbench.utils.config_manager import ConfigManager\nfrom workbench.utils.glue_utils import get_resolved_options\n\n# Convert Glue Job Args to a Dictionary\nglue_args = get_resolved_options(sys.argv)\n\n# Set the WORKBENCH_BUCKET for the ConfigManager\ncm = ConfigManager()\ncm.set_config(\"WORKBENCH_BUCKET\", glue_args[\"workbench-bucket\"])\n\n# Create a new Data Source from an S3 Path\nsource_path = \"s3://workbench-public-data/common/abalone.csv\"\nmy_data = DataSource(source_path, name=\"abalone_glue_test\")\n</code></pre>"},{"location":"glue/#glue-example-2","title":"Glue Example 2","text":"<p>This example takes two 'Job Parameters'</p> <ul> <li>--workbench-bucket : &lt;your workbench bucket&gt;</li> <li>--input-s3-path : &lt;your S3 input path&gt;</li> </ul> <p>The example will convert all CSV files in an S3 bucket/prefix and load them up as DataSources in Workbench.</p> examples/glue_load_s3_bucket.py<pre><code>import sys\n\n# Workbench Imports\nfrom workbench.api.data_source import DataSource\nfrom workbench.utils.config_manager import ConfigManager\nfrom workbench.utils.glue_utils import get_resolved_options, list_s3_files\n\n# Convert Glue Job Args to a Dictionary\nglue_args = get_resolved_options(sys.argv)\n\n# Set the WORKBENCH_BUCKET for the ConfigManager\ncm = ConfigManager()\ncm.set_config(\"WORKBENCH_BUCKET\", glue_args[\"workbench-bucket\"])\n\n# List all the CSV files in the given S3 Path\ninput_s3_path = glue_args[\"input-s3-path\"]\nfor input_file in list_s3_files(input_s3_path):\n\n    # Note: If we don't specify a name, one will be 'auto-generated'\n    my_data = DataSource(input_file, name=None)\n</code></pre>"},{"location":"glue/#exception-log-forwarding","title":"Exception Log Forwarding","text":"<p>When a Glue Job crashes (has an exception), the AWS console will show you the last line of the exception, this is mostly useless. If you use Workbench log forwarding the exception/stack will be forwarded to CloudWatch.</p> <p><pre><code>from workbench.utils.workbench_logging import exception_log_forward\n\nwith exception_log_forward():\n   &lt;my glue code&gt;\n   ...\n   &lt;exception happens&gt;\n   &lt;more of my code&gt;\n</code></pre> The <code>exception_log_forward</code> sets up a context manager that will trap exceptions and forward the exception/stack to CloudWatch for diagnosis. </p>"},{"location":"glue/#glue-job-local-testing","title":"Glue Job Local Testing","text":"<p>Glue Power without the Pain. Workbench manages the AWS Execution Role, so local API and Glue Jobs will have the same permissions/access. Also using the same Code as your notebooks or scripts makes creating and testing Glue Jobs a breeze.</p> <pre><code>export WORKBENCH_CONFIG=&lt;your config&gt;  # Only if not already set up\npython my_glue_job.py --workbench-bucket &lt;your bucket&gt;\n</code></pre>"},{"location":"glue/#additional-resources","title":"Additional Resources","text":"<ul> <li>Workbench Glue Jobs: Workbench Glue</li> <li>Setting up Workbench on your AWS Account: AWS Setup</li> <li>Using Workbench for ML Pipelines: Workbench API Classes</li> </ul> <ul> <li>Workbench Core Classes: Core Classes</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"lambda_layer/","title":"AWS Lambda Layer","text":"<p>Workbench Lambda Layers</p> <p>AWS Lambda Jobs are a great way to spin up data processing jobs. Follow this guide and empower AWS Lambda with Workbench!</p> <p>Workbench makes creating, testing, and debugging of AWS Lambda Functions easy. The exact same Workbench API Classes are used in your AWS Lambda Functions. Also since Workbench manages the access policies you'll be able to test new Lambda Jobs locally and minimizes surprises when deploying.</p> <p>Work In Progress</p> <p>The Workbench Lambda Layers are a great way to use Workbench but they are still in 'beta' mode so please let us know if you have any issues.</p>"},{"location":"lambda_layer/#lambda-job-setup","title":"Lambda Job Setup","text":"<p>Setting up a AWS Lambda Job that uses Workbench is straight forward. Workbench can be 'installed' using a Lambda Layer and then you can use the Workbench API just like normal.</p> <p>Here are the ARNs for the current Workbench Lambda Layers, please note they are specified with region and Python version in the name, so if your lambda is us-east-1, python 3.12, pick this ARN with those values in it.</p>"},{"location":"lambda_layer/#python-312","title":"Python 3.12","text":"<p>us-east-1</p> <ul> <li>arn:aws:lambda:us-east-1:507740646243:layer:workbench_lambda_layer-us-east-1-python312:2</li> </ul> <p>us-west-2</p> <ul> <li>arn:aws:lambda:us-west-2:507740646243:layer:workbench_lambda_layer-us-west-2-python312:3</li> </ul> <p>Note: If you're using lambdas on a different region or with a different Python version, just let us know and we'll publish some additional layers.</p> <p></p> <p>At the bottom of the Lambda page there's an 'Add Layer' button. You can click that button and specify the layer using the ARN above. Also in the 'General Configuration' set these parameters:</p> <ul> <li>Timeout: 5 Minutes</li> <li>Memory: 4096</li> <li>Ephemeral storage: 2048</li> </ul> <p>Set the WORKBENCH_BUCKET ENV Workbench will need to know what bucket to work out of, so go into the Configuration...Environment Variables... and add one for the Workbench bucket that your are using for AWS Account (dev, prod, etc). </p> <p>Lambda Role Details</p> <p>If your Lambda Function already use an existing IAM Role then you can add the Workbench policies to that Role to enable the Lambda Job to perform Workbench API Tasks. See Workbench Access Controls</p>"},{"location":"lambda_layer/#workbench-lambda-example","title":"Workbench Lambda Example","text":"<p>Here's a simple example of using Workbench in your Lambda Function. </p> <p>Workbench Layer is Compressed</p> <p>The Workbench Lambda Layer is compressed (to fit all the awesome). This means that the <code>load_lambda_layer()</code> method must be called before using any other Workbench imports, see the example below. If you do not do this you'll probably get a <code>No module named 'numpy'</code> error or something like that.</p> examples/lambda_hello_world.py<pre><code>import json\nfrom pprint import pprint\nfrom workbench.utils.lambda_utils import load_lambda_layer\n\n# Load/Decompress the Workbench Lambda Layer\nload_lambda_layer()\n\n# After 'load_lambda_layer()' we can use other Workbench imports\nfrom workbench.api import Meta\nfrom workbench.api import Model \n\ndef lambda_handler(event, context):\n\n    # Create our Meta Class and get a list of our Models\n    meta = Meta()\n    models = meta.models()\n\n    print(f\"Number of Models: {len(models)}\")\n    print(models)\n\n    # Onboard a model\n    model = Model(\"abalone-regression\")\n    pprint(model.details())\n\n    # Return success\n    return {\n        'statusCode': 200,\n        'body': { \"incoming_event\": event}\n    }\n</code></pre>"},{"location":"lambda_layer/#exception-log-forwarding","title":"Exception Log Forwarding","text":"<p>When a Lambda Job crashes (has an exception), the AWS console will show you the last line of the exception, this is mostly useless. If you use Workbench log forwarding the exception/stack will be forwarded to CloudWatch.</p> <p><pre><code>from workbench.utils.workbench_logging import exception_log_forward\n\nwith exception_log_forward():\n   &lt;my lambda code&gt;\n   ...\n   &lt;exception happens&gt;\n   &lt;more of my code&gt;\n</code></pre> The <code>exception_log_forward</code> sets up a context manager that will trap exceptions and forward the exception/stack to CloudWatch for diagnosis. </p>"},{"location":"lambda_layer/#lambda-function-local-testing","title":"Lambda Function Local Testing","text":"<p>Lambda Power without the Pain. Workbench manages the AWS Execution Role/Policies, so local API and Lambda Functions will have the same permissions/access. Also using the same Code as your notebooks or scripts makes creating and testing Lambda Functions a breeze.</p> <pre><code>python my_lambda_function.py --workbench-bucket &lt;your bucket&gt;\n</code></pre>"},{"location":"lambda_layer/#additional-resources","title":"Additional Resources","text":"<ul> <li>Workbench Access Management: Workbench Access Management</li> <li>Setting up Workbench on your AWS Account: AWS Setup</li> </ul> <ul> <li> <p>Using Workbench for ML Pipelines: Workbench API Classes</p> </li> <li> <p>Consulting Available: SuperCowPowers LLC</p> </li> </ul>"},{"location":"misc/faq/","title":"Workbench: FAQ","text":"<p>Artifact and Column Naming?</p> <p>You might have noticed that Workbench has some unintuitive constraints when naming Artifacts and restrictions on column names. All of these restrictions come from AWS. Workbench uses Glue, Athena, Feature Store, Models and Endpoints, each of these services have their own constraints, Workbench simply 'reflects' those contraints.</p>"},{"location":"misc/faq/#naming-underscores-dashes-and-lower-case","title":"Naming: Underscores, Dashes, and Lower Case","text":"<p>Data Sources and Feature Sets must adhere to AWS restrictions on table names and columns names (here is a snippet from the AWS documentation)</p> <p>Database, table, and column names</p> <p>When you create schema in AWS Glue to query in Athena, consider the following:</p> <p>A database name cannot be longer than 255 characters. A table name cannot be longer than 255 characters. A column name cannot be longer than 255 characters.</p> <p>The only acceptable characters for database names, table names, and column names are lowercase letters, numbers, and the underscore character.</p> <p>For more info see: Glue Best Practices</p>"},{"location":"misc/faq/#datasourcefeatureset-use-_-and-modelendpoint-use-","title":"DataSource/FeatureSet use '_'  and Model/Endpoint use '-'","text":"<p>You may notice that DataSource and FeatureSet names have underscores but the model and endpoints have dashes. Yes, it\u2019s super annoying to have one convention for DataSources and FeatureSets and another for Models and Endpoints but this is an AWS restriction and not something that Workbench can control.</p> <p>DataSources and FeatureSet: Underscores. You cannot use a dash because both classes use Athena for Storage and Athena tables names cannot have a dash.</p> <p>Models and Endpoints: Dashes. You cannot use an underscores because AWS imposes a restriction on the naming.</p>"},{"location":"misc/faq/#additional-information-on-the-lower-case-issue","title":"Additional information on the lower case issue","text":"<p>We\u2019ve tried to create a glue table with Mixed Case column names and haven\u2019t had any luck. We\u2019ve bypassed wrangler and used the boto3 low level calls directly. In all cases when it shows up in the Glue Table the columns have always been converted to lower case. We've also tried uses the Athena DDL directly, that also doesn't work. Here's the relevant AWS documentation and the two scripts that reproduce the issue.</p> <p>AWS Docs</p> <ul> <li>Athena Naming Restrictions</li> <li>Glue Best Practices</li> </ul> <p>Scripts to Reproduce</p> <ul> <li>scripts/athena_ddl_mixed_case.py</li> <li>scripts/glue_mixed_case.py</li> </ul>"},{"location":"misc/general_info/","title":"General info","text":""},{"location":"misc/general_info/#general-info","title":"General Info","text":""},{"location":"misc/general_info/#workbench-the-scientists-workbench-powered-by-aws-for-scalability-flexibility-and-security","title":"Workbench: The scientist's workbench powered by AWS\u00ae for scalability, flexibility, and security.","text":"<p>Workbench is a medium granularity framework that manages and aggregates AWS\u00ae Services into classes and concepts. When you use Workbench you think about DataSources, FeatureSets, Models, and Endpoints. Underneath the hood those classes handle all the details around updating and managing a complex set of AWS Services. All the power and none of the pain so that your team can Do Science Faster!</p>"},{"location":"misc/general_info/#workbench-documentation","title":"Workbench Documentation","text":"<p>See our Python API and AWS documentation here: Workbench Documentation</p>"},{"location":"misc/general_info/#full-workbench-overview","title":"Full Workbench OverView","text":"<p>Workbench Architected FrameWork</p>"},{"location":"misc/general_info/#why-workbench","title":"Why Workbench?","text":"<ul> <li>The AWS SageMaker\u00ae ecosystem is awesome but has a large number of services with significant complexity</li> <li>Workbench provides rapid prototyping through easy to use classes and transforms</li> <li>Workbench provides visibility and transparency into AWS SageMaker\u00ae Pipelines<ul> <li>What S3 data sources are getting pulled?</li> <li>What Features Store/Group is the Model Using?</li> <li>What's the Provenance of a Model in Model Registry?</li> <li>What SageMaker Endpoints are associated with this model?</li> </ul> </li> </ul>"},{"location":"misc/general_info/#single-pane-of-glass","title":"Single Pane of Glass","text":"<p>Visibility into the AWS Services that underpin the Workbench Classes. We can see that Workbench automatically tags and tracks the inputs of all artifacts providing 'data provenance' for all steps in the AWS modeling pipeline.</p> <p>Image TBD</p> <p> Clearly illustrated: Workbench provides intuitive and transparent visibility into the full pipeline of your AWS Sagemaker Deployments.</p>"},{"location":"misc/general_info/#getting-started","title":"Getting Started","text":"<ul> <li>Workbench Overview Slides that cover and illustrate the Workbench Modeling Pipeline.</li> <li>Workbench Docs/Wiki Our general documentation for getting started with Workbench.</li> <li>Workbench AWS Onboarding Deploy the Workbench Stack to your AWS Account. </li> <li>Notebook: Start to Finish AWS ML Pipeline Building an AWS\u00ae ML Pipeline from start to finish.</li> <li>Video: Coding with Workbench Informal coding + chatting while building a full ML pipeline.</li> <li>Join our Discord for questions and advice on using Workbench within your organization.</li> </ul>"},{"location":"misc/general_info/#workbench-zen","title":"Workbench Zen","text":"<ul> <li>The AWS SageMaker\u00ae set of services is vast and complex.</li> <li>Workbench Classes encapsulate, organize, and manage sets of AWS\u00ae Services.</li> <li>Heavy transforms typically use AWS Athena or Apache Spark (AWS Glue/EMR Serverless).</li> <li>Light transforms will typically use Pandas.</li> <li>Heavy and Light transforms both update AWS Artifacts (collections of AWS Services).</li> <li>Quick prototypes are typically built with the light path and then flipped to the heavy path as the system matures and usage grows.</li> </ul>"},{"location":"misc/general_info/#classes-and-concepts","title":"Classes and Concepts","text":"<p>The Workbench Classes are organized to work in concert with AWS Services. For more details on the current classes and class hierarchies see Workbench Classes and Concepts.</p>"},{"location":"misc/general_info/#contributions","title":"Contributions","text":"<p>If you'd like to contribute to the Workbench project, you're more than welcome. All contributions will fall under the existing project license. If you are interested in contributing or have questions please feel free to contact us at workbench@supercowpowers.com.</p>"},{"location":"misc/general_info/#workbench-alpha-testers-wanted","title":"Workbench Alpha Testers Wanted","text":"<p>Our experienced team can provide development and consulting services to help you effectively use Amazon\u2019s Machine Learning services within your organization.</p> <p>The popularity of cloud based Machine Learning services is booming. The problem many companies face is how that capability gets effectively used and harnessed to drive real business decisions and provide concrete value for their organization.</p> <p>Using Workbench will minimize the time and manpower needed to incorporate AWS ML into your organization. If your company would like to be a Workbench Alpha Tester, contact us at workbench@supercowpowers.com.</p> <p>\u00ae Amazon Web Services, AWS, the Powered by AWS logo, are trademarks of Amazon.com, Inc. or its affiliates.</p>"},{"location":"misc/scp_consulting/","title":"Scp consulting","text":""},{"location":"misc/scp_consulting/#consulting","title":"Consulting","text":""},{"location":"misc/scp_consulting/#workbench-scp-consulting-awesome","title":"Workbench + SCP Consulting = Awesome","text":"<p>Our experienced team can provide development and consulting services to help you effectively use Amazon\u2019s Machine Learning services within your organization.</p> <p>The popularity of cloud based Machine Learning services is booming. The problem many companies face is how that capability gets effectively used and harnessed to drive real business decisions and provide concrete value for their organization.</p> <p>Using Workbench will minimizize the time and manpower needed to incorporate AWS ML into your organization. If your company would like to be a Workbench Alpha Tester, contact us at workbench@supercowpowers.com.</p>"},{"location":"misc/scp_consulting/#typical-engagements","title":"Typical Engagements","text":"<p>Workbench clients typically want a tailored web_interface that helps to drive business decisions and provides value for their organization.</p> <ul> <li>Workbench components provide a set of classes and transforms the will dramatically reduce time and increase productivity when building AWS ML Systems.</li> <li>Workbench enables rapid prototyping via it's light paths and provides AWS production workflows on large scale data through it's heavy paths.</li> <li> <p>Rapid Prototyping is typically done via these steps.</p> </li> <li> <p>Quick Construction of Web Interface (tailored)</p> </li> <li>Custom Components (tailored)</li> <li>Demo/Review the Application with the Client</li> <li>Get Feedback/Changes/Improvements</li> <li> <p>Goto Step 1</p> </li> <li> <p>When the client is happy/excited about the ProtoType we then bolt down the system, test the heavy paths, review AWS access, security and ensure 'least privileged' roles and policies.</p> </li> </ul> <p>Contact us for a free initial consultation on how we can accelerate the use of AWS ML at your company workbench@supercowpowers.com.</p>"},{"location":"misc/workbench_classes_concepts/","title":"Workbench Classes and Concepts","text":"<p>A flexible, rapid, and customizable AWS\u00ae ML Sandbox. Here's some of the classes and concepts we use in the Workbench system:</p> <p></p> <ul> <li>Artifacts</li> <li>DataLoader</li> <li>DataSource</li> <li>FeatureSet</li> <li>Model</li> <li> <p>Endpoint</p> </li> <li> <p>Transforms</p> </li> <li>DataSource to DataSource<ul> <li>Heavy <ul> <li>AWS Glue Jobs</li> <li>AWS EMR Serverless</li> </ul> </li> <li>Light<ul> <li>Local/Laptop</li> <li>Lambdas</li> <li>StepFunctions</li> </ul> </li> </ul> </li> <li>DataSource to FeatureSet<ul> <li>Heavy/Light (see above breakout)</li> </ul> </li> <li>FeatureSet to FeatureSet<ul> <li>Heavy/Light (see above breakout)</li> </ul> </li> <li>FeatureSet to Model</li> <li>Model to Endpoint</li> </ul>"},{"location":"model_utils/","title":"Model Utilities","text":"<p>Examples</p> <p>Examples of using the Model Utilities are listed at the bottom of this page Examples.</p> <p>Model Utilities for Workbench models</p>"},{"location":"model_utils/#workbench.utils.model_utils.cleanlab_model_local","title":"<code>cleanlab_model_local(model)</code>","text":"<p>Create a CleanlabModels instance for detecting data quality issues in a Model's training data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Model used to create the cleanlab models</p> required <p>Returns:</p> Name Type Description <code>CleanlabModels</code> <code>CleanlabModels</code> <p>Factory providing access to CleanLearning and Datalab models. - clean_learning(): CleanLearning model with enhanced get_label_issues() - datalab(): Datalab instance with report(), get_issues()</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def cleanlab_model_local(model: Model) -&gt; CleanlabModels:\n    \"\"\"Create a CleanlabModels instance for detecting data quality issues in a Model's training data.\n\n    Args:\n        model (Model): The Model used to create the cleanlab models\n\n    Returns:\n        CleanlabModels: Factory providing access to CleanLearning and Datalab models.\n            - clean_learning(): CleanLearning model with enhanced get_label_issues()\n            - datalab(): Datalab instance with report(), get_issues()\n    \"\"\"\n    from workbench.algorithms.models.cleanlab_model import create_cleanlab_model  # noqa: F401 (avoid circular import)\n    from workbench.api import Model, FeatureSet  # noqa: F401 (avoid circular import)\n\n    # Get Feature and Target Columns from the existing given Model\n    features = model.features()\n    target = model.target()\n    model_type = model.model_type\n\n    # Backtrack our FeatureSet to get the ID column\n    fs = FeatureSet(model.get_input())\n    id_column = fs.id_column\n\n    # Get the full FeatureSet data\n    full_df = fs.pull_dataframe()\n\n    # Create and return the CleanLearning model\n    return create_cleanlab_model(full_df, id_column, features, target, model_type=model_type)\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.fingerprint_prox_model_local","title":"<code>fingerprint_prox_model_local(model, include_all_columns=False, radius=2, n_bits=1024, counts=False)</code>","text":"<p>Create a FingerprintProximity Model for this Model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Model used to create the fingerprint proximity model</p> required <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in neighbor results (default: False)</p> <code>False</code> <code>radius</code> <code>int</code> <p>Morgan fingerprint radius (default: 2)</p> <code>2</code> <code>n_bits</code> <code>int</code> <p>Number of bits for the fingerprint (default: 1024)</p> <code>1024</code> <code>counts</code> <code>bool</code> <p>Use count fingerprints instead of binary (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FingerprintProximity</code> <code>FingerprintProximity</code> <p>The fingerprint proximity model</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def fingerprint_prox_model_local(\n    model: Model,\n    include_all_columns: bool = False,\n    radius: int = 2,\n    n_bits: int = 1024,\n    counts: bool = False,\n) -&gt; FingerprintProximity:\n    \"\"\"Create a FingerprintProximity Model for this Model\n\n    Args:\n        model (Model): The Model used to create the fingerprint proximity model\n        include_all_columns (bool): Include all DataFrame columns in neighbor results (default: False)\n        radius (int): Morgan fingerprint radius (default: 2)\n        n_bits (int): Number of bits for the fingerprint (default: 1024)\n        counts (bool): Use count fingerprints instead of binary (default: False)\n\n    Returns:\n        FingerprintProximity: The fingerprint proximity model\n    \"\"\"\n    from workbench.algorithms.dataframe.fingerprint_proximity import FingerprintProximity  # noqa: F401\n    from workbench.api import Model, FeatureSet  # noqa: F401 (avoid circular import)\n\n    # Get Target Column from the existing given Model\n    target = model.target()\n\n    # Backtrack our FeatureSet to get the ID column\n    fs = FeatureSet(model.get_input())\n    id_column = fs.id_column\n\n    # Create the Proximity Model from both the full FeatureSet and the Model training data\n    full_df = fs.pull_dataframe()\n    model_df = model.training_view().pull_dataframe()\n\n    # Mark rows that are in the model\n    model_ids = set(model_df[id_column])\n    full_df[\"in_model\"] = full_df[id_column].isin(model_ids)\n\n    # Create and return the FingerprintProximity Model\n    return FingerprintProximity(\n        full_df,\n        id_column=id_column,\n        target=target,\n        include_all_columns=include_all_columns,\n        radius=radius,\n        n_bits=n_bits,\n    )\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.get_model_hyperparameters","title":"<code>get_model_hyperparameters(workbench_model)</code>","text":"<p>Get the hyperparameters used to train a Workbench model.</p> <p>This retrieves the hyperparameters.json file from the model artifacts that was saved during model training.</p> <p>Parameters:</p> Name Type Description Default <code>workbench_model</code> <code>Any</code> <p>Workbench model object</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[dict]</code> <p>The hyperparameters used during training, or None if not found</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def get_model_hyperparameters(workbench_model: Any) -&gt; Optional[dict]:\n    \"\"\"Get the hyperparameters used to train a Workbench model.\n\n    This retrieves the hyperparameters.json file from the model artifacts\n    that was saved during model training.\n\n    Args:\n        workbench_model: Workbench model object\n\n    Returns:\n        dict: The hyperparameters used during training, or None if not found\n    \"\"\"\n    # Get the model artifact URI\n    model_artifact_uri = workbench_model.model_data_url()\n\n    if model_artifact_uri is None:\n        log.warning(f\"No model artifact found for {workbench_model.uuid}\")\n        return None\n\n    return load_hyperparameters_from_s3(model_artifact_uri)\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.instance_architecture","title":"<code>instance_architecture(instance_name)</code>","text":"<p>Get the architecture for the given instance name</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def instance_architecture(instance_name: str) -&gt; str:\n    \"\"\"Get the architecture for the given instance name\"\"\"\n    info = model_instance_info()\n    return info[info[\"Instance Name\"] == instance_name][\"Architecture\"].values[0]\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.load_category_mappings_from_s3","title":"<code>load_category_mappings_from_s3(model_artifact_uri)</code>","text":"<p>Download and extract category mappings from a model artifact in S3.</p> <p>Parameters:</p> Name Type Description Default <code>model_artifact_uri</code> <code>str</code> <p>S3 URI of the model artifact.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[dict]</code> <p>The loaded category mappings or None if not found.</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def load_category_mappings_from_s3(model_artifact_uri: str) -&gt; Optional[dict]:\n    \"\"\"\n    Download and extract category mappings from a model artifact in S3.\n\n    Args:\n        model_artifact_uri (str): S3 URI of the model artifact.\n\n    Returns:\n        dict: The loaded category mappings or None if not found.\n    \"\"\"\n    category_mappings = None\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Download model artifact\n        local_tar_path = os.path.join(tmpdir, \"model.tar.gz\")\n        wr.s3.download(path=model_artifact_uri, local_file=local_tar_path)\n\n        # Extract tarball\n        safe_extract_tarfile(local_tar_path, tmpdir)\n\n        # Look for category mappings in base directory only\n        mappings_path = os.path.join(tmpdir, \"category_mappings.json\")\n\n        if os.path.exists(mappings_path):\n            try:\n                with open(mappings_path, \"r\") as f:\n                    category_mappings = json.load(f)\n                print(f\"Loaded category mappings from {mappings_path}\")\n            except Exception as e:\n                print(f\"Failed to load category mappings from {mappings_path}: {e}\")\n\n    return category_mappings\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.load_hyperparameters_from_s3","title":"<code>load_hyperparameters_from_s3(model_artifact_uri)</code>","text":"<p>Download and extract hyperparameters from a model artifact in S3.</p> <p>Parameters:</p> Name Type Description Default <code>model_artifact_uri</code> <code>str</code> <p>S3 URI of the model artifact (model.tar.gz).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[dict]</code> <p>The loaded hyperparameters or None if not found.</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def load_hyperparameters_from_s3(model_artifact_uri: str) -&gt; Optional[dict]:\n    \"\"\"\n    Download and extract hyperparameters from a model artifact in S3.\n\n    Args:\n        model_artifact_uri (str): S3 URI of the model artifact (model.tar.gz).\n\n    Returns:\n        dict: The loaded hyperparameters or None if not found.\n    \"\"\"\n    hyperparameters = None\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Download model artifact\n        local_tar_path = os.path.join(tmpdir, \"model.tar.gz\")\n        wr.s3.download(path=model_artifact_uri, local_file=local_tar_path)\n\n        # Extract tarball\n        safe_extract_tarfile(local_tar_path, tmpdir)\n\n        # Look for hyperparameters in base directory only\n        hyperparameters_path = os.path.join(tmpdir, \"hyperparameters.json\")\n\n        if os.path.exists(hyperparameters_path):\n            try:\n                with open(hyperparameters_path, \"r\") as f:\n                    hyperparameters = json.load(f)\n                log.important(f\"Performance warning: Loaded hyperparameters from {hyperparameters_path}\")\n            except Exception as e:\n                log.warning(f\"Failed to load hyperparameters from {hyperparameters_path}: {e}\")\n\n    return hyperparameters\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.model_instance_info","title":"<code>model_instance_info()</code>","text":"<p>Get the instance information for the Model</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def model_instance_info() -&gt; pd.DataFrame:\n    \"\"\"Get the instance information for the Model\"\"\"\n    data = [\n        {\n            \"Instance Name\": \"ml.t2.medium\",\n            \"vCPUs\": 2,\n            \"Memory\": 4,\n            \"Price per Hour\": 0.06,\n            \"Category\": \"General\",\n            \"Architecture\": \"x86_64\",\n        },\n        {\n            \"Instance Name\": \"ml.m7i.large\",\n            \"vCPUs\": 2,\n            \"Memory\": 8,\n            \"Price per Hour\": 0.12,\n            \"Category\": \"General\",\n            \"Architecture\": \"x86_64\",\n        },\n        {\n            \"Instance Name\": \"ml.c7i.large\",\n            \"vCPUs\": 2,\n            \"Memory\": 4,\n            \"Price per Hour\": 0.11,\n            \"Category\": \"Compute\",\n            \"Architecture\": \"x86_64\",\n        },\n        {\n            \"Instance Name\": \"ml.c7i.xlarge\",\n            \"vCPUs\": 4,\n            \"Memory\": 8,\n            \"Price per Hour\": 0.21,\n            \"Category\": \"Compute\",\n            \"Architecture\": \"x86_64\",\n        },\n        {\n            \"Instance Name\": \"ml.c7g.large\",\n            \"vCPUs\": 2,\n            \"Memory\": 4,\n            \"Price per Hour\": 0.09,\n            \"Category\": \"Compute\",\n            \"Architecture\": \"arm64\",\n        },\n        {\n            \"Instance Name\": \"ml.c7g.xlarge\",\n            \"vCPUs\": 4,\n            \"Memory\": 8,\n            \"Price per Hour\": 0.17,\n            \"Category\": \"Compute\",\n            \"Architecture\": \"arm64\",\n        },\n    ]\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.noise_model_local","title":"<code>noise_model_local(model)</code>","text":"<p>Create a NoiseModel for detecting noisy/problematic samples in a Model's training data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Model used to create the noise model</p> required <p>Returns:</p> Name Type Description <code>NoiseModel</code> <code>NoiseModel</code> <p>The noise model with precomputed noise scores for all samples</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def noise_model_local(model: Model) -&gt; NoiseModel:\n    \"\"\"Create a NoiseModel for detecting noisy/problematic samples in a Model's training data.\n\n    Args:\n        model (Model): The Model used to create the noise model\n\n    Returns:\n        NoiseModel: The noise model with precomputed noise scores for all samples\n    \"\"\"\n    from workbench.algorithms.models.noise_model import NoiseModel  # noqa: F401 (avoid circular import)\n    from workbench.api import Model, FeatureSet  # noqa: F401 (avoid circular import)\n\n    # Get Feature and Target Columns from the existing given Model\n    features = model.features()\n    target = model.target()\n\n    # Backtrack our FeatureSet to get the ID column\n    fs = FeatureSet(model.get_input())\n    id_column = fs.id_column\n\n    # Create the NoiseModel from both the full FeatureSet and the Model training data\n    full_df = fs.pull_dataframe()\n    model_df = model.training_view().pull_dataframe()\n\n    # Mark rows that are in the model\n    model_ids = set(model_df[id_column])\n    full_df[\"in_model\"] = full_df[id_column].isin(model_ids)\n\n    # Create and return the NoiseModel\n    return NoiseModel(full_df, id_column, features, target)\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.proximity_model_local","title":"<code>proximity_model_local(model, include_all_columns=False)</code>","text":"<p>Create a FeatureSpaceProximity Model for this Model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Model/FeatureSet used to create the proximity model</p> required <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in neighbor results (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FeatureSpaceProximity</code> <code>FeatureSpaceProximity</code> <p>The proximity model</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def proximity_model_local(model: Model, include_all_columns: bool = False) -&gt; FeatureSpaceProximity:\n    \"\"\"Create a FeatureSpaceProximity Model for this Model\n\n    Args:\n        model (Model): The Model/FeatureSet used to create the proximity model\n        include_all_columns (bool): Include all DataFrame columns in neighbor results (default: False)\n\n    Returns:\n        FeatureSpaceProximity: The proximity model\n    \"\"\"\n    from workbench.algorithms.dataframe.feature_space_proximity import FeatureSpaceProximity  # noqa: F401\n    from workbench.api import Model, FeatureSet  # noqa: F401 (avoid circular import)\n\n    # Get Feature and Target Columns from the existing given Model\n    features = model.features()\n    target = model.target()\n\n    # Backtrack our FeatureSet to get the ID column\n    fs = FeatureSet(model.get_input())\n    id_column = fs.id_column\n\n    # Create the Proximity Model from both the full FeatureSet and the Model training data\n    full_df = fs.pull_dataframe()\n    model_df = model.training_view().pull_dataframe()\n\n    # Mark rows that are in the model\n    model_ids = set(model_df[id_column])\n    full_df[\"in_model\"] = full_df[id_column].isin(model_ids)\n\n    # Create and return the FeatureSpaceProximity Model\n    return FeatureSpaceProximity(\n        full_df, id_column=id_column, features=features, target=target, include_all_columns=include_all_columns\n    )\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.published_proximity_model","title":"<code>published_proximity_model(model, prox_model_name, include_all_columns=False)</code>","text":"<p>Create a published proximity model based on the given model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to create the proximity model from</p> required <code>prox_model_name</code> <code>str</code> <p>The name of the proximity model to create</p> required <code>include_all_columns</code> <code>bool</code> <p>Include all DataFrame columns in results (default: False)</p> <code>False</code> <p>Returns:     Model: The proximity model</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def published_proximity_model(model: Model, prox_model_name: str, include_all_columns: bool = False) -&gt; Model:\n    \"\"\"Create a published proximity model based on the given model\n\n    Args:\n        model (Model): The model to create the proximity model from\n        prox_model_name (str): The name of the proximity model to create\n        include_all_columns (bool): Include all DataFrame columns in results (default: False)\n    Returns:\n        Model: The proximity model\n    \"\"\"\n    from workbench.api import Model, ModelType, FeatureSet  # noqa: F401 (avoid circular import)\n\n    # Get the custom script path for the proximity model\n    script_path = get_custom_script_path(\"proximity\", \"feature_space_proximity.template\")\n\n    # Get Feature and Target Columns from the existing given Model\n    features = model.features()\n    target = model.target()\n\n    # Create the Proximity Model from our FeatureSet\n    fs = FeatureSet(model.get_input())\n    prox_model = fs.to_model(\n        name=prox_model_name,\n        model_type=ModelType.PROXIMITY,\n        feature_list=features,\n        target_column=target,\n        description=f\"Proximity Model for {model.name}\",\n        tags=[\"proximity\", model.name],\n        custom_script=script_path,\n        custom_args={\"include_all_columns\": include_all_columns},\n    )\n    return prox_model\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.safe_extract_tarfile","title":"<code>safe_extract_tarfile(tar_path, extract_path)</code>","text":"<p>Extract a tarball safely, using data filter if available.</p> <p>The filter parameter was backported to Python 3.8+, 3.9+, 3.10.13+, 3.11+ as a security patch, but may not be present in older patch versions.</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def safe_extract_tarfile(tar_path: str, extract_path: str) -&gt; None:\n    \"\"\"\n    Extract a tarball safely, using data filter if available.\n\n    The filter parameter was backported to Python 3.8+, 3.9+, 3.10.13+, 3.11+\n    as a security patch, but may not be present in older patch versions.\n    \"\"\"\n    with tarfile.open(tar_path, \"r:gz\") as tar:\n        if hasattr(tarfile, \"data_filter\"):\n            tar.extractall(path=extract_path, filter=\"data\")\n        else:\n            tar.extractall(path=extract_path)\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.supported_instance_types","title":"<code>supported_instance_types(arch='x86_64')</code>","text":"<p>Get the supported instance types for the Model/Model</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def supported_instance_types(arch: str = \"x86_64\") -&gt; list:\n    \"\"\"Get the supported instance types for the Model/Model\"\"\"\n\n    # Filter the instance types based on the architecture\n    info = model_instance_info()\n    return info[info[\"Architecture\"] == arch][\"Instance Name\"].tolist()\n</code></pre>"},{"location":"model_utils/#workbench.utils.model_utils.uq_metrics","title":"<code>uq_metrics(df, target_col)</code>","text":"<p>Evaluate uncertainty quantification model with essential metrics. Args:     df: DataFrame with predictions and uncertainty estimates.         Must contain the target column, a prediction column (\"prediction\"), and either         quantile columns (\"q_025\", \"q_975\", \"q_25\", \"q_75\") or a standard deviation         column (\"prediction_std\").     target_col: Name of the true target column in the DataFrame. Returns:     Dictionary of computed metrics.</p> Source code in <code>src/workbench/utils/model_utils.py</code> <pre><code>def uq_metrics(df: pd.DataFrame, target_col: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate uncertainty quantification model with essential metrics.\n    Args:\n        df: DataFrame with predictions and uncertainty estimates.\n            Must contain the target column, a prediction column (\"prediction\"), and either\n            quantile columns (\"q_025\", \"q_975\", \"q_25\", \"q_75\") or a standard deviation\n            column (\"prediction_std\").\n        target_col: Name of the true target column in the DataFrame.\n    Returns:\n        Dictionary of computed metrics.\n    \"\"\"\n    # Input Validation\n    if df.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n    if target_col not in df.columns:\n        raise ValueError(f\"Target column '{target_col}' not found in DataFrame.\")\n    if \"prediction\" not in df.columns:\n        raise ValueError(\"Prediction column 'prediction' not found in DataFrame.\")\n\n    # Drop rows with NaN predictions (e.g., from models that can't handle missing features)\n    n_total = len(df)\n    df = df.dropna(subset=[\"prediction\", target_col])\n    n_valid = len(df)\n    if n_valid &lt; n_total:\n        log.info(f\"UQ metrics: dropped {n_total - n_valid} rows with NaN predictions\")\n\n    # --- Coverage and Interval Width ---\n    if \"q_025\" in df.columns and \"q_975\" in df.columns:\n        lower_95, upper_95 = df[\"q_025\"], df[\"q_975\"]\n        lower_90, upper_90 = df[\"q_05\"], df[\"q_95\"]\n        lower_80, upper_80 = df[\"q_10\"], df[\"q_90\"]\n        lower_68 = df.get(\"q_16\", df[\"q_10\"])  # fallback to 80% interval\n        upper_68 = df.get(\"q_84\", df[\"q_90\"])  # fallback to 80% interval\n        lower_50, upper_50 = df[\"q_25\"], df[\"q_75\"]\n    elif \"prediction_std\" in df.columns:\n        lower_95 = df[\"prediction\"] - 1.96 * df[\"prediction_std\"]\n        upper_95 = df[\"prediction\"] + 1.96 * df[\"prediction_std\"]\n        lower_90 = df[\"prediction\"] - 1.645 * df[\"prediction_std\"]\n        upper_90 = df[\"prediction\"] + 1.645 * df[\"prediction_std\"]\n        lower_80 = df[\"prediction\"] - 1.282 * df[\"prediction_std\"]\n        upper_80 = df[\"prediction\"] + 1.282 * df[\"prediction_std\"]\n        lower_68 = df[\"prediction\"] - 1.0 * df[\"prediction_std\"]\n        upper_68 = df[\"prediction\"] + 1.0 * df[\"prediction_std\"]\n        lower_50 = df[\"prediction\"] - 0.674 * df[\"prediction_std\"]\n        upper_50 = df[\"prediction\"] + 0.674 * df[\"prediction_std\"]\n    else:\n        raise ValueError(\n            \"Either quantile columns (q_025, q_975, q_25, q_75) or 'prediction_std' column must be present.\"\n        )\n    median_std = df[\"prediction_std\"].median()\n    coverage_95 = np.mean((df[target_col] &gt;= lower_95) &amp; (df[target_col] &lt;= upper_95))\n    coverage_90 = np.mean((df[target_col] &gt;= lower_90) &amp; (df[target_col] &lt;= upper_90))\n    coverage_80 = np.mean((df[target_col] &gt;= lower_80) &amp; (df[target_col] &lt;= upper_80))\n    coverage_68 = np.mean((df[target_col] &gt;= lower_68) &amp; (df[target_col] &lt;= upper_68))\n    median_width_95 = np.median(upper_95 - lower_95)\n    median_width_90 = np.median(upper_90 - lower_90)\n    median_width_80 = np.median(upper_80 - lower_80)\n    median_width_50 = np.median(upper_50 - lower_50)\n    median_width_68 = np.median(upper_68 - lower_68)\n\n    # --- CRPS (measures calibration + sharpness) ---\n    z = (df[target_col] - df[\"prediction\"]) / df[\"prediction_std\"]\n    crps = df[\"prediction_std\"] * (z * (2 * norm.cdf(z) - 1) + 2 * norm.pdf(z) - 1 / np.sqrt(np.pi))\n    mean_crps = np.mean(crps)\n\n    # --- Interval Score @ 95% (penalizes miscoverage) ---\n    alpha_95 = 0.05\n    is_95 = (\n        (upper_95 - lower_95)\n        + (2 / alpha_95) * (lower_95 - df[target_col]) * (df[target_col] &lt; lower_95)\n        + (2 / alpha_95) * (df[target_col] - upper_95) * (df[target_col] &gt; upper_95)\n    )\n    mean_is_95 = np.mean(is_95)\n\n    # --- Interval to Error Correlation ---\n    abs_residuals = np.abs(df[target_col] - df[\"prediction\"])\n    width_68 = upper_68 - lower_68\n\n    # Spearman correlation for robustness\n    interval_to_error_corr = spearmanr(width_68, abs_residuals)[0]\n\n    # --- Confidence to Error Correlation ---\n    # If confidence column exists, compute correlation (should be negative: high confidence = low error)\n    confidence_to_error_corr = None\n    if \"confidence\" in df.columns:\n        confidence_to_error_corr = spearmanr(df[\"confidence\"], abs_residuals)[0]\n\n    # Collect results\n    results = {\n        \"coverage_68\": coverage_68,\n        \"coverage_80\": coverage_80,\n        \"coverage_90\": coverage_90,\n        \"coverage_95\": coverage_95,\n        \"median_std\": median_std,\n        \"median_width_50\": median_width_50,\n        \"median_width_68\": median_width_68,\n        \"median_width_80\": median_width_80,\n        \"median_width_90\": median_width_90,\n        \"median_width_95\": median_width_95,\n        \"interval_to_error_corr\": interval_to_error_corr,\n        \"confidence_to_error_corr\": confidence_to_error_corr,\n        \"n_samples\": len(df),\n    }\n\n    print(\"\\n=== UQ Metrics ===\")\n    print(f\"Coverage @ 68%: {coverage_68:.3f} (target: 0.68)\")\n    print(f\"Coverage @ 80%: {coverage_80:.3f} (target: 0.80)\")\n    print(f\"Coverage @ 90%: {coverage_90:.3f} (target: 0.90)\")\n    print(f\"Coverage @ 95%: {coverage_95:.3f} (target: 0.95)\")\n    print(f\"Median Prediction StdDev: {median_std:.3f}\")\n    print(f\"Median 50% Width: {median_width_50:.3f}\")\n    print(f\"Median 68% Width: {median_width_68:.3f}\")\n    print(f\"Median 80% Width: {median_width_80:.3f}\")\n    print(f\"Median 90% Width: {median_width_90:.3f}\")\n    print(f\"Median 95% Width: {median_width_95:.3f}\")\n    print(f\"CRPS: {mean_crps:.3f} (lower is better)\")\n    print(f\"Interval Score 95%: {mean_is_95:.3f} (lower is better)\")\n    print(f\"Interval/Error Corr: {interval_to_error_corr:.3f} (higher is better, target: &gt;0.5)\")\n    if confidence_to_error_corr is not None:\n        print(f\"Confidence/Error Corr: {confidence_to_error_corr:.3f} (lower is better, target: &lt;-0.5)\")\n    print(f\"Samples: {len(df)}\")\n    return results\n</code></pre>"},{"location":"model_utils/#examples","title":"Examples","text":""},{"location":"model_utils/#feature-importance","title":"Feature Importance","text":"<pre><code>\"\"\"Example for using some Model Utilities\"\"\"\nfrom workbench.utils.model_utils import feature_importance\n\nmodel = Model(\"aqsol_classification\")\nfeature_importance(model)\n</code></pre> <p>Output</p> <pre><code>[('mollogp', 469.0),\n ('minabsestateindex', 277.0),\n ('peoe_vsa8', 237.0),\n ('qed', 237.0),\n ('fpdensitymorgan1', 230.0),\n ('fpdensitymorgan3', 221.0),\n ('estate_vsa4', 220.0),\n ('bcut2d_logphi', 218.0),\n ('vsa_estate5', 218.0),\n ('vsa_estate4', 209.0),\n</code></pre>"},{"location":"model_utils/#additional-resources","title":"Additional Resources","text":"<ul> <li>Workbench API Classes: API Classes</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"plugins/","title":"OverView","text":"<p>Workbench Plugins</p> <p>The Workbench toolkit provides a flexible plugin architecture to expand, enhance, or even replace the Dashboard. Make custom UI components, views, and entire pages with the plugin classes described here.</p> <p>The Workbench Plugin system allows clients to customize how their AWS Machine Learning Pipeline is displayed, analyzed, and visualized. Our easy to use Python API enables developers to make new Dash/Plotly components, data views, and entirely new web pages focused on business use cases.</p>"},{"location":"plugins/#concept-docs","title":"Concept Docs","text":"<p>Many classes in Workbench need additional high-level material that covers class design and illustrates class usage. Here's the Concept Docs for Plugins:</p> <ul> <li>Workbench Plugin Overview</li> </ul>"},{"location":"plugins/#make-a-plugin","title":"Make a plugin","text":"<p>Each plugin class inherits from the Workbench PluginInterface class and needs to set two attributes and implement two methods. These requirements are set so that each Plugin will conform to the Workbench infrastructure; if the required attributes and methods aren\u2019t included in the class definition, errors will be raised during tests and at runtime.</p> <p>Note: For full code see Model Plugin Example</p> <pre><code>class ModelPlugin(PluginInterface):\n    \"\"\"MyModelPlugin Component\"\"\"\n\n    \"\"\"Initialize this Plugin Component \"\"\"\n    auto_load_page = PluginPage.MODEL\n    plugin_input_type = PluginInputType.MODEL\n\n    def create_component(self, component_id: str) -&gt; dcc.Graph:\n        \"\"\"Create the container for this component\n        Args:\n            component_id (str): The ID of the web component\n        Returns:\n            dcc.Graph: The EndpointTurbo Component\n        \"\"\"\n        self.component_id = component_id\n        self.container = dcc.Graph(id=component_id, ...)\n\n        # Fill in plugin properties\n        self.properties = [(self.component_id, \"figure\")]\n\n        # Return the container\n        return self.container\n\n    def update_properties(self, model: Model, **kwargs) -&gt; list:\n        \"\"\"Update the properties for the plugin.\n\n        Args:\n            model (Model): An instantiated Model object\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            list: A list of the updated property values\n        \"\"\"\n\n        # Create a pie chart with the endpoint name as the title\n        pie_figure = go.Figure(data=..., ...)\n\n        # Return the updated property values for the plugin\n        return [pie_figure]\n</code></pre>"},{"location":"plugins/#required-attributes","title":"Required Attributes","text":"<p>The class variable plugin_page determines what type of plugin the MyPlugin class is. This variable is inspected during plugin loading at runtime in order to load the plugin to the correct artifact page in the Workbench dashboard. The PluginPage class can be DATA_SOURCE, FEATURE_SET, MODEL, or ENDPOINT.</p>"},{"location":"plugins/#s3-bucket-plugins","title":"S3 Bucket Plugins","text":"<p>Offers the most flexibility and fast prototyping. Simply set your workbench  S3 Path and Workbench will load the plugins from S3 directly.</p> <pre><code>\"WORKBENCH_PLUGINS\": \"s3://my-s3-bucket/workbench_plugins\"\n</code></pre> <p>Helpful Tip</p> <p>You can copy files from your local system up to S3 with this handy AWS CLI call</p> <pre><code> aws s3 cp . s3://my-s3-bucket/workbench_plugins \\\n --recursive --exclude \"*\" --include \"*.py\"\n</code></pre>"},{"location":"plugins/#additional-resources","title":"Additional Resources","text":"<p>Need help with plugins? Want to develop a customized application tailored to your business needs?</p> <ul> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"presentations/","title":"Workbench Presentations","text":"<p>The Workbench framework makes AWS\u00ae both easier to use and more powerful. Workbench handles all the details around updating and managing a complex set of AWS Services. With a simple-to-use Python API and a beautiful set of web interfaces, Workbench makes creating AWS ML pipelines a snap.</p> <p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"presentations/#workbench-presentations_1","title":"Workbench Presentations","text":"<ul> <li>Workbench Overview</li> <li>Private SaaS Architecture</li> <li>Python API</li> <li>Plugins Overview</li> <li>Plugins: Getting Started</li> <li>Plugins: Pages</li> <li>Plugins: Advanced</li> <li>Exploratory Data Analysis</li> <li>Architected ML Framework</li> <li>AWS Access Management</li> <li>Workbench Config</li> <li>Workbench REPL</li> </ul>"},{"location":"presentations/#workbench-python-api-docs","title":"Workbench Python API Docs","text":"<p>The Workbench API documentation Workbench API covers our in-depth Python API and contains code examples. The code examples are provided in the Github repo <code>examples/</code> directory. For a full code listing of any example please visit our Workbench Examples</p>"},{"location":"presentations/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p></p> <p>\u00ae Amazon Web Services, AWS, the Powered by AWS logo, are trademarks of Amazon.com, Inc. or its affiliates</p>"},{"location":"release_notes/0_8_106/","title":"Release 0.8.106","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.78</code> to <code>0.8.106</code></p>"},{"location":"release_notes/0_8_106/#general","title":"General","text":"<p>A bunch of naming changes from <code>sageworks</code> to <code>workbench</code>. Super... fun....</p> <p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_106/#api-changes","title":"API Changes","text":"<p>All imports have changed! So you can pretty much do a global replace (mind the caps) of sageworks with workbench.</p> <pre><code>from sageworks.api import DataSource\n&lt;is now&gt;\nfrom workbench.api import DataSource\n</code></pre>"},{"location":"release_notes/0_8_106/#agtable-plugin-changes","title":"AGTable Plugin Changes","text":"<p>The AGTable plugin now has 3 properties returned by <code>update_properties()</code>.</p> <pre><code>self.properties = [\n            (self.component_id, \"columnDefs\"),\n            (self.component_id, \"rowData\"),\n            (self.component_id, \"style\"),   &lt;-- This one is new\n        ]\n</code></pre> <p>In general you should be using the 'magic loop' method and shouldn't even have to know what properties are set. Here's a 'magic loop' example:</p> <p><pre><code>def example_callbacks(self):\n    @callback(\n        [Output(component_id, prop) for component_id, prop in my_table.properties],\n        [Input(\"whatever, \"blah\")],\n        )\n    def _example_callbacks(blah):\n        \"\"\"Callback to Populate the models table with data\"\"\"\n        models = self.meta.models(details=True)\n        return my_table.update_properties(models)\n</code></pre> See our presentation on Plugins and the 'magic loop': Workbench Plugins OverView</p>"},{"location":"release_notes/0_8_106/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.78 --&gt; v0.8.106 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What do you call that feeling like you\u2019ve done this before?               Deja-moo</p>"},{"location":"release_notes/0_8_106/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/0_8_158/","title":"Release 0.8.158","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases.</p> <p>Note: These release notes cover the changes from <code>0.8.106</code> to <code>0.8.158</code></p>"},{"location":"release_notes/0_8_158/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_158/#api-changes","title":"API Changes","text":"<ul> <li> <p>Artifact attribute 'uuid' to the more appropriate 'name'</p> <pre><code>my_model.uuid\n&lt;is now&gt;\nmy_model.name\n</code></pre> </li> <li> <p><code>capture_uuid</code> to <code>capture_name</code></p> <pre><code>end.inference(df, capture_uuid=\"my_inference_run\")\n&lt;is now&gt;\nend.inference(df, capture_name=\"my_inference_run\")\n</code></pre> </li> <li> <p>The training holdout method have removed the id_column arg (this information is already known by the FeatureSet)</p> <pre><code>fs.set_training_holdouts(id_column=\"my_id\", holdout_ids):\nfs.get_training_holdouts(id_column=\"my_id\") \n&lt;is now&gt;\nfs.set_training_holdouts(holdout_ids):\nfs.get_training_holdouts()\n</code></pre> </li> </ul>"},{"location":"release_notes/0_8_158/#aws-stack-changes","title":"AWS Stack Changes","text":"<p>The <code>workbench-core</code> stack has undergone some improvements and reorganization to support a more granular set of policies. We've also added a. set of 'ReadOnly' AWS Managed Policies so that folks can mix and match depending on their use cases. </p>"},{"location":"release_notes/0_8_158/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.106 --&gt; v0.8.158 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What do you call that feeling like you\u2019ve done this before?               Deja-moo</p>"},{"location":"release_notes/0_8_158/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/0_8_50/","title":"Release 0.8.50","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.46</code> to <code>0.8.50</code></p>"},{"location":"release_notes/0_8_50/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_50/#featureset-id_column-lock-in","title":"FeatureSet: id_column lock in","text":"<p>We're going to lock in id_columns when FeatureSets are created, AWS FeatureGroup requires an id column, so this is the best place to do it, see API Changes below.</p>"},{"location":"release_notes/0_8_50/#featureset-robust-handling-of-training-column","title":"FeatureSet: Robust handling of training column","text":"<p>In the past we haven't supported giving a training column as input data. FeatureSets are read-only, so locking in the training rows is 'suboptimal'. In general you might want to use the FeatureSet for several models with different training/hold_out sets. Now if a FeatureSet detects a training column it will give the follow message:</p> <pre><code>Training column detected: Since FeatureSets are read only, Workbench \ncreates training views that can be dynamically changed. We'll use \nthis training column to create a training view.\n</code></pre>"},{"location":"release_notes/0_8_50/#endpoint-auto_inference","title":"Endpoint: auto_inference()","text":"<p>We're changing the internal logic for the <code>auto_inference()</code> method to include the id_column in it's output.</p>"},{"location":"release_notes/0_8_50/#api-changes","title":"API Changes","text":"<p>FeatureSet</p> <p>When creating a FeatureSet the <code>id_column</code> is now a required argument.</p> <pre><code>ds = DataSource(\"test_data\")\nfs = ds.to_features(\"test_features\", id_column=\"my_id\") &lt;-- Required\n</code></pre> <p><pre><code>to_features = PandasToFeatures(\"my_feature_set\")\nto_features.set_input(df_features, id_column=\"my_id\")  &lt;-- Required\nto_features.set_output_tags([\"blah\", \"whatever\"])\nto_features.transform()\n</code></pre> If you're data doesn't have a id column you can specify \"auto\"</p> <pre><code>to_features = PandasToFeatures(\"my_feature_set\")\nto_features.set_input(df_features, id_column=\"auto\")  &lt;-- Auto Id (index)\n</code></pre> <p>For more details see: FeatureSet Class</p> <p>The new Meta() API will be used inside of the Artifact classes (see Internal Changes...Artifacts... below)</p>"},{"location":"release_notes/0_8_50/#improvements","title":"Improvements","text":"<p>DFStore</p> <p>Robust handling of slashes, so now it will 'just work' with various upserts and gets:</p> <pre><code>```\n# These all give you /ml/shap_value dataframe\ndf_store.get(\"/ml/shap_values\")\ndf_store.get(\"ml/shap_values\")\ndf_store.get(\"//ml/shap_values\")\n```\n</code></pre>"},{"location":"release_notes/0_8_50/#internal-changes","title":"Internal Changes","text":"<p>There's a whole new directory structure that helps isolate Cloud Platform specific funcitonality.</p> <pre><code>- workbench/src\n     - core/cloud_platform\n        - aws\n        - azure\n        - gcp\n</code></pre> <ul> <li>The <code>DFStore</code> now uses <code>AWSDFStore</code> as its concrete implementation class.</li> <li>Both <code>CachedMeta</code> and <code>AWSAccountClamp</code> have had a revamp of their singleton logic.</li> </ul>"},{"location":"release_notes/0_8_50/#internal-caching","title":"Internal Caching","text":"<p>So as part of our v0.9.0 Roadmap we're continuing to revamp caching. We're experimenting with CachedMeta Class inside the Artifact classes. Caching continues to be challenging for the framework, it's an absolute must for Web Inferface/UI performance and then it needs to get out of the way for batch jobs and the concurrent building of ML pipelines.</p>"},{"location":"release_notes/0_8_50/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.46 --&gt; v0.8.50 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>That feeling like you\u2019ve done this before?       .... Deja-moo</p>"},{"location":"release_notes/0_8_50/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/0_8_55/","title":"Release 0.8.55","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.50</code> to <code>0.8.55</code></p>"},{"location":"release_notes/0_8_55/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_55/#featureset-training-column-values","title":"FeatureSet: training column values","text":"<p>We're got a good suggestion from one of our beta customers to change the training column to use True/False values instead of 1/0. Having boolean values make semantic sense and make filtering easier and more intuitive.</p>"},{"location":"release_notes/0_8_55/#api-changes","title":"API Changes","text":"<p>FeatureSet Queries</p> <p>Since the training column now contains True/False, any code that you have where you're doing a query against the training view.</p> <pre><code>fs.query(f'SELECT * FROM \"{table}\" where training = 1')\n&lt;changed to&gt;\nfs.query(f'SELECT * FROM \"{table}\" where training = TRUE')\n\nfs.query(f'SELECT * FROM \"{table}\" where training = 0')\n&lt;changed to&gt;\nfs.query(f'SELECT * FROM \"{table}\" where training = FALSE')\n</code></pre> <p>Also dataframe filtering is easier now, so if you have a call to filter the dataframe that also needs to change.</p> <p><pre><code>df_train = all_df[all_df[\"training\"] == 1].copy()\n&lt;changed to&gt;\ndf_train = all_df[all_df[\"training\"]].copy()\n\ndf_val = all_df[all_df[\"training\"] == 0].copy()\n&lt;changed to&gt;\ndf_val = all_df[~all_df[\"training\"]].copy()\n</code></pre> For more details see: Training View</p> <p>Model Instantiation</p> <p>We got a request to reduce the time for Model() object instantiation. So we created a new <code>CachedModel()</code> class that is much faster to instantiate.</p> <p><pre><code>%time Model(\"abalone-regression\")\nCPU times: user 227 ms, sys: 19.5 ms, total: 246 ms\nWall time: 2.97 s\n\n%time CachedModel(\"abalone-regression\")\nCPU times: user 8.83 ms, sys: 2.64 ms, total: 11.5 ms\nWall time: 22.7 ms\n</code></pre> For more details see: CachedModel</p>"},{"location":"release_notes/0_8_55/#improvements","title":"Improvements","text":"<p>Workbench REPL Onboarding</p> <p>At some point the onboarding with Workbench REPL got broken and wasn't properly responding when the user didn't have a complete AWS/Workbench setup.</p>"},{"location":"release_notes/0_8_55/#internal-changes","title":"Internal Changes","text":"<p>The decorator for the CachedMeta class did not work properly in Python 3.9 so had to be slightly refactored.</p>"},{"location":"release_notes/0_8_55/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.50 --&gt; v0.8.55 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>That feeling like you\u2019ve done this before?       .... Deja-moo</p>"},{"location":"release_notes/0_8_55/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/0_8_58/","title":"Release 0.8.58","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.55</code> to <code>0.8.58</code></p>"},{"location":"release_notes/0_8_58/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_58/#caching-refactoring","title":"Caching Refactoring","text":"<p>We've created a new set of Cached Classes:</p> <ul> <li>CachedDataSource</li> <li>CachedFeatureSet</li> <li>CachedModel</li> <li>CacheEndpoint</li> </ul> <p>As part of this there's now a <code>workbench/cached</code> directory that housed these classes and the <code>CachedMeta</code> class.</p>"},{"location":"release_notes/0_8_58/#api-changes","title":"API Changes","text":"<p>Meta Imports Yes, this changed AGAIN :)</p> <pre><code>from workbench.meta import Meta\n&lt;change to&gt;\nfrom workbench.api import Meta\n</code></pre> <p>CachedModel Import <pre><code>from workbench.api import CachedModel\n&lt;change to&gt;\nfrom workbench.cached.cached_model import CachedModel\n</code></pre> For more details see: CachedModel</p>"},{"location":"release_notes/0_8_58/#improvements","title":"Improvements","text":"<p>Dashboard Responsiveness</p> <p>The whole point of these Cached Classes is to improve Dashboard/Web Interface responsiveness. The Dashboard uses both the CachedMeta and Cached(Artifact) classes to make both overview and drilldowns faster.</p> <p>Supporting a Client Use Case There was a use case where a set of plugin pages needed to iterate over all the models to gather and aggregate information. We've supported that use case with a new decorator that avoids overloading AWS/Throttling issues.</p> <p>Internal The Dashboard now refreshes all data every 90 seconds, so if you don't see you're new model on the dashboard... just wait longer. :)</p>"},{"location":"release_notes/0_8_58/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.55 --&gt; v0.8.58 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What do you call a nonsense meeting?       .... Moo-larkey</p>"},{"location":"release_notes/0_8_58/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/0_8_60/","title":"Release 0.8.60","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.58</code> to <code>0.8.60</code></p>"},{"location":"release_notes/0_8_60/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_60/#custom-models","title":"Custom Models","text":"<p>We've now exposed additional functionality and API around adding your own custom models. The new custom model support is documented on the Features to Models page.  </p>"},{"location":"release_notes/0_8_60/#api-changes","title":"API Changes","text":"<p>None</p>"},{"location":"release_notes/0_8_60/#notes","title":"Notes","text":"<p>Custom models introduce models that don't have model metrics or inference runs, so you'll see a lot of log messages complaining about not finding metrics or inference results, please just ignore those, we'll put in additional logic to address those cases.</p>"},{"location":"release_notes/0_8_60/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.58 --&gt; v0.8.60 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What do you call a nonsense meeting?       .... Moo-larkey</p>"},{"location":"release_notes/0_8_60/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/0_8_71/","title":"Release 0.8.71","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.60</code> to <code>0.8.71</code></p>"},{"location":"release_notes/0_8_71/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_71/#dash-thread-safety","title":"Dash Thread Safety","text":"<p>We learned that thread safety is good when using plugin classes. We had a model plugin class that was setting an attribute in one callback and then using that attribute in another callback, this mostly worked until it didn't. Anyway so the Inference Run dropdown box on the Models page now actually works correctly.</p>"},{"location":"release_notes/0_8_71/#api-changes","title":"API Changes","text":"<p>None</p>"},{"location":"release_notes/0_8_71/#internal-changes","title":"Internal Changes","text":"<p>When using PandasToFeatures it will overwrite FeatureSets if you give the same name. This behavior is expected. The issue was that it was super eager about doing that and would do it during class initiation, so we've moved that logic to when <code>transform()</code> is called.</p> <pre><code># Create a Feature Set from a DataFrame\ndf_to_features = PandasToFeatures(\"test_features\")\ndf_to_features.set_input(data_df, id_column=\"id\", one_hot_columns=[\"food\"])\ndf_to_features.set_output_tags([\"test\", \"small\"])\ndf_to_features.transform()      &lt;--- Overwrite happens here\n</code></pre>"},{"location":"release_notes/0_8_71/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.60 --&gt; v0.8.71 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What do you call that feeling like you\u2019ve done this before?               Deja-moo</p>"},{"location":"release_notes/0_8_71/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/0_8_74/","title":"Release 0.8.74","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.71</code> to <code>0.8.74</code></p>"},{"location":"release_notes/0_8_74/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_74/#s3-based-plugins","title":"S3 based Plugins","text":"<p>We've added the ability to grab Dashboard plugins directly from an S3 bucket. Please see Dashboard S3 Plugin</p>"},{"location":"release_notes/0_8_74/#artifact-hashes","title":"Artifact Hashes","text":"<p>For content verification purposes we've added a <code>hash()</code> method to all of the Workbench Artifact classes (DataSource, FeatureSet, Model, Endpoint, Graph, etc). Also for DataSources and FeatureSets there is a <code>table_hash()</code> method that will compute a total hash of all data in the Athena table.</p> <pre><code>ds = DataSource(\"abalone_data\")\n\nds.modified()\nOut[2]: datetime.datetime(2024, 11, 17, 19, 45, 58, tzinfo=tzlocal())\n\nds.hash()\nOut[3]: '67a9ebb495af573604794aa9c31eded8'\n\nds.table_hash()\nOut[4]: '622f5ddba9d4cad2cf642d1ea5555de9'\n\nfs = FeatureSet(\"test_features\")\n\nfs.hash()\nOut[5]: '1571eee207b72f14bd5065d6c4acdaaf'\n\n# Note: Model/Endpoint hashes will backtrack to model.tar.gz and can be used for validation\nmodel = Model(\"abalone-regression\")\nend = Endpoint(\"abalone-regression-end\")\n\nmodel.get_model_data_url()\nOut[6]: 's3://sagemaker-us-west-2-507740646243/abalone-regression-2024-11-18-03-09/output/model.tar.gz'\n\nmodel.hash()\nOut[7]: '00def9381366cdd062413d0b395ba70c'\n\n# Verify endpoint is using expected model\nend.hash()\nOut[7]: '00def9381366cdd062413d0b395ba70c'\n\n# Realtime endpoint created from the same model\nend = Endpoint(\"abalone-regression-end-rt\")\nend.hash()\nOut[8]: '00def9381366cdd062413d0b395ba70c'\n</code></pre> <p>Note: You will get a performance warning when running hash() or table_hash() as these operations compute actual content hashes on the underlying data.</p>"},{"location":"release_notes/0_8_74/#custom-model-scripts","title":"Custom Model Scripts","text":"<p>We're expanding our custom model script to include some additional chemical informatics capabilities. In addition to our molecular descriptors and Morgan fingerprints we also have RDKIT canonicalization and tautomerization.</p>"},{"location":"release_notes/0_8_74/#api-changes","title":"API Changes","text":"<p>FeatureSet</p> <p>When creating a FeatureSet the <code>id_column</code> is now NOT required. The old \"auto\" functionality is gone, if you don't specify an <code>id_column</code> and id column named <code>auto_id</code> will be automatically generated.</p> <pre><code>ds = DataSource(\"test_data\")\nfs = ds.to_features(\"test_features\", id_column=\"auto\")\n&lt;replaced by&gt;\nfs = ds.to_features(\"test_features\")\n\n# We do recommend that you specify an id :)\nfs = ds.to_features(\"test_features\", id_column=\"my_id\")\n</code></pre> <ul> <li> <p><code>get_database()</code> has a deprecation warning, it's replaced with just the <code>database</code> property.</p> <pre><code>ds.get_database()\n&lt;replaced by&gt;\nds.database\n</code></pre> </li> <li> <p>Added the <code>hash()</code> method to Artifacts (see above).</p> </li> <li>Added <code>table_hash()</code> method to DataSources and FeatuerSet (see above).</li> </ul>"},{"location":"release_notes/0_8_74/#internal-changes","title":"Internal Changes","text":"<p>There was a small refactor of the cache decorator. We fixed a case where if we blocked on getting a value we also spun up a background thread to get it. This chance will not affect existing code or APIs.</p>"},{"location":"release_notes/0_8_74/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.71 --&gt; v0.8.74 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What do you call that feeling like you\u2019ve done this before?               Deja-moo</p>"},{"location":"release_notes/0_8_74/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/0_8_78/","title":"Release 0.8.78","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.74</code> to <code>0.8.78</code></p>"},{"location":"release_notes/0_8_78/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/0_8_78/#s3-based-plugins","title":"S3 based Plugins","text":"<p>We've added the ability to grab Dashboard plugins directly from an S3 bucket. Please see Dashboard S3 Plugin</p>"},{"location":"release_notes/0_8_78/#tbd-stuff","title":"TBD Stuff","text":""},{"location":"release_notes/0_8_78/#api-changes","title":"API Changes","text":""},{"location":"release_notes/0_8_78/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.74 --&gt; v0.8.78 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What do you call that feeling like you\u2019ve done this before?               Deja-moo</p>"},{"location":"release_notes/0_8_78/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_7_8/","title":"Release 0.7.8","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on [Discord](https://discord.gg/WHAJuz8sw8</p> <p>Since we've recently introduced a View() class for DataSources and FeatureSets we needed to rename a few classes/modules.</p>"},{"location":"release_notes/archived/0_7_8/#featuresets","title":"FeatureSets","text":"<p>For setting holdout ids we've changed/combined to just one method <code>set_training_holdouts()</code>, so if you're using <code>create_training_view()</code> or <code>set_holdout_ids()</code> you can now just use the unified method <code>set_training_holdouts()</code>.</p> <p>There's also a change to getting the training view table method.</p> <pre><code>old: fs.get_training_view_table(create=False)\nnew: fs.get_training_view_table(), does not need the create=False\n</code></pre>"},{"location":"release_notes/archived/0_7_8/#models","title":"Models","text":"<pre><code>inference_predictions() --&gt; get_inference_predictions()\n</code></pre>"},{"location":"release_notes/archived/0_7_8/#webplugins","title":"Web/Plugins","text":"<p>We've changed the Web/UI View class to 'WebView'. So anywhere where you used to have view just replace with web_view</p> <p><pre><code>from workbench.views.artifacts_view import ArtifactsView\n</code></pre> is now</p> <pre><code>from workbench.web_interface.page_views.all_artifacts import AllArtifacts\n</code></pre>"},{"location":"release_notes/archived/0_7_8/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_11/","title":"Release 0.8.11","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover all the changes from <code>0.8.8</code> to <code>0.8.11</code></p>"},{"location":"release_notes/archived/0_8_11/#general","title":"General","text":"<p>The AWSAccountClamp had too many responsibilities so that class has been split up into two classes and a set of utilities:</p> <ul> <li>AWSAccountClamp</li> <li>AWSSession</li> <li>utils/execution_environment.py</li> </ul>"},{"location":"release_notes/archived/0_8_11/#api-changes","title":"API Changes","text":"<p>For all/most of these API changes they include both DataSources and FeatureSets. We're using a FeatureSet (fs) in the examples below but also applies to DataSoources.</p> <ul> <li> <p>Column Names/Table Names</p> <pre><code>fs.column_names() -&gt; fs.columns\nfs.get_table_name() -&gt; fs.table_name\n</code></pre> </li> <li> <p>Display/Training/Computation Views</p> <p>In general methods for FS/DS are now part of the View API, here's a change list:</p> <pre><code>fs.get_display_view() -&gt; fs.view(\"display\")\nfs.get_training_view() -&gt; fs.view(\"training\")\nfs.get_display_columns() -&gt; fs.view(\"display\").columns\nfs.get_computation_columns() -&gt; fs.view(\"computation\").columns\nfs.get_training_view_table() -&gt; fs.view(\"training\").table_name\nfs.get_training_data(self) -&gt; fs.view(\"training\").pull_dataframe()\n</code></pre> <p>Some FS/DS methods have also been removed</p> <p><code>num_display_columns() -&gt; gone    num_computation_columns() -&gt; gone</code></p> </li> <li> <p>Views: Methods that we're Keeping</p> <p>We're keeping the methods below since they handle some underlying mechanics and serve as nice convenience methods. </p> <pre><code>ds/fs.set_display_columns()\nds/fs.set_computation_columns()\n</code></pre> </li> <li> <p>AWSAccountClamp</p> <pre><code>AWSAccountClamp().boto_session() --&gt; AWSAccountClamp().boto3_session\n</code></pre> </li> <li> <p>All Classes</p> <p>If the class previously had a <code>boto_session</code> attribute that has been renamed to <code>boto3_session</code></p> </li> </ul>"},{"location":"release_notes/archived/0_8_11/#glue-job-fixes","title":"Glue Job Fixes","text":"<p>For <code>workbench==0.8.8</code> you needed to be careful about when/where you set your config/ENV vars. With <code>&gt;=0.8.9</code> you can now use the typical setup like this:</p> <pre><code>```\nfrom workbench.utils.config_manager import ConfigManager\n\n# Set the Workbench Config\ncm = ConfigManager()\ncm.set_config(\"WORKBENCH_BUCKET\", args_dict[\"workbench-bucket\"])\ncm.set_config(\"REDIS_HOST\", args_dict[\"redis-host\"])\n```\n</code></pre>"},{"location":"release_notes/archived/0_8_11/#robust-modelnotreadyexception-handling","title":"Robust ModelNotReadyException Handling","text":"<p>AWS will 'deep freeze' Serverless Endpoints and if that endpoint hasn't been used for a while it can sometimes take a long time to come up and be ready for inference. Workbench now properly manages this AWS error condition, it will report the issue, wait 60 seconds, and try again 5 times before raising the exception.</p> <pre><code>(endpoint_core.py:502) ERROR Endpoint model not ready\n(endpoint_core.py:503) ERROR Waiting and Retrying...\n...\nAfter a while, inference will run successfully :)\n</code></pre>"},{"location":"release_notes/archived/0_8_11/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_20/","title":"Release 0.8.20","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.11</code> to <code>0.8.20</code></p>"},{"location":"release_notes/archived/0_8_20/#general","title":"General","text":"<p>The <code>cloud_watch</code> AWS log aggregator, is now officially awesome. It provides a fairly sophisticated way of both doing broad scanning and deep dives on individual streams. Please see our Cloud Watch documentation.</p> <p>The View classes have finished their refactoring. The 'read' class <code>View()</code> can be constructed either directly or with the <code>ds/fs.view(\"display\")</code> methods. See Views for more details. There also a set of classes for constructing views, please see View Overview</p>"},{"location":"release_notes/archived/0_8_20/#api-changes","title":"API Changes","text":"<ul> <li> <p>Table Name attribute</p> <p>The <code>table_name</code> attribute/property has been replaced with just <code>table</code></p> <pre><code>ds.table_name -&gt; ds.table\nfs.table_name -&gt; fs.table\nview.table_name -&gt; view.table\n</code></pre> </li> <li> <p>Endpoint Confusion Matrix</p> <p>The <code>endpoint</code> class had a method called <code>confusion_matrix()</code> this has been renamed to the more descriptive <code>generate_confusion_matrix()</code>. Note: The model method, of the same name, has NOT changed.</p> <pre><code>end.confusion_matrix() -&gt; end.generate_confusion_matrix()\nmodel.confusion_matrix() == no change\n</code></pre> </li> </ul>"},{"location":"release_notes/archived/0_8_20/#bug-fixes","title":"Bug Fixes","text":"<p>Fixed: There was a corner case where if you had the following sequence:</p> <ul> <li>New FeatureSet without a training view</li> <li>Created a traiing view (like with <code>set_training_holdouts()</code> </li> <li>Immediately query/pull from the FS training view</li> </ul> <p>The corner case was a race-condition where the FeatureSet would not 'know' that a training view was already there and would create a default training view.</p>"},{"location":"release_notes/archived/0_8_20/#improvements","title":"Improvements","text":"<p>The log messages that you receive on a plugin validation failure should now be more distinguishable and more informative. They will look like this and in some cases even tell you the line to look at.</p> <pre><code>ERROR Plugin 'MyPlugin' failed validation:\nERROR    File: ../workbench_plugins/components/my_plugin.py\nERROR    Class: MyPlugin\nERROR    Details: my_plugin.py (line 35): Incorrect return type for update_properties (expected list, got Figure)\n</code></pre>"},{"location":"release_notes/archived/0_8_20/#internal-api-changes","title":"Internal API Changes","text":"<p>In theory these API should not affect end user of the Workbench API but are documented here for completeness.</p> <ul> <li>Artifact Name Validation</li> </ul> <p>The internal method used by Artifact subclasses has changed names from <code>ensure_valid_name</code> to <code>is_name_valid</code>, we've also introduced an optional argument to turn on/off lowercase enforcement, this will be used later when we support uppercase for models, endpoints, and graphs.</p>"},{"location":"release_notes/archived/0_8_20/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_22/","title":"Release 0.8.22","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.20</code> to <code>0.8.22</code></p>"},{"location":"release_notes/archived/0_8_22/#general","title":"General","text":"<p>Mostly bug fixes and minor API changes.</p>"},{"location":"release_notes/archived/0_8_22/#api-changes","title":"API Changes","text":"<ul> <li> <p>Removing <code>target_column</code> arg when creating FeatureSets</p> <p>When creating a FeatureSet via DataSource or Pandas Dataframe there was an optional argument for the <code>target_column</code> after some discussion we decided to remove this argument. In general <code>FeatureSets</code> are often used to create multiple models with different targets, so it doesn't make sense to specify a <code>target</code> at the FeatureSet level.</p> <p>Changed for both <code>DataSource.to_features()</code> and the <code>PandasToFeatures()</code> classes.</p> </li> </ul>"},{"location":"release_notes/archived/0_8_22/#minor-bug-fixes","title":"Minor Bug Fixes","text":"<p>Fixed: The SHAP computation was occasionally complaining about the additivity check so we flipped that flag to False</p> <pre><code>shap_vals = explainer.shap_values(X_pred, check_additivity=False)\n</code></pre>"},{"location":"release_notes/archived/0_8_22/#improvements","title":"Improvements","text":"<p>The optional requirements for <code>[UI]</code> now include matplotlib since it will probably be useful in the future.</p>"},{"location":"release_notes/archived/0_8_22/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_23/","title":"Release 0.8.23","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.22</code> to <code>0.8.23</code></p>"},{"location":"release_notes/archived/0_8_23/#general","title":"General","text":"<p>Mostly bug fixes and minor API changes.</p>"},{"location":"release_notes/archived/0_8_23/#api-changes","title":"API Changes","text":"<ul> <li> <p>Removing <code>auto_one_hot</code> arg from <code>PandasToFeatures</code> and <code>DataSource.to_features()</code></p> <p>When creating a <code>PandasToFeatures</code> object or using <code>DataSource.to_features()</code> there was an optional argument <code>auto_one_hot</code>. This would try to automatically convert object/string columns to be one-hot encoded. In general this was only useful for 'toy' datasets but for more complex data we need to specify exactly which columns we want converted.</p> </li> <li> <p>Adding optional <code>one_hot_columns</code> arg to <code>PandasToFeatures.set_input()</code> and <code>DataSource.to_features()</code> </p> <p>When calling either of these FeatureSet creation methods you can now add an option arg <code>one_hot_columns</code> as a list of columns that you would like to be one-hot encoded.</p> </li> </ul>"},{"location":"release_notes/archived/0_8_23/#minor-bug-fixes","title":"Minor Bug Fixes","text":"<p>Our pandas dependency was outdated and causing an issue with an <code>include_groups</code> arg when outlier groups were computed. We've changed the requirements:</p> <p><pre><code>pandas&gt;=2.1.2\nto\npandas&gt;=2.2.1\n</code></pre> We also have a ticket for the logic change so that we avoid the deprecation warning.</p>"},{"location":"release_notes/archived/0_8_23/#improvements","title":"Improvements","text":"<p>The time to <code>ingest</code> new rows into a FeatureSet can take a LONG time. Calling the FeatureGroup AWS API and waiting on the results is what takes all the time.</p> <p>There will hopefully be a series of optimizations around this process, the first one is simply increasing the number of workers/processes for the ingestion manager class.</p> <pre><code>feature_group.ingest(.., max_processes=8)\n(has been changed to)\nfeature_group.ingest(..., max_processes=16, num_workers=4)\n</code></pre>"},{"location":"release_notes/archived/0_8_23/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_27/","title":"Release 0.8.27","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.23</code> to <code>0.8.27</code></p>"},{"location":"release_notes/archived/0_8_27/#general","title":"General","text":"<ul> <li>Plugin Launcher: A flexible way to test plugins</li> <li>FeatureSpaceProximity (FSP): A Class for investigating feature space, neighbors, distances, etc.</li> <li>ProximityGraph: A class to construct NetworkX Graphs. Uses the FSP class to pull neighbors, contruct edges via 'distances', etc.</li> <li>Case</li> </ul>"},{"location":"release_notes/archived/0_8_27/#api-changes","title":"API Changes","text":"<ul> <li> <p>KNNSpider() --&gt; FeatureSpaceProximity()</p> <p>If you were previously using the <code>KNNSpider</code> that class has been replaced with <code>FeatureSpaceProximity</code>. The API is also a bit different please see the documentation on the FeatureSpaceProximity Class.</p> </li> </ul>"},{"location":"release_notes/archived/0_8_27/#minor-improvements","title":"Minor Improvements","text":"<p>The model scripts used in deployed AWS Endpoints are now case-insensitive. In general this should make the use of the endpoints a bit more flexible for End-User Applications to hit the endpoints with less pre-processing of their column names.</p> <p>CloudWatch default buffers have been increased to 60 seconds as we appears to have been hitting some AWS limits with running 10 concurrent glue jobs.</p>"},{"location":"release_notes/archived/0_8_27/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_29/","title":"Release 0.8.29","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.27</code> to <code>0.8.29</code></p>"},{"location":"release_notes/archived/0_8_29/#general","title":"General","text":"<p>Locking AWS Model Training Image: AWS will randomly update the images associated with training and model registration. In particular the SKLearn Estimator has been updated into a non-working state for our use cases. So for both training and registration we're now explicitly specifying the image that we want to use.</p> <pre><code> self.estimator = SKLearn(\n     ...\n     framework_version=\"1.2-1\",\n     image_uri=image,  # New\n )\n</code></pre>"},{"location":"release_notes/archived/0_8_29/#api-changes","title":"API Changes","text":"<ul> <li> <p>delete() --&gt; class.delete(name)</p> <p>We've changed the API for deleting artifacts in AWS (DataSource, FeatureSet, etc). This is part of our efforts to minimize race-conditions when objects are deleted. </p> <pre><code>my_model = Model(\"xyz\")  # Creating object\nmy_model.delete()        # just to delete\n\n&lt;Now just one line&gt;\nModel.delete(\"xyz\")      # Delete\n</code></pre> </li> </ul>"},{"location":"release_notes/archived/0_8_29/#minor-improvements","title":"Minor Improvements","text":"<p>Bulk Delete: Added a Bulk Delete utility</p> <pre><code>from workbench.utils.bulk_utils import bulk_delete\n\ndelete_list = [(\"DataSource\", \"abc\"), (\"FeatureSet\", \"abc_features\")]\nbulk_delete(delete_list)\n</code></pre>"},{"location":"release_notes/archived/0_8_29/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_33/","title":"Release 0.8.33","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.29</code> to <code>0.8.33</code></p>"},{"location":"release_notes/archived/0_8_33/#general","title":"General","text":"<p>Replaced WatchTower Code: Had lots of issues with WatchTower on Glue/Lambda, the use of forks/threads was overkill for our logging needs, so simply replaced the code with boto3 <code>put_log_events()</code> calls and some simple token handling and buffering.</p>"},{"location":"release_notes/archived/0_8_33/#api-changes","title":"API Changes","text":"<p>None</p>"},{"location":"release_notes/archived/0_8_33/#improvementsfixes","title":"Improvements/Fixes","text":"<p>DataSource from DataFrame: When creating a DataSource from a Pandas Dataframe, the internal <code>transform()</code> was not deleting the existing DataSource (if it existed).</p> <p>ROCAUC on subset of classes: When running inference on input data that only had a subset of the classification labels (e.g. rows only had \"low\" and \"medium\" when model was trained on \"low\", \"medium\", \"high\"). The input to ROCAUC needed to be adjusted so that ROCAUC doesn't crash. When this case happens we're returning proper defaults based on scikit learn docs.</p>"},{"location":"release_notes/archived/0_8_33/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_35/","title":"Release 0.8.35","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.33</code> to <code>0.8.35</code></p>"},{"location":"release_notes/archived/0_8_35/#general","title":"General","text":"<p>Workbench REPL: The REPL now has a workaround for the current iPython embedded shell namespace scoping issue. See: iPython Embedded Shell Scoping Issue. So this pretty much means the REPL is 110% more awesome now!</p>"},{"location":"release_notes/archived/0_8_35/#api-changes","title":"API Changes","text":"<p>None</p>"},{"location":"release_notes/archived/0_8_35/#improvementsfixes","title":"Improvements/Fixes","text":"<p>AWS Service Broker: The AWS service broker was dramatic when it pulls meta data for something that just got deleted (or partially deleted), it was throwing CRITICAL log messages. We've refined the AWS error handling so that it's more granular about the error_codes for Validation or ResourceNotFound exceptions those are reduced to WARNINGS.</p> <p>ROCAUC modifications: Version <code>0.8.33</code> put in quite a few changes, for <code>0.8.35</code> we've also added logic to both validate and ensure proper order of the probability columns with the class labels.</p>"},{"location":"release_notes/archived/0_8_35/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.33 --&gt; v0.8.35 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What do you call a cow with no legs?     ........Ground beef.</p>"},{"location":"release_notes/archived/0_8_35/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_36/","title":"Release 0.8.36","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.35</code> to <code>0.8.36</code></p>"},{"location":"release_notes/archived/0_8_36/#general","title":"General","text":"<p>Fast Inference: The current inference method for endpoints provides error handling, metrics calculations and capture mechanics. There are use cases where the inference needs to happen as fast as possible without all the additional features. So we've added a <code>fast_inference()</code> method that streamlines the calls to the endpoint.</p> <pre><code>end = Endpoint(\"my_endpoint\")\nend.inference(df)  # Metrics, Capture, Error Handling\nWall time: 5.07 s\n\nend.fast_inference(df)  # No frills, but Fast!\nWall time: 308 ms\n</code></pre>"},{"location":"release_notes/archived/0_8_36/#api-changes","title":"API Changes","text":"<p>None</p>"},{"location":"release_notes/archived/0_8_36/#improvementsfixes","title":"Improvements/Fixes","text":"<p>Version Update Check: Added functionality that checks the current Workbench version against the latest released and gives a log message for update available.</p> <p>ROCAUC modifications: Functionality now includes 'per label' rocauc calculation along with label order and alignment from previous versions.</p>"},{"location":"release_notes/archived/0_8_36/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.35 --&gt; v0.8.36 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>What\u2019s a cow\u2019s best subject in school?     ......Cow-culus.</p>"},{"location":"release_notes/archived/0_8_36/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_39/","title":"Release 0.8.39","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.36</code> to <code>0.8.39</code></p>"},{"location":"release_notes/archived/0_8_39/#general","title":"General","text":"<p>Just a small set of error handling and bug fixes.</p>"},{"location":"release_notes/archived/0_8_39/#api-changes","title":"API Changes","text":"<p>None</p>"},{"location":"release_notes/archived/0_8_39/#improvementsfixes","title":"Improvements/Fixes","text":"<p>Scatter Plot: Fixed a corner case where the hoover columns included AWS generated fields.</p> <p>Athena Queries: Put in additional error handling and retries when looking for and querying Athena/Glue Catalogs. These changes affect both DataSource and Features (which have DataSources internally for offline storage).</p> <p>FeatureSet Creation: Put in additional error handling and retries when pulling AWS meta data for FeatureSets (and internal DataSources).</p>"},{"location":"release_notes/archived/0_8_39/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.36 --&gt; v0.8.39 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>That feeling like you\u2019ve done this before?       .... Deja-moo</p>"},{"location":"release_notes/archived/0_8_39/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_42/","title":"Release 0.8.42","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.39</code> to <code>0.8.42</code></p>"},{"location":"release_notes/archived/0_8_42/#general","title":"General","text":"<p>Artifact deletion got a substantial overhaul. The 4 main classes received internal code changes for how they get deleted. Specifically deletion is now handled via a class method that allows an artifact to be delteed without instantiating an object. The API for deletion is actually more flexible now, please see API Changes below.</p>"},{"location":"release_notes/archived/0_8_42/#api-changes","title":"API Changes","text":"<p>Artifact Deletion</p> <p>The API for Artifact deletion is more flexible, if you already have an instantiated object, you can simply call <code>delete()</code> on it. If you're deleting an object in bulk/batch mode, you can call the class method <code>managed_delete()</code>, see code example below.</p> <p><pre><code>fs = FeatureSet(\"my_fs\")\nfs.delete()                        # Used for notebooks, scripts, etc.. \nOR\nFeatureSet.managed_delete(\"my_fs\") # Bulk/batch/internal use\n\n&lt;Same API for DataSources, Models, and Endpoints&gt;\n</code></pre> Note: Internally these use the same functionality, the dual API is simply for ease-of-use.</p>"},{"location":"release_notes/archived/0_8_42/#improvementsfixes","title":"Improvements/Fixes","text":"<p>Race Conditions</p> <p>In theory, the changes to a class based delete will reduce race conditions where an object would try to create itself (just to be deleted) and the AWS Service Broker was encountering partially created (or partially deleted objects) and would barf error messages.</p> <p>Slightly Better Throttling Logic</p> <p>The AWS Throttles have been 'tuned' a bit to back off a bit faster and also not retry the list_tags request when the ARN isn't found.</p>"},{"location":"release_notes/archived/0_8_42/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.39 --&gt; v0.8.42 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>That feeling like you\u2019ve done this before?       .... Deja-moo</p>"},{"location":"release_notes/archived/0_8_42/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_46/","title":"Release 0.8.46","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p> <p>Note: These release notes cover the changes from <code>0.8.42</code> to <code>0.8.46</code></p>"},{"location":"release_notes/archived/0_8_46/#general","title":"General","text":"<p>This release is an incremental release as part of the road map for <code>v.0.9.0</code>. Please see the full details of the planned changes here: v0.9.0 Roadmap. </p>"},{"location":"release_notes/archived/0_8_46/#new-deprecation-warnings","title":"New Deprecation Warnings","text":"<p>We're starting to put in deprecation warning as we streamline classes and APIs. If you're using a class or method that's going to be deprecated you'll see a log message like this:</p> <pre><code>my_class = SomeOldClass()\nWARNING SomeOldClass is deprecated and will be removed in version 0.9.\n</code></pre> <p>In general these warning messages will be annoying but they will help us smoothly transistion and streamline our Classes and APIs.</p>"},{"location":"release_notes/archived/0_8_46/#deprecations","title":"Deprecations","text":"<ul> <li>AWSServiceBroker: Replaced by the Meta() class</li> </ul>"},{"location":"release_notes/archived/0_8_46/#api-changes","title":"API Changes","text":"<p>Meta()</p> <p>The new <code>Meta()</code> class will provide API that aligns with the AWS <code>list</code> and <code>describe</code> API. We'll have functionality for listing objects (models, feature sets, etc) and then functionality around the details for a named artifact.</p> <pre><code>meta = Meta()\nmodels_list = meta.models()  # List API\nend_list = meta.endpoints()  # List API\n\nfs_dict = meta.feature_set(\"my_fs\") # Describe API\nmodel_dict = meta.model(\"my_model\") # Describe API\n</code></pre> <p>For more details see: Meta Class</p> <p>The new Meta() API will be used inside of the Artifact classes (see Internal Changes...Artifacts... below)</p> <p>Artifact Classes</p> <p>The artifact classes (DataSource, FeatureSet, Model, Endpoint) have had some old arguments removed.</p> <pre><code>DataSource(force_refresh=True)  -&gt; Gone (remove it)\nFeatureSet(force_refresh=True)  -&gt; Gone (remove it)\nModel(force_refresh=True)       -&gt; Gone (remove it)\nModel(legacy=True)              -&gt; Gone (remove it)\n</code></pre>"},{"location":"release_notes/archived/0_8_46/#improvements","title":"Improvements","text":"<p>Scalability</p> <p>The changes to caching and the Meta() class should allow better horizontal scaling, we'll flex out the stress tests for upcoming releases before <code>0.9.0</code>.</p> <p>Table Names starting with Numbers</p> <p>Some of the Athena queries didn't properly escape the tables names and if you created a DataSource/FeatureSet with a name that started with a number the query would fail. Fixed now. :)</p>"},{"location":"release_notes/archived/0_8_46/#internal-changes","title":"Internal Changes","text":"<p>Meta()</p> <p><code>Meta()</code> doesn't do any caching now. If you want to use Caching as part of your meta data retrieval use the <code>CachedMeta()</code> class.</p> <p>Artifacts</p> <p>We're got rid of most (soon all) caching for individual Artifacts, if you're constructing an artifact object, you probably want detailed information that's 'up to date' and waiting a bit is probably fine. Note: We'll still make these instantiations as fast as we can, removing the caching logic will as least simplify the implementations.</p>"},{"location":"release_notes/archived/0_8_46/#specific-code-changes","title":"Specific Code Changes","text":"<p>Code Diff v0.8.42 --&gt; v0.8.46 </p> <p>Who doesn't like looking at code! Also +3 points for getting down this far! Here's a cow joke as a reward:</p> <p>That feeling like you\u2019ve done this before?       .... Deja-moo</p>"},{"location":"release_notes/archived/0_8_46/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_6/","title":"Release 0.8.6","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines. We've also fixed various corner cases mostly around 'half constructed' AWS artifacts (models/endpoints).</p>"},{"location":"release_notes/archived/0_8_6/#additional-functionality","title":"Additional Functionality","text":"<ul> <li>View Support (phase 1)</li> <li>CloudWatch (phase 1)</li> <li>Exception Log Forwarding Exception Handling</li> <li>Lambda Layers Workbench Lambda Layers</li> <li>Better Docs for Deploying Plugins Deploying Plugins</li> </ul>"},{"location":"release_notes/archived/0_8_6/#issues-addressed","title":"Issues Addressed","text":"<ul> <li> <p>Model to Endpoint under AWS Throttle</p> <p>A corner case where the <code>to_endpoint()</code> method would fail when not 'knowing' the model input. This happened when AWS was throttling responses and the <code>get_input()</code> of the Endpoint returned <code>unknown</code> which caused a <code>NoneType</code> error when using the 'unknown' model.</p> </li> <li> <p>Empty Model Package Groups</p> <p>There are cases where customers might construct a Model Package Group (MPG) container and not put any Model Packages in that Group. Workbench has assumed that all MPGs would have at least one model package. The current 'support' for empty MPGs treats it as an error condition but the API tries to accommodate the condition and will properly display the model group. The group will indicate that it's 'empty' and provides an alert health icons.</p> </li> </ul>"},{"location":"release_notes/archived/0_8_6/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"release_notes/archived/0_8_8/","title":"Release 0.8.8","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p>"},{"location":"release_notes/archived/0_8_8/#additional-functionality","title":"Additional Functionality","text":"<ul> <li>View Support (Views)</li> <li>CloudWatch (CloudWatch Docs)</li> </ul>"},{"location":"release_notes/archived/0_8_8/#api-changes","title":"API Changes","text":"<ul> <li> <p>Auto Inference name change</p> <p>When auto_inference is run on an endpoint the name of that inference run is currently <code>training_holdout</code>. That is too close to <code>model_training</code> and is confusing. So we're going to change the name to <code>auto_inference</code> which is way more explanatory and intuitive.</p> <p>Porting plugins: There should really not be any hard coding for <code>training_holdout</code>, plugins should just call <code>list_inference_runs()</code> (see below) and use the first one on the list.</p> </li> <li> <p><code>list_inference_runs()</code></p> <p>The <code>list_inference_runs()</code> method on Models has been improved. It now handles error states better (no model, no model training data) and will return 'model_training' LAST on the list, this should improve UX for plugin components.</p> </li> </ul> <p>Examples</p> <pre><code> model = Model(\"abalone-regression\")\n model.list_inference_runs()\n Out[1]: ['auto_inference', 'model_training']\n\n model = Model(\"wine-classification\")\n model.list_inference_runs()\n Out[2]: ['auto_inference', 'training_holdout', 'model_training']\n\n model = Model(\"aqsol-mol-regression\")\n model.list_inference_runs()\n Out[3]: ['training_holdout', 'model_training']\n\n model = Model(\"empty-model-group\")\n model.list_inference_runs()\n Out[4]: []\n</code></pre>"},{"location":"release_notes/archived/0_8_8/#glue-job-changes","title":"Glue Job Changes","text":"<p>We're spinning up the CloudWatch Handler much earlier now, so if you're setting config like this:</p> <pre><code>from workbench.utils.config_manager import ConfigManager\n\n# Set the Workbench Config\ncm = ConfigManager()\ncm.set_config(\"WORKBENCH_BUCKET\", args_dict[\"workbench-bucket\"])\ncm.set_config(\"REDIS_HOST\", args_dict[\"redis-host\"])\n</code></pre> <p>Just switch out that code for this code. Note: these need to be set before importing workbench</p> <pre><code># Set these ENV vars for Workbench \nos.environ['WORKBENCH_BUCKET'] = args_dict[\"workbench-bucket\"]\nos.environ[\"REDIS_HOST\"] = args_dict[\"redis-host\"]\n</code></pre>"},{"location":"release_notes/archived/0_8_8/#misc","title":"Misc","text":"<p>Confusion Matrix support for 'ordinal' labels</p> <p>Pandas has an \u2018ordinal\u2019 type, so the confusion matrix method <code>endpoint.confusion_matrix()</code> now checks the label column to see if it\u2019s ordinal and uses that order, if not just it will alphabetically sort. </p> <p>Note: This change may not affect your UI experience. Confusion matricies are saved in the Workbench/S3 meta data storage, so a bunch of stuff upstream will also need to happen. FeatureSet object/api for setting the label order, recreation of the model/endpoint and confustion matrix, etc. In general this is a forwarding looking change that will be useful later. :)</p>"},{"location":"release_notes/archived/0_8_8/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"repl/","title":"Workbench REPL","text":"<p>Visibility and Control</p> <p>The Workbench REPL provides AWS ML Pipeline visibility just like the Workbench Dashboard but also provides control over the creation, modification, and deletion of artifacts through the Python API.</p> <p>The Workbench REPL is a customized iPython shell. It provides tailored functionality for easy interaction with Workbench objects and since it's based on iPython developers will feel right at home using autocomplete, history, help, etc. Both easy and powerful, the Workbench REPL puts control of AWS ML Pipelines at your fingertips.</p>"},{"location":"repl/#installation","title":"Installation","text":"<p><code>pip install workbench</code></p>"},{"location":"repl/#usage","title":"Usage","text":"<p>Just type <code>workbench</code> at the command line and the Workbench shell will spin up and provide a command view of your AWS Machine Learning Pipelines.</p> <p>At startup the Workbench shell, will connect to your AWS Account and create a summary of the Machine Learning artifacts currently residing on the account.</p> <p></p> <p>Available Commands:</p> <ul> <li>status</li> <li>config</li> <li>incoming_data</li> <li>glue_jobs</li> <li>data_sources</li> <li>feature_sets</li> <li>models</li> <li>endpoints</li> <li>and more...</li> </ul> <p>All of the API Classes are auto-loaded, so drilling down on an individual artifact is easy. The same Python API is provided so if you want additional info on a model, for instance, simply create a model object and use any of the documented API methods.</p> <pre><code>m = Model(\"abalone-regression\")\nm.details()\n&lt;shows info about the model&gt;\n</code></pre>"},{"location":"repl/#additional-resources","title":"Additional Resources","text":"<ul> <li>Setting up Workbench on your AWS Account: AWS Setup</li> <li>Using Workbench for ML Pipelines: Workbench API Classes</li> </ul> <ul> <li>Workbench Core Classes: Core Classes</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"road_maps/0_9_0/","title":"Road Map v0.9.0","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p>"},{"location":"road_maps/0_9_0/#general","title":"General","text":"<p>Streamlining</p> <p>We've learned a lot from our beta testers!</p> <p>One of the important lessons is not to 'over manage' AWS. We want to provide useful, granular Classes and APIs. Getting out of the way is just as important as providing functionality. So streamlining will be a big part of our <code>0.9.0</code> roadmap.</p> <p>Horizontal Scaling</p> <p>Our plan for <code>0.9.0</code> is to have formalized horizontal stress testing that tests 32 concurrent ML pipelines. Even though 32 may not seem like much, AWS has various quotas and limits that we'll be hitting, so 32 is a good goal for <code>0.9.0</code>. Eventaully we'll have an architecture that will support 100's of concurrent pipelines.</p> <ul> <li>Feature Processor Quotas</li> <li>Sagemaker Endpoint Quotas</li> </ul> <p>Caching</p> <p>Caching needs an overhaul. Right now Workbench uses caching for 'latency hiding'. The Dashboard and Web Interfaces are the only use case where responsiveness is important. Other use cases like nightly batch processing, scripts or notebooks, will work totally fine waiting for AWS responses.</p> <p>Class/API Reductions</p> <p>The organic growth of Workbench was based on user feedback and testing, that organic growth has led to an abundance of Classes and API calls. We'll be identifying classes and methods that are 'cruft' from some development push and will be deprecating those.</p>"},{"location":"road_maps/0_9_0/#deprecation-warnings","title":"Deprecation Warnings","text":"<p>We're starting to put in deprecation warning as we streamline classes and APIs. If you're using a class or method that's going to be deprecated you'll see a log message like this:</p> <pre><code>broker = AWSServiceBroker()\nWARNING AWSServiceBroker is deprecated and will be removed in version 0.9.\n</code></pre> <p>If you're using a class that's NOT going to be deprecated but currently uses/relies on one that is you'll still get a warning that you can ignore (developers will take care of it).</p> <pre><code># This class is NOT deprecated but an internal class is\nmeta = Meta() \nWARNING AWSServiceBroker is deprecated and will be removed in version 0.9.\n</code></pre> <p>In general these warning messages will be annoying but they will help us smoothly transistion and streamline our Classes and APIs.</p>"},{"location":"road_maps/0_9_0/#deprecations","title":"Deprecations","text":"<ul> <li>AWSServiceBroker: Replaced by the Meta() class</li> <li>Other Stuff</li> </ul>"},{"location":"road_maps/0_9_0/#api-changes","title":"API Changes","text":"<p>Meta()</p> <p>The new <code>Meta()</code> class will provide API that aligns with the AWS <code>list</code> and <code>describe</code> API. We'll have functionality for listing objects (models, feature sets, etc) and then functionality around the details for a named artifact.</p> <pre><code>meta = Meta()\nmodels_list = meta.models()  # List API\nend_list = meta.endpoints()  # List API\n\nfs_dict = meta.feature_set(\"my_fs\") # Describe API\nmodel_dict = meta.model(\"my_model\") # Describe API\n</code></pre> <p>The new Meta() API will be used inside of the Artifact classes (see Internal Changes...Artifacts... below)</p>"},{"location":"road_maps/0_9_0/#improvementsfixes","title":"Improvements/Fixes","text":"<p>FeatureSet</p> <p>When running concurrent ML pipelines we occasion get a partially constructed FeatureSet, FeatureSets will now 'wait and fail' if they detect partially constructed data (like offline storage not being ready).</p>"},{"location":"road_maps/0_9_0/#internal-changes","title":"Internal Changes","text":"<p>Meta()</p> <p>We're going to make a bunch of changes to <code>Meta()</code> specifically around more granular (LESS) caching. Also there will be an <code>AWSMeta()</code> subclass that manages the AWS specific API calls. We'll also put stubs in for <code>AzureMeta()</code> and <code>GCPMeta()</code>, cause hey we might have a client who really wants that flexibility.</p> <p>The new Meta class will also include API that's more aligned to the AWS <code>list</code> and <code>describe</code> interfacts. Allowing both broad and deep queries of the Machine Learning Artifacts within AWS.</p> <p>Artifacts</p> <p>We're getting rid of caching for individual Artifacts, if you're constructing an artifact object, you probably want detailed information that's 'up to date' and waiting a bit is probably fine. Note: We'll still make these instantiations as fast as we can, removing the caching logic will as least simplify the implementations.</p>"},{"location":"road_maps/0_9_0/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"road_maps/0_9_5/","title":"Road Map v0.9.5","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord </p> <p>The Workbench framework continues to flex to support different real world use cases when operating a set of production machine learning pipelines.</p>"},{"location":"road_maps/0_9_5/#general","title":"General","text":"<p>ML Pipelines</p> <p>We've learned a lot from our beta testers!</p> <p>One of the important lessons is that when you make it easier to build ML Pipelines the users are going to build lots of pipelines.</p> <p>For the creation, monitoring, and deployment of 50-100 of pipelines, we need to focus on the consoldation of artifacts into <code>Pipelines</code>. </p> <p>Pipelines are DAGs</p> <p>The use of Directed Acyclic Graphs for the storage and management of ML Pipelines will provide a good abstraction. Real world ML Pipelines will often branch multiple times, 1 DataSource may become 2 FeatureSets might become 3 Models/Endpoints. </p> <p>New Pipeline Dashboard Top Page</p> <p>The current main page shows all the individual artifacts, as we scale up to 100's models we need 2 additional levels of aggregation:</p> <ul> <li>Pipeline Groups<ul> <li>Group_1<ul> <li>Pipeline_1 (DS, FS, Model, Endpoint)</li> <li>Pipeline_2 (DS, FS, Model, Endpoint)</li> </ul> </li> <li>Group_2<ul> <li>Pipeline_3 (DS, FS...)</li> <li>Pipeline_4 (DS, FS...)</li> </ul> </li> </ul> </li> </ul> <p>New Pipeline Details Page</p> <p>When a pipeline is clicked on the top page, a Pipeline details page comes up for that specific pipeline. This page will give all relevant information about the pipeline, including model performance, monitoring, and endpoint status.</p> <p>Awesome image TBD</p>"},{"location":"road_maps/0_9_5/#versioned-artifacts","title":"Versioned Artifacts","text":"<p>Our beta customers have requested versioning for artifacts, so we support versioning for both Model and FeatureSets. Endpoints and DataSources typically do not need versioning, so we may wait on the versioning support for those artifact until a later version.</p>"},{"location":"road_maps/0_9_5/#questions","title":"Questions?","text":"<p>The SuperCowPowers team is happy to answer any questions you may have about AWS and Workbench. Please contact us at workbench@supercowpowers.com or on chat us up on Discord </p>"},{"location":"themes/","title":"Theme Support for Workbench","text":"<p>Need Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and Workbench. So please contact us at workbench@supercowpowers.com or on chat us up on Discord</p> <p>Workbench supports full theming and branding for customization of components and pages, including Workbench Plugins.</p>"},{"location":"themes/#theming-mechanisms","title":"Theming Mechanisms","text":"System What It Styles How It Switches CSS/Bootstrap Layout, buttons, cards, tables <code>data-bs-theme</code> + stylesheet swap Plotly Templates Figure axes, colors, fonts Plugin <code>set_theme()</code> re-renders figures Custom CSS Non-Bootstrap elements Cache-busting stylesheet reload <p>Note: Colorscales require plugins to pull from template metadata when updating figures.</p>"},{"location":"themes/#dynamic-theme-switching","title":"Dynamic Theme Switching","text":"<p>Workbench supports instant theme switching without page reload:</p> <ol> <li>User clicks a theme in the settings menu</li> <li>JavaScript updates localStorage, cookies, and DOM</li> <li>Bootstrap stylesheet swapped via <code>&lt;link&gt;</code> href</li> <li><code>data-bs-theme</code> attribute updated (light/dark)</li> <li><code>workbench-theme-store</code> updated, triggering plugin callbacks</li> <li>Plugins re-render figures with new theme colors</li> </ol> <p>Theme Persistence: Stored in localStorage (JS access) and cookie <code>wb_theme</code> (server access).</p>"},{"location":"themes/#plugin-theme-support","title":"Plugin Theme Support","text":"<p>Plugins support theme changes by overriding the <code>set_theme()</code> method:</p> <pre><code>from workbench.web_interface.components.plugin_interface import PluginInterface\nfrom dash import no_update\n\nclass MyPlugin(PluginInterface):\n    def __init__(self):\n        self.model = None  # Cache data for re-rendering\n        super().__init__()\n\n    def set_theme(self, theme: str) -&gt; list:\n        \"\"\"Re-render when theme changes.\"\"\"\n        if self.model is None:\n            return [no_update] * len(self.properties)\n        return self.update_properties(self.model)\n</code></pre> <p>The base <code>PluginInterface</code> provides:</p> <ul> <li>Default <code>set_theme()</code> returning <code>[no_update] * len(self.properties)</code></li> <li>Shared <code>theme_manager</code> for accessing colors and colorscales</li> </ul> <p>Page-level callback wiring (in <code>callbacks.py</code>):</p> <pre><code>def setup_theme_callback(plugins):\n    @callback(\n        [Output(cid, prop, allow_duplicate=True) for p in plugins for cid, prop in p.properties],\n        Input(\"workbench-theme-store\", \"data\"),\n        prevent_initial_call=True,\n    )\n    def on_theme_change(theme):\n        all_props = []\n        for plugin in plugins:\n            all_props.extend(plugin.set_theme(theme))\n        return all_props\n</code></pre> <p>Plugins stay simple\u2014they just implement <code>set_theme()</code> without knowing about callbacks or store IDs.</p>"},{"location":"themes/#additional-resources","title":"Additional Resources","text":"<p>Need help with plugins? Want to develop a customized application tailored to your business needs?</p> <ul> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"themes/details/","title":"Bootstrap and DBC Theme Hooks in Dash","text":""},{"location":"themes/details/#theme-hooks-overview","title":"Theme Hooks Overview","text":""},{"location":"themes/details/#css-classes","title":"CSS Classes","text":"<p>Bootstrap provides pre-defined classes (<code>container</code>, <code>row</code>, <code>col</code>, <code>btn</code>, etc.) for responsive layouts. DBC components apply these automatically, and you can customize via the <code>className</code> property.</p>"},{"location":"themes/details/#css-variables","title":"CSS Variables","text":"<p>Bootstrap uses CSS variables for flexible theming:</p> <ul> <li><code>--bs-body-bg</code>: Background color</li> <li><code>--bs-body-color</code>: Text color</li> <li><code>--bs-border-color</code>: Border color</li> </ul> <p>These adjust automatically based on the <code>data-bs-theme</code> attribute.</p>"},{"location":"themes/details/#data-attributes","title":"Data Attributes","text":"<p>The <code>data-bs-theme</code> attribute (<code>light</code> or <code>dark</code>) on a root container dynamically adjusts all Bootstrap variables globally.</p>"},{"location":"themes/details/#how-theme-switching-works","title":"How Theme Switching Works","text":"<p>Workbench supports instant theme switching\u2014no page reload required.</p>"},{"location":"themes/details/#what-happens-on-theme-change","title":"What Happens on Theme Change","text":"<ol> <li>User clicks a theme in the settings menu</li> <li>Clientside callback (JavaScript) executes:</li> <li>Saves to <code>localStorage</code> and cookie (<code>wb_theme</code>)</li> <li>Swaps Bootstrap stylesheet URL</li> <li>Updates <code>data-bs-theme</code> attribute</li> <li>Updates <code>workbench-theme-store</code> dcc.Store</li> <li>Page-level callback fires, calling <code>set_theme()</code> on each plugin</li> <li>Plugins re-render figures with new colors from ThemeManager</li> </ol>"},{"location":"themes/details/#server-side-theme-detection","title":"Server-Side Theme Detection","text":"<p>On page load, the server reads the theme from cookies:</p> <pre><code>@app.server.before_request\ndef check_theme_cookie():\n    theme_name = request.cookies.get(\"wb_theme\")\n    if theme_name and theme_name != cls.current_theme_name:\n        cls.set_theme(theme_name)\n</code></pre> <p>This ensures Plotly templates are set correctly for initial figure rendering.</p>"},{"location":"themes/details/#two-main-theming-concepts","title":"Two Main Theming Concepts","text":""},{"location":"themes/details/#css-for-web-interface","title":"CSS for Web Interface","text":"<p>Styles the overall layout, buttons, dropdowns, fonts, and background colors. Use Bootstrap or custom CSS files. Switch themes dynamically via CSS class changes.</p>"},{"location":"themes/details/#plotly-templates-for-figures","title":"Plotly Templates for Figures","text":"<p>Styles Plotly figures (background, gridlines, colorscales, fonts). Use predefined templates or create custom JSON templates.</p> <p>Resource: dash-bootstrap-templates</p>"}]}